<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f73b5ad409304e1c8a4ec689bab7463d"><head><title>Bayes Method</title><objref idref="d651478ba3e84c03851885b895caea97" /><objref idref="e31e837cfea64decae6c39ec99b181d9" /></head><body><p id="b5554e120d054fd2a63c107fac8a71f4"><em style="italic">Bayes Theorem </em>calculates conditional probabilities. Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degenerative issues, Bayes theorem supports accurately assessing that risk based on a certain age range, instead of making assumptions.</p><image id="bfee29da7d8e4055860e0802ae5bd541" src="../webcontent/Bayes.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="ff485191780442c4bff39d8129cabce1"><em style="italic">Bayes Rule-</em></p><p id="ea996d8cb2514b19b8ddb8070e8703de"><em style="italic">*Source https://www.psychologyinaction.org/psychology-in-action-1/2012/10/22/bayes-rule-and-bomb-threats</em></p></caption><popout enable="false"></popout></image><table id="f203990078574afe81e0ad3a11ccd6a2" summary="" rowstyle="plain"><cite id="ieeb83215e23f4e5991fa7d89494b567b" /><caption><p id="ce190101998849b68d2c9225afeae703" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="db3a717ca10248869d4986aba8b7d5fe">Additional Reading:<link href="https://plato.stanford.edu/entries/bayes-theorem/#:~:text=Bayes&apos;%20Theorem%20relates%20the%20%22direct,%2C%20PH(E).&amp;text=Bayes&apos;%20Theorem." target="new" internal="false">Bayes Theorem</link></p></td></tr></table><p id="ee117cb137b74833b23f82019e53a00e"><em style="italic">Bayesian Inference</em><em> </em>is applied when Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. Used in sports, medicine, and law among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule but it is widely used. </p><p id="f2617b9480b24b079c440815572fd50d"><em>Naive Bayes (NB) Method</em></p><p id="b0e07801ba364e50bd211f476502ec1a">Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier locates all observations that have similar predictor values to the observation that is to be classified, and then assigns it to the class that it belongs to; When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and it assumes that all predictors are independent. Although this is a naive assumption, naive bayes performs quite well for real world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honey dew melon, the NB classifier will assume that all features listed above independently contribute to the probability that a fruit with these features is a honey dew melon. Naive  Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that <link href="https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf" target="new" internal="false">studies</link><sup> </sup>have been conducted showing it does not perform as well as methods like random forests. It is said to be a good classifier but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high dimensional data. This is because when the dimension of a dataset is large or it grows, datapoints are more likely to be further apart than in cases with low dimensional data. </p><p id="d4bb8f94614947039eb2d296bc505d73">Naive Bayes is not considered the go-to algorithm for estimating the probability of an observation&apos;s class as it is biased in its results. It is quite useful for ranking and classification tasks. Assume that you introduce a new record to your model, if this new record has a predictor category that is different from those in the training dataset, naive Bayes will assume zero probability to that record. Let&apos;s make it real: if your response is <em style="italic">has diabetes </em>and a predictor category is <em style="italic">past pregnancy</em>. Now assume that your training dataset has all observations with <em style="italic">past pregnancy =0</em>. All new observations with <em style="italic">past pregnancy =1 </em>will be classified as not having diabetes.  </p><p id="ebd16c2ba1134875b057754b6869f692">There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied statistics courses. </p><wb:inline idref="newe7dfbf18a7b04fab95c701025bb26ad6" purpose="learnbydoing" /></body></workbook_page>
