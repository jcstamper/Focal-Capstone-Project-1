<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c0da183109594ae09b9ec5303ac457a9"><head><title>Classification and Regression Trees</title><objref idref="e31e837cfea64decae6c39ec99b181d9" /><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="a6f38a40081f4660943b942d8d69fa25">Tree based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an &quot;If-Then&quot; statement and are easily digestible to an individual with a growing statistics knowledge. </p><p id="d48b58a7d8674fc7a3ceda1954212491">We will explore the different tree based methods starting with one of the most popular methods: <em style="italic">Decision Trees</em>. Using a very simple example, let us build a decision tree: <em>Decision Trees: </em><link href="https://scikit-learn.org/stable/modules/tree.html" target="new" internal="false"><em>scikit-learn</em></link><em>.</em></p><p id="dec13bca61b24f58a2121b9c349868c4">A decision tree consists of a root node, the leaf nodes and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a <em style="italic">Classification Tree. </em>When your outcome or target variable is a continuous value, you have a <em style="italic">Regression Tree. </em></p><table id="f3d4876a395046dd9ebd00e29ec0ba38" summary="" rowstyle="plain"><cite id="i67f6d3607a464146b6aca6599ae5ae3e" /><caption><p id="ff4076d78e564a3caa76b4a961cf9619" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b57c0c3ad4264e33924cde90dc59eba2">Additional Reading: <link href="https://hbr.org/1964/07/decision-trees-for-decision-making" target="new" internal="false">Decision Trees for Decision Making</link></p></td></tr></table><p id="f7b550da1af94260ac53064f82a90677"><em>Building Classification Trees</em></p><p id="f528605a53724f6bb4e28aa9c1eb542b">Building a classification tree involves <em style="italic">recursive partitioning </em>and <em style="italic">pruning. </em>Both concepts are used to ensure the model has a low error rate and overfitting is not an issue. </p><p id="f9b7944b053e45c19869ba358a5ae838"><em style="italic">Recursive Partitioning</em> creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure. Purity means all observations belong to a single class. </p><p id="cfb8d040183948a0a58a2a61ed10ee05">Recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue. </p><p id="ab9ed5b507a342fe8adbde998ff3c498">Another technique is the <em style="italic">Chi-square automatic interaction detection </em>(CHAID). It is used for both classification and prediction and can be used to show the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer, CHAID will help Capital One&apos;s marketing firm to predict how your age, income, credit score will affect your response to the interest rate offered.</p><p id="fc1083c770444c709c1d8d780ab046f7"><em style="italic">Measures of Impurity. </em>You can measure impurity using <em style="italic">entropy </em>and <em style="italic">the Gini index. </em>The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen, the index varies from 0 to 1. 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. <em style="italic">Entropy </em>is a measure of uncertainty within a model. Decision trees will always seek to maximize entropy.</p><p id="e84a989e11494f7995ee182fb3713448"><em>Reading: </em><link href="https://victorzhou.com/blog/gini-impurity/" target="new" internal="false">Gini Index and Impurity Measures</link></p><p id="fd3542b909be48e9ba3e5aead1c1fba2"><em style="italic">Pruning. </em>If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will <em style="italic">prune </em>the weakest branches to reduce complexity of your model and improve accuracy. Pruning can be done using two techniques. </p><ul id="e8536ce2aa6a4762b4d6106d73cc6955"><li><p id="a68ecdebb0da48b4ae861695cb391749">Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm. The best tree is chosen by generalized accuracy as measured by a training set or cross-validation.</p></li><li><p id="e50cd66abbc14da283793fbf1cd398de">Reduced error pruning is done by replacing each node with the node&apos;s most popular class,however that replacement is temporary unless the it does not negatively affect the prediction accuracy. It is an efficient technique for pruning. </p></li></ul><table id="c761935ee095473a8a31097bf376f8f5" summary="" rowstyle="plain"><cite id="ie85d9d017cee46b49cb1cc8cd370248c" /><caption><p id="abe8ee02b93b487996bf733c110d9cb5" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b8fc32d197e24af68a3ece997404111c"><em>Application</em>: <link href="http://faculty.washington.edu/fxia/courses/LING572/decison_tree99.pdf" target="new" internal="false">Decision Trees and NLP: A Case Study in POS Tagging</link>. </p></td></tr></table><p id="efd4e1eae4794c0782c8411e0077d142">When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving for this means pruning the tree. The least complex tree with the smallest validation error is called a <em style="italic">Minimum Error Tree. </em>The least complex tree with a validation error that is &quot;within one standard error of the minimum error tree&quot; is called a <em style="italic">Best Pruned Tree.</em></p><p id="f85a567ab31b49ec9230d959190b9077">The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree, this way it will generalize new cases well. Misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree. </p><p id="c82fb8a5ff0e49408e69352085e52876"><em>Building Regression Trees</em></p><p id="f9b5687b2133493b99b1bf1feb085a37">You will find that decision trees are more explainable than linear regression models. A smaller tree is easily interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node. The predictive accuracy of CART models are not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE). </p><p id="ef890bf9f05f44eb9c63c6fa8ddae5d2">We will explore some methods that can be used to improve this prediction accuracy and performance; on the next page, you will learn more about <em style="italic">Random Forests, Bagging and Boosting. </em></p><wb:inline idref="neweb70f10349b1444dbcbeb67e57ccf43b" purpose="learnbydoing" /></body></workbook_page>
