<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="b54b9560ac4f474e948597c8f4cc8ce7"><head><title>Model Interpretation Strategies</title><objref idref="aaaf4d4a01484fd3aff18868f882e298" /></head><body><p id="a3617db632b34b28bfb14a476f76fda1">As a Data Scientist, model interpretation means more than one thing to you and to your clients; in most cases, it will mean different things to both parties. The Data Scientist is interested in understanding the results of a task and how it can assist the client and their end users in making decisions. A great resource by Marco Ribeiro explains end user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu. There is a middle &quot;man&quot; between the prediction and the explanation of the prediction. This explanation is what the decision maker (Doctor in this case) will use to make the decision on the right diagnosis and treatment. </p><p id="b51bb141101b49ee98e43bd5f17ef852">Interpretability is important to data science and machine learning because it directly affects the human decision makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module), it is often important to know what is predicted and why the prediction was made. Understanding the why will make the problem clearer and affect problem solving for future challenges. </p><p id="f42281afdb2f4cd29c67b143896f2618">Doshi Velez &amp; Kim (2017) have explained a great detail some of the reasons why interpretability is important. The ever growing and unsatisfied curiosity of humans (and by extension our thirst for learning). <em style="italic">Bias</em> <em style="italic">identification</em> is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores, and income? Detecting bias can also lead to better <em style="italic">acceptance</em>. Finally, the data scientist and machine learning engineers can <em style="italic">debug </em>and <em style="italic">audit</em> models when those models are easily interpretable. </p><p id="f9d544ba777f4195b84bd5778da39f12">Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).</p><p id="e5bcea29f49747b6a5f4dd085aa8c979">Looking to the next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are a useful tool in deciding whether a model will be considered trustworthy. </p><p id="b8521f8a15b64b9abfb87006be3142c7"><em>Reading: </em><link href="https://arxiv.org/pdf/1602.04938.pdf" target="new" internal="false">Should you trust that model?</link></p><p id="e001fac9b36c4cdb8ae22e18ff78a82f">The authors of the above article proposed a technique to explain predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers including neural networks for text and image classification.</p><p id="cc9d919873714b51b1c3c8ca3341ea8a"><link href="https://christophm.github.io/interpretable-ml-book/lime.html" target="new" internal="false"><em style="italic">Local Surrogate Models</em></link><em style="italic"> </em>&quot;explain individual predictions of black box models.&quot;</p><p id="d662fb56abb1491c9844dd268216fa2a"><link href="https://christophm.github.io/interpretable-ml-book/shapley.html#shapley" target="new" internal="false"><em style="italic">Shapley Value</em></link> is concerned with explaining a prediction by assessing the importance of features to the task. </p><p id="ec4bd467ba244bd6b494f183ae6e38b9"><em>Additional Resource: </em><link href="https://drive.google.com/file/d/1xD5HY7HU5mBiKQ1nEQvRYqwBkNfHG3yM/view" target="new" internal="false">Sara Hooker: The Myth of the Perfect Model</link></p><wb:inline idref="newe131dd72d65a44499220491554114a99" purpose="learnbydoing" /></body></workbook_page>
