<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f92219fb485c4e96bbb649d63e54a7c5"><head><title>Active Learning </title><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="b9b5dfd22c6b483f93269d0df3ddd9d0">This data science pattern is different from what you have studied in this course, it makes assumptions about an algorithm and the data that is used to construct it. <em style="italic">Active Learning </em>pattern posits that if an algorithm or <em style="italic">learner </em>can choose the data it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data, and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling. </p><p id="c83efbeac0b94e9db3e07fd172d65b77">Active learning presents <em style="italic">Scenarios </em>that allow a learner to query the labels of observations in a dataset. </p><p id="bffe42a4ddb04657be8d1582876ffd21"><em style="italic">Membership Query Synthesis </em>is a scenario that means a learner will generate an observation i.e. the learner will create a data point that is similar to one or more in the dataset. Once it is created, the new observation will be labeled by the <em style="italic">oracle </em>(an information source or teacher).</p><p id="c7094a27f0db464398da62f1c0297c5f"><em style="italic">Stream Based Selective Sampling</em> scenario involves unlabeled data points or observations that are examined with the algorithm evaluating its informativeness against query parameters. The learner will decide if it should assign a label or query the oracle.</p><p id="bade3f7605f14e5fa4cc32ebb5a96694"><em style="italic">Pool Based Sampling </em>as shown in the figure below, assumes that you have a pool of unlabeled data and observations are collected from the pool according to an <em style="italic">informativeness measure </em>(certainty that a classifier has when classifying data points)<em style="italic">. </em>The informativeness measure is applied to all observations in your dataset and then the observations that have the most important measures are selected. The selected observations are then labeled.</p><p id="fee186245df94caf806a8e9ac782475a"><em><em style="highlight">Thought:</em></em><em> </em><em style="italic">Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm&apos;s abilities (prediction and otherwise).</em></p><image id="b9a9409fad9b46d293b270cb970f789a" src="../webcontent/Active_Learning.jpg" alt="" style="inline" vertical-align="middle" height="283" width="500"><caption><p id="d1a2c4cf6ab2476db12e6622b79d7d23"><em style="italic">Pool Based Active Learning Cycle-Source: Settles Active Learning Survey</em><em style="italic"><sup>1</sup></em></p></caption><popout enable="false"></popout></image><p id="a35510417f9b4c4bbc78f49e7450c837"><em>Query Strategies</em></p><p id="b4b098affc3e4f9681e58aa09754c0ba">How does the algorithm decide on the most informative measures? Let&apos;s highlight some of the strategies used to evaluate the informativeness of unlabeled data.</p><p id="efddeb66e27d46b59911a611a9651c81"><em style="italic">Uncertainty Sampling </em>is an approach that allows the active learner to query the observations about which it is not able to label. </p><p id="d6b5f65c1b554b2193fb7bf54e363110"><em style="italic">Query-by-committee </em>involves using group or committee of models that have been trained on a labeled dataset but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labelings. Identify the query that all voting models disagree on, that becomes the most informative query.</p><p id="e88fc9f3b1164436a0b493262f58d69c"><em style="italic">Expected Model Change </em>will use an approach that selects the observation that would introduce the most change to a current model if its label was known.</p><p id="b5da2be714a6415e943518fe72527ea2"><em style="italic">Expected Error Change </em>involves labeling the data points that would reduce the model&apos;s out of sample error (measure of how accurately your learner can make predictions on new data).</p><p id="d485d221681c499a9c3a45568c42258c" /><table id="e3d64a166c2a483b82447ae099bb582e" summary="" rowstyle="plain"><cite id="i012d0d4e081f4d80afac00b8a9e151ee" /><caption><p id="e0aad8aa0d3d4d4d82fa57bb634c1d38" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="d3130897bc2640e8a65bd7663f77d063"><em>Additional Reading: </em><link href="http://burrsettles.com/pub/settles.activelearning.pdf" target="new" internal="false">Survey of Active Learning</link>. This report gives an in depth review of active learning in machine learning and artificial intelligence.</p></td></tr></table><wb:inline idref="newd631c6984cb74935b6b407731f36c2f6" purpose="didigetthis" /></body><bib:file><bib:entry id="c008003605554b9d920d51d8c73ec226"><bib:article><bib:author>Burr Settles</bib:author><bib:title>Active Learning Literature Survey</bib:title><bib:journal>University of Wisconsin Madison</bib:journal><bib:year>2009</bib:year></bib:article></bib:entry></bib:file></workbook_page>
