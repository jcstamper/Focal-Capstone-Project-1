<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="afe43b3ca23848d29a3af1d12f9c426d"><head><title>Principal Component Analysis</title><objref idref="bb8cd74aab6c47c39d8c9786d3b7eb30" /></head><body><p id="cfe74d7f5abd43c792282882839c5e53"><em>Dimensionality</em></p><p id="cd463fb0e2044fe8b693db44669c0419">As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleansing, think about when you transform variables by creating new variables e.g. dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting, and computing costs. We think about the <em style="italic">dimensionality </em>of a model when we consider the number of variables used by the model. Famous mathematician, R. Bellman defined the <em style="italic">curse of dimensionality </em>as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective.  How can you address the curse of dimensionality without losing useful information? We use <em style="italic">dimensionality reduction: </em>this technique is sometimes referred to as feature extraction or factor selection. This technique is approached using mathematical modeling. </p><p id="b64913761c0d4be2b76bae075cea19ee"><em>Feature Extraction</em></p><p id="cd82e64ae6714d178598fdafccfaa626">So far we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build and its techniques involve looking at features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique. </p><p id="eb716f040adc447caa906c8aade19b8b"><em style="italic">Principal Component Analysis (PCA) </em>is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. Well, when you have a dataset that has a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. When implementing PCA, you will be reducing the dimension of your feature space. </p><p id="bf82edf5d6ec4a5c9fc288d410f5c651">You use the Principal Component Analysis technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped, but dropping these variables with PCA is better justified than using the omission technique since you are using mathematical modeling.</p><p id="de1fb80591874fcc8f476e006a1380a6">There are other techniques for dimension reduction including Linear Discriminant Analysis (LDA) and those techniques are mentioned in a future unit as well as in your upcoming Machine Learning courses.</p><p id="b792eb49dc8147698f5a256ffdb94875">Principal component analysis involves performing the eigendecomposition on the covariance matrix.  A principal component analysis is a linear transformation technique as it finds a low dimensional representation of your high dimensional data. PCA will seek out a &quot;small&quot; number of dimensions in the dataset that are useful to the analytic task. PCA is considered an unsupervised technique and will be mentioned in that unit as well. </p><p id="ab7127ce862144a2904563dfe26dc876">The following steps are used when performing PCA. </p><ul id="cc8a24b6cd864e88aa7d1e44c8c6abd1"><li><p id="deb2ecfdb44d4aeabacec7412b2c53fe">Standardize the data.</p></li><li><p id="b63efa13a60542098c3a93b6125a86de">Compute the Covariance matrix of dimensions in the data. The covariance matrix</p></li><li><p id="abfbb2ef88234e538770d075b28d3cd3">Compute the Eigenvectors and Eigenvalues from the covariance matrix. <em style="italic">Eigenvector</em> is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile an eigenvalue is known as a characteristic value<sup>1 </sup>or a set of scalars. </p></li><li><p id="f84ad25edaa04f358a55c9779d46dc19">Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.</p></li><li><p id="ce6f8e36d9bb4a42b4f166ee00a371a9">Construct the projection matrix W from the selected k Eigenvectors.</p></li><li><p id="d2bfb21c500345978ee8289cefde43a5">Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.</p></li></ul><table id="ce9fc51513604828a7627ccef562e26d" summary="" rowstyle="plain"><cite id="icd59f63592124bc48ac7c60d4392daef" /><caption><p id="b96b6f7b47db46fba21b56e091f77e78" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c9be03c8b36e4c84865417a93d4e7da0"><em>PCA in Python Example: </em><link href="https://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/dimensionality_reduction/projection/principal_component_analysis.ipynb" target="new" internal="false">Principal Component Analysis in three (3) steps.</link></p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="e37ad350c3414f499deb60d473e0aaee"><em>Reading: </em><link href="https://www.nature.com/articles/nmeth.4346" target="new" internal="false"><em style="italic">A Brief Article</em><em><em style="italic">-</em></em>Principal Component Analysis (Lever, Krzywinski and Altman, 2017)</link></p></td></tr></table><wb:inline idref="newae3a2609b31c43b9a4bb794b561fe058" purpose="didigetthis" /></body><bib:file><bib:entry id="adba2ed55b3648b78bb7d30daea0955f"><bib:book><bib:author>Hoffman, K. &amp; Kunze. R.</bib:author><bib:title>Linear Algebra</bib:title><bib:publisher>Englewood Cliffs</bib:publisher><bib:year>1971</bib:year><bib:edition>Second</bib:edition></bib:book></bib:entry></bib:file></workbook_page>
