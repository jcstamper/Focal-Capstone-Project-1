<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f4e02624db114a8a9c8cd78840a1f707"><head><title>Errors</title><objref idref="fb91d7452cca428ebc8f86c4062f67ff" /></head><body><p id="bcc07f9e395647dca3b7917c8865e0cd"><em style="italic">Errors At All Phases</em></p><p id="fbc57f5c2d5c4048b5bbfa8eef0b6881">A quick survey of both scholarly and practitioner literature, you will read that errors can make or break the analytic development process and render your analytic solution &quot;of little use&quot; to your client. Errors can start from the business understanding phase, when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding will lead to bloated costs, and scope creep. A data science team might also encounter errors during the data understanding phase. Issues with data that is not prepared adequately, or even worse collecting data that is not relevant to your analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team, and an attempt to deliver a solution hurriedly.</p><p id="c258dc4012d046cd845e2882d467008d"><em style="italic">Errors in Model Understanding</em> </p><p id="b5488dc3b389459bbf75e02d9a89d8d0">The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data related problems. Errors in this phase are used to validate models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will hear about throughout your career as data scientists. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.</p><p id="fad17bb165bc491a8dc78765eb50b7de"><em style="italic">Training Error </em>is derived by calculating the classification error of a model on the exact data that was used to train the model.</p><p id="d034ce8bece6490d808c089e7fdd2507"><em style="italic">Test Error </em>is important as it gives insight into the amount of errors to expect when making future predictions and it is used for model selection. None of your training data set should be part of the test data as it can affect the accuracy of the test error.</p><p id="cd4a28583fee49d9904fd28de2817f69"><em style="italic">Irreducible error</em>, is the noise term in the true relationship that cannot fundamentally be reduced by any model. When <em style="italic">x </em>can not determine <em style="italic">y </em>because there are other predictors that might improve the prediction error, you incorporate those variables. </p><p id="a86b1c8a139b4224a4520f1a229fc97c"><em style="italic">Reducible Error </em>is the error resulting from a mismatch between <em style="italic">f </em>and <em style="italic">f hat</em> or the estimate of the relationship between <em style="italic">x </em>and <em style="italic">y </em>and its true relationship. </p><p id="b11330dc787a4500a0f87214a7caa4b7">As we explore the different methods, we will dive deeper into calculating the error rate for the models trained using those methods.</p><wb:inline idref="newf81eb254180f42dab9523c4e03f7f431" purpose="didigetthis" /></body></workbook_page>
