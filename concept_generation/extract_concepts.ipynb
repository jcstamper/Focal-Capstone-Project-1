{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY') \n",
    "\n",
    "\n",
    "def find_keywords_fcds(row):\n",
    "    try:\n",
    "        query = ' '.join(row)\n",
    "        response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"extract key concepts from the following text such that we can assess data science knowledge based on them and put them in a numbered list with no header: {query}\"\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "        concepts = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        concept_list = concepts.split('\\n')\n",
    "        cleaned_concept_list = []\n",
    "        for item in concept_list:\n",
    "            cleaned_item = item[item.find('. ') + 2:]\n",
    "            cleaned_concept_list.append(cleaned_item)\n",
    "        print('done')\n",
    "        return cleaned_concept_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 'Error'\n",
    "\n",
    "def find_keywords_chem(row):\n",
    "    try:\n",
    "        query = ' '.join(row)\n",
    "        response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"extract key concepts from the following text such that we can assess chemistry knowledge based on them and put them in a numbered list with no header: {query}\"\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=256,\n",
    "        top_p=0.1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "        concepts = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        concept_list = concepts.split('\\n')\n",
    "        cleaned_concept_list = []\n",
    "        for item in concept_list:\n",
    "            cleaned_item = item[item.find('. ') + 2:]\n",
    "            cleaned_concept_list.append(cleaned_item)\n",
    "        print('done')\n",
    "        return cleaned_concept_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 'Error'\n",
    "\n",
    "\n",
    "\n",
    "def fill_empty(row):\n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcds_df = pd.read_csv('f_22_oli_content_with_concepts.csv')\n",
    "fcds_df.fillna('', inplace=True)\n",
    "#chem_df = pd.read_csv('chem_oli_content_with_concepts.csv')\n",
    "#chem_df.fillna('', inplace=True)\n",
    "#fcds_df['concepts']= None\n",
    "#fcds_df['concepts'] = fcds_df.apply(find_keywords_fcds, axis=1)\n",
    "#chem_df['concepts'] = chem_df.apply(find_keywords_chem, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit</th>\n",
       "      <th>Module</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subheaders</th>\n",
       "      <th>concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep Learning and Model Deployment</td>\n",
       "      <td>Model Deployment</td>\n",
       "      <td>Quiz 10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>['Data science', 'Knowledge assessment', 'Key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Project Planning</td>\n",
       "      <td>Developing a Vision</td>\n",
       "      <td>Module 4 Summary</td>\n",
       "      <td>This module focuses on the foundation of a doc...</td>\n",
       "      <td></td>\n",
       "      <td>['Foundation of a documentation set', 'Vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analytic Algorithms and Model Building</td>\n",
       "      <td>Model Selection</td>\n",
       "      <td>Model Selection for Prediction</td>\n",
       "      <td>To replicate the setting of performing predict...</td>\n",
       "      <td></td>\n",
       "      <td>['Replicating the setting of performing predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Project Planning</td>\n",
       "      <td>Requirements Gathering</td>\n",
       "      <td>Successful Requirements Gathering</td>\n",
       "      <td>The requirements gathering process is not line...</td>\n",
       "      <td>Validating Requirements</td>\n",
       "      <td>['Requirements gathering process is not linear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exploratory Data Analysis</td>\n",
       "      <td>Feature Engineering</td>\n",
       "      <td>t-SNE</td>\n",
       "      <td>In the last section, we explored Principal Com...</td>\n",
       "      <td>References:</td>\n",
       "      <td>Error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Unit                  Module  \\\n",
       "0      Deep Learning and Model Deployment        Model Deployment   \n",
       "1           Data Science Project Planning     Developing a Vision   \n",
       "2  Analytic Algorithms and Model Building         Model Selection   \n",
       "3           Data Science Project Planning  Requirements Gathering   \n",
       "4               Exploratory Data Analysis     Feature Engineering   \n",
       "\n",
       "                               Title  \\\n",
       "0                            Quiz 10   \n",
       "1                   Module 4 Summary   \n",
       "2     Model Selection for Prediction   \n",
       "3  Successful Requirements Gathering   \n",
       "4                              t-SNE   \n",
       "\n",
       "                                                Text               Subheaders  \\\n",
       "0                                                                               \n",
       "1  This module focuses on the foundation of a doc...                            \n",
       "2  To replicate the setting of performing predict...                            \n",
       "3  The requirements gathering process is not line...  Validating Requirements   \n",
       "4  In the last section, we explored Principal Com...              References:   \n",
       "\n",
       "                                            concepts  \n",
       "0  ['Data science', 'Knowledge assessment', 'Key ...  \n",
       "1  ['Foundation of a documentation set', 'Vision ...  \n",
       "2  ['Replicating the setting of performing predic...  \n",
       "3  ['Requirements gathering process is not linear...  \n",
       "4                                              Error  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "display(fcds_df.head())\n",
    "print(len(fcds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "35\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4271 tokens (4015 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "36\n",
      "done\n",
      "37\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4364 tokens. Please reduce the length of the messages.\n",
      "38\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4462 tokens. Please reduce the length of the messages.\n",
      "39\n",
      "done\n",
      "40\n",
      "done\n",
      "41\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6371 tokens. Please reduce the length of the messages.\n",
      "42\n",
      "done\n",
      "43\n",
      "done\n",
      "44\n",
      "done\n",
      "45\n",
      "done\n",
      "46\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4607 tokens. Please reduce the length of the messages.\n",
      "47\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 9308 tokens. Please reduce the length of the messages.\n",
      "48\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5095 tokens. Please reduce the length of the messages.\n",
      "49\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 18130 tokens. Please reduce the length of the messages.\n",
      "50\n",
      "done\n",
      "51\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 18443 tokens. Please reduce the length of the messages.\n",
      "52\n",
      "done\n",
      "53\n",
      "done\n",
      "54\n",
      "done\n",
      "55\n",
      "done\n",
      "56\n",
      "done\n",
      "57\n",
      "done\n",
      "58\n",
      "done\n",
      "59\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6290 tokens. Please reduce the length of the messages.\n",
      "60\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5662 tokens. Please reduce the length of the messages.\n",
      "61\n",
      "done\n",
      "62\n",
      "done\n",
      "63\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4880 tokens. Please reduce the length of the messages.\n",
      "64\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 10591 tokens. Please reduce the length of the messages.\n",
      "65\n",
      "done\n",
      "66\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4274 tokens (4018 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "67\n",
      "done\n",
      "68\n",
      "done\n",
      "69\n",
      "done\n",
      "70\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4161 tokens (3905 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "71\n",
      "done\n",
      "72\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5554 tokens. Please reduce the length of the messages.\n",
      "73\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 13782 tokens. Please reduce the length of the messages.\n",
      "74\n",
      "done\n",
      "75\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5635 tokens. Please reduce the length of the messages.\n",
      "76\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 10975 tokens. Please reduce the length of the messages.\n",
      "77\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4290 tokens (4034 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "78\n",
      "done\n",
      "79\n",
      "done\n",
      "80\n",
      "done\n",
      "81\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6488 tokens. Please reduce the length of the messages.\n",
      "82\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 9081 tokens. Please reduce the length of the messages.\n",
      "83\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5104 tokens. Please reduce the length of the messages.\n",
      "84\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4553 tokens. Please reduce the length of the messages.\n",
      "85\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6894 tokens. Please reduce the length of the messages.\n",
      "86\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 11485 tokens. Please reduce the length of the messages.\n",
      "87\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4759 tokens. Please reduce the length of the messages.\n",
      "88\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 8653 tokens. Please reduce the length of the messages.\n",
      "89\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5787 tokens. Please reduce the length of the messages.\n",
      "90\n",
      "done\n",
      "91\n",
      "done\n",
      "92\n",
      "done\n",
      "93\n",
      "done\n",
      "94\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4359 tokens. Please reduce the length of the messages.\n",
      "95\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4121 tokens. Please reduce the length of the messages.\n",
      "96\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6539 tokens. Please reduce the length of the messages.\n",
      "97\n",
      "done\n",
      "98\n",
      "done\n",
      "99\n",
      "done\n",
      "100\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4206 tokens (3950 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "101\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4999 tokens. Please reduce the length of the messages.\n",
      "102\n",
      "done\n",
      "103\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6198 tokens. Please reduce the length of the messages.\n",
      "104\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4176 tokens (3920 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "105\n",
      "done\n",
      "106\n",
      "done\n",
      "107\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4121 tokens (3865 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "108\n",
      "done\n",
      "109\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 6025 tokens. Please reduce the length of the messages.\n",
      "110\n",
      "done\n",
      "111\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4120 tokens. Please reduce the length of the messages.\n",
      "112\n",
      "Error: This model's maximum context length is 4097 tokens. However, you requested 4137 tokens (3881 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\n",
      "113\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5366 tokens. Please reduce the length of the messages.\n",
      "114\n",
      "done\n",
      "115\n",
      "done\n",
      "116\n",
      "done\n",
      "117\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5233 tokens. Please reduce the length of the messages.\n",
      "118\n",
      "done\n",
      "119\n",
      "done\n",
      "120\n",
      "done\n",
      "121\n",
      "done\n",
      "122\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 13224 tokens. Please reduce the length of the messages.\n",
      "123\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4584 tokens. Please reduce the length of the messages.\n",
      "124\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 10642 tokens. Please reduce the length of the messages.\n",
      "125\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 5232 tokens. Please reduce the length of the messages.\n",
      "126\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4848 tokens. Please reduce the length of the messages.\n",
      "127\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 8944 tokens. Please reduce the length of the messages.\n",
      "128\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4428 tokens. Please reduce the length of the messages.\n",
      "129\n",
      "Error: This model's maximum context length is 4097 tokens. However, your messages resulted in 4766 tokens. Please reduce the length of the messages.\n",
      "130\n",
      "done\n",
      "131\n",
      "done\n",
      "132\n",
      "done\n",
      "133\n",
      "done\n",
      "134\n",
      "done\n",
      "135\n",
      "done\n",
      "136\n",
      "done\n",
      "137\n",
      "done\n",
      "138\n",
      "done\n",
      "139\n",
      "done\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "for i in range(35,len(fcds_df)):\n",
    "    fcds_df['concepts'][i] = find_keywords_fcds(fcds_df.iloc[i]['Text'])\n",
    "    fcds_df.to_csv('f_22_oli_content_with_concepts.csv', index=False)\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
