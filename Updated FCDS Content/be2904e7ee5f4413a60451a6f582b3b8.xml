b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="be2904e7ee5f4413a60451a6f582b3b8"><head><title>Feature Engineering</title><objref idref="bb8cd74aab6c47c39d8c9786d3b7eb30" /><objref idref="e2347f3cd48240c28451f5daa9cbb8c9" /></head><body><p id="ca06153abcb24444b95df7b6a9beb499"><em>Importance of Feature Engineering</em></p><p id="f16dc9d6166e4e9b8d0014f769b33ab3"><link href="https://en.wikipedia.org/wiki/Feature_engineering#:~:text=Feature%20engineering%20is%20the%20process,as%20applied%20machine%20learning%20itself." target="new" internal="false">Feature engineering</link> is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering. </p><image id="f2f25e6c6b8c4b0f9569f934f9f88a23" src="../webcontent/Feature_engineering.jpg" alt="" style="inline" vertical-align="middle" height="292" width="650"><caption><p id="fbc073996fa0486cba34f9d0d23e0853">Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course)</p></caption><popout enable="false"></popout></image><p id="a4faeedcd0f14bf090a1ee47bb781739"><em>Feature Engineering Techniques </em></p><p id="e16f276e145d45219b3f7a419579af54"><em>Imputation</em></p><p id="cb3972312dee4f40b3458d93b0b6369c">Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.</p><p id="d771eebdf0384906939ae25bd0775284"><em style="italic">Numerical Data Imputation. </em>If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with &quot; - &quot; or &quot;<em style="italic">NA</em>&quot; can be replaced with a &quot;0&quot;. Other methods used include using the median or mean values of that variable. </p><p id="f139de017480484aa12b673206b180f0"><em style="italic">Categorical Data imputation. </em>In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the \xe2\x80\x9cmost frequently  occurring value,\xe2\x80\x9d and you can impute \xe2\x80\x9cother\xe2\x80\x9d in a situation where there is no dominant value in the categorical column.</p><p id="d391bb01f82f40569558fc2e3baac0ed"><em>Binning </em></p><p id="c0082eedf8164f619d246321f1afe297">Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and <link href="https://en.wikipedia.org/wiki/Overfitting" target="new" internal="false">overfitting</link>. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance. </p><example id="b8d6c677eacb439da77a7cb61c782ea3"><title>Title</title><p id="a7e77f3d2dfa4f3eab7fe59589a0ec07">In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.</p></example><p id="b1f2c9be34e74394bbd8cb7e43e9d443"><em>Handling Outliers </em></p><p id="b8058f8f726a427e9520dfc397a6c19f">You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the &quot;Exploratory Data Analysis&quot; module. </p><p id="d14a2f12059e4f059e94ccc7cb504921"><em>Log Transform </em></p><p id="ae7c2075fea0453098183d104726d372">Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.</p><p id="da4e41cd294b4238bb9262ebf8cf780d"><em>Categorical Encoding</em></p><p id="a25a469974c048c091ab49e889a9275f"><em style="italic">One hot encoding </em></p><p id="af18e32ccdac401480d9c1e25953e5ac">One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called <em style="italic">dummy variables</em>. </p><p id="a23b2853ab3a4e519f1a27e8e7f5d8ea">One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive. </p><p id="f7de8e676da6410f800ff0e386c24e19">Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable \xe2\x80\x9chigh school,\xe2\x80\x9d which becomes one if the person has high school or 0 if the person has less than high school. </p><p id="e73e0202eb874926b1af10366f7d9351"><em style="italic">Binary Feature Encoding</em></p><p id="b1100fbaaf9a44acaeceb43aa4276ee6">When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information. </p><p id="efc821cf63cc4691abb834099208719d"><em style="italic">Ordinal Feature Encoding</em></p><p id="d76a8146b71d473db28538addb9d7e01">When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers. </p><table id="e6b394ccacf24bd4a362434c8baaebd2" summary="" rowstyle="plain"><cite id="ie283b6167b604cb094c0036a8e0bab32" /><caption><p id="f759232df1754930a8882030d1cd10b5" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ea09496b6c774c8a8bc63142953b566a">Highly Satisfied</p></td><td colspan="1" rowspan="1" align="left"><p id="f8043648edbc41a4b25a1a7a1d8a7257">1</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="e1fb9de1e41f4733b1117d956f32776c">Satisfied</p></td><td colspan="1" rowspan="1" align="left"><p id="c3a17af8653f43c7ac9117435c7479a2">2</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="a2aaaba935cd4dbc8a1d91c37c58021f">Somewhat Satisfied</p></td><td colspan="1" rowspan="1" align="left"><p id="f6576630cdbd4013b79b124fd872179e">3</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="aa87ee325eb941f1baf478495c18ba68">Not Applicable</p></td><td colspan="1" rowspan="1" align="left"><p id="bfb358287b424e22a648ca9ed8a20efc">4</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="d2b2e981b61c4ec7a2e8fa3ac8aab4e2">Dissatisfied</p></td><td colspan="1" rowspan="1" align="left"><p id="bcd21847301e41199ff6db1ff04d7e46">5</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="fc24149a0a384a3e836ef8260b35850b">Highly Dissatisfied</p></td><td colspan="1" rowspan="1" align="left"><p id="cb2d2c8b41bb4acdb45bdc72b095cbb7">6</p></td></tr></table><p id="d2ae660c506947a893bf2a5c1a120157"><em>Feature Split </em></p><p id="c6a387c8f9b54390ac34ef9bf274d8d0">When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions. </p><p id="ed0aac34c03640a1bd8ba57a9ccaf411"><em>Scaling </em></p><p id="d8e4328a903b4072b87430aad770a749">Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range. </p><p id="ebda41b2e07f4e1db89d8a0b858cf5aa">There are instances that require the use of scaled data, including algorithms that use <link href="https://en.wikipedia.org/wiki/Gradient_descent" target="new" internal="false">gradient descent</link></p><definition id="f800544af7fd46b2a43f87f78d741b1f"><term>Gradient descent</term><meaning id="baad86d155fc4755a4dfbefa90ab085a"><material><p id="c856ff5ecbed4d0f978a4a49adea8a51"><em style="italic">An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.</em></p></material></meaning></definition><p id="fa47aac60504445594b68ca2c20badb7">Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled. </p><p id="edd33584f1a1426fb2b146de0d2e5649"><em>Quick thought: </em>How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.</p><p id="ef786abe1d7041029188b92371846c85"><em style="italic">Normalization. </em>This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled. </p><p id="a3bc2ebedfac49b5b938cafbc63644b1"><em style="italic">Standardization. </em>Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks. </p><table id="ffcb72331c5a4f3b9eded20594f87a61" summary="" rowstyle="plain"><cite id="i5811f487ec87443caa724d6fca56a87e" /><caption><p id="ebfdf6319d3a4234acc70316b70f9bf9" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c0d4552574744e52b41aea24f0cf6633"><em>Reading: </em><link href="https://scikit-learn.org/stable/modules/preprocessing.html" target="new" internal="false">Data Preprocessing in scikit-learn</link>.</p></td></tr></table></body></workbook_page>\n'