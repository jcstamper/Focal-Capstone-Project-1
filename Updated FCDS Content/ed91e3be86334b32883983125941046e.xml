b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="ed91e3be86334b32883983125941046e"><head><title>k-Nearest Neighbors Method</title></head><body><p id="c5303ff2322c4e25abf6220c0e1107b7">Have you come across the term &quot;lazy learning&quot;? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers&apos; preferences is available. </p><p id="e470527efada40439d07b0cf5d14506e">The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. , An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.</p><p id="a8ae68f352c6459a8aa32728198f83a0">A well-known lazy learner is the <em style="italic">k</em>-Nearest Neighbor (<em style="italic">k</em>NN) method. This method can be used to solve both classification and regression problems. <em style="italic">k</em>NN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. <em style="italic">k</em>NN is also simple to implement in Python or R). <em style="italic">k</em>NN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. <em style="italic">k</em>NN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a <link href="https://en.wikipedia.org/wiki/Normal_distribution" target="new" internal="false">Gaussian distribution</link>.</p><p id="ed6de4577c8d4272bb929fb3ea1fd01d"><em style="italic">k</em>NN will perform its task just in time as it does not learn a  model but keeps its training observations in an explicit form, so it spends less time learning and more time classifying or predicting.</p><p id="c0d7aaa90a874df5b9dbe83e8cf88fc9">There are three steps involved in making predictions with <em style="italic">k</em>NN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.</p><table id="cde953af408046d3aba969a72260d9a0" summary="" rowstyle="plain"><cite id="i6a063dbd8b0b478eb2e113bda24e4449" /><caption><p id="ec94c24616b14e70929162aa869bf3a0" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="bb25aa43ef5340c78502b5a782f680be"><em>Reading: </em><link href="https://arxiv.org/pdf/1701.07266.pdf" target="new" internal="false"><em><em style="italic">k-Nearest Neighbors: From Global to Local</em></em></link></p></td></tr></table><p id="fa138fc0c2a549468b8925d15c6be02a">The <em style="italic">k</em>NN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The <em style="italic">k</em>NN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in <em style="italic">k</em>NN is the Euclidean Distance. The formula for the Euclidean distance between observations (\\(x_{i}\\) and \\(y_{i}\\)) is shown below:</p><image id="b024a17deb6d4fd4ae7f59fca4ec051d" src="../webcontent/KNN_similarity_(3).jpg" alt="" style="inline" vertical-align="middle"><caption><p id="c09e8760b2ec4b9f939289e0d47f73d8">Euclidean Distance Function. (Source: Sayad (2010))</p></caption><popout enable="false"></popout></image><p id="c7079cf4156a47999a45063ddbe06fad">There are other distance measures, including <link href="https://www.sciencedirect.com/topics/mathematics/manhattan-distance" target="new" internal="false">Manhattan</link> (distance between vectors using the sum of their absolute differences) and <link href="https://en.wikipedia.org/wiki/Minkowski_distance" target="new" internal="false">Minkowski</link> distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the <link href="https://arxiv.org/pdf/1708.04321.pdf" target="new" internal="false"><em style="italic">Hamming Distance</em></link><em style="italic">,</em> which can be used as long as you do not have more than two (2) classes.</p><p id="cf36bcac7d374c78b3362375f24e9bd9">Once distances are computed, the class that a new observation will be assigned is based on the classes of its neighbors. Now, you can see why <em style="italic">k</em>NN is considered a <em>similarity function</em>. </p><p id="c5e10259d4b7425aab63ac62a5a7e176">How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., <em style="italic">k</em> = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a <em style="italic">k</em> that is greater than 1. If you assign <em style="italic">k</em> = <em style="italic">n</em>, i.e., <em style="italic">n</em> being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of <em style="italic">k</em> are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes <em style="italic">k</em> can be chosen using cross-validation, and the validation dataset will validate the best <em style="italic">k</em>.</p><p id="f608ef6555a94e3dab4006e2432ff79d">Although <em style="italic">k</em>NN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.</p></body></workbook_page>\n'