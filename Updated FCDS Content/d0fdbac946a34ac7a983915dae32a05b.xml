b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d0fdbac946a34ac7a983915dae32a05b"><head><title>Traditional Sequence2Sequence Models</title></head><body><p id="d3769c49ab4543b1aa2056ca4eaa8206">Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.</p><ol id="c80887979b28460891b5ccdbf6f2b5ce"><li><p id="c20c688e6a7e4501857b7e5593b8e60d">The <em>Encoder</em> processes the input sequence and compresses the information into a context vector (also known as sentence embedding or \xe2\x80\x9cthought\xe2\x80\x9d vector) of a <em style="italic">fixed length</em>. At a particular timestep, the encoder takes a word embedding and produces an output called the \xe2\x80\x9chidden state,\xe2\x80\x9d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.</p></li><li><p id="bd7cdd8e7ed943fd8259924c4470155a">The<em> Decoder</em> is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.</p></li></ol><image id="b39f49247d734292a73fc1708dd6c57e" src="../webcontent/image-d0fdbac946a34ac7a983915dae32a05b-1.png" alt="" style="inline" vertical-align="middle" height="225" width="650"><caption><p id="d1a9b1c6902a45aea119c3d3117c7615">Figure 1: Traditional Sequence2Sequence model architecture.</p></caption><popout enable="false"></popout></image><p id="ac875fd4a2a942608a0dd8ad80594e0f">A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.</p><image id="b21f13e2732642f181e6d6fa378dad8d" src="../webcontent/image-d0fdbac946a34ac7a983915dae32a05b-2.png" alt="" style="inline" vertical-align="middle" height="270" width="667"><caption><p id="c8d71ec1521d4bd38b1816b06497d653">Figure 2: Limitation of the traditional Sequence2Sequence model architecture.</p></caption><popout enable="false"></popout></image><p id="cdf80f3cdb3346a7806ca70a6eacf227">The attention mechanism introduced in <link href="https://arxiv.org/pdf/1409.0473.pdf" target="new" internal="false">Bahdanau et al., 2015</link> tried to resolve this \xe2\x80\x9cbottleneck problem\xe2\x80\x9d.</p><section id="f69a51bd203f434288fb25c0b7c5d6ee"><title>Attention</title><body><p id="c43b4010a7a540ac9f44ae60910bc7fa">Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.</p><p id="f3d20c4a131a47e5a0d29ddf94049eb8">In the following example, when we see \xe2\x80\x9ceating,\xe2\x80\x9d we expect to encounter a food word very soon. The color term (\xe2\x80\x9dgreen\xe2\x80\x9d) describes the food but is probably not related much to \xe2\x80\x9ceating\xe2\x80\x9d directly.</p><image id="dbf828126f0647a8a6bbd7859d7c43ef" src="../webcontent/image-d0fdbac946a34ac7a983915dae32a05b-3.png" alt="" style="inline" vertical-align="middle" height="112" width="468"><caption><p id="c962c8ce2df24484920cee0824b1925d">Figure 3: Attention between words in a sequence.</p></caption><popout enable="false"></popout></image><p id="ec17d15c293c420e89c5ed27f27d9911">In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of \xe2\x80\x9cattention\xe2\x80\x9d to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.</p><p id="b1a5d81d84764d0e8292726ea2e2faf5">Mathematically, attention in deep learning can be broadly interpreted as a <em style="italic">vector of importance weights</em> given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or \xe2\x80\x9cattends to\xe2\x80\x9d) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.</p></body></section><section id="d323b2501fff460da04f3afb4cf15cb0"><title>Attention in Sequence2Sequence Models</title><body><p id="dcb3fec7de6c41ea9f6721d4db1f73bd">In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to \xe2\x80\x9cattend to\xe2\x80\x9d at each step without the need to manually provide this information. The decoder weighs the encoder&apos;s hidden states to give higher importance to words \xe2\x80\x94 from the input sentence \xe2\x80\x94 that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.</p><image id="fe079a9ef3cb4e1ba41d403c853e9aa7" src="../webcontent/image-d0fdbac946a34ac7a983915dae32a05b-4.png" alt="" style="inline" vertical-align="middle" height="382" width="650"><caption><p id="c61456dd3901498e9d37bd8b95fe0013">Figure 4: Attention in Seq2Seq RNN architectures.</p></caption><popout enable="false"></popout></image></body></section><section id="b65244f4b40047e793262c81b208f8fe"><title>Towards Transformer Models</title><body><p id="f1909ec44edd46a79da19af8ead35ff7">Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or \xe2\x80\x9cattends to\xe2\x80\x9d) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.</p></body></section></body></workbook_page>\n'