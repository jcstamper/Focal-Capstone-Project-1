b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c0da183109594ae09b9ec5303ac457a9"><head><title>Tree-based Methods</title></head><body><p id="a6f38a40081f4660943b942d8d69fa25">Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an &quot;If-Then&quot; statement and are easily digestible to an individual with a growing statistics knowledge. </p><p id="d48b58a7d8674fc7a3ceda1954212491">We will explore the different tree-based methods starting with one of the most popular methods: <em style="italic">Decision Trees</em>. Using a very simple example, let us build a decision tree: <em>Decision Trees: </em><link href="https://scikit-learn.org/stable/modules/tree.html" target="new" internal="false"><em>scikit-learn</em></link><em>.</em></p><p id="dec13bca61b24f58a2121b9c349868c4">A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a <em style="italic">Classification Tree. </em>When your outcome or target variable is a continuous value, you have a <em style="italic">Regression Tree. </em></p><table id="f3d4876a395046dd9ebd00e29ec0ba38" summary="" rowstyle="plain"><cite id="i909f89fcc490403ca487bf3e968c7cdf" /><caption><p id="ff4076d78e564a3caa76b4a961cf9619" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b57c0c3ad4264e33924cde90dc59eba2">Additional Reading: <link href="https://hbr.org/1964/07/decision-trees-for-decision-making" target="new" internal="false">Decision Trees for Decision Making</link></p></td></tr></table><p id="f7b550da1af94260ac53064f82a90677"><em>Building Classification Trees</em></p><p id="a5df9e591ee84d66ac5d23fca8f05244">Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.</p><p id="fced94c331c04d13a7427bb22836aa8f"><em style="italic">Recursive Partitioning</em> creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.</p><p id="e8616327d534418486b39316cff247b3">Another technique for building decision trees is the <em style="italic">Chi-square automatic interaction detection</em> (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One&apos;s marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.</p><p id="aefc601ce7ee42328e692b29ccfbf692"><em style="italic">Measures of Impurity.</em> You can measure impurity using <em style="italic">entropy</em> and the <em style="italic">Gini index</em>. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.</p><table id="bfbf594ac63a43dfaf016eee897b5d32" summary="" rowstyle="plain"><cite id="i257a63e4879643b9b6bdf8111077e094" /><caption><p id="d048ebe41932481ba8bb3dde04bb7d28" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="f7de26c47f6f4f85b1720201b9cdaf84"><em>Reading: </em><link href="https://victorzhou.com/blog/gini-impurity/" target="new" internal="false">Gini Index and Impurity Measures</link></p></td></tr></table><p id="fd3542b909be48e9ba3e5aead1c1fba2"><em style="italic">Pruning. </em>If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.</p><ul id="e8536ce2aa6a4762b4d6106d73cc6955"><li><p id="a68ecdebb0da48b4ae861695cb391749">Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.</p></li><li><p id="e50cd66abbc14da283793fbf1cd398de">Reduced error pruning is done by replacing each node with the node&apos;s most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning. </p></li></ul><table id="c761935ee095473a8a31097bf376f8f5" summary="" rowstyle="plain"><cite id="i5a40b81f245a48c9b51ea62a17778838" /><caption><p id="abe8ee02b93b487996bf733c110d9cb5" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b8fc32d197e24af68a3ece997404111c"><em>Application</em>: <link href="http://faculty.washington.edu/fxia/courses/LING572/decison_tree99.pdf" target="new" internal="false">Decision Trees and NLP: A Case Study in POS Tagging</link>. </p></td></tr></table><p id="b973b0a13c024ce6958c941d89fc7645">When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a <em style="italic">Minimum Error Tree.</em> The least complex tree with a validation error that is within one standard error of the minimum error tree is called a <em style="italic">Best Pruned Tree</em>.</p><p id="b0b58bd1b8ff4cbebeb491e4cb2c8c28">The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.</p><p id="c82fb8a5ff0e49408e69352085e52876"><em>Building Regression Trees</em></p><p id="c636f21dd61c4579915ce72593fb1dea">We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).</p><p id="ef890bf9f05f44eb9c63c6fa8ddae5d2"><em style="italic">Random Forests, Bagging, and Boosting </em>can be used to improve this prediction accuracy and performance. We will learn about those next.</p><section id="afedd832b7f14117b189bebeaa8f972f"><title>Ensemble Techniques</title><body><p id="e2ac599b42314cebb82932542a5c8c1d"><em>Bagging </em>reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. <em style="italic">Out-of-Bag Error Estimation</em> measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.</p><table id="c130d13669714105ba40f885df340536" summary="" rowstyle="plain"><cite id="i3ef330f9e798478e871ef218e7e3869f" /><caption><p id="a7e3e785653a4a779eac37585a2f4ed0" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="bb0f75e62bd64320a805abedf3dee179">Additional Reading: <link href="https://www.tandfonline.com/doi/full/10.1080/21642583.2014.956265" target="new" internal="false">History of Random Forest Algorithm</link></p></td></tr></table><p id="d7e5fd6647464eb18b54a3311c1c41b1"><em>Random Forests. </em>This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).</p><p id="c02702c86f6349868af1bd3d41b1b1e5"><em>Boosting. </em>Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.</p><p id="ff635ace5782441c9b0e3e4c69d1dd94">Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: <em style="italic">Adaptive Boosting (AdaBoost)</em>. AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model&apos;s predictive power, which is helpful for reducing dimensionality and improving computation time. </p><p id="dbe1f0631c894b2392cb5aea39597e3c">Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.</p><table id="a5a3145c71d14d9eb8f33ac383ead233" summary="" rowstyle="plain"><cite id="iac0258dc57064ea0a7b1b11b93804fb4" /><caption><p id="b5c23f0d62e64e81ad376adb919bcb26" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ffc33d32e9974258b45208357073dc09"><em>Reading: </em><link href="https://en.wikipedia.org/wiki/Ensemble_learning" target="new" internal="false">Ensemble Methods-General Use</link></p></td></tr></table></body></section></body></workbook_page>\n'