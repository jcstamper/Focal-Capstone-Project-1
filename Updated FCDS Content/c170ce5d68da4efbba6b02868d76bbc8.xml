b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c170ce5d68da4efbba6b02868d76bbc8"><head><title>Data Wrangling</title><objref idref="a2e2091d25524d3c8470d18640f8f76e" /><objref idref="d94c43426bb240b391a6e1690ca8c30c" /><objref idref="e419ad50b62747dfb8c1a4bfc9e7d729" /></head><body><p id="e271b69ea212431ba90620647a1cb46d">The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that <em style="italic">it</em> is usable. Data that is collected from different sources are considered <em style="italic">raw data</em>. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.</p><p id="c0fa80e5df9f42d2a7cbc3cde763a5f4">At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called <em style="italic">Data Wrangling.</em></p><p id="a72e4e156f67443d83741a98a1364a69"><em>Data Wrangling</em> is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.</p><p id="b8ef48e2c42147a58d539daeb5bb3e58">Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.</p><p id="aada93d38ce544908dec91bb8a693d00">Despite its importance, data wrangling presents some challenges that are common in data science projects.</p><p id="e99df6f21c584295bbe6a62a23a414e1">So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with \xe2\x80\x9ccleaned\xe2\x80\x9d datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.</p><p id="dfbc246dcdde4235b5a98c07715c91dc">Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization&apos;s architecture.</p><p id="b7a47debb5e2461f90a1b8200078096d">A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.</p><p id="b4d11d07549a40f69b5779d0d8da9d8d">The <link href="https://scikit-learn.org/stable/modules/preprocessing.html" target="new" internal="false">scikit-learn preprocessing package</link> is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.</p><section id="f97536cbf9f749a9a98db8ddb6af0cde"><title>Inspecting Data</title><body><p id="a26d4b0b0e4b4597910279410da59f5e">Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.</p><p id="eaa844a28d944349bb77bd9edb57d6aa"><em>Outliers</em></p><p id="a53a0943afff4b28b30525279be091a4">Rather small or large observation within your dataset compared to other values in the dataset is called <em style="italic">an outlier. </em>Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can \xe2\x80\x9chandle\xe2\x80\x9d outliers by following these steps:</p><p id="d24276a2b7a74b58b24a56ca3d759fe6">Construct a <em style="italic">Box plot </em>or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.</p><p id="ff874d4d0fec4562993c75e17650a1e6">The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called <em style="italic">whiskers</em>. They extend to the maximum and minimum, except for outliers, which are marked separately.</p><p id="e76149df64a445e0b0948ee902fb037e">Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.</p><image id="b5f46fe0cf4c440ea38a08e6e8ea7f0b" src="../webcontent/1*USCc7cKa-8M0tfCPkFPZQg.png" alt="" style="inline" vertical-align="middle" height="256" width="600"><caption><p id="b0d215740d8a42bd918606ce22777346" /></caption><popout enable="false"></popout></image><p id="da07e03907f949cfa0bfde155d371568">In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.</p><p id="b7a1f8d41d9245a5ad427b20bd1f7bb3">Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.</p><p id="fffb7b0804974bc08c390465ae2bbde1">In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.</p><p id="de65a79af16b4ebca82cbabdafa9e4a9"><em>The z-score</em></p><p id="b90d797832b24d648b2af67f394b11b0">Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its <em style="italic">z-score</em>.</p><p id="c23434d3983441eb9c91ba02aae56b65">\\(z = (x-\\mu )/\\sigma\\)</p><p id="aa47407207a74357a7b3b9cfdcfa405d">The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the <link href="https://www.investopedia.com/terms/e/empirical-rule.asp" target="new" internal="false">Empirical Rule</link>, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!</p></body></section><section id="ce9fa1c08d8447eaa9c44bd6cf52bf5c"><title>Transforming Data</title><body><p id="bd9ad79629ca42a0af015daf3af6f2b8">The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task</p><p id="e57b9d8d83d2445ea4552f329a64e433"><em>Handling Missing Values</em></p><p id="cc8140bcb5064b07b56c3d8de2d6a08d">One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.</p><p id="a32180b3678f421689f3caf8a6f9c6c7">In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.</p><ul id="e791867dccfc433b8a4206cdf71d9975"><li><p id="a55f28ae1bb44e13b86993cbff73f44f"><em style="italic">Missing completely at random (MCAR) </em>implies that missing data is not related to the data. The probability of data being missing is the same for all observations.</p></li><li><p id="f4ac8f7b2c3c449a9b2e9251d503b66b"><em style="italic">Missing at random (MAR) </em>is the probability that the missing data is the same within certain groups.</p></li><li><p id="bb124e3e51f84866a15abea879f95f6b"><em style="italic">Not missing at random (NMAR) </em>means that the probability of data being missing varies for reasons that are unknown.</p></li></ul><p id="b8ed3a895c3c45f39b1cbac39f22eb43"><em>Imputation</em></p><p id="c6f01c2b1bef4dfabc9de0f160e9e62b">The common strategies that are employed in handling missing values are imputation and omission. <em style="italic">Imputation </em>replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include <em style="italic">hot and cold deck imputation, regression imputation, </em>and <em style="italic">interpolation and extrapolation.</em></p><p id="fcdb5d1163194b9e98a6df73e44fd703"><em>Omission</em></p><p id="b14bbf22280646418ee233588f63e726"><em style="italic">Omission </em>is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you <em>will suffer a loss of data</em> if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small. </p><p id="b9dd5f57356147bc92a488246218bf54"><em style="italic">Pairwise deletion </em>is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size. </p><p id="c594ead082554859a255535d45a28ebd"><em style="italic">Listwise deletion </em>removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.</p><p id="d33e409f323e4e158236590a461dbea8">You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.</p><p id="ba285fd59320412eafe004e5d5589672"><em>Subsetting. </em>This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.</p><p id="f00ad26b8cb9472c89d675cd64862221"><em>Outliers</em></p><p id="b7ace29ae88e4991abafaf8755cc37d8">When we discussed inspecting the data, there was mention of visualizing the data to identify <em style="italic">outliers. </em>Outliers are unusual values in the dataset. The value is unusual because it \xe2\x80\x9clies at an abnormal distance from other values in your dataset.\xe2\x80\x9d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.</p><p id="efe2eea270fe492fb6ddbfcba717db1d">As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.</p><p id="b1ec69b313e94429b562e7a5c93c6f27"><em>Transforming Categorical Data</em></p><p id="db6e04950e36489d902652ed824167c4">Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.</p><p id="d7cce4cba9c14355a1befa414085e0f5"><em style="italic">Category Reduction. </em>Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an &quot;other&quot; category for the categories with few occurrences.</p><ul id="ce070fba6b614ff9ab6cb27ace51769f"><li><p id="e00082694d8846079ffc7d9ce7a75d5b"><em style="italic">Creating Category Scores. </em>Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: \xe2\x80\x9cThe instructor provided out-of-class support for the course\xe2\x80\x9d could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.</p></li><li><p id="b5fc5ddf8f8f40bfa9d835ed6f7e4d3f"><em style="italic">Creating Dummy Variables. </em>Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.</p></li><li><p id="cd12678c9f064bcea7518e356d5da3e0"><em style="italic">Creating Dummy Variables for more than one category. </em>What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:</p><ul id="d1b5676b707448e88408014e3411a9f5"><li><p id="a8c5283a2dba4aa2b5320d829c2494cb">For a variable with \\(k&gt;2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Let\xe2\x80\x99s call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).</p></li><li><p id="d7d9de6656f54512a99660ed2f4ee349">One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.</p></li><li><p id="e374d7730b1b4da3af2ba34af822ef1a">Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.</p></li></ul></li></ul><p id="c0dfa008689d44b5b6e7681759d4be5d"><em>Transforming Quantitative Data</em></p><p id="cbd281c4eb6f4681b3b9744507e77fd2">Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.</p><p id="d06b5a2173f74e3aa58037659e44efc4">An exampleof a popular quantitative transformation is converting the date of birth to age.</p><p id="d3f2a2fe7fad4d50a5cfa129c45e9056">Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.</p><ul id="e1f27006c90e4c419f52d654bae5d5dc"><li><p id="e5c84856fdfc49c5ac05d23bb2c9828f"><em style="italic">Binning</em> transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.</p></li><li><p id="cf9070b25df2453dac45db37bcdfcf62"><em style="italic">Using Mathematics. </em>One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.</p></li></ul></body></section><section id="bdac158021d44522b9523a4e84d2f1c4"><title>Integrating Data</title><body><p id="a87fb1adc97b47a3a885ba3de5338fa1">Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.</p><p id="a514973e75264c0a9b2ee83ded25ed26">Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.</p><p id="c6a38434b2574818be6a659e85c78eea"><em>Data Wrangling to Data Exploration</em></p><p id="a06fe9fa4c7b4f12809402acd31ab35d">In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to <em>descriptive</em> analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.</p><p id="ba1681d24d524e9eb057b15e3816c1d2">The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion. </p></body></section></body></workbook_page>\n'