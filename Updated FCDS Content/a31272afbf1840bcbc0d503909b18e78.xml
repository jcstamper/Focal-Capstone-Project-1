b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="a31272afbf1840bcbc0d503909b18e78"><head><title>Model Selection for Prediction</title><objref idref="fc59dc700a3e4c95abcf79e8bd3bd4f9" /></head><body><section id="aa73c9dc6cf4463a8d5d723046634563"><title> </title><body><p id="bf3eea2275c24b58aeaae63e962c099b">To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:</p><table id="b87898a791d442da85646c6fe34c6440" summary="" rowstyle="plain"><cite id="ibf0a531f3a434549a9e022571c9dbaed" /><caption><p id="d65bc54f5c6c461789a5a266ba4025e5" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b83139609d364fd092f6d5fbc1da0dd5"><em>Input</em>: candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub></p><p id="a50869002b3a4eedb1effb5d663c086b"><em>Procedure</em>:</p><p id="d2b353bd4caf4a08834fb9b39b4c543c">Split the dataset into <em>train set</em> and <em>test set</em>.</p><p id="f3bf068e1a024b7085f4b61aeb4068f5">For each candidate model M<sub>i</sub>:</p><p id="a4dd426136c2443186244d15be6598bf">(i) Train M<sub>i</sub> on the <em>train set</em>.</p><p id="d83fdda350314638982e615f58e26545">(ii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em></p><p id="df7bdd7917e04d0b830db76613fded99"><em>Output</em>: the model M<sub>j</sub> with the best performance on the test set.</p></td></tr></table><p id="cc9716ab196f49418bd890f053af086e">If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.</p><table id="ef5905ec55e54ea88a4da18d3cf7a122" summary="" rowstyle="plain"><cite id="i33e236d1509d4aa48e6692e0522cc397" /><caption><p id="c06f6d7f6b7f4508b59cef6a2af6a221" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e63bb97317044429a192bca35b764850"><em>Input</em>: candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub> and hyperparameter space S.</p><p id="e2419de82a6d463dbd38a18cbb04df1e"><em>Procedure</em>:</p><p id="f2d52d445a7843269710bd7b5344c9ec">Split the dataset into <em>train subset</em>, <em>validation subset </em>and <em>test set</em>.</p><p id="f57ee9006c8e412baade596f897e505d">For each candidate model M<sub>i</sub>:</p><p id="a46bb0529b9b4a49a547ba8bbaf8fab9">(i) Pick the hyperparameters that give the best performance on the <em>validation subset </em>when M<sub>i</sub> is trained on the <em>train subset</em>. We call these the <em style="italic">best hyperparameters</em>.</p><p id="f71eaddd230240c99777397dddf78e21">(ii) Retrain a new model M<sub>i</sub> using the <em style="italic">best hyperparameters</em> on the combined data from the <em>train subset </em>and <em>validation subset</em>.</p><p id="a53df861b6934b12a2d0c638801167c5">(iii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em>.</p><p id="dbd29567edc947f499d3b11682106600"><em>Output</em>: the model M<sub>j</sub> and the associated <em style="italic">best hyperparameters</em> with the best performance on the test set.</p></td></tr></table><p id="e983b71a684343d99e157485f84a80e6">Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.</p><p id="b7def37d3b9742ed918c662a637c048a">While the above procedure is sufficient to demonstrate the prediction of a  model\xe2\x80\x99s validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative is to perform k-fold cross-validation as in the inner loop of the model selection procedure.</p><table id="d6355ee8a020416381bd761e30b84819" summary="" rowstyle="plain"><cite id="i526b69c289994861b8c0cc3b4917285f" /><caption><p id="d85d74addb584e6fb6085b7741c7570a" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c796e9701e3f4e36b7b3e5554f6ba7ba"><em>Input</em>: number of folds K, candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub> and hyperparameter space S.</p><p id="b1940c3eef6e4e91b910f4623238abf0"><em>Procedure</em>:</p><p id="f6a609396a5c4d4ca5e5dfacb21605b2">Split the dataset into <em>train set</em> and <em>test set</em>. Split the <em>train set</em> into K folds.</p><p id="ad7b79adc76a4bc1bce9dcd2bc831cb2">For each candidate model M<sub>i</sub>:</p><p id="ee641efe49bf4bf897336cadd1ef0c26">(i) Pick the hyperparameters that give the best <em style="italic">cross-validated performance</em> for M<sub>i</sub> on the <em>train set</em>. We call these the <em style="italic">best hyperparameters</em>.</p><p id="f31ad3b391444df48a764f720a349c99">(ii) Retrain a new model M<sub>i</sub> using the <em style="italic">best hyperparameters</em> on the <em>train set</em>.</p><p id="ddbea9cacc794717b68ee56830d20c4a">(iii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em>.</p><p id="fe94943a8fa244e5bd0b188fbbe81694"><em>Output</em>: the model M<sub>j</sub> and the associated <em style="italic">best hyperparameters</em> with the best performance on the test set.</p></td></tr></table></body></section></body></workbook_page>\n'