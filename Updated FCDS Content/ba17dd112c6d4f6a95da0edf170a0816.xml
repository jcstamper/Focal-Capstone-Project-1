b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="ba17dd112c6d4f6a95da0edf170a0816"><head><title>Asymptotic Complexity</title><objref idref="c514a3963013475b840e38db813125f0" /><objref idref="f9023ae790c84b0080551d755838ac10" /></head><body><p id="ad2adb215368440890d00f33c0414c70">Above, we alluded to concepts such as the time <em style="italic">an operation takes</em> or the memory <em style="italic">a data structure requires</em>. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.</p><p id="c3459cbd7969458a97287c5b66cd0c61">In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.</p><p id="ef72fa6093a64042ad22548994a27a8a">Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called <em style="italic">asymptotic analysis</em>.</p><p id="a0f50812d4414e848d095ce1b6760d30">The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).</p><p id="de45e43f14ca4d11957d04ebb6b1b9aa">The most important tool is based on the <em style="italic">big-O</em> notation. We say a function \\(f(n)\\) is \\(O(g(n)\\) if there is are positive constants \\(c\\) and \\(n_0\\) such that for \\(n \\geq n_0\\) \\(f(n) \\leq c\\cdot g(n)\\). That is, beyond \\(n_0\\), \\(f(n)\\) grows at most as fast as \\(c\\cdot g(n)\\), that is, \\(c\\cdot g(n)\\) always dominates \\(f(n)\\) in growth, as shown below.</p><image id="f6d120c683c04cfaa0596455f2ec2ceb" src="../webcontent/bigo.jpg" alt="" style="inline" vertical-align="middle" height="373" width="600"><caption><p id="a3a1ed142d2f4e8cac91b80a6734d40d" /></caption><popout enable="false"></popout></image><p id="dc67ec2f634a4be0958b090b112a9338">So when analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of the input. Based on the definition above, we express the number of steps with a function \\(f(n)\\), which captures the size of the \\(n\\). We then try to find the function \\(g(n)\\) such that \\(f(n)\\) is \\(O(g(n)\\).</p><p id="bfda32d7517e43428e87191ed2f0d5ad">Let&apos;s give a very simple example to explain this. Suppose we have an unsorted array of \\(n\\) integers, and we want to search if a given integer \\(x\\) appears in the array or not. A simple algorithm for this would a simple loop that searches if the next entry in the array is equal to \\(x\\). So <em style="italic">in the worst case</em>, we will need to look at each of the \\(n\\) integers until we know what the result is. Assume of these steps take, say, 3 operations; our function \\(f(n)\\) will be \\(3n\\). Now with the choice of \\(c=4\\) and \\(n_0 = 0\\) you can easily convince that \\(f(n)\\) is \\(O(n)\\). That is, the number of steps of our algorithm grows linearly as \\(n\\). While this is a very simple example, not every algorithm analysis will be simple as that. You can try and come up with an algorithm that searches \\(x\\) in a <em style="italic">sorted</em> array whose number of steps \\(f(n)\\) is \\(O(\\log n)\\). So ignoring the cost of initial sorting (which can be amortized over many searches if necessary), you can see that this latter algorithm uses much less time than the former one, as the function \\(\\log n\\) grows much much slower than the function \\(n\\).</p><p id="b6f0b7161ce74351a9a98e4b98e0525f">The following are some additional important concepts regarding asymptotic analysis:</p><ul id="a4b3a0df1fc04707b25bf1f73059af2e"><li><p id="ac7c94a34da74354890deb5b31fff2b3"><em style="italic">Upper bound </em>is the asymptotically maximum time that a given algorithm needs for all inputs of size \\(n\\). Algorithms have upper bounds. For example, we say, &quot;mergesort has an upper bound time complexity of \\(f(n) = c_1 n \\log n\\)&quot; or &quot;selection sort has an upper bound time complexity of \\(f(n) = c_2 n^2\\)&quot;.</p></li><li><p id="a6a10adb97bf46cca1b344b2183ec4bd"><em style="italic">Lower bound</em> is the minimum asymptotically the minimum time that <em style="italic">any</em> algorithm for a problem needs for all inputs of size \\(n\\). For example, it can be shown that no sorting algorithm that works by comparing elements can take less than \\(c_3n \\log n\\) steps.</p></li></ul><p id="af1734707e35464c9ea3f1a06323fbee">For more details on asymptotic analysis one can refer to any book on algorithm analysis<cite id="ie4134347b5434a57b7e8ab53f0c98578" entry="ad8d87ed1a304d0581aa81977db7757e" />, or to <link href="https://en.wikipedia.org/wiki/Asymptotic_analysis" target="new" internal="false">https://en.wikipedia.org/wiki/Asymptotic_analysis</link></p></body><bib:file><bib:entry id="ad8d87ed1a304d0581aa81977db7757e"><bib:book><bib:author>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein</bib:author><bib:title>Introduction to Algorithms</bib:title><bib:publisher>MIT Press</bib:publisher><bib:year>2022</bib:year><bib:edition>4th Edition</bib:edition></bib:book></bib:entry></bib:file></workbook_page>\n'