b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="b4787980c6a54f0bb3010dac6f7fff17"><head><title>Bidirectional Encoder Representations from Transformers (BERT)</title></head><body><p id="e684f5b91bed4a7390d0b900961d283c">Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.</p><section id="b8b02f8ac88a4d31ae34638ca02d8c0d"><title>Encoder-Only Models</title><body><ol id="e2790b7f57f2447a884436c385ecbd33"><li><p id="a0587e2f917945069ba0548d5dce3558">Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \xe2\x80\x9cbi-directional\xe2\x80\x9d attention and are often called <em style="italic">auto-encoding models</em>.</p></li><li><p id="a1cd80191fe642fa89020bc47d497bd5">The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p></li><li><p id="e621af4a69b44112a916d35763850f62">Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.</p></li><li><p id="c356d619e8aa4290a66e308f4818417a">Representatives of this family of models include BERT, ALBERT, RoBERTa.</p></li></ol></body></section><section id="e491437cb2a9406dbed9f146d79b4160"><title>Decoder-Only Models</title><body><ol id="abd9a11fc4e64061bb8af507e6bf3c99"><li><p id="a812a41ecf3249e7a19d840d444296a6">Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em style="italic">auto-regressive models</em>.</p></li><li><p id="d21532ba99c64644888bc317a5cf528b">The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p></li><li><p id="c462b33d99d848318ae1d8566e9b6c84">These models are best suited for tasks involving text generation.</p></li><li><p id="e758d7a8577e49a48e513227f17dd811">Representatives of this family of models include CTRL, GPT, GPT-2.</p></li></ol></body></section><p id="e76cf2c1afad464a9fdd7520c28ba1dc">We are now finally ready to study arguably the most famous Encoder Model, BERT and its variants in detail.</p><section id="b2f52de92ee14926abcec8fd8a3c742e"><title>BERT</title><body><p id="d1308d6b3ddf4e0da862305f8afeaa2b"> </p><p id="cbfa924adc2f4302b6c0766c7395a3e6">One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.</p><p id="a86df6e346c14084ba6cfc17c003ad69">BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms \xe2\x80\x94 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\xe2\x80\x99s goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained Transformer Encoder stack.</p><p id="cee3dedea6364315af5411770f47045d">As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\xe2\x80\x99s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p><p id="c67b194bb7a54618aefe73509cf6e479">The original BERT<link href="https://arxiv.org/abs/1810.04805" target="new" internal="false"> paper</link> presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.</p><ol id="cd317b8aae724c14ae6f04e67a2fc90f"><li><p id="bd14232dec3f4b86be728b5823b292cf">BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.</p></li><li><p id="a873546d4dc8405f95f70dfd498f768c">BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.</p></li></ol></body></section></body></workbook_page>\n'