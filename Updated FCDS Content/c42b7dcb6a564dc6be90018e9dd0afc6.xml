b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c42b7dcb6a564dc6be90018e9dd0afc6"><head><title>Language Models</title><objref idref="aeec69dc53454bf6a58706a3fc5ccff2" /></head><body><p id="b9d2fa6b6afb4e11847729e9ca5fc989">While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they don\xe2\x80\x99t take the context of relative positivity between words into consideration. For example, \xe2\x80\x9cgood food and terrible service\xe2\x80\x9d and \xe2\x80\x9cterrible food and good service\xe2\x80\x9d mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately. </p><p id="b8fd1666207849f59557798cec14b2a3"><em>Probabilistic language models</em> are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-gram\xe2\x80\x99s probability is the conditional probability that the n-gram\xe2\x80\x99s last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase \xe2\x80\x9cgood food and terrible service\xe2\x80\x9d would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = \xe2\x80\x9cfood\xe2\x80\x9d/\xe2\x80\x9dgood\xe2\x80\x9d), P(word = \xe2\x80\x9cservice\xe2\x80\x9d/\xe2\x80\x9dterrible\xe2\x80\x9d).</p><p id="c8ca6eb569eb4f8aa756abf35661ffb2">\\[ P(W_{n}|W_{n-1})=\\frac{P(W_{n-1},W_{n})}{P(W_{n-1})} \\]</p><p id="dfd547edcbec4910be5bad2bf672c8d9">Where the probability \\(P()\\) of a token \\(W_{n}\\) given the preceding token \\(W_{n-1}\\) is equal to the probability of their bigram \\(P(W_{n-1},W_{n})\\), divided by the probability of the preceding token.</p><p id="db9b87858c5748a9b6b9845852c9adaa">Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences \xe2\x80\x9cBig blue skies look appealing.\xe2\x80\x9d and \xe2\x80\x9cColorless green ideas sleep furiously.\xe2\x80\x9d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one \xe2\x80\x93 a sentence she can say someone could use.</p><p id="e8f78f023fb84e878799c883067073d4">Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc. </p><p id="bb230cb124424005beb1a6540e4f4d61">These days people build (classical) language models using well-established toolkits:</p><ul id="a2026fb0780940bb9ba056200c82343f"><li><p id="f2a644c3ba6348db871d529faa4247ee">SRILM Toolkit: <link href="https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit" target="new" internal="false">https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit</link></p></li><li><p id="c5afc1d5646347cab08e171082c04474">CMU Statistical Language Modeling Toolkit: <link href="http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html" target="new" internal="false">http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html</link></p></li><li><p id="c054a228205847c8ba9da1f34be718d8">KenLM Language Model Toolkit: <link href="https://kheafield.com/code/kenlm/" target="new" internal="false">https://kheafield.com/code/kenlm/</link></p></li></ul><p id="df72c717b797434a8d719bf219fd7f47">Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.</p></body></workbook_page>\n'