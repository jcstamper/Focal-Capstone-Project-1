b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="de3aa30f6fc149fbaeafd8e13c0df272"><head><title>Algorithmic Bias</title></head><body><p id="b88891c6c54143e3b87e721507bb3d33">One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term \xe2\x80\x9cdata-driven\xe2\x80\x9d reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the future\xe2\x80\x93but we cannot <em style="italic">capture</em> information about the future, in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientist\xe2\x80\x99s work is bound up with information about the past. So is data science backward-looking?</p><p id="c1e797ccfca840e8b15886f10b841838">For example, let\xe2\x80\x99s say you have a longitudinal dataset about income and education and you want to make some inferences about it. If the data go back far enough, you would get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didn\xe2\x80\x99t take that into account, or consider it when creating your framing questions, you could end up with conclusions like \xe2\x80\x9cgirls aren\xe2\x80\x99t smart enough to go to college\xe2\x80\x9d, or \xe2\x80\x9cwomen don\xe2\x80\x99t like high-paying jobs.\xe2\x80\x9d</p><p id="f7a28811fd2d454fa36944e85e2e3edc">The importance of recognizing that these conclusions, while technically possible, maybe problematic to assume, is that in history there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.</p><p id="ba7d7a92a84f49cf900c85dca07d9fcf">One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward the members of group A. For example, if your job is to analyze a group of nurses at a hospital and your job is to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you may wrongly decline a qualified male candidate. </p><p id="c27aa5e032674831833ebbb1da59c1ec">As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most \xe2\x80\x9cnormal,\xe2\x80\x9d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.</p><table id="ef49a362c8f740219d35998acfb2d761" summary="" rowstyle="plain"><cite id="i4cff8d9e3ca94a55a304b57c4076674c" /><caption /><tr><td colspan="1" rowspan="1" align="left"><p id="c8b165f9280f401d8b1fcfd5459b05cd">Reading: <link href="https://doi-org.cmu.idm.oclc.org/10.7551/mitpress/9302.001.0001" target="new" internal="false">Introduction section of &quot;Raw Data&quot; Is An Oxymoron by Lisa Gitleman and Virginia Jackson.</link></p></td></tr></table><section id="bc00d758115e4ad4b9cf7fca947d87e0"><title>Case Study</title><body><p id="e1952468d20d4201bb623a958bd05281" /></body></section><p id="fb2cc8d5182a49f0b9c04176b8e749cc"><link href="https://news.northeastern.edu/2018/07/16/heres-what-happened-when-boston-tried-to-assign-students-good-schools-close-to-home/" target="new" internal="false">The school assignment algorithm implemented by the Boston Public School (BPS)</link> starting in the 2014-2015 school year uses a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program&apos;s implementation shortening students&apos; distances and minimizing travel time to and from school, the system fails to allocate students to high-quality neighborhood schools due to the disparity in </p><p id="a260a737394d4815b23ba9e2fd3b5d71">allocations of high-quality schools across Boston&apos;s neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program&apos;s intention was just, and reducing the distance to school and busing is desirable, the algorithm&apos;s design phase did not account for the limited quality-school situation among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families, but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.</p></body></workbook_page>\n'