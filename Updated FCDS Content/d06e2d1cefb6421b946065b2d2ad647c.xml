b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d06e2d1cefb6421b946065b2d2ad647c"><head><title>k-Means Clustering </title></head><body><section id="ec92654e10e84e7089ccc1e585f91537"><title>Unsupervised Learning Techniques</title><body><p id="ee9c81c5ccf14b59af50eb2c86b46332">When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied  Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.</p><p id="f96cce74a98c4dcea31406125183e9a1">You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.</p><image id="bb883c08539741dfada45516a43509c8" src="../webcontent/Clusterexample.jpg" alt="" style="inline" vertical-align="middle" height="426" width="650"><caption><p id="f9a6f1cc0b3c475ea11ccd936d2feecb">Clustering (Source: www.pyarmy.com)</p></caption><popout enable="false"></popout></image></body></section><section id="decefc0ec48f4d58a6f2a6f3a77aab8f"><title>Types of Clustering</title><body><p id="df818ed009a64521a9a6aecf38ea072e"><em style="italic">Hard Clustering</em> divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.</p><p id="c60d3fa2e1524db18a0bdc1c52956b32"><em style="italic">Soft Clustering</em> groups data into clusters, but a data point can belong to more than one cluster to a degree.</p><p id="ab356d67fa624cdc87b4789de1ec7d6f"><em style="italic">Overlapping Clustering</em> allows data to belong to more than one cluster.</p><p id="ade53e37721042e88eec7d3d7168cf86"><em style="italic">Hierarchical Clustering</em> organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.</p><table id="cd30989e5544425b8c0876cae19e581f" summary="" rowstyle="plain"><cite id="i6b199aec782742ffa452354b7735d3b8" /><caption><p id="a54367be4c9f4727850e813333e13f06" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e6ba5fae2f97452db4fcbdd144ad96e8"><em>Reading: </em><link href="https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering" target="new" internal="false">An Expansion on Clustering</link>.</p></td></tr></table></body></section><section id="bc4424576bc749bd990659b80605c5f2"><title>k-Means Clustering</title><body><p id="eca991b543c74320bf2b3aebef341fcf">This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:</p><ul id="ee73afd5c77145b4a77c845e34c9bc4a"><li><p id="ea40729f9fbf4db0874fc5ce8911ff67">A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.</p></li><li><p id="a01e94479b364b4e885c00ec8bfe598b">Each data point is then assigned to the cluster with the nearest mean.</p></li><li><p id="ff39abc04b664072976ad527994038fb">After a pass, using the points assigned to a cluster, new means are computed.</p></li><li><p id="a4dedf08e2d24654bf528055a0c42ef3">The last two steps are repeated.</p></li><li><p id="fee2d93dc9fa4f7c87f772199d309196">The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.</p></li></ul></body></section><p id="a95ec12efea644fe9c92720351f02a31">How do we decide <em style="italic">k</em>? Similar to <em style="italic">k</em>NN, there are empirically studied recommendations for the best <em style="italic">k</em> to select. You can also select <em style="italic">k</em> based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for <em style="italic">k</em> and then compare the results obtained from each value of <em style="italic">k</em>. It is good practice to also run the <em style="italic">k</em>-Means cluster method by using different values for <em style="italic">k</em> based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of <em style="italic">k</em>.</p><p id="e86d450f9d8c46b8a7e72a67aa8b528f">k can also be chosen by calculating the <em style="italic">Within Cluster Sum of Squares (WCSS)</em>. This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.</p><p id="bd84a3e93c7b4ebe9261ff1a3b56f441">Assuming that we have 1000 observations in a dataset, and we have decided that <em style="italic">k = 1000</em>, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.</p><p id="c8e59e18989840ac9ca4482ec77124a8">When you randomly initialize with a range of <em style="italic">k</em> values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for <em style="italic">k</em>. The Elbow method produces a graph that shows this optimum value at the &quot;elbow&quot; of the line, as shown below. You select <em style="italic">k</em> as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.</p><image id="a0bf20c38e5d48eba52f17abfc1ca84f" src="../webcontent/Elbow_method.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="c147bb88f81c44f78940634fd8b0772e">Elbow Method</p></caption><popout enable="false"></popout></image><table id="e056006746734dfb85260bb3cd8b13ca" summary="" rowstyle="plain"><cite id="i8c29b857b5344606b0bcd716ac4c29b9" /><caption><p id="caa4ede768494efd94d4cd334dcbab52" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ffa6798ba14241118c390280faed34bf"><em>Reading: </em><link href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="new" internal="false"><em style="italic">k-</em>Means Clustering-sklearn</link>.</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="fad8cf706494459884a63034c36f61cc"><em>Additional Reading: </em><link href="https://en.wikipedia.org/wiki/K-means_clustering" target="new" internal="false">K-Means Clustering Algorithm</link></p></td></tr></table><p id="cbf61b1f51f84a4d8271f466f762103a"><em style="italic">K-Means Clustering</em> and <em style="italic">k-Nearest Neighbors</em> have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. <em style="italic">k</em>NN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points. </p><p id="ad99311ff79246898eb740fee2d3bf41"><em style="italic">K-means </em>does not provide a labeled dataset to the model for learning purposes. <em style="italic">K-means</em> will partition the data into a number of clusters. <em style="italic">kNN</em> works best with data that is of the same scale, but <em style="italic">k-means</em> does not need the same scale data to perform well. Remember when you learned about <em style="italic">kNN</em> being a lazy learner? <em style="italic">K-means</em> is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.</p></body></workbook_page>\n'