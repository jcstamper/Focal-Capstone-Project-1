b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f73b5ad409304e1c8a4ec689bab7463d"><head><title>Bayes Method</title></head><body><p id="adbcf0287aa7482a82afe47e033cb0fb"><em style="italic">The Bayes Theorem </em>describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.</p><image id="bfee29da7d8e4055860e0802ae5bd541" src="../webcontent/Bayes.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="ff485191780442c4bff39d8129cabce1">Bayes Rule (Source: https://www.psychologyinaction.org)</p></caption><popout enable="false"></popout></image><table id="f203990078574afe81e0ad3a11ccd6a2" summary="" rowstyle="plain"><cite id="ib794910a114a497ca32b1931217d3692" /><caption><p id="ce190101998849b68d2c9225afeae703" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="db3a717ca10248869d4986aba8b7d5fe">Additional Reading: <link href="https://plato.stanford.edu/entries/bayes-theorem/#:~:text=Bayes&apos;%20Theorem%20relates%20the%20%22direct,%2C%20PH(E).&amp;text=Bayes&apos;%20Theorem." target="new" internal="false">Bayes Theorem</link></p></td><td colspan="1" rowspan="1" align="left"><p id="bba6836ed9064e56b37829ecf6245eac">Additional Reading: <link href="https://www.stat.cmu.edu/~hseltman/726/OverviewOfBayesianStatistics.pdf" target="new" internal="false">Overview of Bayesian Statistics</link></p></td></tr></table><p id="ee117cb137b74833b23f82019e53a00e"><em style="italic">Bayesian Inference </em>is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used. </p><p id="f2617b9480b24b079c440815572fd50d"><em>Naive Bayes (NB) Method</em></p><p id="f3cc14cd799442b4bc8901f5498e1130">Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that <link href="https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf" target="new" internal="false">studies</link> have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.</p><p id="e80b26db58b64153bf351979ce638270">NB  is not considered the go-to algorithm for estimating the probability of an observation&apos;s class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let&apos;s put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.</p><p id="f947d638feef433688398f1096aba6e6">There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.</p></body></workbook_page>\n'