b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="a3fbdc66fe9747eeb172d065384945d9"><head><title>t-SNE</title></head><body><p id="d2049d46d5424d25a79236b44b7eb60b">In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points \xe2\x80\x9cfarther\xe2\x80\x9d apart. Let\xe2\x80\x99s consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:</p><image id="edb437fcc88b430794f9b31a771ea9bd" src="../webcontent/image-b828391b93d94786a2f859460d133290-1.png" alt="" style="inline" vertical-align="middle" height="258" width="431"><caption><p id="d790ecd18fed48978c828a16b7488442">Source: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/</p></caption><popout enable="false"></popout></image><p id="e901fecdde6e4486a0c5cbb36e4d0b75">As we can see, while some groups are visible, most of the digits are clumped together, making them really hard to distinguish. Given that MNIST has 10 distinct classes, what we would like is an algorithm that lets us differentiate between local distances and global distances a little better to make classes more visible.</p><p id="a92db7ee26914a8f97aff514cb677e97">Enter <em>t-distributed stochastic neighbor embedding</em> or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.</p><p id="f504cbc6056548b8b34837ed780f5a78">How t-SNE works is that it tries to model the distribution of points in the higher-dimensional space as a set of Gaussian distributions, where we define the probability of some data point <em style="italic">i</em> picking another point <em style="italic">j</em> to be neighbors through the following equation:</p><p id="b8023b37b5ef4807b1da2495e9cb566d">\n\\[ p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_k\\right\\|^2 / 2 \\sigma_i^2\\right)} \\]</p><p id="f732a2766be1484c96d7c3f119cbefcb">(If you\xe2\x80\x99re familiar with deep learning, you might notice the similarities between this method and the softmax function. It\xe2\x80\x99s also important to note that the distance function here is more of a suggestion, and you can use any custom distance function you want with most t-SNE implementations).</p><p id="c68033cbac8a41b3bdf86fc8a3f84650">After creating this high-dimensional model, we then optimize a low-dimensional embedding, with the lower-dimensional embedding using a t-distribution instead with the following similarity function:</p><p id="c1a4b7f2d1df4c989c0932289e6fb32e">\\[ q_{i j}=\\frac{\\left(1+\\left\\|\\mathbf{y}_i-\\mathbf{y}_j\\right\\|^2\\right)^{-1}}{\\sum_k \\sum_{l \\neq k}\\left(1+\\left\\|\\mathbf{y}_k-\\mathbf{y}_l\\right\\|^2\\right)^{-1}} \\]</p><p id="fe5225c7733142b48acb24d84f61c8c8">By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a \xe2\x80\x9cgood enough\xe2\x80\x9d representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.</p><p id="b41c5869feee41f698dc2d3600b29c9d">If we go to our MNIST example, using t-SNE gets us the following results, which make the clusters a lot easier to see:</p><image id="c8537eb9d1b84a75b938c70468cd11a6" src="../webcontent/image-b828391b93d94786a2f859460d133290-4.png" alt="" style="inline" vertical-align="middle" height="450" width="600"><caption><p id="b8030071f5974162b85dc48b8cd656c5" /></caption><popout enable="false"></popout></image><p id="dacad58a115a4ed1a56281b23a86f31b">Overall, t-SNE can be a far better visualization tool than PCA. However, it is <em>very </em>important to know the limitations of any new tool you consider for data science, especially data visualization. Given that visualizations can be misleading, it is important to treat a potentially better tool with skepticism.</p><p id="f535d791bac04050b0d67ab2dd51868d">In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the \xe2\x80\x98perplexity,\xe2\x80\x99 and this roughly is the number of neighbors you want to consider as \xe2\x80\x9cclose\xe2\x80\x9d to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:</p><image id="f6d59d6d60eb47a3a755d582befc28f7" src="../webcontent/image-b828391b93d94786a2f859460d133290-5.png" alt="" style="inline" vertical-align="middle" height="129" width="600"><caption><p id="d8e34d15b535492f92720f358ffb1278" /></caption><popout enable="false"></popout></image><p id="a30cb920238846c88efbe6fa257644fc">Secondly, note that it is important to not really think too much about the distances between clusters and the relative size of any clusters reported. Given that t-SNE is stochastic, the distances between points tend to mean very little, if anything, and instead, it is important to look at the data in its entirety.</p><image id="c54f27bbc4d34422aadbd3e914b088ad" src="../webcontent/image-b828391b93d94786a2f859460d133290-6.png" alt="" style="inline" vertical-align="middle" height="125" width="600"><caption><p id="d5e52154e40949cfbfcbd0f0eb70e5f3" /></caption><popout enable="false"></popout></image><image id="db169d17060c43cabfe96ad717367af8" src="../webcontent/image-b828391b93d94786a2f859460d133290-7.png" alt="" style="inline" vertical-align="middle" height="125" width="600"><caption><p id="fe723a3b1f654baa9d020581b79f7b05" /></caption><popout enable="false"></popout></image><p id="b63b22ff292b4a01994a54f342b3162e">Thirdly, it is important to note that t-SNE is sensitive to data scaling. Applying a standard scalar or another scaling system is important to get interpretable results, as otherwise, certain features will be scaled in a very different manner than any other.</p><p id="d86c0363d43d4aaf9b6755f135cae387">Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like <link href="https://arxiv.org/abs/1802.03426" target="new" internal="false">UMAP</link> instead. </p><p id="bafa0fe992be43a7804304c80d251c85" /><p id="da85ec08318c4a42bbb1e84dbee19dcc"><em>References:</em></p><p id="de9a7af061de417f8cb1b2950f0aa6d3">Hinton, G. E., &amp; Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, &amp; K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from <link href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf" target="new" internal="false">https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf</link> </p><p id="d28d0f96286a402f88469e3178c7f068">Maaten, L.van der and Hinton, G. (2008) Visualizing data using T-Sne, Journal of Machine Learning Research. Available at: <link href="https://jmlr.org/papers/v9/vandermaaten08a.html" target="new" internal="false">https://jmlr.org/papers/v9/vandermaaten08a.html</link></p><p id="efcece9edb404db190173b1325e6b7be">van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15(93), 3221\xe2\x80\x933245. Retrieved from <link href="http://jmlr.org/papers/v15/vandermaaten14a.html" target="new" internal="false">http://jmlr.org/papers/v15/vandermaaten14a.html</link> </p></body></workbook_page>\n'