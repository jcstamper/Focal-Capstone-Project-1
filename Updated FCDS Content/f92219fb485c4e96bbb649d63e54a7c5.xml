b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f92219fb485c4e96bbb649d63e54a7c5"><head><title>Active Learning </title></head><body><p id="d0c23901f37f4a93b050be6a4be1f2ed">This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. <em style="italic">Active Learning</em> pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.</p><p id="cb850b42e41d4d03849466d5fa57304c">Active learning presents <em style="italic">Scenarios</em> that allow a learner to query the labels of observations in a dataset.</p><p id="ba90c699717f49d897f6059bf3c56bc9"><em style="italic">Membership Query Synthesis</em> is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).</p><p id="dff0adcdc347455390179d723fe5c6f3"><em style="italic">Stream-based Selective Sampling</em> scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. <em style="italic">Pool Based Sampling</em>, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The <em style="italic">informativeness measure</em> is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.</p><table id="ad3fc035bba74db78824f1aedab148d1" summary="" rowstyle="plain"><cite id="i9d6b1ab0cb444f069962570b0e4c0552" /><caption /><tr><td colspan="1" rowspan="1" align="left"><p id="cada1aeddeaa4eb89b89fb8db5aa284c"><em>Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm&apos;s abilities (prediction and otherwise).</em></p></td></tr></table><image id="b9a9409fad9b46d293b270cb970f789a" src="../webcontent/Active_Learning.jpg" alt="" style="inline" vertical-align="middle" height="283" width="500"><caption><p id="d1a2c4cf6ab2476db12e6622b79d7d23"><em style="italic">Pool Based Active Learning Cycle-Source: Settles Active Learning Survey</em><em style="italic"><sup>1</sup></em></p></caption><popout enable="false"></popout></image><p id="a35510417f9b4c4bbc78f49e7450c837"><em>Query Strategies</em></p><p id="d7140b10ab8a4d7cabda3d7fffa7488c">How does the algorithm decide on the most informative measures? Let&apos;s highlight some of the strategies used to evaluate the informativeness of unlabeled data.</p><p id="e4311a9b16384d57b6e10355645bdb14"><em style="italic">Uncertainty Sampling</em> is an approach that allows the active learner to query the observations about which it is not able to label.</p><p id="baf0e51b268c4a3d976620dae346b0c8"><em style="italic">Query-by-committee</em> involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.</p><p id="ceeecf855a764670bcda3ef77a2d1290"><em style="italic">Expected Model Change</em> would use an approach that selects the observation that would introduce the most change to a current model if its label was known.</p><p id="f0bfed2ff9b445ef83bed163d19b9197"><em style="italic">Expected Error Change</em> involves labeling the data points that would reduce the model&apos;s out-of-sample error (a measure of how accurately your learner can make predictions on new data).</p><table id="e3d64a166c2a483b82447ae099bb582e" summary="" rowstyle="plain"><cite id="i7c311f10df1e4294a78e8a7dcd85a38d" /><caption><p id="e0aad8aa0d3d4d4d82fa57bb634c1d38" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="d3130897bc2640e8a65bd7663f77d063"><em>Additional Reading: </em><link href="http://burrsettles.com/pub/settles.activelearning.pdf" target="new" internal="false">Survey of Active Learning</link>. This report gives an in depth review of active learning in machine learning and artificial intelligence.</p></td></tr></table></body><bib:file><bib:entry id="c008003605554b9d920d51d8c73ec226"><bib:article><bib:author>Burr Settles</bib:author><bib:title>Active Learning Literature Survey</bib:title><bib:journal>University of Wisconsin Madison</bib:journal><bib:year>2009</bib:year></bib:article></bib:entry></bib:file></workbook_page>\n'