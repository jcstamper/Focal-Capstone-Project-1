b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bb16329a3c9145988127e4388ebc07b4"><head><title>Computer Vision Architectures</title></head><body><section id="d7dbb49d0c504303b3514ce037332b19"><title>Computer Vision Architectures</title><body><p id="bbe8da36a9014ac5afe4eabaa577d484"> </p></body></section><section id="d22c2e5f51cb44bea9a0867785a3313d"><title>Types of Layers In a Convolutional Neural Network</title><body><p id="f2087a4af46f4009976064b77d293e13"> </p></body></section><p id="b280134d7b5e456e87810b054f29930a">The CNN architecture incorporates a number of layer types as follows:</p><p id="cabb3904e637449aac10d13ce2c6d09f" /><ol id="f7d2d4f6ddc4402a95fb98aa234eaec0"><li><p id="d328afbf079a4a91abad41ed3274a354">The <em>input layer </em>accepts a 3D matrix of size W1\xc3\x97H1\xc3\x97D1</p></li><li><p id="a45ffcb3fac747b2904d21f60ac6ffa4">The <em>convolutional layer</em> accepts a 3D matrix of size W1\xc3\x97H1\xc3\x97D1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2\xc3\x97H2\xc3\x97D2 where</p></li></ol><p id="b688f107a87c435d8d4ea40b28e83ef5" /><p id="c70c7cc202474747af40b32843dd496b">W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K</p><p id="f54af12ad5b4409da9111c6186589927" /><ol id="fb27751ea38e4d239a6ab9eca6fe5c2e"><li><p id="f936be6e8eea4315bb4100ff53a0cb18">The <em>pooling layer</em> accepts a 3D matrix of size W1\xc3\x97H1\xc3\x97D1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2\xc3\x97H2\xc3\x97D2 where</p></li></ol><p id="c4182839f81f4105bc8dcf7f0b25e229" /><p id="cfbfc6177ab34045991eedbc5e3f1e4a">W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K</p><p id="c79d5e92660446c182511006b2687dab" /><ol id="c9a132bc8db5474098dd5a3dbf5ec65d"><li><p id="dd06f5df263c4afa9ff482bcf0328e25">The <em>fully connected layer</em> is identical to a fully connected network layer. It accepts a k-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).</p></li></ol><section id="e56360f24cf34bdbb4be37d858fbfa02"><title>Classic Architectures</title><body><p id="c7a1eb0841db4b4e9421c7561da50b9b"> </p></body></section><ol id="afe5b536a6f34da482354e44a0a828c1"><li><p id="cbe6bd67bb384a4490fe0736c9933cf2"><em>LeNet</em></p></li></ol><p id="ac2ca379dddd4abeb1d79a087eb49c2a" /><p id="dd5d919559744ed888f4df18e19ac3c9"><link href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="new" internal="false">LeNet</link> is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.</p><p id="f0c69a011f74451cbd4e93832b1ec646" /><image id="e65407dc5169425b9202cba573b1479f" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-1.png" alt="" style="inline" vertical-align="middle" height="140" width="468"><caption><p id="cb4f2eb5e63e497e99faad7b13b025d6" /></caption><popout enable="false"></popout></image><p id="bf8acdfba38a4dd0850fdbada93b49b7">Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.</p><p id="f4d6d6aa9a0b47d49858a75a7d2d9fd9" /><ol id="d2eeab36426a4f4b9bc6f2a427ce6013"><li><p id="ffb32d7a273341c882097269690ef093"><em>AlexNet</em></p></li></ol><p id="fb70c33260234b29ba2dbec7cd956a4b" /><image id="a3646f47c3fc46ca844a2d91f4580879" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-2.png" alt="" style="inline" vertical-align="middle" height="144" width="468"><caption><p id="fabe33fa763f46a5bad37c4f7dfb6cd4" /></caption><popout enable="false"></popout></image><p id="bf3a6af95fdb49dfa834e84a74d16b70">Figure 10. Data flow of AlexNet</p><p id="ba22b08f69ae4d0893d1664052ebe654" /><p id="a522a6148a9440f9abdc69f547e9b9e4"><link href="https://en.wikipedia.org/wiki/AlexNet" target="new" internal="false">AlexNet</link> was the first successful application of CNNs to the <link href="https://en.wikipedia.org/wiki/ImageNet" target="new" internal="false">ImageNet</link> dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state of the art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of AlexNet convolutional filter.</p><p id="d6e436a4d7f44836bcc06f35f286e57b" /><p id="b635ae7cab4245d3af7507403757effb" /><image id="f0d850c3002140188109748bc71d7a83" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-3.png" alt="" style="inline" vertical-align="middle" height="224" width="333"><caption><p id="cfd8e7e6a7cb4ae8a817ad12fd5825c7" /></caption><popout enable="false"></popout></image><p id="a4cf2f67a06b4822adb90be62dd5476f" /><p id="dec7bbd42714403eb7a2ddc5118a0c77">Figure 11. Image filters learned by the first layer of AlexNet.</p><p id="f05a7bc01fa946e9bd1160af4068dd2e" /><ol id="ef955641566244f2ba38cf4962353afd"><li><p id="e207de3b179e45ac8eb6aa52a1d3d979"><em>VGGNet</em></p></li></ol><p id="dc57ce6c32374d46a60d1a2841b6ac9f" /><p id="a152f56cbce34a0580404cf91e51be4d">Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called <link href="https://arxiv.org/pdf/1409.1556.pdf" target="new" internal="false">VGGNet</link> from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.</p><p id="b3142cfc88334c8a91ff12df1e977a26" /><image id="caec96866f9c43c9956bd7bfc3ccad3c" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-4.png" alt="" style="inline" vertical-align="middle" height="403" width="468"><caption><p id="ca0688d869644dfca6a84ab3bab2f30c" /></caption><popout enable="false"></popout></image><p id="ced4d40696a7433390eeb76e1c71cd64" /><p id="a2fdd978a9274ac196375995f492fc89">Figure 12. Comparison of AlexNet and VGGNet.</p><p id="e69ebf3949b84dbe9c326df557c9781a" /><p id="a829e208bdef428d9437770187e0652d">VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).</p><p id="c7692e83c1f34e648bf03ae6ebf92d8c" /><ol id="db688f82ae04419689ae465399972a7c"><li><p id="f5c5682fb11846e685fa405c54f3e69a"><em>ResNet</em></p></li></ol><p id="d13158ca968b4585bc8fd5df9ec11c47" /><p id="b131d64f8760420ebb21cf9a19c5b18f"><link href="https://arxiv.org/pdf/1512.03385.pdf" target="new" internal="false">He et al. (2015)</link> introduced the Residual Network or ResNet as &quot;shortcut connections&quot; that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.</p><image id="dc65213c9cd648159f3b72fcd3a8c78b" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-5.png" alt="" style="inline" vertical-align="middle" height="261" width="468"><caption><p id="cf8b408b998d4969bb89e77d62954514" /></caption><popout enable="false"></popout></image><p id="d9347322b9f946b19c6d0c15a397cf12">Figure 13. Comparison of Normal CNN layer and Residual layer.</p><p id="e8643f645bb342108f717200416b6001" /><p id="f7fd995c511a4603b295c6fbfc3e9903">Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.</p><p id="f899c6f8d7fa4e069c4e7ef87ca203bd" /><image id="c4563e52f51342b282d042bce5f0e868" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-6.png" alt="" style="inline" vertical-align="middle" height="674" width="249"><caption><p id="e0b6548070024dd38b60d599af4adfe9" /></caption><popout enable="false"></popout></image><p id="f6e77a66b24b45979b10596e8963c227">Figure 14. ResNet-18 architecture.</p><p id="a3058a52fc654ce1800b9aa580704f8b" /><ol id="af6741266c9c4d6b880f00a9b15a5481"><li><p id="fbd6d8f610244fb2bd9f657c5e3e076f"><em>The current state of CNN research</em></p></li></ol><p id="f39fcfee66604869946b71b862aa7d82" /><p id="ce17ed89d5244509ba7bf3de33a400e7">Now that we have seen several milestone architectures for CNNs, let&apos;s now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a <link href="https://arxiv.org/pdf/1605.07678.pdf" target="new" internal="false">2017 paper by Canziani et al.</link>, shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.</p><p id="f275e1157f9c4f848fe2aee7ec1c7e62" /><image id="f2e7ab380f664d3c98b52f2c90681b3f" src="../webcontent/image-bb16329a3c9145988127e4388ebc07b4-7.png" alt="" style="inline" vertical-align="middle" height="168" width="468"><caption><p id="d899b7b20af4490eb9e40405938b660c" /></caption><popout enable="false"></popout></image><p id="c689df47121e45e2bedb7b8da1a4070c">Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017)</p></body></workbook_page>\n'