b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bedf187269ec44a5a8575853a517c929"><head><title>Deep Learning Activation Functions</title></head><body><section id="c5f26e7de70145e393b7e50e461f8870"><title> </title><body><p id="a3158fed9da74fa6aa46a51cf6caca61">The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as \xe2\x80\x9cactivated\xe2\x80\x9d or \xe2\x80\x9cinhibited,\xe2\x80\x9d thus affecting the transformations in the subsequent layers.</p><p id="eb52e7961b2e45e3aa2de15365e9706d">Following are some common activation functions:</p><section id="a3514640a41c499ba6283be9341cc515"><title>Sigmoid</title><body><p id="d8a6157d082e44ed9b6ce683de87cafa">The sigmoid function transforms an input to a value between 0 and 1. Assuming the \\(s\\) represents the weighted sum of the inputs, the logistic function, which is an example of a sigmoid function, computes</p><p id="ad2cc27ac8414f35a38f047faece8258">\\[\\frac{1}{1+e^{-s}}\\]</p><p id="d465386a1fc5471bb44b920348556acf">It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.</p></body></section><section id="b23c216a4ba744cfb4c708477f80e676"><title>ReLU</title><body><p id="c9fcf17b79f640068c2c6759b6ba0f15">ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.</p></body></section><section id="c47a79e6fc4e4502b0d2d94ccf948814"><title>LeakyReLU</title><body><p id="d2e5d6bf956f466889c944de3f3ce076">Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.</p></body></section><section id="d6d7b3de51eb431abf2195c6a7c00aa2"><title>Tanh</title><body><p id="b91d72db0a9149d4bf36bb1c79e5a444">Tanh, the hyperbolic tangent function is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the function is closer to having a zero mean therefore, data is more centered, which makes the learning for the next layer easier and faster. The tanh function shares the same disadvantage as that of the sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.</p></body></section><section id="e579ea74da6d49628907b682e70e6da4"><title>Softmax</title><body><p id="eaac6ff4c0cf4d97bae145473fb3ffb7">The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.</p></body></section></body></section></body></workbook_page>\n'