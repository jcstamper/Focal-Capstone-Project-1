b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="e460b784b07741489941626e769375a7"><head><title>Introduction to Deep Learning</title></head><body><section id="c6a458c27d904aa8b8cc952bc66d7168"><title>Introduction</title><body><p id="c7c8252b55a345d7a86f4ea8296fae42"> </p></body></section><p id="a726ab017b1748fa98503c6a4c501e17">Deep Learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep Learning is a subset of Machine Learning that draws inspiration from how the human brain functions. Like most Machine Learning algorithms, Deep Learning also involves transformation of data from an input to an output. For example, using speech samples as inputs to predict the speaker, or taking some text in one language and translating text in another language. As the problem becomes more and more complex, classical Machine Learning algorithms fail to adequately learn this transformation or mapping of input to output from the data. Deep Learning algorithms overcome this using many successive transformations of input-data, thus the <em style="italic">deep </em>in Deep Learning.</p><p id="f835231a817f4ee0bc4dc43f8fdd2ec1" /><p id="b547f400bed4437987917a353d10b4ac">Neurons in the brain exchange information, such as visual signals detected by the retina, which is weighted by synaptic weights to determine the effect of the input. The weighted inputs can be thought of as being aggregated or summed in a neuron, then being passed through some nonlinear processing, and then finally being passed onto the next neuron in the network. Inspired from this, Neural Networks consist of neurons that are connected to each other via <em style="italic">weights</em>. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the <em style="italic">activation function, </em>and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The <em style="italic">deep</em> nature of deep neural networks refers to having multiple layers of such connected neurons. </p><p id="cd4bdd17a49a4205bea77423e94658a6" /><p id="a3c4bb0e5a0c4648ba08c335f40ef197">This module, we discuss some of the key concepts in Deep Learning.</p></body></workbook_page>\n'