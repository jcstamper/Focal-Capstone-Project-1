b'<?xml version="1.0" encoding="UTF-8"?>\r\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bd5ac3e3535640598fc045e480a49068"><head><title>Classification and Regression Metrics</title><objref idref="c0047158bffa477cacf9095f445caa08" /><objref idref="f884b110ee3f48bfaf5a73e27e20b647" /></head><body><p id="bda867944d154057aa084f0f64da3d9b"><em>Classification Metrics</em></p><p id="c56aa769f77a437395dfae1aa23e390c">The metrics used to evaluate the results of your task is of great importance. The performance of your algorithms need to be measured and compared to ensure that you select the right algorithm.</p><ul id="b4ba196cb448440da5f30661f65ab6b9"><li><p id="f49ec44ea5984fc38c597be908f715af">Confusion Matrix is quite easy to interpret and it is straightforward in its duties. This metric simply shows how well a classifier has performed, it is a simple visualization of the task&apos;s performance. You might also find that it is referred to as a <link href="https://en.wikipedia.org/wiki/Contingency_table" target="new" internal="false"><em style="italic">Contingency Table</em></link><em style="italic">. </em>Confusion Matrix can present the prediction results of a binary or multi class classification problem. As shown in the example below, the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category. Those categories are defined as follows:</p><p id="c546afa706954013b3c0639d10eca65e"><em style="italic">True Positives (TP) </em>is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values.</p><p id="bb08632edbc54f24bb640731e4eb6b5a"><em style="italic">True Negatives (TN) </em>indicates that the classifier correctly predicted no-event values. An observation in the negative class is correctly classified as being in said class. </p><p id="b6fd627511eb470197b77abe293c2da2"><em style="italic">False Positives (FP) </em>is an error in which a classifier improperly indicates classifies as observation in the positive class, when in reality it belongs to the negative class. This is considered a Type I error and is considered a false alarm. The conditional probability of a positive test result given an event that was not present is called the <em style="italic">False Positive Rate. </em></p><p id="fc485d15e57942f7bc07a13c580dd0d7"><em style="italic">False Negatives (FN) </em>is an error in which a classifier improperly classifies an observation in the negative class, when it belongs to the positive class. This is considered a Type II error. The conditional probability of a negative test result given an event that was present is called the <em style="italic">False Negative Rate. </em> This error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society. The premise of <em style="italic">Blackstone&apos;s formulation</em> supports the above claim. It states that &quot;it is better that 10 guilty persons escape than that one innocent person should suffer.&quot; It can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease, they could possibly die (or spread said disease) because of this type II error.</p><p id="bb926fd5b006469aaa7715dc7fd83507">Let us use the example of predicting the diagnosis classes for a dataset with 300 observations. The matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes. The table below shows a perfect classification exercise. It goes without saying that this can not be the case when real data is introduced to a trained classifier. The confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model.</p><table id="be16b44687e94ea78a031099387158f3" summary="" rowstyle="plain"><cite id="iaa67445ef801447b8d3ccf147429cd11" /><caption><p id="ab21796f33394fd7b865a676f924df35" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a880d807ae694f86a0038021291e3565"> </p></td><td colspan="1" rowspan="1" align="left"><p id="ad2fa3f1b9da444498c9d0b552b902cb">Diabetic</p></td><td colspan="1" rowspan="1" align="left"><p id="c43ab549d15f4c3e9eaff760ac74a86c">Not-Diabetic</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="a26812dd78924a1ea97403186c7943a3">Diabetic</p></td><td colspan="1" rowspan="1" align="left"><p id="e5da99d750734298aa4bdf0e5bd16198">110</p></td><td colspan="1" rowspan="1" align="left"><p id="d1cb666372064a6e8e5279f29cd187f0">0</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="c0fe6f3c7fe54e73871cdaf59510cb23">Not-Diabetic</p></td><td colspan="1" rowspan="1" align="left"><p id="c27ea1990dbf41ed81d19407bf679d26">0</p></td><td colspan="1" rowspan="1" align="left"><p id="efea5763b90a471da7c047360403728e">90</p></td></tr></table></li><li><p id="d472d45003fd41a38239d55d6025f7ee">Accuracy is simply a measure of how accurately the classifier performed with classifying data.  Accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier&apos;s performance. </p></li><li><p id="a4f591b725dc4fcb8a3d5d6137a23eb2">Recall sometimes referred to as <em style="italic">Sensitivity</em> is the True Positive rate and is calculated as:</p><p id="fef38e3e415d445780be1bdc816587f1"># of True Positives/ (# of True Positives + # of False Negatives). It is not holistic because it does not account for the False Positive and True Negative. </p><p id="d9fb693830dc49c9b38aa9f6af156d40">Recall = TP/(TP+FN)</p></li><li><p id="cbb55e2a7e454b298c7ba25925efcad7">Specificity is the True Negative rate and similar to the recall, it can result in biased results. It is calculated as # of True Negatives/(# of True Negatives + # of False Positives).</p><p id="f7cad9829eaf4aa99911d78c25ef039d">Specificity = TN/(TN + FP)</p></li><li><p id="be134526443e483d9b118881bcebfaa8">Precision is the positive predictive value and calculated as:</p><p id="ed792410f02241469833a2c99ff64542"># of True Positives/ (# of True Positives/# of False Positives).</p><p id="a7a47cbc39dd4ff19a61ffda7c25dfa1">Precision = TP/ (TP + FP)</p><p id="ae26c7e520164e98bcb02cec3047b292"><em>F1 Score: </em>All the metrics above were highlighted to introduce biased results because they do not account for all four rates. The F1 score will is calculated as:</p><p id="d742fe5eaa6646af89e71039fa5947f6">2 * (Precision * Recall) / (Precision + Recall).</p><p id="e91da5e6ac214509a0f6a71b0ed4ba75"><em>Matthews Correlation Coefficient </em>will solve for the issue of biased results. It is often used to assess the quality of a binary classification model. It is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events, and finally -1 means the classifier misclassified observations. MCC is considers symmetric meaning that no class is more important than another (switch Negatives and Positives and the result will remain the same).</p><p id="cd3f25d321dc4d08be27ea3ddda87e3d"><em>Resource: </em><link href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html" target="new" internal="false">MCC-scikitlearn</link>.</p><image id="dee5413cd9c54b73b7b714f02ee55eee" src="../webcontent/MCC.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="c40bff12b3ad4d1286f3278453838399"><em style="italic">MCC Formula</em></p></caption><popout enable="false"></popout></image></li><li><p id="a198b478fe9a4c9ebf35cf329b9b59e7">Logistic Loss (Log loss) is a metric that evaluates the predictions of probabilities of an observation&apos;s membership to a specific class. The prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value. Log loss will &quot;take into account the uncertainty of a prediction based on how much it varies from the actual label, while accuracy is the count of predictions where the predicted value equals the actual value. &quot; </p><p id="efcc38b6262a4add88933a62eaa95e3a"><em>Resource: </em>The <link href="http://wiki.fast.ai/index.php/Log_Loss#:~:text=Logarithmic%20loss%20(related%20to%20cross,a%20log%20loss%20of%200." target="new" internal="false">math</link> behind Log Loss.</p></li><li><p id="da9340d21cab4f058d8d8bd98a8e13f0"><link href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="new" internal="false">Receiver Operating Characteristic (ROC) Curve</link> is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds. It is the comparison of two operating characteristics. The ROC curve has been useful for many years, according to research it was developed and used by engineers during the World War II. It was used for battlefield detection analysis and has been used in many fields. Its most prominent use in recent days is for machine learning and model performance assessment. </p><p id="b80bf3f8ac5c49fa884c0f6073338a55">Consider the ROC curve to be &quot;sensitivity as a function of false positive rate.&quot; It can be used to select the optimal models. </p><p id="abe52cc3aa8c4ef6927c2bc383e7fc13"><em style="italic">Area Under the ROC Curve otherwise known as AUC</em> measures the entire area underneath the ROC curve and it is the measure of the classifier&apos;s ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios such as it does not tell you much about the &quot;cost of different errors&quot;. It gives similar weight to errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes e.g. the model has predicted observations that are apples as apples and observations that are not apples as not apples. When the AUC is close to 1, it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability. </p><p id="a3295295761f46e5a3e68d806f5fe7fd">So far, we have talked about the AUC ROC for binary classification, it can be used in a multi class model as well. Note that, there will be multiple AUC&apos;s plotted for each class. </p><p id="b0e99d7fe9a045c7a024c591f44c7d40"><link href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#:~:text=Area%20under%20ROC%20for%20the%20multiclass%20problem,-The%20sklearn.&amp;text=roc_auc_score%20function%20can%20be%20used,the%20OvR%20and%20OvO%20schemes." target="new" internal="false">AUC-ROC for MultiClass </link></p><p id="cda11a76aa48430e864bcebe76242b2d"><link href="http://people.cs.bris.ac.uk/~flach/ICML04tutorial/ROCtutorialPartIII.pdf" target="new" internal="false">ICML Presentation on MultiClass ROC Analysis</link></p><p id="d8523ba6adb3456f8351f4b33f419be0" /></li></ul><image id="d93c110340064f6fb4a4ba3d7f251277" src="../webcontent/AUC.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="f23790bbd56d478cbda3dc39d2247ca9"><em style="italic">ROC-AUC Source</em></p></caption><popout enable="false"></popout></image><p id="cea82e91a3124ef1a4b2cf90d35976c0"><em>Regression Metrics</em></p><p id="dc8f4458418a47a09c2669c916aaaa5c">Regression tasks will predict the state of a target variable based on other correlated input variables. As a quick reminder, target variables in these tasks are continuous values. Let us discuss the metrics that are used to evaluate the outcome of regression tasks:</p><ul id="e534fdd7d7be417ebf740620183e49cd"><li><p id="c6c364c8c483472a854b0949262d8332"><em style="italic">R-squared also known as the Coefficient of Determination </em>is the proportion of the <link href="https://en.wikipedia.org/wiki/Explained_variation" target="new" internal="false">variance</link> in the outcome variable that can be predicted using the predictor variables. It tells you how well &quot;observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model.&quot; When interpreting r-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r<sup>2</sup>). If R<sup>2 </sup>is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R<sup>2</sup>. </p><p id="eb83e90d95e14f03b039a42f22555fb7">When there are multiple regressors, then R<sup>2</sup> is the square of the coefficient of multiple correlation (&quot;<em style="italic">correlation between the variable&apos;s values and the best predictions that can be computed linearly from the predictive variables&quot;)</em></p><p id="e466034b7a76425492457b9cd00b12ef">This metric will provide an indication of how well new data will be predicted by the model. </p></li><li><p id="e32bca5875424561a145e7bc3f23a75f">R<sup>2</sup> does not come without some issues, as a metric it cannot determine whether the coefficient estimates and predictions might be biased. The R<sup>2 </sup>will typically increase when a predictor is added to a model and as you add more predictors, your model will likely overfit and result in a high R<sup>2 </sup>, <em style="italic">Adjusted R-squared </em>will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R<sup>2 </sup>will calculate R<sup>2</sup> from the predictors that have a significant impact on the model. Adjusted R<sup>2 </sup>is best used to compare models with different number of predictors. </p></li><li><p id="be308d0889084ee49ee27c6ed7b77868"><em style="italic">Mean Squared Error (MSE) </em>is measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: <em style="italic">the residual sum of squares divided by the number of degrees of freedom</em>. </p></li><li><p id="c830ed2f80274264b160f7965df330d7">Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the <em style="italic">Mean Absolute Percentage Error</em> (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error. </p></li><li><p id="d794ca173a9a4c9d901b36b8a75be565">Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: &quot;square root of (1-r<sup>2</sup>) multiplied by the standard deviation of <em style="italic">y</em>. </p></li></ul><p id="bd06a6ae86d541e2aa151ddd7a1545a0"><em>Resource: </em><link href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" target="new" internal="false">Regression Metrics-scikit-learn</link></p></body></workbook_page>\r\n'