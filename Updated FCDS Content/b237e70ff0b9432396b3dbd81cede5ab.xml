b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="b237e70ff0b9432396b3dbd81cede5ab"><head><title>Deep Learning Objective / Loss Functions</title></head><body><section id="a48498b7ac594d9eb46a6ce8b595e109"><title>Objective / Loss Functions</title><body><p id="e752242cd27541d182e59b30fc2fdfb8">An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called <em style="italic">loss functions</em>. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.</p><p id="aa566cac4862497eaeec451c4fb737ab">Following are some of the commonly used loss functions based on the problem at hand:</p><p id="d80c5313c6af4693b3ac61e3200ae5ae"><em>Regression Problem</em>: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function. </p><p id="fed3a71ada6146cfab469f3eafbd9faa"><em>Binary Classification Problem</em>: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.</p><p id="c9bf4271aad2409cb75595b6075781ab"><em>Multi-Class Classification Problem</em>: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.</p></body></section><p id="fc020cac67b4415a9440d6ef726469d2">For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as \xe2\x80\x9cmaking a step\xe2\x80\x9d and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.</p><p id="a0b8d4cad4f44dfda34faf9aedf39c71">Following are some common optimization algorithms along with their advantages and disadvantages:</p><section id="fb3c3b0a25d34c5580988f78b10837ae"><title>Gradient Descent</title><body><p id="cdcdb1f2f52b4298bb85a20233bee787">Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the model\xe2\x80\x99s parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.</p><p id="e262909d6ff44e63bf97f682cf0ec5dc">\\[ w:=w-\\eta \\nabla Q_{i}(w) \\]</p><p id="ecdc1335972340b6aa4894c709daaa96"><em>Advantages</em></p><ul id="b9340f7abaad474aa5a5969906d0995b"><li><p id="e0f31ac3e29d4a4797c3c1c21644e525">Easy to compute</p></li><li><p id="dd9a847b445248c48cc73f8e06f832b3">Easy to implement</p></li></ul><p id="fd30444033be4f978e5eb35da1aa4327"><em>Disadvantages</em></p><ul id="aa8da3c850e743d4ba542e408d752e6c"><li><p id="d4f0f4027790400fada9d1a8ca2524dd">Susceptible to getting stuck in a local minima</p></li><li><p id="a7ef9089ec4347278f6cffb77854931a">Convergence is slow as updates are calculated after calculating the gradient for the entire dataset</p></li><li><p id="b2ea13c8b14e412d9e8f1df2c5d7c59a">Computation for entire dataset requires a large memory</p></li></ul></body></section><section id="bc67429e7c574f289f955f5663721cac"><title>Stochastic Gradient Descent (SGD)</title><body><p id="b0a2b310c469468e8d071655296665a1">Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title <em style="italic">stochastic</em>.</p><p id="f03b9d3d8a2f4a809c84ec3145c0a3a2">\\[ w:=w-\\eta \\nabla Q_{i}(w) \\]</p><p id="c595cb8e6ef1417aa7e9adc3bf5199c1"><em>Advantages</em></p><ul id="e8eab0787efb449aadaca9c0c624da1b"><li><p id="dfbab9373fb149d3bc56699169550aa7">Converges in lesser time because of frequent updates</p></li><li><p id="ec7ca06ae78e4039ade324a496262bf8">Lesser memory requirements for calculating updates</p></li><li><p id="dd48303456024087b6b1ab026bdcda9a">Less likely than Gradient Descent to get stuck in a local minima</p></li></ul><p id="fbc6b13195164e4e8fc4b48391190e28"><em>Disadvantages</em></p><ul id="dd56e43a43ec4ca9a9531ede2d1d501f"><li><p id="b3220a264c4e494fa89e53c3d48c4ef6">High variance in parameter updates due to high frequency in updates</p></li><li><p id="f249296f6e844033b772a4613172dfa8">Learning rate needs to be correctly chosen and adjusted for effective training</p></li></ul></body></section><section id="fe0f79408fde43a68b5210f2f0407032"><title>Mini-Batch Gradient Descent</title><body><p id="c73d58eff3684e43bbdc386001963343">To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given <em style="italic">batch</em>. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.</p><p id="ba07867d177a4951a4872100d813902f"><em>Advantages</em></p><ul id="b970a4fc35924ffabd002ede2ad3f2c5"><li><p id="f9e829e94b034b7993fec173784467b3">Frequent updates and lesser variance as compared to SGD</p></li><li><p id="d5f383d277094036b9b16d732dda2575">Moderate amount of memory requirements</p></li></ul><p id="c69431887bab48e984145e4deeaa8696"><em>Disadvantages</em></p><ul id="c88c41b7cf82460996a103cf7d02d4f5"><li><p id="d8b1706f16004f9cb7ee0265456f387b">Learning rate needs to be correctly chosen and adjusted for effective training</p></li><li><p id="bb5560e6dfdb4993a28dda36299e47c9">Learning rate is constant for all parameters which might not be desirable</p></li><li><p id="ed9a2df4846f48cda693f64cfd36524d">Susceptible to getting trapped in a local minima</p></li></ul></body></section><section id="a200c152bb964e658deb563165594949"><title>SGD with Momentum</title><body><p id="da29db619b784a9a89447bf8bf0e9f59">The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.</p><p id="d62ed68fcb1a4392ab6339c3b254d573"><em>Advantages</em></p><ul id="eb55b282f37a45a9ba2de602edef049c"><li><p id="fa9cf0aef5dc4598a7f1900857040002">Reduces the high variance in model updates by SGD</p></li><li><p id="cd8dd39cfa4c459e8d319a96723a9bc6">Faster convergence than Gradient Descent</p></li></ul><p id="d3165e1c96ef40dfbe8d19ff4cf162fe"><em>Disadvantages</em></p><ul id="b09571a9c2b148179246a7402b212bd2"><li><p id="d0315f6be6544518b39104b5cb06e2a5">The momentum hyperparameter needs to be additionally tuned</p></li><li><p id="af313dbed5d34fbdad2dee813fc2c0c5">Learning rate needs to be correctly chosen and adjusted for effective training</p></li></ul></body></section><section id="f708d56369534831966268aab3b3fcc3"><title>AdaGrad (Adaptive Gradient Descent)</title><body><p id="c3f0f43c99b54b6a93051e3e42308100">The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.</p><p id="c404dbcfcbea4016a048434f24aea2f1">\\[ \\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\eta_{\\mathrm{t}}^{\\prime} \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}(\\mathrm{t}-1)} \\]</p><p id="ef815133695c435798ef5975c567fc55">\\[ \\eta_{\\mathrm{t}}^{\\prime}=\\frac{\\eta}{\\operatorname{sqrt}\\left(\\alpha_{\\mathrm{t}}+\\epsilon\\right)} \\]</p><p id="d5afa9a94ffa4b74adeab19e5a028cc6"><em>Advantages</em></p><ul id="c72863c06f184e5da03e209a2d1a7076"><li><p id="f6c43cf77076476982b340455e563846">Reduces the need to manually modify learning rate</p></li><li><p id="b79105b1a80d4622ab20fc2c786895dc">Tends to have faster convergence than Gradient Descent and SGD</p></li></ul><p id="e6cc5058ded945a0a4a620fec83b54d9"><em>Disadvantages</em></p><ul id="e304a96e1f12429aa6245878e0df6d20"><li><p id="c38d006ea0984a5bb97c6429b618dc92">The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate</p></li></ul></body></section><section id="ae7f8ff8cf094b5b85f94d10c69d03a6"><title>RMS (Root Mean Square) Prop</title><body><p id="a53dc721e183428f821f494804c8d41f">RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.</p><p id="c70976eecc5746f0bb349846c68ecca1">\\[ v(w, t):=\\gamma v(w, t-1)+(1-\\gamma)\\left(\\nabla Q_{i}(w)\\right)^{2} \\]</p><p id="c80df2a17f51449ab5db51722405e4c0">\\[ w:=w-\\frac{\\eta}{\\sqrt{v(w, t)}} \\nabla Q_{i}(w) \\]</p><p id="d192083306c44e3797e355197143f668"><em>Advantages</em></p><ul id="ac374c060e584768a35ade1a4068d39e"><li><p id="e987e075d23f4cfc9c97244b9a5acb0b">Requires lesser tuning than other optimization algorithms</p></li><li><p id="cfc38a0666414c078b4f93a21720314c">Faster convergence</p></li></ul><p id="ba88031549a346c1bdcce5e954cffde5"><em>Disadvantages</em></p><ul id="eb9ec11b3f794d13b142ec6f3fd9d109"><li><p id="ad3957d4773641bebac6163afb1dd057">The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks</p></li></ul></body></section><section id="a00179e5f4164f49a13870b6ff115530"><title>AdaDelta</title><body><p id="c5566bd02eb043f1bb793c4e29029ec6">AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size <em style="italic">w</em>. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.</p><p id="d34dca3b75504530a0de534cf96c5789">\\[ \\mathbf{s}_{t}=\\rho \\mathbf{s}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{2} \\]</p><p id="c8c3899c5515436498fc8c162f8dc422">\\[ {x}_{t} = {x}_{t-1} - {g}_{t}^{\\prime} \\]</p><p id="aee2d081aef441abbb57afdb122538df">\\[ \\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} \\]</p><p id="aa18cb4d8b0d4c449fe659ff4276132b">\\[ \\Delta \\mathbf{x}_{t}=\\rho \\Delta \\mathbf{x}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{\\prime 2} \\]</p><p id="e3f92077e91b4c818e43f63c2d036bb3"><em>Advantages</em></p><ul id="bbfdbd3a25084c218e69d56a10e10f83"><li><p id="afd935a7ddfb49fabd0a782a34ab3878">Elevates the learning rate decay problem in AdaGrad</p></li></ul><p id="b58df0d8ee654f42a3e59e7a4b0412e4"><em>Disadvantages</em></p><ul id="e87ea9deade94ade8ec90978f387301c"><li><p id="ede87d476f604ba898462fd6ed72fbc5">Computationally expensive</p></li></ul></body></section><section id="f23121c7df95497e9dda30c26e33fdc8"><title>Adam (Adaptive Moment Estimation)</title><body><p id="a538fe005d0347eb813b2897a673ed8c">Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.</p><p id="c59ff6b09fa24777a5f033a455139f14">\\[ m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right)\\left[\\frac{\\delta L}{\\delta w_{t}}\\right] v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right)\\left[\\frac{\\delta L}{\\delta w_{t}}\\right]^{2} \\]</p><p id="deea54153b7947368cbb74c7187e90a4"><em>Advantages</em></p><ul id="eca942224c3140629a21c030b6253bd9"><li><p id="e52d60bbb83f4c91ab5e5c4b943ef5c8">Rapid convergence</p></li><li><p id="eb9d8aa487a04cbf99961deeb75e35df">Rectifies vanishing learning rate and high variance</p></li></ul><p id="c2c9c7320b6747b8b60c402f46a562ef"><em>Disadvantages</em></p><ul id="f7274f9eb97b4da4a12ca1d9750354c8"><li><p id="f871883438094b63aa4b83250858f951">Computationally expensive</p></li></ul><p id="acc14b6777b24a1889e08bab01e4b13f">Equations Reference: <link href="https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/" target="new" internal="false">Link</link></p></body></section><section id="b2ac4d5e0d7b49a899332978f84a13b0"><title>Convergence Behavior of Various Optimization Algorithms</title><body><image id="c14d29d2cacc466d9a996eee52cc3ef7" src="../webcontent/1obtV.gif" alt="" style="inline" vertical-align="middle"><caption><p id="e383cf6ab2d04457a61fd73481a269f9">(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)</p><image id="fce11bcc17444f73af9f659d03eed570" src="../webcontent/qAx2i.gif" alt="" style="inline" vertical-align="middle"><caption><p id="d54cfd87b9994930892e45eb91901756">(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)</p></caption><popout enable="false"></popout></image></caption><popout enable="false"></popout></image></body></section><wb:inline idref="mooclet_activity" purpose="didigetthis" /></body></workbook_page>\n'