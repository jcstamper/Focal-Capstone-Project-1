b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d5e79d8d2a0643428754d3b9dc3ad975"><head><title>Model Interpretation Strategies</title><objref idref="c0047158bffa477cacf9095f445caa08" /></head><body><p id="d25a0a3862b94c9eba08c84f77a6071e">As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. <link href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf" target="new" internal="false">A great resource by Marco Ribeiro</link> explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle &quot;man&quot; between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.</p><p id="a05aa9163d7448bca411443fbc5f937a">Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was made\xe2\x80\x94understanding the why will make the problem clearer and affect problem-solving for future challenges.</p><p id="a80e61888d3a49e093091e9a6011ac46"><link href="https://arxiv.org/pdf/1702.08608v2.pdf" target="new" internal="false">Doshi Velez &amp; Kim (2017) </link>have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.</p><p id="f63b51d480f84231b7e22b964595b537">Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).</p><p id="e2897998cf774ab185deb454a696ffd0">The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.</p><table id="e1442cfd540244d285d1ac68a75f3016" summary="" rowstyle="plain"><cite id="i2e3f8e8aa61c4922816b3623b3763a90" /><caption><p id="d10a7e40e7f54578a68d88892febb94c" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="d4cf3467ff2a4eeaac282e00e6b0936d"><em>Reading: </em><link href="https://arxiv.org/pdf/1602.04938.pdf" target="new" internal="false">Should you trust that model?</link></p></td></tr></table><p id="c102f34d7028477784de0e06b154ae39">The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.</p><p id="e55b48751c1649af9c172bf1eb0b6e8f"><em style="italic"><em>Local Surrogate Models</em></em>&quot;explain individual predictions of black box models.&quot;</p><p id="e160d07a2c734b50a66a41c6da18e35c"><em style="italic"><em>Shapley Value</em></em> is concerned with explaining a prediction by assessing the importance of features to the task. </p><table id="debd7e03fbbe49a79671113e0c76fa45" summary="" rowstyle="plain"><cite id="ib5698509258d459e9cc6ef9a29a74af4" /><caption><p id="edfde27c6abf4c80a4a759b55fdcac5a" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="cecb06e8f4ab4a1ca8cc482382bf1b7e"><em>Additional Resource: </em><link href="https://drive.google.com/file/d/1xD5HY7HU5mBiKQ1nEQvRYqwBkNfHG3yM/view" target="new" internal="false">Sara Hooker: The Myth of the Perfect Model</link></p></td></tr></table><section id="fd35901112f54feebea72a2d3a6f8914"><title>Accuracy versus Interpretability</title><body><p id="cfa32bf058a24e63989d3fe3df22f990" /></body></section><p id="ab5abac09a6a4e4d954b2ae978ca8638">Throughout this course, you have learned about understanding your client&apos;s needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results. </p><p id="d0ea596aae67414980f4968d977b9f20">Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.</p><p id="a8f379977f934f7d8ba276a3373cb239">Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).</p><p id="a56d146539664363a6a86df417e28053">There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers\xe2\x80\x99 data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, &quot;how can we retain interpretability while improving accuracy?&quot;</p><p id="e2f2fc8cac6f47018339dafe32ae5c6c"><link href="https://www.oreilly.com/content/predictive-modeling-striking-a-balance-between-accuracy-and-interpretability/" target="new" internal="false">Hall (2016)</link> has recommended the following steps: </p><ol id="f6e360a1438346e4b6f06d31b9db9597"><li><p id="e2cff5ba953649d79528dcd0f5e0a26a">Train black-box models and use them as benchmarks.</p></li><li><p id="e2824689c68f4b89a941374fba4546c8">Use different regression techniques.</p></li><li><p id="e5059f9062f24d86baa13828f6e026d2">Use black-box models in the deployment process.</p></li><li><p id="d1c2bd8b6ced4eb59426ac3b220490c9">Train small interpretable ensemble models.</p></li><li><p id="a4b272f2f4284ce594a019faa2a1d8f7">Create nonlinear predictors using black-box techniques.</p></li><li><p id="c2fd05e4b9754af48245d7a96cf80259">Explain black box models better using variable importance measures.</p></li></ol><image id="e61da184f7194b509df45105100ec1e3" src="../webcontent/Accuracy_vs_Interpretability.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="f2f5adda3a5445d38264cd8b94ed96af">Figure 1. Accuracy versus Interpretability (Source: Rane-20181)</p></caption><popout enable="false"></popout></image></body></workbook_page>\n'