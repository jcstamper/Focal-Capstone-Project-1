b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c0d8e2d38a6c4f06844ae7dde6b38d69"><head><title>Introduction to Model Selection</title><objref idref="df2f73d563e74d48851925fc75d7da68" /><objref idref="d5d5bf4121234996b170fd1b3a348e4a" /></head><body><section id="de80f72c56f64b8f9dc25e7c8c99bf76"><title>Machine Learning Terminologies</title><body><p id="b7c14f5f4876430197ff0416f8d53419">Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:</p><ul id="c1252eacc3e24e15be3a9065634f6b80"><li><p id="fd490cc9794e403d8e2e1f18dccd68cd">A <em>model </em>is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search</p></li><li><p id="e7b436c962c043dfa8653e3942d29efa">The <em>model parameters</em> are the numeric values or structures that are derived from the learning process.</p></li><li><p id="bc4141ebd6224e4fa8cdfe317fa4ad84">The <em>model hyperparameters</em> are the numeric values or structures that impact the learning process but are not selected by the learning process.</p></li><li><p id="bad31dc67c1a4dd29c50bd9893419c0f">The <em>learning algorithm</em> specifies the way in which model parameters are updated or derived from the input data.</p></li></ul></body></section><p id="a7041db3137e4bcca03b3b1510851648">With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.</p><table id="ed8ac9a1a9d146f481de0a54ef8ba2e4" summary="" rowstyle="plain"><cite id="ic3fead4ff4f8461689a9d6d5d34413b0" /><caption><p id="cbd2cfdc7acf463abdde9cf2512a34bb" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a109ca6c5352487395387647dae0e5f9">Component</p></td><td colspan="1" rowspan="1" align="left"><p id="fd482e0fabbf41c39bc115ae59e8444c">Model 1</p></td><td colspan="1" rowspan="1" align="left"><p id="d4854ac6028d4453914c9103171a503d">Model 2</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="dbd7885496124cb9b189cda465b6a35a">Hyperparameters</p></td><td colspan="1" rowspan="1" align="left"><p id="c610b7c58f114d89a181a765e5a39f8d">Learning rate \\(\\alpha =0.1\\), regularizer \\(\\lambda =1\\), number of iterations \\(N = 1000\\)</p></td><td colspan="1" rowspan="1" align="left"><p id="c20b0b77b8f5456aab347fd23da35f7a">Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="b97b3eeca74c423c83ae07a7b8ca3849">Learning algorithm</p></td><td colspan="1" rowspan="1" align="left"><p id="dc97c9b173814d6b8e564c1e79c24b7f">Gradient descent over the logistic loss function with L2 regularization</p></td><td colspan="1" rowspan="1" align="left"><p id="ad4f2d1ae77741dab150704ad2c6e5c5">Gradient descent over the logistic loss function with L2 regularization</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="ecd37fbdbb17493c8924706860b55cf0">Pre-processing</p></td><td colspan="1" rowspan="1" align="left"><p id="d9ac2edda1634f10959d1e263efb93e6">None</p></td><td colspan="1" rowspan="1" align="left"><p id="d1803f783dec49c3a3e58910addb6a95">Normalize the data to have zero mean and unit variance</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="e7ed7479797d48b7a445d5b2fe555d6c">Post-processing</p></td><td colspan="1" rowspan="1" align="left"><p id="e031e286ff524265a19e9cb34105f4cc">None</p></td><td colspan="1" rowspan="1" align="left"><p id="ed078d5a0a3b439aba198911bc3347de">None</p></td></tr></table><p id="a1fcf6ee90954b99a949c6e01ce21990">Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1\xe2\x80\x99s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2\xe2\x80\x99s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what \xe2\x80\x9cbest\xe2\x80\x9d means.</p><section id="a60ffe7937e24514aad9f07d92dc76b8"><title>Prediction versus Inference</title><body><p id="ad440da8f14e427c867a14538ce67dcb" /></body></section><p id="c452466eefc241ddb46f5f87f8f5ed11">Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on <em>generalization</em>, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.</p><p id="bd020649731640d1aa1d8c433f592a03">Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on <em>interpretability</em>. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring people\xe2\x80\x99s political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.</p><p id="bfea061fe40f4eb284f151ab86dc225b">Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).</p><image id="f70667ed2e654276885cf2ee26809e88" src="../webcontent/IMG_8105.jpg" alt="" style="inline" vertical-align="middle" height="472" width="500"><caption><p id="c33a5db6a9d04e9aa149b97d3f2b1cf7" /></caption><popout enable="false"></popout></image><table id="fe88b09a97134acf999f84c1fa0422ec" summary="" rowstyle="plain"><cite id="i215ea85fa98b4c4aab6e49d9289f90d3" /><caption><p id="e41884269e604992b58d55d8c43df5d2" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="dd069f3492f54f159993b6f000130a7a">Optional Reading: <link href="https://www.nature.com/articles/s41586-021-03659-0" target="new" internal="false">Integrating explanation and prediction in computational social science</link></p></td></tr></table></body></workbook_page>\n'