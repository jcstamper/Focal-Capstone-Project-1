b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f471cfbbd151446d825497987a3eeeb1"><head><title>Deep Learning Model Architectures</title></head><body><p id="b525eb18c77c431cbdff69f3beaeda89">Following are some of the key deep learning architecture/algorithms:</p><section id="ce7490cc9ea34e4399bb5f6e62a9837e"><title>Artificial Neural Networks (ANN): </title><body><p id="dc9626315906495e88168e8a7e4c623f">Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a <em style="italic">multi-layer perceptron</em>.</p><image id="aa4f025e440e40eaa1112ffb5527535c" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-1.png" alt="" style="inline" vertical-align="middle" height="462" width="650"><caption><p id="a52d5329f15349568c0dff8f865cafb6">Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks</p></caption><popout enable="false"></popout></image></body></section><section id="ee6445c18d8b461d93cb76806e87a9c9"><title>Convolutional Neural Networks (CNN): </title><body><p id="deac620adb2540eeb9f81bf6dd91e633">A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation<cite id="ie2a31fa11d4f4404a36b148a728c19d0" entry="b95bb7b6e992473b867d1eebb60e51a1" />. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into <em style="italic">receptive fields</em> that feed into a <em style="italic">convolutional layer</em>, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.</p><image id="dee3ee12c414424bad563da4fe28fcdd" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-2.png" alt="" style="inline" vertical-align="middle" height="120" width="468"><caption><p id="f472a4167e814892a5f6de4d58ce84f4">Figure 2. LeNet CNN. Source: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures</p></caption><popout enable="false"></popout></image><p id="e34b4b8fd563451788a7cf6c661aee17">The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.</p></body></section><section id="dbc5b3b2646c46abadcd51c6a7d90f11"><title>Recurrent Neural Network (RNN): </title><body><p id="b328bb5abfe94105a99f993146dec6ea">The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).</p><p id="dc46052d450843d5a364b0f5d5b00e8c">RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been  developed and are described below:</p><p id="b75606b0421d456bb5bf939ceea0cea0"><em><em style="italic">Long Short-Term Memory (LSTM) Networks</em></em>: The LSTM was created in 1997 by Hochreiter and Schmidhuber<cite id="ic8e1aec8d0cc4cd38509e1ce582e6b91" entry="af69391a70734d1e982e110f87b83e8c" />, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what&apos;s important and not just its last computed value.</p><p id="ce9e3bbaa1c64613a525576d623d2fcc">The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.</p><p id="d59540479de349828faf10d6f70cce84"><em><em style="italic">Gated Recurrent Unit (GRU) Networks</em></em>: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.</p><p id="fb766bfd255b49a8888da545bd82c55f">The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.</p><image id="e6c4ec453fc34e4b924059a68dd9d050" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-3.png" alt="" style="inline" vertical-align="middle" height="196" width="650"><caption><p id="ae94ba1dcfef4d01baa7e06d796dff38">Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/</p></caption><popout enable="false"></popout></image></body></section></body><bib:file><bib:entry id="b95bb7b6e992473b867d1eebb60e51a1"><bib:article><bib:author>Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel</bib:author><bib:title>Backpropagation Applied to Handwritten Zip Code Recognition</bib:title><bib:journal>AT&amp;T Bell Laboratories</bib:journal><bib:year></bib:year></bib:article></bib:entry><bib:entry id="af69391a70734d1e982e110f87b83e8c"><bib:article><bib:author>S Hochreiter, J Schmidhuber</bib:author><bib:title>Long short-term memory</bib:title><bib:journal>Neural computation</bib:journal><bib:year>1997</bib:year><bib:volume>9 (8)</bib:volume><bib:pages>1735-1780</bib:pages></bib:article></bib:entry></bib:file></workbook_page>\n'