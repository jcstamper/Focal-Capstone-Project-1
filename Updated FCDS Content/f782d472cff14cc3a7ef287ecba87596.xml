b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f782d472cff14cc3a7ef287ecba87596"><head><title>BERT Training</title></head><body><section id="cc773acf88d84c18a4270a58d37461d2"><title>BERT Training</title><body><p id="eb60f5decb4847ebb554d85e252bbbef"> </p></body></section><p id="e244bf6ca26c44ccbbf6a296ebef685f">For any NLP task, BERT is generally trained in two steps:</p><ol id="b8b0375f3954406690c4895876d42628"><li><p id="b3e89f9fe6a84537ab2f28370aaa74fd">First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.</p></li><li><p id="ca7ed9fa9c7648ed9737fae621835175">Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.</p></li></ol><p id="c65e0cfefe83404fb0c82e0ff10cf4aa">The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.</p><section id="bb05092e122848eaa534b1db96ad4232"><title>Masked Language Modeling (MLM)</title><body><p id="ec931cd5c6e2459fba769a0708f82f7a"> </p></body></section><p id="d85b6a31d91e4b48a6ea09513edbf6e8">Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERT\xe2\x80\x99s prediction of the masked token.</p></body></workbook_page>\n'