{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_fcds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhWmgxGPqgkh"
      },
      "outputs": [],
      "source": [
        "# Packages to install\n",
        "!pip install textract\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip3 install tensorflow==1.15\n",
        "!pip3 install bert-serving-server\n",
        "!pip3 install bert-serving-client\n",
        "!pip install numpy==1.19.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textract\n",
        "import re, json, os\n",
        "from os.path import join\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.summarization import keywords\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import json\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "q7bFMuASrxgw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloads\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
        "\n",
        "!unzip -u \"/content/drive/MyDrive/uncased_L-12_H-768_A-12.zip\"\n",
        "!unzip -u \"/content/drive/MyDrive/Concept-Acquisition-Pipeline.zip\""
      ],
      "metadata": {
        "id": "Qhn4HGczsHVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurables"
      ],
      "metadata": {
        "id": "8-PE8od3vHmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEMESTER = \"f19\"\n",
        "data_path = \"/content/drive/MyDrive/oli/\""
      ],
      "metadata": {
        "id": "aphQEiGWvHAM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Extraction from OLI Functions"
      ],
      "metadata": {
        "id": "_8MKd0WrsmGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_content_from_pdf(filename):\n",
        "    # text variable which contains all the text derived from our PDF file\n",
        "    full_text = textract.process(filename, method='pdfminer', language='eng').decode('utf-8')\n",
        "    return full_text.encode('ascii','ignore').lower().decode('utf-8')\n",
        "\n",
        "def extract_keyword(full_text):\n",
        "    keyword_summaries = keywords(text = full_text, split = \"\\n\", scores = True)\n",
        "    data = pd.DataFrame(keyword_summaries, columns = [\"keyword\", \"score\"])\n",
        "    return data.sort_values(\"score\", ascending = False)\n",
        "\n",
        "def get_module_unit_from_org(page_id):\n",
        "    try:\n",
        "        resource_ref = oli_org_soup.find('resourceref', {'idref': page_id})\n",
        "        curr_module = resource_ref.find_parent('module').find('title').get_text()\n",
        "        curr_unit = resource_ref.find_parent('unit').find('title').get_text()\n",
        "        return curr_module, curr_unit\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def is_header(p):\n",
        "    # a header paragraph should have the form <p><em>...</em></p>, with no other inner tag\n",
        "    n_contents = len([c for c in p.contents if not str(c.string).isspace()])\n",
        "    return p.find(\"em\") is not None and n_contents == 1\n",
        "\n",
        "def get_file_content(filename):\n",
        "    with open(data_path+f'{SEMESTER}/content/x-oli-workbook_page/' + filename ) as file:\n",
        "        soup = BeautifulSoup(file.read(), 'lxml')\n",
        "    page_id = soup.find('workbook_page')['id']\n",
        "    curr_module, curr_unit = get_module_unit_from_org(page_id)\n",
        "    title = soup.find(\"title\").get_text().strip()   \n",
        "    \n",
        "    # extract the sub-headers <p><em>text</em></p> and remove them from the text content\n",
        "    sub_headers = []\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        if is_header(p):\n",
        "            sub_headers.append(p.find(\"em\").get_text().strip())\n",
        "            p.extract()\n",
        "    \n",
        "    all_text = \"\\n\".join(p.get_text().strip() for p in soup.find_all(\"p\"))\n",
        "    all_text = re.sub(r\"\\n+\", r\"\\n\", all_text.strip())\n",
        "    return {\n",
        "        \"Unit\" : curr_unit, \"Module\" : curr_module, \"Title\" : title,\n",
        "        \"Text\": all_text, \"Subheaders\" : \",\".join(sub_headers)\n",
        "    }"
      ],
      "metadata": {
        "id": "RJ1MkcMVsJJR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oli_org = open(data_path + f'{SEMESTER}/organizations/default/organization.xml', \"r\").read()\n",
        "oli_org_soup = BeautifulSoup(oli_org, \"lxml\")\n",
        "df_oli = pd.DataFrame([\n",
        "    get_file_content(filename)\n",
        "    for filename in os.listdir(data_path + f\"{SEMESTER}/content/x-oli-workbook_page\")\n",
        "    if filename.endswith(\".xml\")\n",
        "]).dropna()\n",
        "df_oli.to_csv(\"oli_content.csv\", index = False)\n",
        "df_oli.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "Yj7eMcgTvdBU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Dataframe"
      ],
      "metadata": {
        "id": "KRnEdTNtvlSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_for_answer_extraction(text):\n",
        "  print(\"\")\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  selected = list()\n",
        "  for s in sentences:\n",
        "    if s[-1] != '.':\n",
        "      continue\n",
        "    parts = s.split(\" \")\n",
        "    if parts[0] in ['We']:\n",
        "      continue\n",
        "    selected.append(s)\n",
        "  \n",
        "  return text\n",
        "\n",
        "df_oli['cleaned_text'] = df_oli.apply(lambda row: clean_text_for_answer_extraction(row['Text']), axis=1)"
      ],
      "metadata": {
        "id": "vEs6xw6Ivnb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Generation - Answer Generation from Headings"
      ],
      "metadata": {
        "id": "ev6eXkehwJUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(answer, context, max_length=64):\n",
        "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
        "  features = tokenizer([input_text], return_tensors='pt')\n",
        "\n",
        "  output = model.generate(input_ids=features['input_ids'], \n",
        "               attention_mask=features['attention_mask'],\n",
        "               max_length=max_length)\n",
        "\n",
        "  return tokenizer.decode(output[0])\n",
        "\n",
        "def clean_and_extract_questions(text):\n",
        "  return text.replace(\"<pad> question: \", \"\").replace(\"</s>\", \"\")\n",
        "\n",
        "def get_question_headings():\n",
        "  df_oli['question_unit'] = df_oli.apply(lambda row: clean_and_extract_questions(get_question(answer = row['Unit'], context = row['Text'])), axis=1)\n",
        "  df_oli['question_module'] = df_oli.apply(lambda row: clean_and_extract_questions(get_question(answer = row['Module'], context = row['Text'])), axis=1)\n",
        "  df_oli['question_title'] = df_oli.apply(lambda row: clean_and_extract_questions(get_question(answer = row['Title'], context = row['Text'])), axis=1)\n",
        "\n",
        "  df_oli.to_csv(data_path + \"oli_heading_answers.csv\", index = False)\n",
        "\n",
        "get_question_headings()\n",
        "\n",
        "def save_questions():\n",
        "  unit = df_oli['question_unit'].to_csv(data_path + 'unit_questions.csv')\n",
        "  module = df_oli['question_module'].to_csv(data_path + 'module_questions.csv')\n",
        "  title = df_oli['question_title'].to_csv(data_path + 'title_questions.csv')\n",
        "\n",
        "save_questions()"
      ],
      "metadata": {
        "id": "trpR3puNwRju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355a0226-35d3-4591-a977-aca005cf427a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concept Hierarchy and Extraction"
      ],
      "metadata": {
        "id": "xD6-JMkWxu6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_text(line_text):\n",
        "  with open(data_path + 'content.txt', 'a') as filehandle:\n",
        "    filehandle.write(line_text)\n",
        "\n",
        "df_oli.apply(lambda row: get_content_text(row['cleaned_text']), axis=1)"
      ],
      "metadata": {
        "id": "u7DxhdxdxdhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start this as another process (background execution on terminal)\n",
        "#!bert-serving-start -model_dir uncased_L-12_H-768_A-12 -num_worker=1"
      ],
      "metadata": {
        "id": "CXuoRl4FTGZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move content.txt into Concept-Acquisition-Pipeline/input_data/context\n",
        "# Update result_path (line 17 in Concept-Acquisition-Pipeline/config.py) to the name of results of file"
      ],
      "metadata": {
        "id": "XtjhIrJRcblR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd content/Concept-Acquisition-Pipeline-master"
      ],
      "metadata": {
        "id": "pLryYlFeTPcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3575a52-0882-47c8-95d5-aa1412b9b89d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/content/Concept-Acquisition-Pipeline-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python confidence_propagation.py -l en -task extract"
      ],
      "metadata": {
        "id": "WEnVYiM7UE0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/Concept-Acquisition-Pipeline.zip /content/content/Concept-Acquisition-Pipeline-master"
      ],
      "metadata": {
        "id": "6q7LiME9UFdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concept Pruning"
      ],
      "metadata": {
        "id": "yB5ZopKhX1hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_article(word):\n",
        "  if word in ['a', 'the', 'an']:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_preposition(word):\n",
        "  if word in ['aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as', 'at', 'before', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following', 'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'save', 'since', 'than', 'through', 'to', 'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without']:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def is_verb(word): #Which is better, wordnet or pos tagging\n",
        "  if \" \" not in word:\n",
        "    syn = wn.synsets(word)\n",
        "    if len(syn) > 0:\n",
        "      category = syn[0].pos()\n",
        "      if str(category) == \"v\":\n",
        "        return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def is_single_letter(word):\n",
        "  if len(word) == 1:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def is_valid_concept(concept):\n",
        "  is_valid = True\n",
        "  if is_article(concept):\n",
        "    is_valid = False\n",
        "  \n",
        "  if concept.isdecimal():\n",
        "    is_valid = False\n",
        "  \n",
        "  if is_preposition(concept):\n",
        "    is_valid = False\n",
        "  \n",
        "  if is_verb(concept):\n",
        "    is_valid = False\n",
        "  \n",
        "  if is_single_letter(concept):\n",
        "    is_valid = False\n",
        "\n",
        "  return is_valid"
      ],
      "metadata": {
        "id": "wVVTIop6aWGc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can be pruned manually, or programatically, code for which is given below:\n",
        "\n",
        "valid_concepts = list()\n",
        "concept_file_path = \"/content/content/Concept-Acquisition-Pipeline-master/processed_data/propagation_results/result_oli.json\"\n",
        "#concept_file_path = \"/content/result_oli.json\"\n",
        "with open(concept_file_path, \"r\") as filehandle:\n",
        "  concepts = filehandle.readlines()\n",
        "  for c in concepts:\n",
        "    concept = c.split(\",\")[0].split(\":\")[1].replace('\"', '').strip()\n",
        "    if is_valid_concept(concept):\n",
        "      valid_concepts.append(concept)\n",
        "  \n",
        "for c in valid_concepts:\n",
        "  print(c)"
      ],
      "metadata": {
        "id": "pFkrvdEiX27s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking"
      ],
      "metadata": {
        "id": "cDGqO1UvXatT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rank(question, standardise=False):\n",
        "  rank = 0\n",
        "  tokens = question.split(\" \")\n",
        "  for t in tokens:\n",
        "    if t in valid_concepts:\n",
        "      rank = rank - 1\n",
        "  \n",
        "  if standardise == True:\n",
        "    rank = rank/len(tokens)\n",
        "  return rank\n",
        "\n",
        "def adjust_rank(df, std=True):\n",
        "  min_rank = abs(df['rank'].min())\n",
        "  df['rank'] = df.apply(lambda row: row['rank'] + min_rank, axis=1)\n",
        "\n",
        "  min_rank_std = abs(df['std_rank'].min())\n",
        "  df['std_rank'] = df.apply(lambda row: row['std_rank'] + min_rank_std, axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "module_questions = pd.read_csv(\"/content/drive/MyDrive/oli/module_questions.csv\")\n",
        "module_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "title_questions = pd.read_csv(\"/content/drive/MyDrive/oli/title_questions.csv\")\n",
        "title_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "unit_questions = pd.read_csv(\"/content/drive/MyDrive/oli/unit_questions.csv\")\n",
        "unit_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Calculate rank\n",
        "module_questions['rank'] = module_questions.apply(lambda row: get_rank(row['question_module']), axis=1)\n",
        "title_questions['rank'] = title_questions.apply(lambda row: get_rank(row['question_title']), axis=1)\n",
        "unit_questions['rank'] = unit_questions.apply(lambda row: get_rank(row['question_unit']), axis=1)\n",
        "\n",
        "# Calculate standardized rank\n",
        "module_questions['std_rank'] = module_questions.apply(lambda row: get_rank(row['question_module'], standardise=True), axis=1)\n",
        "title_questions['std_rank'] = title_questions.apply(lambda row: get_rank(row['question_title'], standardise=True), axis=1)\n",
        "unit_questions['std_rank'] = unit_questions.apply(lambda row: get_rank(row['question_unit'], standardise=True), axis=1)\n",
        "\n",
        "# Scale rank\n",
        "module_questions = adjust_rank(module_questions, std=True)\n",
        "title_questions = adjust_rank(title_questions, std=True)\n",
        "unit_questions = adjust_rank(unit_questions, std=True)\n",
        "\n",
        "# Write output to csv files\n",
        "module_questions.to_csv('ranked_module_qs.csv')\n",
        "title_questions.to_csv('ranked_title_qs.csv')\n",
        "unit_questions.to_csv('ranked_unit_qs.csv')"
      ],
      "metadata": {
        "id": "MsRFLOxEXijy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all questions together\n",
        "\n",
        "module_questions = pd.read_csv(\"/content/drive/MyDrive/oli/module_questions.csv\")\n",
        "module_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "module_questions.rename(columns={\"question_module\": \"question\"}, inplace = True)\n",
        "title_questions = pd.read_csv(\"/content/drive/MyDrive/oli/title_questions.csv\")\n",
        "title_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "title_questions.rename(columns={\"question_title\": \"question\"}, inplace = True)\n",
        "unit_questions = pd.read_csv(\"/content/drive/MyDrive/oli/unit_questions.csv\")\n",
        "unit_questions.rename(columns={\"question_unit\": \"question\"}, inplace = True)\n",
        "unit_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "combined_df = module_questions.append(title_questions, ignore_index=True)\n",
        "combined_df = combined_df.append(unit_questions)\n",
        "\n",
        "combined_df.to_csv('all_questions.csv')"
      ],
      "metadata": {
        "id": "36j3F0Q3VGsH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df['rank'] = combined_df.apply(lambda row: get_rank(row['question']), axis=1)\n",
        "combined_df['std_rank'] = combined_df.apply(lambda row: get_rank(row['question'], standardise=True), axis=1)\n",
        "combined_df = adjust_rank(combined_df, std=True)\n",
        "combined_df.to_csv('all_questions.csv')"
      ],
      "metadata": {
        "id": "AXHr6qtKdAvw"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}