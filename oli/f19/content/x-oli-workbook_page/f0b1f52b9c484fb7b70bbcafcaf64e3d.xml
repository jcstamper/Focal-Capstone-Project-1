<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f0b1f52b9c484fb7b70bbcafcaf64e3d"><head><title>Bagging-Boosting Methods (Ensemble Models)</title><objref idref="e31e837cfea64decae6c39ec99b181d9" /><objref idref="a1dd8edc91d54a498e896685273b78c3" /><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="fd3db351218f4ab984f372247c5d2ec1">CART methods do not have good predictive performance; there are methods that can be used to compensate for this deficiency called <em style="italic">Ensemble Models. </em>Ensemble models combine other models to produce an optimal predictive model. Ensemble models also solve the problem of overfitting as faced by single tree models. Ensemble models are not typically displayed using a tree diagram, it combines a group of single tree models to form an ensemble tree model with better predictive power. Ensemble learning can reduce the errors when detecting Distributed denial of service attacks, and for detecting disorders in MRI datasets. </p><p id="dd4db3c9aaf04af2a3f66d6d8df828a6"><em>Bagging </em>reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset , using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high dimensionality data. <em style="italic">Out-of-Bag Error Estimation </em>measures the prediction error of models that use bagging. It is also used to validate models created using random forest. It is computed on data that was not used in the analysis of a model, unlike your validation metrics. </p><table id="ba7a481d6b08471c8d85f683d70a76ff" summary="" rowstyle="plain"><cite id="ibff259b4ca5a4a67a2b17fec0a886fdb" /><caption><p id="fa02479a7b6148de8ed42abb3399c1dd" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="b8e591801766464b882056e11e01a476">Additional Reading: <link href="https://www.tandfonline.com/doi/full/10.1080/21642583.2014.956265" target="new" internal="false">History of Random Forest Algorithm</link></p></td></tr></table><p id="ee5197d6cdc049c9afd4f21f663dbca8"><em>Random Forests. </em>This is an extension of bagging, and makes needed changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also assess the importance of features and scales the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests creates subsets (random) of features and combines those subsets which prevents overfitting. A best practice that you should keep in mind is that a data scientist can derive the number of features to be included in the tree by calculating the square root of the predictor variables. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train). </p><p id="d6134c1cc9384f1c8c165beb825b520c"><em>Boosting. </em>Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the estimate of <em style="italic">f </em>is improved.</p><p id="c65549af50b4475e964e31f9394515ed">Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting including one of the most popular: <em style="italic">Adaptive Boosting (AdaBoost)</em>. AdaBoost is used to improve performance of models. It is sensitive to outlier data but on the upside, it is considered as the best out of the box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm, so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the predictive power of the model and this is helpful for reducing dimensionality and improving computation time. </p><p id="a29041c6e8b44e6abe8e92a76418548e">Ensemble methods were represented as an extension of the tree method, take note that they are used for other methods as well.</p><table id="bea9f0b6ce954ec09f2d8119867269be" summary="" rowstyle="plain"><cite id="i6b016e90cb2f40a0876824fe8ece4e56" /><caption><p id="a7c74d9d0ad445c09f46dccd9645a2f1" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e76a110f38384c8f98265bfa221ce660"><em>Reading: </em><link href="https://en.wikipedia.org/wiki/Ensemble_learning" target="new" internal="false">Ensemble Methods-General Use</link></p></td></tr></table><wb:inline idref="newfa0fe02db43b4852b5f46ca57c00356d" purpose="learnbydoing" /></body></workbook_page>
