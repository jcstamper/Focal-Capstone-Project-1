<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bac22ecf62614913b53e8b09ee38b8a6"><head><title>Clustering Evaluation Metrics</title><objref idref="f884b110ee3f48bfaf5a73e27e20b647" /><objref idref="c0047158bffa477cacf9095f445caa08" /></head><body><p id="df95dd068ddb41768f8e75a1c5974581">The previous page focused on the metrics for evaluating supervised learning problems.  The presence of labeled data makes it somewhat straightforward to train and test the model&apos;s performance. Now, we will focus on metrics that can be used when labeled data is not present.  There are two approaches to evaluating clustering. The <em style="italic">Internal </em>and <em style="italic">External </em>evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert. </p><ul id="b43440b6f4d44682bb009a085b4647de"><li><p id="a935fa916255429e90a0969a396fa407">Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters.  A sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.</p><p id="a98d561ea8c5439c98b46ddb00cd5150">Let&apos;s look at internal evaluation techniques that are used to assess the quality of clustering methods:</p><p id="ff577da79fc6496c8bc1dc4d275bc6a2"><em style="italic">Silhouette Coefficient </em>shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters. </p><p id="b900576359e04a01902a593cc66cd8b7"><em style="italic">Dunn Index</em> is also used to evaluate clustering techniques and similar to the Silhouette coefficient, it is dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (Max.diameter). Finally, the Dunn index will be:</p><p id="a9dd439e8b5b4fe9bff433b009398625">min.separation/max.diameter</p><quote>If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, Dunn index should be <link href="https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/#dunn-index" target="new" internal="false">maximized</link>.</quote></li><li><p id="f682f9615eb540ed9eae95a55ac63ea1">External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering. </p><p id="b1154cd5e8de44849f145d918d5f37fc"><em style="italic">Rand Index </em>tells you how similar a cluster or clusters are to a set benchmark. This is similar a classification evaluation technique. You can calculate the Rand index thus:</p><p id="c114465ded0a4a74936e18981f5e4981">(TP + TN)/(TP+FP+FN+TN)</p><p id="ab3b63ccdad445388a2a9536fb31abaf"><em style="italic">Purity </em>is considered a no frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between quality of clustering and number of clusters when using purity as a metric. The <link href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#:~:text=Normalized%20Mutual%20Information%20(NMI)%20is,and%201%20(perfect%20correlation)." target="new" internal="false"><em style="italic">normalized mutual information</em> </link>(NMI) can be used to measure and compare the quality of clustering between different clusterings with varying number of clusters.</p><p id="e6a59df127e447bdbf75e999ffc6dfd4"><em style="italic">Jaccard Index </em>is used in cluster analysis evaluation and convolutional neural networks among others. It is defined as &quot;the size of the intersection divided by the size of the union of the sample sets.&quot; The Jaccard distance measures dissimilarity between sample sets. </p><p id="b4600886757a49859a90da098704f608"><em style="italic">F-Measure </em>is simply computed as the 2 * ((Precision * Recall)/(Precision + Recall)). You might remember it from the classification metrics, it is also known as the F<sub>1 </sub>score.</p><p id="aa810683641740f69162a9567762f1b7"><em style="italic">Dice Index </em>also known as the Sorensen-Dice indexor Dice Coefficient can assess the similarity of two samples. It ranges from 0 to 1. Dice index is a semimetric version of Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words. </p><table id="a626ac24f62e4d5d994587364e448f90" summary="" rowstyle="plain"><cite id="iacabc2bc7de74b5f94eaf3906adc0260" /><caption><p id="c3d4733968c849419e659fabbfee6ae4" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ced120cd62844e458f0d6e08990e3062"><em>Reading: </em><link href="https://www.dbs.ifi.lmu.de/~zimek/publications/ICDE2012/ICDE12_ELKI_0_5.pdf" target="new" internal="false">Clustering Evaluation Techniques</link> (20min Read)</p></td></tr></table></li><li><p id="c47cb7534c6f48c1a957f05cae6d70c4">You should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis. This is sometimes done by comparing a dataset against another (one without clusters). The Hopkins statistic is used to measure cluster tendency. A resulting value of 1 or close enough will show that the data is clustered. Data that is uniformly distributed will be closer to 0. Hopkins statistic is quite good at estimating randomness in a dataset. </p></li></ul><wb:inline idref="newb8957500e03349cc8051dbeb27aeb04b" purpose="didigetthis" /></body></workbook_page>
