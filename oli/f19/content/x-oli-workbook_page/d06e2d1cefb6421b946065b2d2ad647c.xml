<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d06e2d1cefb6421b946065b2d2ad647c"><head><title>k-Means Clustering </title><objref idref="c83092ab158e4b5ca482b24ad6e967af" /><objref idref="e31e837cfea64decae6c39ec99b181d9" /><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="be25efc461b648b6a5d20fcb419b0097"><em>Unsupervised Learning Techniques</em></p><p id="debc1709bfb8497cbbbeca1a26793cca">When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing; We have touched on the Principal Component Analysis when we discussed feature engineering. This is one of the unsupervised techniques. We will also look further into the different types of cluster analysis techniques.</p><p id="f12c116159d54d4797202bdb2d4a7aa4">You can categorize data according to characteristics using a technique called <em style="italic">Cluster Analysis</em>. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad or people categorized into close friends, acquaintances, and mentors, among others. You might even consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation; this is the segmenting of customer data based on certain criteria including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or application of customized marketing strategies that will might elicit positive responses, increase sales, and engagement.</p><image id="a2bf76470a2c4f899d13e8c2b561773d" src="../webcontent/Clusterexample.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="e584c1e6a052483085fb25ccd8e88ff0"><em style="italic">Clustering-Source www.pyarmy.com</em></p></caption><popout enable="false"></popout></image><p id="dc472011b2294d4c83041d5716848b94"><em>Types of Clustering</em></p><p id="d51032401049438a91dd75343ca0d521"><em style="italic">Hard Clustering </em>divides data into a number of groups and can only belong to one cluster. All clusters are independent of each other. </p><p id="be36952e9e2e42d2b25e6ed09aef5e42"><em style="italic">Soft Clustering</em> groups data into clusters but a data point can belong to more than one cluster to a degree. </p><p id="c50c028c8c4444a5bae4ac7535949be8"><em style="italic">Overlapping Clustering </em>allows data to belong to more than one cluster.</p><p id="b6396e9255824c31ab55526994a29218"><em style="italic">Hierarchical Clustering </em>organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.</p><p id="d4d7d92d7ba840e3bc61de5f82c84846"><em>Reading: </em><link href="https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering" target="new" internal="false">An Expansion on Clustering</link>.</p><p id="d9fadec505924b42904afbd13e7430ce"><em>k-Means Clustering</em></p><p id="c32d32dcc56f431e9d1f9b46c873b059">This method involves identifying the number of clusters <em style="italic">k </em>that the dataset will be grouped into; the data in each cluster should share similarities to the other records within its cluster. Assume you have <em style="italic">k = 5</em>, cluster <em style="italic">1 </em>will contain data that is homogeneous but quite dissimilar to the records in cluster <em style="italic">5. </em> The data within clusters adhere to distance measures to ensure that dispersion is minimized. <em style="italic">k-</em>Means Clustering technique abides by a number of distance measures but the most popular is the Euclidean Distance. Let us look at how clusters are created using this technique:</p><ul id="e0187ab633734b6c9effb5c772704e84"><li><p id="c60127277bd14536b7873a85426f7e53">Partition Data: The dataset is partitioned into <em style="italic">k</em> clusters are pre-specified (chosen by the Data Scientist). </p></li><li><p id="c5c0d0374c6246358104c2c788081998">Initialize Centroids: Within each cluster, the distance between the observations is modified so that dispersion is minimized and each observation is closest to nearest <em style="italic">centroid</em>. Centroids are data points that are considered to be the center of a cluster, you can also think of this datapoint as the mean of all the observations within the cluster. </p></li><li><p id="c5ec99cd01cb4aa7a03b2617ef2bb5be">Iteratively initialize centroids from the previous step until the means of the newly formed clusters are negligible.</p></li><li><p id="fa15c9bc2143497e92b4ef45dba7c4be">You know that you have a good cluster when there is &quot;high similarity among within-cluster data and low similarity among inter-cluster data.&quot;<sup>1</sup></p></li></ul><p id="f3aebc4d417a47c0b588b3042dea9bde">How do we decide <em style="italic">k</em>, similar to <em style="italic">k</em>NN, there are empirically studied recommendations for the best <em style="italic">k </em>to select. You can also select <em style="italic">k</em> based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for <em style="italic">k</em> and then compare the results gotten from each value of <em style="italic">k. </em>It is good practice to also run the <em style="italic">k-</em>Means cluster method by using different values for <em style="italic">k</em> based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of <em style="italic">k. </em></p><p id="e10eab605471482fa9a8ad74df1a6a8e"><em style="italic">k</em> can also be chosen by calculating the <em style="italic">Within Cluster Sum of Squares(WCSS)</em>. This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.</p><p id="e0929b78745f42bf80fbbb6733801d71">Assume that we have 1000 observations in a dataset, and we have decided that <em style="italic">k = 1000, </em>the WCSS should be zero (0). This is because all the observations are considered as centroids and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also think about the information to be gleaned from the cluster analysis; you will lack useful information.</p><p id="d1c53ac47ec84163aef5b6aa1d33394c">When you randomly initialize with a range of <em style="italic">k </em>values for the 1,000 observations mentioned above i.e. between 2-10. You can use the Elbow method to find out the optimum value for <em style="italic">k</em>. The Elbow method produces a graph that shows this optimum value at the &quot;elbow&quot; of the line as shown below. You select <em style="italic">k</em> as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.</p><image id="a0bf20c38e5d48eba52f17abfc1ca84f" src="../webcontent/Elbow_method.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="c147bb88f81c44f78940634fd8b0772e"><em style="italic">Elbow Method: </em>Source<sup>1</sup></p></caption><popout enable="false"></popout></image><table id="e056006746734dfb85260bb3cd8b13ca" summary="" rowstyle="plain"><cite id="i05e9784d53fe445dbce335d9bb5c983a" /><caption><p id="caa4ede768494efd94d4cd334dcbab52" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ffa6798ba14241118c390280faed34bf"><em>Reading: </em><link href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="new" internal="false"><em style="italic">k-</em>Means Clustering-sklearn</link>.</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="fad8cf706494459884a63034c36f61cc"><em>Additional Reading: </em><link href="https://en.wikipedia.org/wiki/K-means_clustering" target="new" internal="false">K-Means Clustering Algorithm</link></p></td></tr></table><p id="b116f7fc1d74448582c4a1018e7e81aa"><em style="italic">K-Means Clustering and k-Nearest Neighbors </em>have been known to cause confusion for data scientists who are new to the field. Afterall, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique and the other is supervised. <em style="italic">k</em>NN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points. <em style="italic">k</em>Means does not provide a labeled dataset to the model for learning purposes. <em style="italic">k</em>Means will partition the data into a number of clusters. <em style="italic">KNN </em>works best with data that is of the same scale but <em style="italic">k</em>Means do not need same scale data to perform well. Remember when you learned about <em style="italic">k</em>NN being a lazy learner? <em style="italic">k</em>Means is an eager learner. It is slow to train but fast to learn and it tends to deal with noise in the training dataset better than a lazy learner. </p><wb:inline idref="newc3e42fdda65c47a4939ca101932ff772" purpose="learnbydoing" /></body></workbook_page>
