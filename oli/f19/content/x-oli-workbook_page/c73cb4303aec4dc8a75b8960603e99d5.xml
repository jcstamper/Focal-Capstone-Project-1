<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c73cb4303aec4dc8a75b8960603e99d5"><head><title>Regression</title><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="f22dcb600b70437c994fd9e2e855d843">When your output variable is a continuous value, you are able to make predictions using the widely known <em style="italic">Regression analysis</em>. The input variables for a regression task can be categorical, discrete or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s). </p><table id="a9bd8cfc8d364652ac86df352310d046" summary="" rowstyle="plain"><cite id="i1db0cf6b2a604afd9141af950efb7e0c" /><caption><p id="f59a4888bac74c289ab8b9243430da6a" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="fbbb9f5c41754644bc10e66e4a80bcde"><em>Thought: </em>The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses, those techniques include kNN among others.</p></td></tr></table><p id="a7ff1304ade6400694ebb41e617d7dc4">Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, you can tell the effect of changes to temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the x and y variables, you can use regression techniques. When you identify independent variables that are highly correlated, you can say that the variables are <em style="italic">multicollinear</em>. If the correlation between two independent variables is &quot;1 or -1&quot;, then you have perfect multicollinearity. You can detect multicollinearity when there are large changes in the estimated regression coefficient, when an independent variable is added or removed. </p><p id="cddcea68ac7d44c2aee45fc42aa67f05">A regression model will have certain components including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error ε, the random error is not found in the dataset, instead it is the difference between an expected outcome and the actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters β. Your goal with a regression model is to estimate the function <em style="italic">f</em>(X, β) with the best fit to the data. <em style="italic">f </em>should be specified when performing regression analysis.  This will ensure that you are deciding on the right regression methods to use. </p><p id="bcaf2b0e2fad4104b33d83a8ec42b69c">When performing regression analysis, you might encounter data that has <em style="italic">outliers</em>, if not handled during the data understanding phase it can affect the results of your regression analysis. </p><p id="bbc7ed05ee474c2fbe5ed62ad298cf18">Let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13.</p><p id="f58f61aadbfa411090e4e0abefc9223c"><em>Linear Regression</em></p><p id="d3dd474c8f7846bdb2c01c8bc09d7a59">This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector x=(<em>X</em><em><sub>1t...</sub></em><em>X</em><em><sub>kt</sub></em>), where <sub>t</sub> denotes a row of data, k is the number of inputs. The model is said to be linear because the output is a linear combination of independent variables. </p><p id="adca6e23a1944b59abd4a6565e07109b">There is the <em style="italic">simple linear regression model </em>that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the <em style="italic">multiple linear regression method </em>to account for multiple predictors. </p><p id="b31a65655053448fa1e4a4092d98ba12">This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution<sup>1</sup>.  Let us represent a regression model as: </p><p id="a89afd78b2e8457db07987add59a14b0"><em>𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽</em><sub><em>k</em></sub><em>𝑥</em><sub><em>k</em></sub><em> + 𝜀</em></p><p id="fb6cc898ba3846d38fcb29ba5e4bf8dc">Regression function for multiple linear regression: </p><p id="e322216c41fe42208383bf73e2e2639d"><em><em style="italic">f(x</em></em><em><sub><em style="italic">1...</em></sub></em><em><em style="italic">x</em></em><em><sub><em style="italic">k)</em></sub></em><em> = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽</em><sub><em>k</em></sub><em>𝑥</em><sub><em>k</em></sub></p><p id="dd3150f46f134c368db65d5456888f97">Y is a straight line function of each independent variable X. The slopes of the individual straight line relationships of X<sub>1....</sub>X<sub>k</sub> with Y are the constants b<sub>1</sub>...b<sub>k</sub> also known as the <em style="italic">coefficients </em>of the variables. Translate this to mean b<sub>i</sub> is the change in the predicted value of your dependent variable Y per unit of change in X<sub>i</sub> with other things being equal. Consider b<sub>0  </sub>as the <em style="italic">intercept</em> (prediction that your model will make if all the independent variables were zero). You must also account for the random error e in the equation. </p><p id="c01e6a4184b64410ab72de4d81ab9480">You estimate b<sub>1</sub>...b<sub>k </sub>and b<sub>0</sub> using<em style="italic"> </em><em>Least Squares</em>. This method will minimize the sum of squared <em style="italic">residuals </em>(a residual is the difference between an observed value and the fitted value given by a model). Least squares can be linear or ordinary or nonlinear.  <em style="italic">Ordinary Least Squares</em> chooses the parameters of a linear function of a set of independent variables by the principle of least squares. <em style="italic">Non-linear least squares </em>will fit a set of observations with a model that is non-linear in unknown parameters, it will approximate the model by a linear model and refine its parameters by iterations. </p><p id="be56650e690e48579659e43d5d221799"><em style="italic">Performance </em>of a regression model can be assessed using the coefficient of determination or R<sup>2</sup> which shows the amount of variation in y that is dependent on x. The larger the R<sup>2</sup>, the better the model can explain variation of the response with various predictors. </p><image id="bd22f59f05e74e42bc918a0b648dc150" src="../webcontent/OLS.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="ed947fcfa09042639fd510b07abf0fef"><em style="italic">Ordinary Least Squares Source</em><em style="italic"><sup>3</sup></em></p></caption><popout enable="false"></popout></image><p id="b4419ce5a03e460cbf7bb8909eca2f65"><em style="italic">Assumptions of Linear Regression</em></p><p id="bb73429c739c4ea0bac66c89919149f5"><em>Reading: </em><link href="http://people.duke.edu/~rnau/testing.htm" target="new" internal="false">Four Principal Assumptions</link>. These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights. </p><p id="ce35dc9313a3410db80c4a6625f69d00"> <em>Polynomial Regression </em></p><p id="b6b6b5e1f37b412caa7030ea71a1f9a8">When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called a <em style="italic">polynomial regression. </em>It seeks to model the expected value of y in relation to the value of x. Pay attention to the figure below, you will note that using a linear regression line to fit the data would result in a high value of error. </p><image id="a806856313aa49d096dbd22680608b27" src="../webcontent/Polynomial.gif" alt="" style="inline" vertical-align="middle" height="375" width="500"><caption><p id="f8f0bc847477426f936098746801cf68"><em style="italic">Trying to fit a simple linear regression line. Source</em><em style="italic"><sup>4</sup></em></p></caption><popout enable="false"></popout></image><p id="a7db537952f94becae768c92db693884">Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the y and x variables. A polynomial regression can fit a broader range of function. However, it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis.</p><image id="e74b715b1566452b8bed9febc5a2a7fc" src="../webcontent/Poly.gif" alt="" style="inline" vertical-align="middle" height="375" width="500"><caption><p id="cb41dddc330b46fe9fc9d4268b58b610"><em style="italic">Polynomial regression with lower error. Source</em><em style="italic"><sup>4</sup></em></p></caption><popout enable="false"></popout></image><p id="ebd9df7dcf334bcb974f9d5c0fd00c4e"><em>Stepwise Regression</em></p><p id="fff9f0031056412ea02a8b02afdc418d">When you have a regression analysis task, you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors. <em style="italic">Stepwise Regression </em>will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression including:</p><ul id="fc074b77ce2249268e65e76d3fe790e3"><li><p id="de8b64d67ff24cb1b349acbef7d7fc67">Backward elimination which tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the &quot;most statistically insignificant deterioration of the model fit&quot;.  This technique should not be used if predictors are more than the observations in the dataset. </p></li><li><p id="a3dc33143fe944aeabe1d902413d2586">Forward selection is the reverse of the backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit. </p><p id="a6efc79798b74fff8733f51fb7cc58b3">We also have the <em style="italic">mixed selection </em>technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques. </p></li></ul><p id="faaac7a117374facb7e2c8139699f294"><em style="italic">Model Accuracy</em></p><p id="e2c23a7dfde74769aefbdecca1782434">Stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample. You can check the extent to which a model fits the data with the <em style="italic">residual standard error(RSE is </em>standard deviation of error e) i.e &quot;the average amount that the response will deviate from the true regression line&quot;. A large RSE means the model was not a good fit to the data.<em style="italic"> </em>and the <em style="italic">R</em><em style="italic"><sup>2</sup></em><em style="italic"> </em>is independent of your response variable, unlike the RSE. </p><p id="d1c2efbf96864ba587ef87f0217cb92e">R-squares is calculated using the <em style="italic">total sum of squares </em>which is the total variance in Y and RSS is the &quot;discrepancy between the data and an estimation model&quot;. </p><p id="be4040354eae4577810c0f323751aafa"><em>Selecting the Right Regression Method</em></p><ul id="c76cffbeb843454c8c829b3dffc2b07e"><li><p id="e906f2c004394be6828cc6ad8b876cae">A goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values). When you want to select the right method, you can use the different metrics below including:</p><ul id="dedadf8675da459a80caa57bd07664b9"><li><p id="c87e15b8b0f4469ea5664d3cff45958f">AIC known as the Akaike Information Criterion is used to select models, and you choose the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models<sup>5</sup>. </p></li><li><p id="e2195fac43e142bfb69d1bcd992799b1">BIC known as Bayesian Information Criterion and a model with the lowest BIC is considered the best model. It is related to the AIC and is appropriate for models fit under the maximum likelihood estimation. The BIC penalizes complex models unlike the AIC. </p></li><li><p id="aec4b7da38804c8b9c22acf8c5990c30">R<sup>2 </sup>can be defined as 1-Residual Sum of Squares/Total Sum of Squares. The R<sup>2 </sup>will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability and 1 means the model explains full variability. </p></li><li><p id="d243b8cdf2164c639e69b354d966cae0">Adjusted R<sup>2</sup> addresses the issue highlighted with the R<sup>2 </sup>, an independent variable that has a strong correlation to the dependent variable increases the adjusted R-squared and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted R<sup>2 </sup>is a suitable criteria to use. </p></li><li><p id="c21c0457103e4bb9a42900f042a2cf09">Mallow&apos;s Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.</p></li></ul></li></ul><p id="bbe08c7f13014712b583c27c74d7936f">We will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in Regression analysis.</p><table id="c738028b8e204a4da0f2db2977aef147" summary="" rowstyle="plain"><cite id="i797a7a57d7a14c2b856e791ec77ca2fe" /><caption><p id="e573e1f3003246268002ad6176dc6476" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e5c09f4e64f14745a0711b95b830555b"><em>Additional Reading: </em><link href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf" target="new" internal="false">Introduction to Statistical Learning</link>.</p></td></tr></table><wb:inline idref="newb7b52312befd4e5b800d1df393ccb1dc" purpose="didigetthis" /><wb:inline idref="newf50c981e38044434bf4351b391fe6c80" purpose="checkpoint" /></body><bib:file><bib:entry id="a68a18e5010e4e2baa960e8c03bdb1f0"><bib:inbook><bib:author>Murphy, K.P</bib:author><bib:title>Machine learning: A probabilistic perspective.</bib:title><bib:chapter>7</bib:chapter><bib:publisher>MIT Press</bib:publisher><bib:year>2012</bib:year></bib:inbook></bib:entry></bib:file></workbook_page>
