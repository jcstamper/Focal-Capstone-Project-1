<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Inline Assessment MathML 1.4//EN" "http://oli.cmu.edu/dtd/oli_inline_assessment_mathml_1_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newe131dd72d65a44499220491554114a99"><title>New Formative Assessment</title><page id="e01e414105c54373939c733da58a989c"><title>Page 1</title><question id="a870b69be9774e399fe16081959ffcbf"><body><p id="ecd667f45ccb4fee8d71c2fb43eb9ec0">Model interpretation is useful because:</p></body><multiple_choice shuffle="true" select="single" id="bbc6395311c34a4cb98e5c0555928fd7" labels="false"><choice value="add70a0e12bc474f97a464220f5feede">It can detect bias and assists with model debugging.</choice><choice value="da1197005c504925ba510865847c2808">It increases the accuracy of models.</choice></multiple_choice><part id="e49c68096bec436facd2540ef48870ae"><response match="add70a0e12bc474f97a464220f5feede" score="1"><feedback><p id="d3fccc9c0c3b4ba1a6974b698df6f701">Correct: Interpretability can help us detect bias and ensure that it is noted and fixed.</p></feedback></response><response match="da1197005c504925ba510865847c2808" score="0"><feedback><p id="fe7d0289b41c4f73a6fd4632a753e7d8">Incorrect: Typically a highly interpretable model does not increase model accuracy.There are strategies that are used to balance this tradeoff.</p></feedback></response></part></question><question id="e2670f71cb114b31a1118d962ec4d43e"><body><p id="e9d25202fb3d42d2bcfc4a183d496f94">Model interpretability strategies include:</p></body><multiple_choice shuffle="true" select="single" id="bb8ccd56092f4369b9c2557a9339ac9b" labels="false"><choice value="d83fc3f8ce014217a60820643f8ed682">Use of Local Surrogate Models and Shapley Values.</choice><choice value="f3fe5753a1d04c7f93a51cb41ae8b091">Use of dimension reduction strategies.</choice><choice value="f918f8229e3b4af28d36f709663370f6">Use of different algorithms that are highly interpretable.</choice></multiple_choice><part id="ff0898bd0c1f49ada9790c4dd3742f9c"><response match="d83fc3f8ce014217a60820643f8ed682" score="1"><feedback><p id="d4a71855727b4b5aa88fe737b59f41a9">Correct: These methods are useful for interpretability and in some cases accuracy, including LIME and SHapley.</p></feedback></response><response match="f3fe5753a1d04c7f93a51cb41ae8b091" score="0"><feedback><p id="ba289cca1c244ef992f6b53114bbc9a4">Incorrect: In Fact there are circumstances were you need to use interpretable dimension reduction techniques.</p></feedback></response><response match="f918f8229e3b4af28d36f709663370f6" score="0"><feedback><p id="a4aaced56e524131b969b04f4bebfda4">Incorrect: The tradeoff exists because there are contexts were it is not possible to use high accuracy models  over high interpretability models.</p></feedback></response></part></question></page></assessment>
