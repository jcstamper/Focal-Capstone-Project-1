<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newda97168f6d354565945f5c624768282b" recommended_attempts="1" max_attempts="1"><title>Quiz 7</title><page id="dbbb393cb58942f385c918c417be9592"><title>Page 1</title><content available="always"><p id="a237f7a99c6d42caa96d494667e321ad">You are about to attempt Quiz 7! Please select the option that answers the questions posed. You will have additional space to justify any answers/ or provide explanations for your responses. Be sure to label the responses accordingly. </p></content><multiple_choice id="newda97168f6d354565945f5c624768282b_1a" grading="automatic" select="single"><body><p id="be0c0c5cf03d44e78354b8c033104eb1">Which of the splits below is considered a single iteration (one round) of Leave-one-out-cross-validation approach for a dataset of 300 observations:</p></body><input shuffle="true" id="ans" labels="false"><choice value="yes">Dataset is split into two subsets: Training set {(x<sub>2</sub>,y<sub>2</sub>)....(x<sub>300</sub>,y<sub>300</sub>)} and Validation set (x<sub>1</sub>,y<sub>1</sub>).</choice><choice value="no">Dataset is split into two subsets: Training set (x<sub>1</sub>,y<sub>1</sub>) and Validation set {(x<sub>2</sub>,y<sub>2</sub>)....(x<sub>300</sub>,y<sub>300</sub>)}.</choice><choice value="d6a3363ff5da422781ed3213d0dddb3c">Dataset is split into two subsets: Training set(x<sub>1</sub>,y<sub>1....</sub>x<sub>210</sub>,y<sub>210</sub>) and Validation set(x<sub>211</sub>,y<sub>211....</sub>x<sub>300</sub>,y<sub>300</sub>).</choice><choice value="e299539597284eb2a10ec72a939522d5">Dataset is split into two subsets: Training set(x<sub>1</sub>,y<sub>1</sub>) and Validation set(x<sub>211</sub>,y<sub>211....</sub>x<sub>300</sub>,y<sub>300</sub>).</choice></input><part id="a4a3514459b5498e8d01938f9d50f68a"><response match="yes" score="10"><feedback><p id="da0ae802297442e29b6765c9a3693758">Correct: The LOOCV approach will divide the dataset into two subsets, holding out one for validation and the rest as the training set.</p></feedback></response><response match="no" score="0"><feedback><p id="e0b795af4cfe47ee95c8046afe6b7201">Incorrect: The LOOCV approach will divide the dataset into two subsets, holding out one observation for validation and the rest as the training set.</p></feedback></response><response match="d6a3363ff5da422781ed3213d0dddb3c" score="0"><feedback><p id="f891b3c155594f72896374a07d5eb842">Incorrect: This resembles a 70-30 split which can be considered a validation set approach and not the LOOCV approach.</p></feedback></response><response match="e299539597284eb2a10ec72a939522d5" score="0"><feedback><p id="ae3f1870d0334fd1834aedccace57fbd">Incorrect: This split trains the model with one observation and validates with 30% of the dataset. This will surely lead to an imbalanced model. </p></feedback></response></part></multiple_choice><multiple_choice id="a9d1c488e3834f3baa176aca9a15597d" grading="automatic" select="single"><body><p id="f6d40384a8594428a182be493ff42d3a">Which statement below is true about the Validation Set approach:</p></body><input shuffle="true" id="e54e12edc46547bdb99d8056b6903a21" labels="false"><choice value="dc71bdbc73fc4c5397ef2d9ccc2ac897">The Validation Set approach uses the data in the training set alone to fit the model and this can result in overestimating the test error if it is trained on a small training set.</choice><choice value="a24800b7eb60412cbe8b6e0e8e8bb1ec">The Validation Set approach uses the data in the training set and validation set to fit the model and this can result in underestimating the test error if it is trained on a small training set.</choice><choice value="b7093d82289f49ba9aa511a351186a71">The Validation Set approach uses the data in the training set alone to fit the model and this will not result in overestimating the test error even if it is trained on a small training set.</choice><choice value="d59e2fa6e01a411cb37b4201b7660bb8">The Validation Set approach uses the data in the validation set alone to fit the model and this can result in underestimating the test error if it is trained on a large training set.</choice></input><part id="a23bb012fbaa430182cae7c68ca8915c"><response match="dc71bdbc73fc4c5397ef2d9ccc2ac897" score="10"><feedback><p id="b1ef5b7d96a0423bb66ead1cdac4b3ab">Correct: This can be remedied if the training dataset is large enough.</p></feedback></response><response match="a24800b7eb60412cbe8b6e0e8e8bb1ec" score="0"><feedback><p id="b8416ae3a9764c31905e4d2ae536ec51">Incorrect: If the training set is small, it will not under estimate the test error. You do not use both in this approach.</p></feedback></response><response match="b7093d82289f49ba9aa511a351186a71" score="0"><feedback><p id="fe625d7a29cd44428556fa0a6c8e8401">Incorrect: This will cause an overestimate of the test error because the model is trained on a small dataset.</p></feedback></response><response match="d59e2fa6e01a411cb37b4201b7660bb8" score="0"><feedback><p id="ae58481e146a4462bf846fbd2bab74a4">Incorrect: This approach will not use the validation set to fit the model.</p></feedback></response></part></multiple_choice><multiple_choice id="e7d9d6e1b5e94801b089415dfe9cdfc3" grading="automatic" select="single"><body><p id="ed923235b6aa4d5d8f7a8adf078d0d86">How would you define the difference between the average prediction of <em style="italic">f </em>and the accurate value you are trying to predict:</p></body><input shuffle="true" id="f44e6748fe8f4773bc37a6f74c7b7523" labels="false"><choice value="d881d438bac3432fa23d2ae06fe4d4db">This is defined as an error caused by Bias.</choice><choice value="f038ba7fcd7d40cba439f592445b4978">This is defined as an error caused by Variance.</choice><choice value="df410dbf8f7e4f58b7a41dce233b350c">This is defined as the Bias-Variance Tradeoff.</choice><choice value="cc4fb63c400e4abda7c0f8f4073731fa">This is defined as a Random Error.</choice></input><part id="e05a8b5323ad419f92cba02660b48f70"><response match="d881d438bac3432fa23d2ae06fe4d4db" score="10"><feedback><p id="e1c08325d3184f27939144b1b2e16761">Correct: This is the right definition for error caused by Bias. </p></feedback></response><response match="f038ba7fcd7d40cba439f592445b4978" score="0"><feedback><p id="b89dd637d52a4af7b88837b7a08299f3">Incorrect: An error caused by Variance is the <em>variability</em> of the prediction of <em style="italic">f </em>that highlights the spread of the data within the dataset.</p></feedback></response><response match="df410dbf8f7e4f58b7a41dce233b350c" score="0"><feedback><p id="ec227bd8be0b4e609526fb509e3c1d22">Incorrect: Think of the Bias Variance tradeoff as the concept of finding the right balance for your model (low bias and low variance).</p></feedback></response><response match="cc4fb63c400e4abda7c0f8f4073731fa" score="0"><feedback><p id="d6edf25ffbce411f8e13b8859de07164">Incorrect: These are errors that are caused by factors that are unknown. </p></feedback></response></part></multiple_choice><multiple_choice id="c70db35e28d44493ac39a54719b620e0" grading="automatic" select="single"><body><p id="c68b6ffa450649e39883387ad918cbf4">When a model is balanced, which of the following statements is correct:</p></body><input shuffle="true" id="f82cee6c9f574401a80a81753a9545e6" labels="false"><choice value="c00a173baad842da8c5dab6eb5b218de">The training error should be<em> slightly</em> lower than the test error. </choice><choice value="e297d8694f4249fca4936ab0755c5bbd">The training error should be <em>much lower</em> than your test error.</choice><choice value="d27cbbff6c4444578387a1f041346583">The training error is <em>equal</em> to the test error.</choice><choice value="f7badea3aa3c41c2b5bf890063fbed09">The training error is <em>much higher</em> than your test error. </choice></input><part id="bda35d81ca254f2aaf0031a9f008da8c"><response match="c00a173baad842da8c5dab6eb5b218de" score="10"><feedback><p id="ed288ffe773e4e4895069ea2b5c20356">Correct: This will result in a model that has low bias and low variance.</p></feedback></response><response match="e297d8694f4249fca4936ab0755c5bbd" score="0"><feedback><p id="bf65f5e3c4574344849e21f5e94e314f">Incorrect: This will result in a model with high variance.</p></feedback></response><response match="d27cbbff6c4444578387a1f041346583" score="0"><feedback><p id="f84a3869b62e47dd937e714c429447cb">Incorrect: This will result in a model that has a high bias.</p></feedback></response><response match="f7badea3aa3c41c2b5bf890063fbed09" score="0"><feedback><p id="a1f8093e179c43098ae92caaebc4a5db">Incorrect: This will still result in a case of underfitting.</p></feedback></response><hint><p id="b09b026de1154b5c90575927dad9b587">Here, you would have a large dataset and a carefully built model!</p></hint></part></multiple_choice><essay id="fd2f129407814307b2a8aebb34563d75" grading="automatic"><body><p id="b6c56b939d8641f9a5a31911ab006f36">Why did you select the answer above?</p></body><part id="c4c504700a0c43928a1d95a12ab959c2"><response match="*" score="1"><feedback /></response></part></essay><multiple_choice id="f9d8e93f18b243ddb7fe119264fb3b75" grading="automatic" select="single"><body><p id="f20ed5f52868488bbbb4bc9fce1914e6">There are concerns that the LOOCV approach is computationally expensive. Which option below will be a suitable alternative to LOOCV for cross validation to a 50,000 record dataset:</p></body><input shuffle="true" id="e2ab3b42942e47b4876087da97e17024" labels="false"><choice value="b7160e16d18f4c409387377b76a2404b">Randomly divide the dataset into five (5) groups that are the same size. Use four (4) groups as the training set and one (1) group as the validation set.</choice><choice value="fe8d3d3ab6d54385982d8bbaa4223211">Divide the dataset into 2 groups with 40,000 records in the training dataset and 10,000 in the validation dataset. </choice><choice value="f0219e2677034136a9279da1bb5d433c">Randomly divide the dataset into two (2) groups. Use 10,000 as the training set and the group with 40,000 as the validation set.</choice><choice value="d1dc865fb9164c1e8327d279142d1bb2">Randomly divide the dataset into <em style="italic">k</em> groups that are the same size. Use <em style="italic">k-2</em> groups as the validation set, one (1) group as the training set and hold out one incase there are any issues.</choice></input><part id="cbf7e8c6a4794ae58bd59c58f17cbd9c"><response match="b7160e16d18f4c409387377b76a2404b" score="10"><feedback><p id="f95036302f3b4caba529269224e86b6b">Correct: This is the <em style="italic">k</em>-Fold Cross Validation Technique and is a less computationally expensive alternative to the LOOCV approach.</p></feedback></response><response match="fe8d3d3ab6d54385982d8bbaa4223211" score="0"><feedback><p id="c35f4b740ec34c89a2f992aa4fb1d4dc">Incorrect: This approach is an 80-20 split that is referred to as the Validation Set Approach.</p></feedback></response><response match="f0219e2677034136a9279da1bb5d433c" score="0"><feedback><p id="f4a7efcb89514eccbc36907e9afd9020">Incorrect:  This approach is an 80-20 split that is referred to as the Validation Set Approach but in this case you have more in the validation set than the training set. This will certainly cause issues.</p></feedback></response><response match="d1dc865fb9164c1e8327d279142d1bb2" score="0"><feedback><p id="b6e3e9608c904f43a9552718daf115c5">Incorrect: This method is not cross validation and will not be a sufficient solution. </p></feedback></response></part></multiple_choice><multiple_choice id="a552259800c642c08625853df8900aab" grading="automatic" select="single"><body><p id="a4888e650997495fba837a63b7feda34">Most models will have an error that is considered noise to your model, this error can be improved by identifying the influences of the noise and including it/them as predictor(s). The error described is called:</p></body><input shuffle="true" id="c879ca503c4f4f28ab65d64c76c09419" labels="false"><choice value="a4201088b5a54eff8f3d658998770a00">Irreducible error.</choice><choice value="fe16bd03884b429a946b95fdea98abd0">Reducible error.</choice><choice value="b51b9674adf64a7392f42f862cb19cb0">High Bias Error.</choice><choice value="e3ddc01fe8b448afa86ca95fb648104d">High Variance Error.</choice></input><part id="a8875647c5634327be8355238466c14a"><response match="a4201088b5a54eff8f3d658998770a00" score="10"><feedback><p id="c8f7a619306146edbe896092e89f457f">Correct: Irreducible error is one that you can not control and you can possibly include in your dataset to reduce the noise.</p></feedback></response><response match="fe16bd03884b429a946b95fdea98abd0" score="0"><feedback><p id="c4aff63a9ab3405bbafe5319bbbebd8f">Incorrect: Reducible error is a mismatch between <em style="italic">f </em>and <em style="italic">f hat </em>and differs from the scenario in the question.</p></feedback></response><response match="b51b9674adf64a7392f42f862cb19cb0" score="0"><feedback><p id="b83461df923b42f497b12171374f0cdb">Incorrect: This error can be fixed and is usually due to reducible errors and it leads to underfitting. </p></feedback></response><response match="e3ddc01fe8b448afa86ca95fb648104d" score="0"><feedback><p id="fa275fce61e84bef98e7ebea36cff589">Incorrect: This error can be fixed and is usually due to reducible errors and it leads to overfitting. </p></feedback></response></part></multiple_choice><multiple_choice id="dd37e413ce0642279dbc267271902714" grading="automatic" select="single"><body><p id="cd488f70cf414877b42beb842884485b">What occurs when a model has become over familiarized with the training dataset and can not be applied to unseen data to produce a good model:</p></body><input shuffle="true" id="c4c7071edf68439ea33a66d2399f75a0" labels="false"><choice value="b32b8e52229a443bae3d4fba2e825abb">The model is overfitted.</choice><choice value="a216ecf169cb4c16a119c015e7586633">The model is underfitted.</choice><choice value="ffc54221a6134d0bb9410f3c79661b6a">The model is balanced.</choice><choice value="ee8d6b39448340208e100487755b5829">The model definitely needs a new algorithm.</choice></input><part id="f220d3d47b8c41bca792201d25e94a01"><response match="b32b8e52229a443bae3d4fba2e825abb" score="10"><feedback><p id="f7fd06511db3427e803cb7359cdd05f5">Correct: An overfitted model has understood the structure of the data. </p></feedback></response><response match="a216ecf169cb4c16a119c015e7586633" score="0"><feedback><p id="b16f67728d804526908dffb45f5e55b4">Incorrect: An underfitted model does not capture the underlying structure of the data and might have poor performance and accuracy. </p></feedback></response><response match="ffc54221a6134d0bb9410f3c79661b6a" score="0"><feedback><p id="bd05f6fe475e48358f70e8382410043f">Incorrect: If the model is balanced, it should not have issues as listed in the question.</p></feedback></response><response match="ee8d6b39448340208e100487755b5829" score="0"><feedback><p id="e017007898f04f19b13e999a0fd2ea1d">Incorrect: This is not the go to option to fix the issue. </p></feedback></response></part></multiple_choice><multiple_choice id="fef64ec977de450eb6bbd137ce89716c" grading="automatic" select="single"><body><p id="dbd7a6b2502245ebbae58b232b13639d">You have trained a model resulting in a high training error that is close to the test error(it is equally high). You have experienced a phenomena called:</p></body><input shuffle="true" id="a21fcd5a87a94fa28af4e193611d31f8" labels="false"><choice value="b51e56088ef746669d73f5dd47e4c69b">Underfitting.</choice><choice value="ea093aef90dc479abfa28b4eb0b895f8">Overfitting.</choice><choice value="cb9fdb18d44b437d8b1c6f802ba5aa78">High Variance.</choice><choice value="b1a25fb8b43e4a5baab1d5619555f907">Low Bias.</choice></input><part id="b8ae0509d374478487e355765a1d9ac4"><response match="b51e56088ef746669d73f5dd47e4c69b" score="10"><feedback><p id="bcadf9dfddb247dc83d25a0a6ee7793c">Correct: If you train your model longer and add more features, you will remedy the issue of underfitting.</p></feedback></response><response match="ea093aef90dc479abfa28b4eb0b895f8" score="0"><feedback><p id="edc37d39f7e14151896b0c7b71997836">Incorrect: If your model&apos;s training accuracy is high and your test accuracy is much lower, this would be overfitting.</p></feedback></response><response match="cb9fdb18d44b437d8b1c6f802ba5aa78" score="0"><feedback><p id="beb0adb01c004b1d8897113a7a2f3bcc">Incorrect: This means that the data points are widely spread from the mean. </p></feedback></response><response match="b1a25fb8b43e4a5baab1d5619555f907" score="0"><feedback><p id="fb4f25938ab5405e969587f5743ed9dc">Incorrect: You want a model that has low bias AND low variance. Low Bias and High Variance models have inconsistent averages but in this solution, we are only presenting Low Bias. </p></feedback></response></part></multiple_choice><multiple_choice id="c778d6149b8d4ee38aea899400fe1a43" grading="automatic" select="single"><body><p id="bead8dd85f2b4ec0a9757929573b72a5">Select the <em>best</em> scenario among the options below that will lead to a low bias model:</p></body><input shuffle="true" id="e6f7a7079f8a4d2583d884f7bfc49a63" labels="false"><choice value="c5eb183a8c024007beb252d54d7cca84">A k = 10 for <em style="italic">k</em>-Fold Cross Validation with a 4,000 dataset will lead to a low bias model.</choice><choice value="e725525093bb4af28df11d56c428e7a0">Performing Leave-one-out-cross-validation will lead to a low bias model.</choice><choice value="e15c676bdf7446289fdf632749e4413b">Performing any of the cross validation approaches.</choice><choice value="beeb9c731d6c4d27ba511e0e9d22e6c7">Using the Validation Set technique will lead to a low bias model.</choice></input><part id="c5aff0fe6b6b4557a7554d35246c2a3c"><response match="c5eb183a8c024007beb252d54d7cca84" score="0"><feedback><p id="f29e21e5b6aa46c19162d54e4cb72872">Incorrect: Using a small k value will introduce bias. Using a larger <em style="italic">k </em>will result in a low bias model. </p></feedback></response><response match="e725525093bb4af28df11d56c428e7a0" score="10"><feedback><p id="d1b1b62b81be478c825f0014d6e05267">Correct: LOOCV tends to produce a lower bias model than the validation set technique and even k folds.</p></feedback></response><response match="e15c676bdf7446289fdf632749e4413b" score="0"><feedback><p id="bc55f86f0c094b6fa061af148428dcd9">Incorrect: Some of the CV techniques will lead to high bias unless there are certain measures adopted.</p></feedback></response><response match="beeb9c731d6c4d27ba511e0e9d22e6c7" score="0"><feedback><p id="f5a806f294c543b0ba7624dbcfe6aa6c">Incorrect: This technique is prone to a high bias model unless certain steps are followed to address it.</p></feedback></response></part></multiple_choice><multiple_choice id="b9374f8876da4b5d9f25b22ea43c1379" grading="automatic" select="single"><body><p id="eff035614fbd43438e09bc1cd7faa877">Cross validation (CV) is a resampling technique that is useful for assessing the results of an analysis. Select the option below that captures the statement(s) that is/are true about this technique:</p><p id="ed725e3a43e940159fbda6f0820e4872">I) CV can be used to estimate the test error associated with a learning method.</p><p id="eed0d0e4206e428b9dbd442bcf671d49">II) Although CV can estimate test error, it is not helpful with parameter tuning.</p><p id="f0552698612045bdb7f68a1f329a0ae4">III) CV techniques like LOOCV can be seen as a means to using all your data for training and testing; when performed more than one round, it will lead to less bias.</p><p id="f84851726d1f4b869380a6128f909c4f" /></body><input shuffle="true" id="e748df76675840b98e4e789437596302" labels="false"><choice value="a5491537b8a24e6cb5261b07030996cb">I &amp; III.</choice><choice value="ed8a79e4f4d746db8ce37f2468d7ca71">II &amp; III.</choice><choice value="a3119db0c7c3403aa8c0cc377b1f7390">III alone.</choice><choice value="c5fc9e40513445579da9386032c1f267">I &amp; II.</choice></input><part id="d987ea70cf544a87b5d027e68288c497"><response match="a5491537b8a24e6cb5261b07030996cb" score="10"><feedback><p id="a2654e3c14cd46a0a7cd32fcd39d5c89">Correct: You have selected the correct option. </p></feedback></response><response match="ed8a79e4f4d746db8ce37f2468d7ca71" score="0"><feedback><p id="b430c4db387e450491ba1d67fbbe8e0b">Incorrect: Cross validation is helpful with tuning parameters.</p></feedback></response><response match="a3119db0c7c3403aa8c0cc377b1f7390" score="0"><feedback><p id="d29e79fdafb84eef89fd5e638a193866">Incorrect: Yes, It is a flexible means of using all your data for training and testing and in certain cases, there will be no overfitting but you might be neglecting one or more others in the list.</p></feedback></response><response match="c5fc9e40513445579da9386032c1f267" score="0"><feedback><p id="ac5b378d17a54900b55027ac294f3ef0">Incorrect: CV is helpful with tuning parameters.</p></feedback></response></part></multiple_choice><essay id="c88fbbdbed3d437ca11fe93b29c03553" grading="automatic"><body><p id="b022511bc6854915b073e92725088d4c">Additional Space for answer clarification: (Enter N/A if you have nothing to add)</p></body><part id="d64238898c3348bab9a4671db5bbc1a2"><response match="*" score="1"><feedback /></response></part></essay></page></assessment>
