<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newc6e7e55ccdab4390831b3ad6b677956f" recommended_attempts="3" max_attempts="3"><title>Quiz 10</title><page id="b95415be93634eb58c8e0f621a7629f6"><title>Page 1</title><multiple_choice id="d11aa540d1f94f21aeb43a58c78cc916" grading="automatic" select="single"><body><p id="abdde8c50d5749ed9b7421740e16be4d">Refer to the statements below and select the option (s) that<em> best</em> describes what the ROC curve demonstrates:</p><p id="f0622a06a66d446dbc15405e0b02c320">I) The ROC curve shows the tradeoff between the true positive rate and the false positive rate.</p><p id="f3f29d10357e4596a9199d5db4346898">II) The ROC is useful in deciding the optimal models to select.</p><p id="a1941300d5244d11a4903e830c35f66f">III) The ROC curve shows that specificity and sensitivity are directly proportional to each other meaning when the sensitivity increases, specificity increases.</p></body><input shuffle="true" id="db1170822ed54e80aeab0532adba0456" labels="false"><choice value="e81b87925347457ab686466df1c03793">I &amp; III.</choice><choice value="af51c06bc8cb4dcfbabb026fb620b319">III alone.</choice><choice value="d15ffd84d64e4107bba4dd527bba225e">I &amp; II.</choice><choice value="b84e84a982a34f3585685596631ee47b">All the options.</choice></input><part id="f06e0ec19cd74091b60e10e521212da0"><response match="e81b87925347457ab686466df1c03793" score="0"><feedback><p id="bcaa9b61e9a64b3cb53ef7bed4824ec1">Incorrect: Although the ROC curve shows a tradeoff, it shows that specificity and sensitivity are INVERSELY proportional to each other.</p></feedback></response><response match="af51c06bc8cb4dcfbabb026fb620b319" score="0"><feedback><p id="c4f8cc90fb2448fe9ab251f4a204204b">Incorrect: It shows that specificity and sensitivity are INVERSELY proportional to each other.</p></feedback></response><response match="d15ffd84d64e4107bba4dd527bba225e" score="10"><feedback><p id="e39a0db8b65a45798bd3c004641f0e1c">Correct: This is the right option!</p></feedback></response><response match="b84e84a982a34f3585685596631ee47b" score="0"><feedback><p id="c11627114331438ba7d10a985e63ec8e">Incorrect: ROC curve shows that specificity and sensitivity are INVERSELY proportional to each other and this makes all options incorrect.</p></feedback></response></part></multiple_choice><multiple_choice id="f7988897cdae403e909148c92df6aabf" grading="automatic" select="single"><body><p id="d84a9c836e5f4f5aaccbfbf65d6523dc">As a Data Scientist for Bank of America&apos;s fraud detection unit, you are evaluating a model developed to classify appropriate transactions as fraudulent. Using the Specificity metric, what question are you trying to answer:</p></body><input shuffle="true" id="fddbf93c032f47e2b7032bacb271276f" labels="false"><choice value="d25ea55e76c1464886927345ac84414b">What proportion of predicted target class cases belong to the target class?</choice><choice value="ef77e07e66424932ababd9adc58c2d14">What proportion of predicted target class cases were classified correctly?</choice><choice value="a24faed98bd848d48f8fb5e3971b971c">What proportion of non-target class cases were classified correctly?</choice><choice value="cbb970fd8bab447ba06dba7043a31ce5">What is the entire two-dimensional area underneath the generated ROC curve?</choice></input><part id="ae0658ffb1624757a31a5dae10ff7014"><response match="d25ea55e76c1464886927345ac84414b" score="0"><feedback><p id="eeabd36e77164b4ead0046414000259b">Incorrect: This question will answer the question of the Precision of the classifier.</p></feedback></response><response match="ef77e07e66424932ababd9adc58c2d14" score="0"><feedback><p id="add4a10801944c82937b6127b2af09b2">Incorrect: This question will answer the question of the Sensitivity of the classifier.</p></feedback></response><response match="a24faed98bd848d48f8fb5e3971b971c" score="10"><feedback><p id="a0a016f5aea64350a64a1c84952d5ddd">Correct: This is what the Specificity would be seeking to answer. </p></feedback></response><response match="cbb970fd8bab447ba06dba7043a31ce5" score="0"><feedback><p id="e0c9c56ffcf7406987fc4dcd90934c99">Incorrect: This is the Area Under the Curve .</p></feedback></response></part></multiple_choice><multiple_choice id="effdb5ed5f3842598cd51f65608d2c91" grading="automatic" select="single"><body><p id="d82341939ab643d7925ff17697ad816b">This evaluation metric is the measure of how similar a data point is to its own cluster compared to other clusters. A high value would indicate that an observation belongs to its cluster:</p></body><input shuffle="true" id="dda65c64765545efb515f7ccea954a71" labels="false"><choice value="bfee3498c72e4f27b464b713d872e9b8">Silhouette Coefficient.</choice><choice value="a1883e8602a14e93890dbc7e0be7332b">AUC-ROC.</choice><choice value="b793827591274b08885ed9761550f6ba">Mean Absolute Average.</choice><choice value="ac64813f032f4d598f7800d872f985a1">Adjusted R<sup>2.</sup></choice></input><part id="f791c0786f0d41efb07dab097e9b389d"><response match="bfee3498c72e4f27b464b713d872e9b8" score="10"><feedback><p id="eb8ac96fc8124c0b9d9dec6e2dbf5f36">Correct: This technique is useful for evaluating cluster goodness of fit and is calculated  with any of the distance metrics discussed.</p></feedback></response><response match="a1883e8602a14e93890dbc7e0be7332b" score="0"><feedback><p id="f669f3873f494cb884d8ba932b95cc96">Incorrect: This technique is not used for cluster analysis. </p></feedback></response><response match="b793827591274b08885ed9761550f6ba" score="0"><feedback><p id="e65c7c5278fd4de6b976c59f7027168e">Incorrect: This is not a metric to be used for clustering.</p></feedback></response><response match="ac64813f032f4d598f7800d872f985a1" score="0"><feedback><p id="ea475af769374b5581f7e294422eddb7">Incorrect: This is a metric used for regression models.</p></feedback></response></part></multiple_choice><multiple_choice id="da16d7be1f234c99b0af6475f8edf24f" grading="automatic" select="single"><body><p id="fd136972ef8e4fd2a40183262f5657e9">The coefficient of determination is the proportion of the variance in the dependent variable that is predictable from the independent variables. However, it does not do a good job at accounting for independent variables that are not useful in a model and it increases in value as more independent variables are added. What metric can be used in place of the coefficient of determination to solve for this issue:</p></body><input shuffle="true" id="b479de15963b4ddd8dae9ebfead3efaf" labels="false"><choice value="f144682047d5495a8ed179feac26350e">Adjusted R<sup>2</sup></choice><choice value="dce4bc525bb64eb38916bffb4237852a">R<sup>2.</sup></choice><choice value="aa44d5ab44464fbdba8a8a9ce8dcfe53">Dunn index</choice><choice value="f73397ccb0814149b078ec15ceab16c5">Jaccard index</choice></input><part id="c31eb97d15dc43fe9035e8babdab4030"><response match="f144682047d5495a8ed179feac26350e" score="10"><feedback><p id="df072dac35d54cfdb79fe7343617ef61">Correct: Adjusted R-squared solves for the drawbacks of the coefficient of determination.</p></feedback></response><response match="dce4bc525bb64eb38916bffb4237852a" score="0"><feedback><p id="e3a432d3c38547a2bc29cc49eb45f8ae">Incorrect: Coefficient of Determination is also known as R-squared.</p></feedback></response><response match="aa44d5ab44464fbdba8a8a9ce8dcfe53" score="0"><feedback><p id="a21271c95309492c95ce8b0f94b35401">Incorrect: This is a cluster analysis metric that would not be a good alternative for r-squared.</p></feedback></response><response match="f73397ccb0814149b078ec15ceab16c5" score="0"><feedback><p id="c1684119e2654934b4549b51b0c6065b">Incorrect: This is a cluster analysis metric that would not be a good alternative for r-squared.</p></feedback></response></part></multiple_choice><multiple_choice id="e2cfc66ac0764d4d8f0cc37ea1ecf40e" grading="automatic" select="single"><body><p id="ad0d234de6454b48ad968db60d7cf4b1">Based on the confusion matrix below with a validation set of 138, Class 1 reflects the respondents to a targeted advertisement who purchased services and Class 0 reflects the non-targeted respondents who did not purchase services. How would you calculate the precision:</p><table id="a9ea84eef8f949caae823198d7d8184e" summary="" rowstyle="plain"><cite id="i1fc6b7bd57574531b709bff7e4927bba" /><caption><p id="b605b0ce25574c7a8247ff544853bfd9" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ea487aff91424038a4d63e4594c0cc25">Actual Class</p></td><td colspan="1" rowspan="1" align="left"><p id="a52426bce54d44c8a24f01ad189b7ee9">Predicted Class 1</p></td><td colspan="1" rowspan="1" align="left"><p id="cd86bb8795ca44468a73e0fd607a6188">Predicted Class 0</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="fd78a59edd8e443ba925a9d44d64d763">Class 1</p></td><td colspan="1" rowspan="1" align="left"><p id="d75824a360534a3d82440b96afa8e0ca">35</p></td><td colspan="1" rowspan="1" align="left"><p id="d9bb9c85cae14e6d84dc071c351b84b6">20</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="ddaf0946b8684bdb972ee733bdff2f4d">Class 0</p></td><td colspan="1" rowspan="1" align="left"><p id="c31f3c35796244da8857e0fdd404343c">11</p></td><td colspan="1" rowspan="1" align="left"><p id="e7da6fd632064e4db7b89998f6131aa8">72</p></td></tr></table></body><input shuffle="true" id="e8d2c4df9bfe42e3b229e273a95690a6" labels="false"><choice value="e1405c91b108430a995df029167e1532">64%</choice><choice value="c2a1f734bfc4449b9ac194a20e2ee2d9">13%</choice><choice value="fc7022b34f8e46e3a53f851aaf9e5125">33%</choice><choice value="b902994341d64029bc0e1b7751ac06b0">86%</choice></input><part id="d09681d783434c86b21adba85e2d0ab6"><response match="e1405c91b108430a995df029167e1532" score="10"><feedback><p id="a5963ae3363d4b0b80ac3f348358aaaa">Correct: The sensitivity will be calculated as TP ÷ (TP + FP). TP=35, FN=20</p></feedback></response><response match="c2a1f734bfc4449b9ac194a20e2ee2d9" score="0"><feedback><p id="e4949c14612148f58baecabe53c87bde">Incorrect: The sensitivity rate will be calculated as TP ÷ (TP + FN)</p></feedback></response><response match="fc7022b34f8e46e3a53f851aaf9e5125" score="0"><feedback><p id="e669ab5274884dbcac1eb7ab7119bd85">Incorrect: The sensitivity rate will be calculated as TP ÷ (TP + FN)</p></feedback></response><response match="b902994341d64029bc0e1b7751ac06b0" score="0"><feedback><p id="d3e776ae1b7d4de9b510ebdb8547ac64">Incorrect: The sensitivity rate will be calculated as TP ÷ (TP + FN)</p></feedback></response></part></multiple_choice><multiple_choice id="d5327bcebc4f43e2a5a61685f73def52" grading="automatic" select="single"><body><p id="ba6c9c90cf33427ea29f959be2a04255">Using a KNN algorithm, you have developed a model that will predict the number of absences for each 10th grade student in the Pittsburgh Public School District. What metric can be used to evaluate the performance of your model:</p></body><input shuffle="true" id="aac4d46c6f06421ebb910fc12d070e20" labels="false"><choice value="d8ad218d170741fa9165efc93ad51d9c">Root Mean Square Error (RMSE).</choice><choice value="cb8181637f81482184aa0bb14f1ac6a8">Misclassification Rate.</choice><choice value="f055cd0ff6ca49f4ab8fc28f8f8abac4">Accuracy Rate.</choice><choice value="f24b967cef9d4866967bbe6606d5a2e5">True Positive Rate.</choice></input><part id="b917a6a8a12c41f0b9d1200863e0740f"><response match="d8ad218d170741fa9165efc93ad51d9c" score="10"><feedback><p id="ed184b54109f46699aac2ff8f6ed4585">Correct: You are faced with a regression problem and will use this metric to evaluate your model.</p></feedback></response><response match="cb8181637f81482184aa0bb14f1ac6a8" score="0"><feedback><p id="a31c0f6081af4622833ad09467fe4e3f">Incorrect: You would only use this metric when faced with a classification problem. Although you are working with the KNN method, you will be using KNN regression algorithm and not a classification algorithm.</p></feedback></response><response match="f055cd0ff6ca49f4ab8fc28f8f8abac4" score="0"><feedback><p id="bcadd574f0044109baafce0f85f10b0e">Incorrect: You would only use this metric when faced with a classification problem. Although you are working with the KNN method, you will be using KNN regression algorithm and not a classification algorithm.</p></feedback></response><response match="f24b967cef9d4866967bbe6606d5a2e5" score="0"><feedback><p id="c016028f57924675b46332e767b03942">Incorrect: You would only use this metric when faced with a classification problem. Although you are working with the KNN method, you will be using KNN regression algorithm and not a classification algorithm.</p></feedback></response><hint><p id="f9da29f26d7e420f85f4e473485dfea7">You are not predicting whether a student will be absent!</p></hint></part></multiple_choice><multiple_choice id="d6bc35f9f35c4a6f9dabca04dd3c6604" grading="automatic" select="single"><body><p id="d2114c2cf95948e49bf41dc3b7727c42">The R-squared will summarize the percent of variation in response that a regression model explains and is a useful regression metric. You always want a high R-squared but this high value might be misleading. Would you deem this a drawback of this metric:</p></body><input shuffle="true" id="ef90516024c6400391570eb805e65c83" labels="false"><choice value="bb98774123104622ae2362bd02d1219a">Yes, It might classify the true positives in the false positive category.</choice><choice value="ac788df1085a479f951c2abff3e22a48">Yes, It is not as easy to interpret like other metrics because it depends on the scale of data.</choice><choice value="d35f6704cdac4b959afaaa0bc2bfcf94">Yes, It stays the same or increases with the addition of more predictor variables.</choice><choice value="a11d2e90577f46f5985e02f1e6c71650">No,there are no drawbacks to this useful metric.</choice></input><part id="fc523315ccc04e0da4427ba00ac3682b"><response match="bb98774123104622ae2362bd02d1219a" score="0"><feedback><p id="e45a88be370a4b21a3902ec6ce16c86e">Incorrect: It is used for regression  problems and not classification problems.</p></feedback></response><response match="ac788df1085a479f951c2abff3e22a48" score="0"><feedback><p id="ca9416c1183345a5913f2eb334e9d150">Incorrect: It is infact one of the easiest to interpret and it does not depend on the scale of data.</p></feedback></response><response match="d35f6704cdac4b959afaaa0bc2bfcf94" score="10"><feedback><p id="b5159537f6304c3fb715b83b6942fe7e">Correct: This is a major limitation to r-squared. However,  the adjusted r-squared is a metric that overcomes this issues.</p></feedback></response><response match="a11d2e90577f46f5985e02f1e6c71650" score="0"><feedback><p id="d66724893e674d52aec7ee75a96c18c7">Incorrect: It is not an intuitive metric because R-squared does not do a good job of acknowledging the effect of predictors.</p></feedback></response></part></multiple_choice><multiple_choice id="c6e906650ffa49b1846a5d41f5d01de5" grading="automatic" select="single"><body><p id="d0c946ddbcdf4db09cdbf3fb833fd6dc">You have performed a cluster analysis and are missing any known result to assist with evaluating the performance of your clustering. You are aware that cluster analysis can still be evaluated using intrinsic criteria i.e. assessing goodness of fit within the data. What internal evaluation metric can we use in this scenario:</p></body><input shuffle="true" id="aa1a8f7cd05a4fb3bd89f04ca50879f8" labels="false"><choice value="d31b0d91f43a4d73bbf4544936d16fac">Silhouette Coefficient.</choice><choice value="c92f7cf8bcab4178902b7a4a443867d4">Purity.</choice><choice value="dac3a07a69834b0e832be4586741d013">Rand index.</choice><choice value="aa520b7faa1e49579354598859ca154e">F-Measure.</choice></input><part id="aa11d21e4a564b478819b68067371096"><response match="d31b0d91f43a4d73bbf4544936d16fac" score="10"><feedback><p id="af0c55cc5a274372a05da70c73d7de67">Correct: You should use an internal evaluation technique when there is no ground truth data for evaluation.</p></feedback></response><response match="c92f7cf8bcab4178902b7a4a443867d4" score="0"><feedback><p id="af89d07b9797411c9844e3475d7ac6be">Incorrect: This is an external evaluation technique and needs ground truth.</p></feedback></response><response match="dac3a07a69834b0e832be4586741d013" score="0"><feedback><p id="c109d345baff4c6aa178aa52f2a155ba">Incorrect: This is an external evaluation technique and needs ground truth.</p></feedback></response><response match="aa520b7faa1e49579354598859ca154e" score="0"><feedback><p id="ae27beebc2a942d3ba1d84e333532783">Incorrect: This is an external evaluation technique and needs ground truth. Remember this technique from classification metrics.</p></feedback></response><hint><p id="ef5e169c8bf146568edb3f93c782c235">Known result would be prelabeled data.</p></hint></part></multiple_choice><multiple_choice id="c3d4eaca0c37481fbb97b657a04c4eff" grading="automatic" select="single"><body><p id="ee93c9b878d84037b227ca6839e45ebf">When modeling a problem, you are often faced with a tradeoff between interpretability and accuracy.  Are there ways to improve (even slightly) the accuracy of your models without trading interpretability:</p></body><input shuffle="true" id="b50eea120a2444dcb42561c60d75b847" labels="false"><choice value="fb4f18591b9e4abf8cc5779e0fb38270">Yes, you can combine predictions between different models (especially highly interpretable ones) and this has proven to result in better predictions.</choice><choice value="f7db0c2a37004291981d3218381f2e4f">No, this tradeoff means you must sacrifice accuracy for interpretability and vice versa.</choice><choice value="af05ef973ee94974933f4ed9fc6fc810">Yes, but bear in mind that you can not increase the accuracy of interpretable linear models by any means.</choice><choice value="de6cdbfe968943e68df8bbf75e1691d4">No, attempting to create a balance will lead to models that are not useful to the analytic solution.</choice></input><part id="aa8adce686a94da9b243d4c86823aca4"><response match="fb4f18591b9e4abf8cc5779e0fb38270" score="10"><feedback><p id="a36aeac7efe44a4db0239dc61778ba61">Correct: This is a good option to balancing the tradeoff. </p></feedback></response><response match="f7db0c2a37004291981d3218381f2e4f" score="0"><feedback><p id="a9bd6f311793408daac4dd6bb5c24960">Incorrect: This is not always the case, there are options or routes to take to balance this tradeoff.</p></feedback></response><response match="af05ef973ee94974933f4ed9fc6fc810" score="0"><feedback><p id="b11bc7c5e4d84d30ae84ced6f07ef4a8">Incorrect: You can increase the accuracy of traditional models by introducing nonlinear predictors to the model.</p></feedback></response><response match="de6cdbfe968943e68df8bbf75e1691d4" score="0"><feedback><p id="f5b0412e4e7e46719cab7b4150f9698a">Incorrect: Revisit the material for the week, there are ways to increase accuracy or interpretability.</p></feedback></response></part></multiple_choice><multiple_choice id="b08f55f0258049d0a99514a6ccc63f10" grading="automatic" select="single"><body><p id="fb4ffd5dd54745b3b7b431bad368c653">Internal and external evaluation criteria for clustering performance differ in many ways including one of the options below:</p></body><input shuffle="true" id="d63245bff630431891637af57fd4eace" labels="false"><choice value="f9f843e0a1444881a39f946fcae58d37">External evaluation is based on previous knowledge about the data and internal evaluation is based on information that is intrinsic to the data.</choice><choice value="e666b2969a9c4e6fb3c46854a7077c3b">Internal evaluation is based on previous knowledge about the data. and external evaluation is based on information that is intrinsic to the data.</choice><choice value="fd608b0b921f4a3db5d31a26cabe9ce5">Internal evaluation involves techniques such as purity that measure the purity of cluster class labels and external evaluation such as Dunn index that defines the intercluster distance.</choice><choice value="e77d99cb57054843a06c333023ba65d0">External evaluation criteria evaluate the results of a cluster model based on the fit between the data and its expected structure and internal evaluation criteria evaluate the clustering result based on information that is not in the dataset.</choice></input><part id="c69ccb6eadf34e24aebddb074d4296d8"><response match="f9f843e0a1444881a39f946fcae58d37" score="10"><feedback><p id="f958571bac0743c9921af1d6677c6ff1">Correct: When you use an external evaluation technique, you are evaluating the clustering/clusters based on previous knowledge about the data. Internal evaluation looks at the information that is within the clusters and no previously known data.</p></feedback></response><response match="e666b2969a9c4e6fb3c46854a7077c3b" score="0"><feedback><p id="f430fb56914546679500c38aebedb010">Incorrect: This option would mean that internal evaluation uses prelabeled data or ground truth data. This is not the case. External evaluation is based on previous knowledge about the clustered data (aka having prelabeled data).</p></feedback></response><response match="fd608b0b921f4a3db5d31a26cabe9ce5" score="0"><feedback><p id="ed6f7c9689b149fe813c1a810509dc1a">Incorrect: Look closely at this option. External evaluation techniques include purity and internal evaluation would include Dunn index.</p></feedback></response><response match="e77d99cb57054843a06c333023ba65d0" score="0"><feedback><p id="f4f9f492af80469eab780f0e1c736ced">Incorrect: Look closely at this option. Internal evaluation evaluates results of clustering based on the fit between the data and its expected structure.</p></feedback></response></part></multiple_choice></page></assessment>
