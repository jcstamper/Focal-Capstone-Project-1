<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bc51037f932144ffbb61e8b00654c0e1"><head><title>Summary and Quiz 7</title></head><body><ul id="b345fe7f45714df0ab9340bdcc285bc0"><li><p id="ef201df6fa6c4bc6a38fdbd83bad69da"><em style="italic">Training Error </em>is derived by calculating the classification error of a model on the exact data that was used to train the model.</p><p id="d8b10254113a4e5493ff4c150f976de0"><em style="italic">Test Error </em>is important as it gives insight into the amount of errors to expect when making future predictions and it is used for model selection. </p><p id="cd1aca308a854f9386b805b49ece84b4"><em style="italic">Irreducible error</em>, is the noise term in the true relationship that cannot fundamentally be reduced by any model. </p><p id="a251a608a3bb46d19e04848b44d9d27a"><em style="italic">Reducible Error </em>is the error resulting from a mismatch between <em style="italic">f </em>and <em style="italic">f hat</em> or the estimate of the relationship between <em style="italic">x </em>and <em style="italic">y </em>and its true relationship. </p></li><li><p id="dea327879b6f41e287f57008c73a05a7">The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting).</p><p id="d7d443e49f9548dfb7dc7f359727ed34">The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</p><p id="a2c1503de7d5433bb408a55fe4311a60">Bias-Variance can be decomposed as: σ² + Var(g) + Bias(g)²</p></li><li><p id="c97c60ebcf614a97a44224756d7b3d0d"> <em style="italic">Model Assessment </em>and <em style="italic">Model Selection </em>are key concepts of importance to every data scientist and necessary in the model understanding phase.</p><p id="a3c4699966064f4f93231cf5f77eb04c">Cross-validation can be performed by using the Leave one out or <em style="italic">k-fold </em>techniques. Both techniques have their pros and cons. </p></li><li><p id="c8c4c305f02c4653973344cd556c1e7c">In the next module, we will begin exploring the different supervised learning techniques. You will begin to see more about applying CV with those techniques, assessing and selecting the best models using supervised techniques.</p><p id="f0d018599f6942f8ae34692d4b4d0776"><em>Please post your quiz questions in Piazza as PRIVATE or email your questions during the test taking period.</em></p></li></ul><activity idref="newda97168f6d354565945f5c624768282b" purpose="checkpoint" /></body></workbook_page>
