<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newc7b41a6dc19842928a84ed5bd2b88e19" recommended_attempts="3" max_attempts="3"><title>Quiz 9</title><page id="c9e7dade8fb341e48707b862aa6685bf"><title>Page 1</title><content available="always"><p id="b724e6c0d7b045c9b6860b8573117d01">Answer the following questions to the best of your knowledge. Always pick the solution that you believe is the best answer for the scenario or question. If you have any questions about the test, please post a <em>private</em> note on Piazza or send an email.</p></content><multiple_choice id="c444a439db624bcb9e49f107a822c06e" grading="automatic" select="single"><body><p id="f7b3a1383b134156a8283d2481d89365">Which statement below <em>BEST</em> describes the agglomerative clustering method.</p></body><input shuffle="true" id="a8c62bc4a660428999ac8caa67ba9042" labels="false"><choice value="e3a912211cb241a1907affdd8616a7ef">A bottom-up approach that allows for each observation to be assigned to its own cluster at the start of the cluster analysis. </choice><choice value="ba467ad346d14bd4895efe48d9606d0d">A top-down approach that allows for each observation to be assigned to its own cluster at the start of the cluster analysis process.</choice><choice value="aff32d417e034f96992a5bcde5c29e19">An approach that can be done with either the top down or bottom up approach but requires that each observation is assigned to its own cluster at the start of the cluster analysis process.</choice><choice value="c6de2291a51a482992424d0a0b85636f">Each observation is assigned to a cluster and the number of clusters are pre-defined at the start of the cluster analysis process. </choice></input><part id="ba0746fdeb6b4fa393231beddbe6d7f7"><response match="e3a912211cb241a1907affdd8616a7ef" score="10"><feedback><p id="d138b0380dc54b929261ee076cece1fe">Correct: The agglomerative approach starts by having each observation in its own cluster and using pairwise computations, it clusters observations.</p></feedback></response><response match="ba467ad346d14bd4895efe48d9606d0d" score="0"><feedback><p id="b789db9eb915466ba412dbcbbc56c6ed">Incorrect: This is the divisive approach and not the agglomerative approach.</p></feedback></response><response match="aff32d417e034f96992a5bcde5c29e19" score="0"><feedback><p id="a867e8cb04ab4c67b982a8fd4c87c770">Incorrect: The agglomerative approach is a bottom up approach.</p></feedback></response><response match="c6de2291a51a482992424d0a0b85636f" score="0"><feedback><p id="e93d0abd2f5340d4b68d8e90a5f97d10">Incorrect: This definition is quite similar to the <em style="italic">k</em> means clustering technique.</p></feedback></response></part></multiple_choice><multiple_choice id="d18c1e950f2e4237a6304a4f5680895f" grading="automatic" select="single"><body><p id="ae1a54abad454fa3b1b4fe74cd7eeb5e">Consider the <em style="italic">k</em>-Means clustering method and the steps involved in training a model with a basic k-means algorithm. Which one of the following is <em>not</em> considered a step in the training process?</p></body><input shuffle="true" id="c6a034dc7f5a4fc299c489a9679b30d4" labels="false"><choice value="ffda5c3025a243119f577aa869a5cb68">Start by assigning each observation to its nearest observation as a means of forming clusters.</choice><choice value="c9891727a5f44440984600589d25f04f">Start by specifying the desired <em style="italic">k </em>value.</choice><choice value="c0de99175d054726970048ddcb31a97c">Randomly assign all observations to their nearest centroid.</choice><choice value="f8b4db5a25e9421389ee1e5044958a57">Calculate the cluster centroids for each cluster so that cluster assignments can be done iteratively.</choice></input><part id="ec355e0002f543dfbd350231b1a90c3f"><response match="ffda5c3025a243119f577aa869a5cb68" score="10"><feedback><p id="b9d9a8bf3ed04964a0ca88ebd397d6ca">Correct: k means clustering technique will reassign an observation to its nearest centroid and not the nearest observation.</p></feedback></response><response match="c9891727a5f44440984600589d25f04f" score="0"><feedback><p id="baad778cfed544e58368e17d04335fec">Incorrect; This answer would be one of the steps in the k means clustering process.</p></feedback></response><response match="c0de99175d054726970048ddcb31a97c" score="0"><feedback><p id="c5e53c5dfe6a439090335b719528dd3d">Incorrect; This answer would be one of the steps in the k means clustering process.</p></feedback></response><response match="f8b4db5a25e9421389ee1e5044958a57" score="0"><feedback><p id="a960a4d617954bd0ada28e9d3bfc4f6c">Incorrect; This answer would be one of the steps in the k means clustering process.</p></feedback></response></part></multiple_choice><multiple_choice id="dce6b69f6ff5467d84c0215470ddc65b" grading="automatic" select="single"><body><p id="b5d12b03627c4dbb9ce98c8b73ddd30c">This linkage method states that the distance between two clusters, is how much the sum of squares will increase when those clusters are merged, it is also considered to be an alternative to the single linkage method:</p></body><input shuffle="true" id="b73a4187f13a4574bde5363685d277d1" labels="false"><choice value="e80a8f3fa6d44a2aaebcf188bd1c5f5f">Ward&apos;s Method.</choice><choice value="fb71014a5f9c49c8bcaea95bfa4e2bd2">Centroid Linkage.</choice><choice value="ecf91e2330c240f2bd2de00920289da1">Complete Linkage.</choice><choice value="fa9bc201d3fa448ea9faa042a8e10518">Average Linkage.</choice></input><part id="c864728f8db64b7d8fc773b137450e8a"><response match="e80a8f3fa6d44a2aaebcf188bd1c5f5f" score="10"><feedback><p id="e29ab16e89b742a4b0358f228b4866d8">Correct: This method will assess the variance of clusters and it is quite suitable for quantitative variables.</p></feedback></response><response match="fb71014a5f9c49c8bcaea95bfa4e2bd2" score="0"><feedback><p id="ec6e48e1db33498aab62582b12d32082">Incorrect: This method uses the centroid to determine the average distance between clusters.</p></feedback></response><response match="ecf91e2330c240f2bd2de00920289da1" score="0"><feedback><p id="c6bf115ae82a4d7fbe9cec642feebb08">Incorrect: This method involves clustering based on maximum distance.</p></feedback></response><response match="fa9bc201d3fa448ea9faa042a8e10518" score="0"><feedback><p id="dc47da73d70448928a0c8177f5a74121">Incorrect: This method is similar to the complete  linkage but the clusters are formed and the distance is measured  based on the average distance between observations in the cluster to other observations in another cluster.</p></feedback></response></part></multiple_choice><multiple_choice id="c3e45153fd794a0799690058a0600cfd" grading="automatic" select="single"><body><p id="c55da903a87a4ba5ac2f951a48d2626f">The output below (6 cities) shows the distance measurements for a dataset that has been clustered using <em style="italic">k</em> = 2. Cluster A and B centroids were computed and the measurements you see are the distance of each observation from each of the centroids. Which option below is the right grouping in cluster A and cluster B:</p><image id="f90d0fd944f043d793a48cbd8d918ee1" src="../webcontent/Cluster_Analysis1.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="e97bbba45c1444029710c38397063e6d"><em style="italic">Six &quot;Cities&quot; belonging to Clusters A and B</em></p></caption><popout enable="false"></popout></image></body><input shuffle="true" id="ef37b36953f24b98a35457e44439f9f6" labels="false"><choice value="dfbd537116a14aefa3b2aa3bf0513c7d"><p id="a331353c509849b8b9c0e2b01d2e2134">A(Beijing, Dakar)</p><p id="ca09f598374b4f08b65e83ef9035965d">B(Paris, Abuja, Chennai, Seoul)</p></choice><choice value="a06fd2d5a23a4e92963a4c9f39429ca0"><p id="f3d1a4674d5646ba9253cd3633ed78ba">A(Dakar, Chennai)</p><p id="a08a208c8828479ab56085ccdbfb9134">B(Paris, Abuja, Dakar, Seoul, Beijing)</p></choice><choice value="a05d50323cc8479495f2832995bc957b"><p id="f179c75a58604e4d9b0d9a3433ca4cd7">A(Beijing, Paris, Abuja)</p><p id="f402a0336b2948b0881737a9b9813012">B(Dakar, Chennai, Seoul)</p></choice><choice value="b20ea633976745ecad1b789b44a4d623"><p id="d91768b82f7a42f6be6319d633e10e61">A(Paris, Abuja, Chennai, Seoul)</p><p id="a0dc3e8f7fc44538b981ca84e5157d80">B(Beijing, Dakar)</p></choice></input><part id="b52a439d2a434ccdbebac47faed371a4"><response match="dfbd537116a14aefa3b2aa3bf0513c7d" score="10"><feedback><p id="f0c587e2f28e4349913828f1360b4a56">Correct: This is the right grouping!</p></feedback></response><response match="a06fd2d5a23a4e92963a4c9f39429ca0" score="0"><feedback><p id="a1769eb54928408392b993016f186939">Incorrect: Compare both distance measures and cluster a city in the group where it has the lowest value.</p></feedback></response><response match="a05d50323cc8479495f2832995bc957b" score="0"><feedback><p id="b222bb94e5854024ad83f4f90b6ca42d">Incorrect: Compare both distance measures and cluster a city in the group where it has the lowest value.</p></feedback></response><response match="b20ea633976745ecad1b789b44a4d623" score="0"><feedback><p id="c1385494819b49fcae84b18a3298d35a">Incorrect: Compare both distance measures and cluster a city in the group where it has the lowest value.</p></feedback></response></part></multiple_choice><multiple_choice id="aa89828031d14a55956512cb183c0863" grading="automatic" select="single"><body><p id="fc8cf24c6b3448b9a6486077cca1ef98">When performing cluster analysis especially the <em style="italic">k-</em>Means method, you can plot the sum of squared distances or the dispersion, as a function of the clusters to evaluate and select the optimal <em style="italic">k</em>. This value for the optimal <em style="italic">k </em>is typically located at the optimization point. This approach is called:</p></body><input shuffle="true" id="fb360a36482e4386b4ccb33dc04167bb" labels="false"><choice value="c1499f88b99344cda77aec9ee82087d7">Elbow Method.</choice><choice value="c5a735222fea47c1ab0906d1f836ec98">Silhouette Approach.</choice><choice value="c946ed61b3764350a42f52f52657a500">ROC Curve Approach.</choice><choice value="f3e795aad8d7442cb90d5d44a28f54cf">Box Plot Approach.</choice></input><part id="ab55bd3a1d774a0496a816152e52d9a0"><response match="c1499f88b99344cda77aec9ee82087d7" score="10"><feedback><p id="c1565720598b4d779f40c13f9f17dec5">Correct: This is the best graphical approach as it shows the decline of cluster heterogeneity  as more clusters are formed.</p></feedback></response><response match="c5a735222fea47c1ab0906d1f836ec98" score="0"><feedback><p id="ea159cffcd904a55b1ffa8a50fa7b75d">Incorrect: This method measures the quality of the clustering  and determines how well a data point fits to its cluster.</p></feedback></response><response match="c946ed61b3764350a42f52f52657a500" score="0"><feedback><p id="a70328e1184b4a92be5de439f025b442">Incorrect: This is not a straightforward metric to use in determining the optimal <em style="italic">k </em>value.</p></feedback></response><response match="f3e795aad8d7442cb90d5d44a28f54cf" score="0"><feedback><p id="ed8278c961ea43c683f8c97258ad387c">Incorrect: This can allow you to see outlier data but it is not considered the best way to identify the optimal k compared to the other options.</p></feedback></response></part></multiple_choice><multiple_choice id="d70b4ffd13f04edcadbc21c8850df33d" grading="automatic" select="single"><body><p id="a5c37ba4190841a4b26b2a9fef5e49b4">Hierarchical clustering (agglomerative) is known to be fairly easy to understand and it works by clustering observations one after the other. Which statement below is also true about this approach:</p></body><input shuffle="true" id="c99f4700b58b4c0c9742d128e446da22" labels="false"><choice value="f125ff477885463a9cbd78b64f653490">Hierarchical (agglomerative) clustering is sensitive to outliers, and is better suited for smaller datasets.</choice><choice value="cf4ea4e0aed840aca058b0d40bcb27aa">Hierarchical (agglomerative) clustering will usually give you the same solution/results when data is reordered or observations are dropped. This is because it is considered high stability (without the application of a linkage method).</choice><choice value="f4614125771f4dbea726b06ea45f75da">Hierarchical (agglomerative) clustering requires pre-specifying the desired <em style="italic">k</em> and this tends to maximize dispersion within clusters.</choice><choice value="dd3a85d2877b494db462c1edb04cc6cd">Hierarchical (agglomerative) clustering is quite efficient and does not result in long computation times like <em style="italic">k-</em>Means clustering.</choice></input><part id="cf34c2e565c04982ad955cc4e9f1681c"><response match="f125ff477885463a9cbd78b64f653490" score="10"><feedback><p id="f7925637265244d1b86a8452e7144db2">Correct: It is sensitive to outliers and is better with smaller datasets  because a large dataset with large number of clusters will result in a dendrogram that is difficult to interpret.</p></feedback></response><response match="cf4ea4e0aed840aca058b0d40bcb27aa" score="0"><feedback><p id="e8273cae91d04ae194ce5ec3afeac9ae">Incorrect: Hierarchical clustering <em>is</em> low stability but when data is reordered, results are usually different.</p></feedback></response><response match="f4614125771f4dbea726b06ea45f75da" score="0"><feedback><p id="a1d687ff70924d5d87b1395f57721e80">Incorrect: Hierarchical clustering does not require pre-specified value of k.</p></feedback></response><response match="dd3a85d2877b494db462c1edb04cc6cd" score="0"><feedback><p id="cfc52b59b95d4db7956ff40099363207">Incorrect: Hierarchical clustering is not as efficient as k-means clustering and can result in long computation times.</p></feedback></response></part></multiple_choice><multiple_choice id="fb5ebd4c2471439e8772237a53fba306" grading="automatic" select="single"><body><p id="e9110035301e48eb85c00fa5b2593f07">The idea behind using the within-clusters sum of squares measurement for <em style="italic">k</em>-means clustering is that:</p></body><input shuffle="true" id="d8cf1d87d8114c4d8ba41592bc67c5cc" labels="false"><choice value="c2772ba19c2244caa5fbb6a669710183">A good clustering is one for which the within-cluster sum of squares variance is as small as possible.</choice><choice value="bd60967cdc324c42843bc3f339ce6288">A good clustering is one for which the  within-cluster sum of squares is as large as possible.</choice><choice value="ce6b49f925064f69b34db21affcdbbde">A good clustering is not dependent on the within-cluster sum of squares variance, it makes no difference if the  within-cluster sum of squares variance is small or large.</choice><choice value="ae1181d568e7411ba9a210986d59a237">A good clustering is one for which the within-cluster sum of squares variance is equal to the value of <em style="italic">k.</em> </choice></input><part id="ca32d68366c64609abc17b2d467737f3"><response match="c2772ba19c2244caa5fbb6a669710183" score="10"><feedback><p id="cc7c346a70f540c0af7de9943a8bba03">Correct: The WCSS should be minimized to ensure there is as little dispersion as possible.</p></feedback></response><response match="bd60967cdc324c42843bc3f339ce6288" score="0"><feedback><p id="beb219caaabd48b4b54a66e8f1976ecc">Incorrect: This would mean there is a wider spread and this could mean erroneous clustering.</p></feedback></response><response match="ce6b49f925064f69b34db21affcdbbde" score="0"><feedback><p id="d76a84984dab443d825c2a5ebff216df">Incorrect: This would mean there could be wider spread and this will mean erroneous clustering.</p></feedback></response><response match="ae1181d568e7411ba9a210986d59a237" score="0"><feedback><p id="d0377c77cfc74ba5a40cabe54604b960">Incorrect: The value of <em style="italic">k</em> will more than likely not equal the WSS.</p></feedback></response></part></multiple_choice><multiple_choice id="d3949166997e417c9a99830bbad6eb3c" grading="automatic" select="single"><body><image id="bc147295a8f24e6d90a43987865bc35f" src="../webcontent/Cluster2.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="d338e637cb3a496f9438875aed8b90cd" /></caption><popout enable="false"></popout></image><p id="eaeba80fe57749828b90ba85224816d5">According to the figure above, the distance between each cluster is measured as L(a, b) = max(D(x<sub>ai, </sub>x<sub>bj</sub>)) using this method:</p></body><input shuffle="true" id="fc0f706f5a844728b6cbf6f47eec870c" labels="false"><choice value="b7c05f387b0f4188b97caa6b7864c906">Complete Linkage.</choice><choice value="c58d06cf90a3476b9a42a2a3813bd6e5">Single Linkage.</choice><choice value="d7e2cc7acf1047daa701e3338d3efd49">Average Linkage.</choice><choice value="a1007914d2a44614bb661e28037e2c51">Centroid Linkage.</choice></input><part id="f47b3cf734d647a6b9e8c0a583c6510d"><response match="b7c05f387b0f4188b97caa6b7864c906" score="10"><feedback><p id="c8e2b626aee1404295c031325578d74c">Correct:This linkage method involves merging two clusters with the maximum distance until there is only one cluster left.</p></feedback></response><response match="c58d06cf90a3476b9a42a2a3813bd6e5" score="0"><feedback><p id="bd33afe21f6f408295de4e673a780171">Incorrect:This linkage method involves merging two clusters with the minimum distance until there is only one cluster left.</p></feedback></response><response match="d7e2cc7acf1047daa701e3338d3efd49" score="0"><feedback><p id="dcad9137153a4d9fbeedcfba8f1c3707">Incorrect: This linkage method involves merging two clusters with the lowest average distance until there is only one cluster left.</p></feedback></response><response match="a1007914d2a44614bb661e28037e2c51" score="0"><feedback><p id="cba1f49ae6694e65872dcd2b56656ece">Incorrect: This linkage method involves merging two clusters with the lowest centroid distance until there is only one cluster left.</p></feedback></response></part></multiple_choice><content available="always"><p id="bc366469251e4be78831622c10602755">Nike is interested in grouping its catalogue shoppers based on their past transactions and shopping history. The popular shoe brand would like to target specific advertisements to each identified group. Those groups should contain shoppers who are <em>similar</em>. Please use this information to answer questions 9 and 10.</p></content><multiple_choice id="d3e0970ed2944fd69eff2dacbd1ca7d8" grading="automatic" select="single"><body><p id="b40fd368e85041c3a4362a457d9ba2be">Given the information, which statement(s) below would support completing this task:</p></body><input shuffle="true" id="a48bac22b8204d74ae79e24296f53d81" labels="false"><choice value="cb1fcbc7643d411b8a1dbd49c55babbf">You must consider what dissimilarity measures should apply based on the type of data.</choice><choice value="bc0edb327eec49c18d6d76dbbe2da437">You must consider whether the variables in your shopper history and transaction dataset should be scaled.</choice><choice value="bf9a7b22aa5f4d0697fde9cd4740c9cf">You must also determine what type of linkage method would be used.</choice><choice value="ec9766545a0940a08d5d3ba27f9085fa">All the statements provided are correct.</choice></input><part id="eb6928704b654af1aa6f4fb6e458a6e4"><response match="cb1fcbc7643d411b8a1dbd49c55babbf" score="0"><feedback><p id="cb000431895d4d57826e69281c81b6da">Incorrect: This statement is correct but there are other options that are valid as well.</p></feedback></response><response match="bc0edb327eec49c18d6d76dbbe2da437" score="0"><feedback><p id="d710c60924ca4db798b16284be26147b">Incorrect: This statement is correct but there are other options that are valid as well.</p></feedback></response><response match="bf9a7b22aa5f4d0697fde9cd4740c9cf" score="0"><feedback><p id="a37066051cf54190b8817b3c539fd37b">Incorrect: This statement is correct but there are other options that are valid as well.</p></feedback></response><response match="ec9766545a0940a08d5d3ba27f9085fa" score="10"><feedback><p id="b19e44c03d0d4811bda4cf2a6a186561">Correct: All statements listed are correct.</p></feedback></response></part></multiple_choice><multiple_choice id="e62f0fe553904739afc6816abc2d683d" grading="automatic" select="single"><body><p id="d6f1c429f4b745d287aab5f38a255cf2">Assume that you performed a <em style="italic">k</em>-means cluster analysis on the dataset and you have applied the elbow method. Based on the figure below, what is your optimal <em style="italic">k</em>:</p><image id="e0f692f0761749c59e980a9ea65b968b" src="../webcontent/Elbow.jpg" alt="" style="inline" vertical-align="middle" height="298" width="400"><caption><p id="d0882923e13541f29978df083e6e8c43"><em style="italic">Elbow Chart</em></p></caption><popout enable="false"></popout></image></body><input shuffle="true" id="d4fba93e4cfb4decacb985cc1d19a87c" labels="false"><choice value="f8ff0ff4abe64497811929ae06793eb4"><em style="italic">k</em> = 5 is the optimal number of clusters. It is located at the optimization point.</choice><choice value="f7817326c5e9413b8512104af7f99593"><em style="italic">k</em> = 3 is the optimal number of clusters. It is located at the first &quot;knee&quot; of the plot.</choice><choice value="f5a025f477b84448965431ebf433789c"><em style="italic">k</em> = 2 is the optimal number of clusters. The WCSS is not minimized at this point.</choice><choice value="f44979e4fc254f25a51781215f45dadf"><em style="italic">k</em> = 10 is the optimal number of clusters. The WCSS is at its lowest.</choice></input><part id="fc198b7cc5664188821209c3b5e92516"><response match="f8ff0ff4abe64497811929ae06793eb4" score="10"><feedback><p id="b89adb9404f54a1a8f8f1fbb1b1a6aec">Correct: The optimal <em style="italic">k </em>from an elbow chart is the value at the knee of the curve. This is the right value.</p></feedback></response><response match="f7817326c5e9413b8512104af7f99593" score="0"><feedback><p id="af5bde3ef05d4d7e83ea55d4e968f707">Incorrect: The optimal <em style="italic">k </em>from an elbow chart is the value at the knee of the curve.</p></feedback></response><response match="f5a025f477b84448965431ebf433789c" score="0"><feedback><p id="f6e1583ee6c44908be304cbc876f2ebd">Incorrect: The optimal <em style="italic">k </em>from an elbow chart is the value at the knee of the curve.</p></feedback></response><response match="f44979e4fc254f25a51781215f45dadf" score="0"><feedback><p id="d7df4116ec304389aa2b261db959a391">Incorrect: The optimal <em style="italic">k </em>from an elbow chart is the value at the knee of the curve and the WCSS should be at the lowest possible, anything after the optimization point is not significant.</p></feedback></response></part></multiple_choice></page></assessment>
