b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f65523c9e29b4324bcb047ee80427ae7"><head><title>[Research paper] Attention is all you need</title></head><body><p id="fddfb486f9ab447c8a815c0d774f8705"><em>Citation</em>: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. <em style="italic">Advances in neural information processing systems</em>, <em style="italic">30</em>. <link href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="new" internal="false">[pdf]</link></p><p id="f0a3074b133f4d979b5cf49e023fd45b" /><p id="d8c0ded79891464bb7b1e549c2e5c833">The content of this paper is essentially similar to the Language Representation and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.</p><p id="c363fefc5407442986f501926a56b894" /><p id="dab674e2a4de416a8bbb31bb2a0246f1"><em>Who are the papers&apos; authors? Why are they qualified to write about this topic?</em></p><p id="c0186f45194a4646857c47e2e5dddc4d">The authors are a group of researchers and engineers from Google Brain, Google Research and University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they have worked on various language research projects related to Google\xe2\x80\x99s services, such as language translation, speech tagging and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.</p><p id="a440c8c09e3340b99dcfd7c5532a4213" /><p id="d5f0eb25190b474db5338ac3a4e616e5"><em>Who is the audience of the paper?</em></p><p id="c8d048f849cd4ebbaca097cbac3501a5">The paper is targeting ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.</p><p id="ecff4d0804874305bfb6cb76c4d4849b" /><p id="dd196df82e7d49c48ce5d8c5f6423fc6"><em>Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?</em></p><p id="c708571a21454989bc32602529a79043">Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasn\xe2\x80\x99t a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.</p><p id="af760375ec4540879960a095dde26dad" /><p id="b6b871b2ce444accbbd925abfef79c25"><em>What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?</em></p><p id="e2c1042f37ef46119f6022f9679470b3">There are two primary innovations from the paper.</p><p id="df823b8d4eaf48349c07e3dafa737ba6" /><p id="cc771a6fbd514437a7ed868fc063a58e"><em>Positional Encoding</em> is a novel way of representing word order in a sentence. Given a sentence \xe2\x80\x9cI like data science,\xe2\x80\x9d an RNN knows that \xe2\x80\x9clike\xe2\x80\x9d comes after \xe2\x80\x9cI\xe2\x80\x9d because it processes the tokens sequentially and therefore receives \xe2\x80\x9cI\xe2\x80\x9d as input before \xe2\x80\x9clike.\xe2\x80\x9d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(\xe2\x80\x9cI\xe2\x80\x9d, 1), (\xe2\x80\x9clike\xe2\x80\x9d, 2), (\xe2\x80\x9cdata\xe2\x80\x9d, 3), (\xe2\x80\x9cscience\xe2\x80\x9d, 4)]. In addition to learning the embedding of the tokens, it will also learn the encoding of these index locations and therefore the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, $PE_{(pos, 2i)}$ and $PE_{(pos, 2i+1)}$, because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens, because their ordering within the input sentence has already been represented by the index locations.</p><p id="f9b8f0a0b87c4f4688c6f36426dca07e" /><p id="db39579adfa5481c944cb123da80f285"><em>Self-attention</em> is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it \xe2\x80\x93 for example, it may know that \xe2\x80\x9cback\xe2\x80\x9d has different meanings in \xe2\x80\x9cI came back from work\xe2\x80\x9d and in \xe2\x80\x9cmy back hurts\xe2\x80\x9d because it attends to the token \xe2\x80\x9ccame\xe2\x80\x9d in the first sentence and \xe2\x80\x9churts\xe2\x80\x9d in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title \xe2\x80\x9cAttention is all you need\xe2\x80\x9d comes from. As an unrelated note, the template \xe2\x80\x9cX is all you need\xe2\x80\x9d subsequently became popular in the machine learning literature, with a <link href="https://arxiv.org/pdf/2201.09792.pdf" target="new" internal="false">recent paper</link> from CMU that both makes use of it and pokes fun at it.</p><p id="adeefd382b674b3bb2c8be6b46a40c0f" /><p id="c79fddaf2a3548e9a3fa383bd70016b3"><em>Summarize the paper\xe2\x80\x99s experiments and findings</em></p><p id="d3e33ea4fc03446eb82e85774e69722a">The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and output sequence of tokens (e.g., a French sentence):</p><p id="bb5f3590a80f4d61b81fb2ca35e6bdfc" /><p id="ffd6a111003a4c3fa7fb759bae5d3535"><em>Step 1</em>: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., $PE_{(pos, 2i)}$ or $PE_{(pos, 2i+1)}$ to yield the word vector with positional information for each token.</p><image id="d9eefe9bedc8471ebfaac3721796b697" src="../webcontent/image-f65523c9e29b4324bcb047ee80427ae7-1.png" alt="" style="inline" vertical-align="middle" height="201" width="287"><caption><p id="fa599cc7e5704c86a0a3373ea7e7d5d8" /></caption><popout enable="false"></popout></image><p id="c1232f0f9c0b4f44a49618fd95ac8428"><em>Step 2</em>: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed $h = 8$ times with different, learned linear projections to different dimensions. Thus, every input token yields $h$ attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.</p><image id="fa13cf648f71410a904bedb0be6f573c" src="../webcontent/image-f65523c9e29b4324bcb047ee80427ae7-2.png" alt="" style="inline" vertical-align="middle" height="279" width="226"><caption><p id="dd5cdf58f3584d78a7a3782261ccfdc6" /></caption><popout enable="false"></popout></image><p id="ba975961fd984f6a90ae91360933c450" /><p id="ac3ddeefabc14c40b25e15b45c61ba3c"><em>Step 3</em> is similar to Step 1 but carried out on the output sequence tokens.</p><image id="c288a15b2e4a4e349e511bff6fb96345" src="../webcontent/image-f65523c9e29b4324bcb047ee80427ae7-3.png" alt="" style="inline" vertical-align="middle" height="169" width="213"><caption><p id="d9f32b799f5042fc835e64acfbc8577f" /></caption><popout enable="false"></popout></image><p id="f5dbbf6a17b74abab5c8c91ea7edf663" /><p id="be1f365157c645d6a37c670b9eca87d3"><em>Step 4</em>: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens <em style="italic">before and including it</em> in the same sentence (this is where the term \xe2\x80\x9cmasked\xe2\x80\x9d comes from \xe2\x80\x93 we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in <em style="italic">both the input and output sequence</em> to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).</p><image id="f9f16464f9e645089496df3162a1336b" src="../webcontent/image-f65523c9e29b4324bcb047ee80427ae7-4.png" alt="" style="inline" vertical-align="middle" height="327" width="185"><caption><p id="d076c8f7d2824071b1acac966449e662" /></caption><popout enable="false"></popout></image><p id="f68c5ad4277c4d48909e46bf686c5eaf" /><p id="e6b1b1c8a0544b9daa713cf0d015cd0c"><em>Step 5</em>: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).</p><p id="ee61e2d812334631ace2194379a084e7" /><image id="a38718fad9f249b8a96be2a922a2ea03" src="../webcontent/image-f65523c9e29b4324bcb047ee80427ae7-5.png" alt="" style="inline" vertical-align="middle" height="157" width="136"><caption><p id="ccd8585807ab40e0832613f370624bef" /></caption><popout enable="false"></popout></image><p id="c9aaf41da5c947fea7f4ad3171589e29">Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.</p><p id="e292174b90ec40ea8b728d9c809bfede" /><p id="f7fdd8ff187d4369b57406f701985bc6">During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Seq2Seq models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.</p><p id="a54e3c0647da4eb9af52345900cb6257" /><p id="fe7bf17e1d084e4c97833c09a290f766">The above architecture is evaluated on two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed better <link href="https://en.wikipedia.org/wiki/BLEU" target="new" internal="false">BLEU</link> scores than previous state-of-the-art models, at a fraction of the training cost.</p><p id="d49590104edc4b9cb1daf6f9ca4a57bb" /><p id="c90e66a510d34f97b19c1e11e1945008"><em>What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?</em></p><p id="c5dd4f8198844ff7aa78ac038c9f8c3c">The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research, following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as <link href="https://huggingface.co/docs/transformers/model_doc/bert" target="new" internal="false">BERT</link>, <link href="https://openai.com/api/" target="new" internal="false">GPT-3</link> and <link href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" target="new" internal="false">T5</link>, which anyone can utilitize for their own projects.</p><p id="b14c6d9b7a0b4849b517aadf1939472c" /><p id="f9fc7fd2ad294087a4fc461b67830921" /></body></workbook_page>\n'