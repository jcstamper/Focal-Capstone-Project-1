b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f2b72539c48944dda3e74af784c6f6cf"><head><title>Bidirectional Encoder Representations from Transformers (BERT)</title></head><body><p id="ece270514eeb438f8c99de3afc748510">Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.</p><section id="c98aeaf8c7244f68b4b9acc53c68c5bd"><title>Encoder-Only Models</title><body><ul id="ee043f990d0146edaab144a96fc53ba8"><li><p id="d94d5242ae80471da4e5dca4cb853e78">Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \xe2\x80\x9cbi-directional\xe2\x80\x9d attention and are often called <em style="italic">auto-encoding models.</em></p></li><li><p id="f136af7ed9f34d7f9fedbc35c05a645c">The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p></li><li><p id="a2971cf676394c8380bed8352707a8c7">Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.</p></li><li><p id="aae911402fab411b802df960a0027ae4">Representatives of this family of models include BERT, ALBERT, RoBERTa.</p></li></ul></body></section><section id="f2e153529de14bc18d592e9e450d575f"><title>Decoder-Only Models</title><body><ul id="f5ca13a677b2471bb1f1b073561d0b2a"><li><p id="d8d658f542c3477f82e6ac814facbcfa">Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em style="italic">auto-regressive models.</em></p></li><li><p id="b6fd4a409f1a4bbf96660cdbbebf78ea">The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p></li><li><p id="d087c734fe8a4e03866161f9f8635e53">These models are best suited for tasks involving text generation.</p></li><li><p id="fe5dcd0461df472fb6b32ef8b8d3a113">Representatives of this family of models include CTRL, GPT, GPT-2.</p></li></ul><p id="e060d03aaa0343a687d37215374f9bbe">We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.</p></body></section><section id="cb3e5515a5cf4e8483a00870445d7252"><title>BERT</title><body><p id="e363e71f746d42a988502dcaee8d7a24">One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.</p><p id="b35779f9e08d43f390ebc893503c05d3">BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms \xe2\x80\x94 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\xe2\x80\x99s goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.</p><p id="ae1d2ebc866b4d0887aeabd4a1af619e">As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\xe2\x80\x99s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.</p><p id="a115627b56304dffa8a5f99633502519">The original BERT <link href="https://arxiv.org/abs/1810.04805" target="new" internal="false">paper</link> presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.</p><ul id="d7e84f43ee034880924086eb80945ab8"><li><p id="d97a5610f0c74d438202644143bfeb21">BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.</p></li><li><p id="d8564e9b41b64a9c8b12ec04ba8a9c55">BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.</p></li></ul></body></section><section id="ede603fb53ac4855822bdbd3f1954062"><title>BERT Training</title><body><p id="b68398cd71e74c5ea82f1db89b7d67dc">For any NLP task, BERT is generally trained in two steps:</p><ol id="bfbc7bf57a424c8e9e543790a7e8e7bd"><li><p id="c211f55b1a4e42e993ed67aabcd0a749">First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.</p></li><li><p id="b9422eae98184e23a654b3b6be7faccb">Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.</p></li></ol><p id="fd22b26bf0b34abe883a2535240119ba">The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.</p></body></section><section id="fe34cd686ed140cfb040ad039ca5408d"><title>Masked Language Modeling (MLM)</title><body><p id="c9ad9e9277cb4d90bbea25854880554b">Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERT\xe2\x80\x99s prediction of the masked token.</p><image id="abfd81aec4c848ae8be660c941b8c231" src="../webcontent/image-f2b72539c48944dda3e74af784c6f6cf-1.png" alt="" style="inline" vertical-align="middle" height="315" width="468"><caption><p id="bf5308458a3b44b19f145504a0a2dbf8">Figure 1. Illustration of masking and input flow across the model in BERT.</p></caption><popout enable="false"></popout></image></body></section><section id="f6d4e49656d643898b678752b90de138"><title>Next Sentence Prediction (NSP)</title><body><p id="c96e778354c4434bba35caa595fc1538">While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.</p><p id="d84513bc12d945108c8418fc62ed306c">Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.</p><image id="c97cd8ce3e844a23975c2e952d0c2b0c" src="../webcontent/image-f2b72539c48944dda3e74af784c6f6cf-2.png" alt="" style="inline" vertical-align="middle" height="314" width="468"><caption><p id="a9220fc16ba24c16bbb0c680221d808f">Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training.</p></caption><popout enable="false"></popout></image><p id="a523b60592d747b0b531f1ecb19f9663">When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.</p></body></section><section id="f96a9f2f95624eff8ead0f91daac70fa"><title>Using BERT</title><body><image id="aaaee56927264f7b83bedca138b0c9b7" src="../webcontent/image-f2b72539c48944dda3e74af784c6f6cf-3.png" alt="" style="inline" vertical-align="middle" height="191" width="468"><caption><p id="ecf6078a100045d8930be76bbd14d898">Figure 3: Overview of fine-tuning BERT and usage of BERT in various downstream tasks.</p></caption><popout enable="false"></popout></image><p id="d7d58175de2b4f1cafd4689a9bcecb8c">Pre-trained BERT models can be used for a wide variety of language tasks by fine-tuning. Some examples are:</p><ol id="e1cb363f13fb441bb264612fa70e5f1b"><li><p id="e744f2d5c83f4856b44fee1be3babbf4">Classification tasks such as sentiment analysis are done like Next Sentence Prediction classification, by adding a classification layer on top of the Transformer output for the [CLS] token.</p></li><li><p id="a22d2ec05aa14d0b8cce511b6595c2ad">Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&amp;A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.</p></li><li><p id="b1367668a46c4dbd9e8d5fe3b966edb4">Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.</p></li><li><p id="abc7a16b57754e29bcd608ffe5c0fcd0">Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.</p></li></ol></body></section><section id="ba6ec33a6331436988b45da9814c592b"><title>BERT Variants</title><body><p id="c1bbf68d9f984d63a2a88efa429796a3">The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.</p><ul id="eb2dfb8516eb4b91a21765d10a7211d6"><li><p id="b35813600f664b1b9aface872cd5c865"><em>ALBERT</em> (A Lite BERT)</p><ul id="baf432524be84338b80412e814aec156"><li><p id="bb7c802075e44daa93df0488a8789feb">ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.</p></li><li><p id="d564c4b03f194893a5fe8a621868e4c4">To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.</p></li><li><p id="b71b1155c3114c1aa8d701186106177d">In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a \xe2\x80\x9cclassification model\xe2\x80\x9d where the goal is to \xe2\x80\x9cclassify\xe2\x80\x9d whether the 2 given sentences are swapped or not i.e., whether they are in the right order.</p></li></ul></li><li><p id="cffd1399f85449eeaabfc4bc9d878e7a"><em>DistilBERT</em> (Distilled BERT)</p><ul id="ef4297b275ea4248850ed662f2582cfd"><li><p id="e521e9953f8b4b98aaaa88b7e9297f06"><link href="https://medium.com/huggingface/distilbert-8cf3380435b5" target="new" internal="false">DistilBERT</link> has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERT\xe2\x80\x99s performances. It reduced the number of layers in BERT by a factor of two.</p></li><li><p id="d9d4a191665c4d058868add9190c86a7">DistilBERT uses a technique called <em style="italic">distillation</em>, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called <em style="italic">teacher-student training</em>.</p></li></ul></li><li><p id="aa620440a81549e3a8021bc30efc3950"><em>RoBERTa</em> (Robustly Optimized BERT pre-training Approach)</p><ul id="d207b8633ab04e48a53ad9a686c3f694"><li><p id="c5939d9274884a70bf3e876cbeed7ab7">Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.</p></li><li><p id="c9ef46dbdeee4a17b3632b473880b26f">To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERT\xe2\x80\x99s pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.</p></li></ul></li><li><p id="b8beec898d34427486c06065f8939af2"><em>ELECTRA</em> (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</p><ul id="abd13ea750214c73ba94d5eaf78cf89d"><li><p id="fe24be8d31164622a22f2b7e3e44ce0f">Instead of MLM for pre-training, ELECTRA uses a task called \xe2\x80\x9cReplaced Token Detection\xe2\x80\x9d (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.</p></li><li><p id="d3baa73843b247c58afd274d75dee7a1">This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.</p></li></ul></li></ul></body></section></body></workbook_page>\n'