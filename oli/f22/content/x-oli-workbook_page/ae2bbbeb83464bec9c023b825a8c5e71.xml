b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="ae2bbbeb83464bec9c023b825a8c5e71"><head><title>Hypothesis Testing</title></head><body><section id="fd150814d44045e1a5603a42de99c2b6"><title>Introduction to Statistical Hypothesis Testing</title><body><p id="d15fb6fa80994152b496b162654a8f60"> </p></body></section><p id="aee03bcc6fd0405699c96eb4eb159a99">When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly \xe2\x80\x9cdifferent enough\xe2\x80\x9d for you to pick one model over another.</p><p id="d3e4331442a345b38824fc6bf6340404">To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set , with your label also being uniformly at random chosen from  . If you run k-means cross fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but \xe2\x80\x9cgood enough\xe2\x80\x9d classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are <em>no</em> associations between the features and the label.</p><p id="b1d2d30c48754efe9850c7dfdbf2dbbc">Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.</p><p id="e9ea7da7ffda47c98641ae31b04ec9cc">In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a <em>hypothesis test</em> on your performance measures.</p><p id="cfe4c249c45540f2a63828b1a2c91561">A <em>hypothesis test</em> is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a <em>population parameter</em>, and decide if you have a <em>statistically significant result.</em></p><p id="efc058960a964abdadaae97e37f9d187" /><p id="b08803bcfb684bf380afa2d2839a8d71" /><p id="f9d079ef31b34bff867a42ab6c36b317" /><p id="dfa3c5f99abd49ef979cb61abecc37e9" /><p id="a783ae8ebad2423d98e6962d108f8f25" /><p id="c79a5f71d9a04772bd1af76a29f64d76" /><p id="a08f4a21f96a47bbbfef4cb3fc763a61">&#x1f4a1; Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.</p><p id="c4bd1d595e9e48289fcf4fb7eb7ac410" /><p id="e08475e0c80d41dea1392ac8adb17162" /><p id="f1d014130fb143ffb90c3656ac8fc79a">Performing a hypothesis test involves three major steps:</p><ol id="b8509d68ab574ab988be06842afafa4c"><li><p id="f6a842945d4b42ecbdd548c913ec23a4">Deciding what is your <em>Null Hypothesis,</em> , and what is your <em>Alternative Hypothesis,</em> . This will depend on the test you perform, but in general  refers to what you wish to \xe2\x80\x9cdisprove\xe2\x80\x9d, and  refers to what you wish to demonstrate as more possible than the null.</p></li><li><p id="c482207c0c7e4a839a1b949c53bcd5a3">Computing some sort of \xe2\x80\x9ctest-statistic\xe2\x80\x9d. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.</p></li><li><p id="a6d0f5e0ab3247baa75cd279511e2b7f">Looking up the p-value for that test-statistic, and comparing it to some pre-defined confidence \xe2\x80\x9cthreshold\xe2\x80\x9d, . This  is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than , we can reject the null, and tentatively suggest the alternative is more possible.</p></li></ol><p id="f8e2568fce5740eabdae15495b3002e4" /><p id="ee58574e90324266be482bc4777ec19f">&#x1f4a1; It is important to note that \xe2\x80\x9crejecting the null\xe2\x80\x9d does NOT mean \xe2\x80\x9caccepting the alternative\xe2\x80\x9d. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.</p><p id="cb8275a7f7dd459ca73dbad04d4115a2" /><p id="aacf388eac5942cf9d463185e0737c6b" /><p id="daf8fc3960b24869961db39ec3b7a208">When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are <em>unlikely</em> given the assumptions and evidence; they are <em>NOT</em> iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.</p><p id="d2deb927b8a342ee85b89468b7699168">That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.</p><p id="aeed865772d245f4a65f7e1646d0a80b">Without further ado, let\xe2\x80\x99s discuss the first statistical test of this chapter, and one of the fore-runners of hypothesis testing: <em>Welch\xe2\x80\x99s t-test.</em></p><p id="d70c094d19594a8ab44b5a7e22797135" /><section id="bc2d64713f8347e8936f6415350335c9"><title>Welch\xe2\x80\x99s t-test</title><body><p id="b719c2cdc7b94352a3ac6311bc49972e"> </p></body></section><p id="ed2a5ed40450446094e0f756b10cc82d">Something we generally wish to do when we compare different metrics or other values about data or models are means, or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e. is it likely to have happened due to random chance, or not.</p><p id="e40ae460ce074bf090769e751fe46d75">If we want to compare these means $\\mu_\\alpha$ and $\\mu_{\\beta}$ against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.</p><p id="ca195aefac2246f39ff6a9f2177ab784" /><p id="ae3cf2c5e76f412c880b4cb46f11e97e">We could try to test if they are just different from each other with the following hypotheses:</p><ol id="d8038831cffa4f34a1a506570be3736f"><li><p id="ebae68fb64354a59870ddf301609a4a5"> </p></li><li><p id="a9b23139885f40509ddb57f2eb51ddbe" /></li></ol><p id="b07a7a37feed4669bc8ef5379a1fd78b">Or we could test that one is strictly larger than the other:</p><ol id="e89951590af543a89bb0c83199c4b998"><li><p id="f7afc3987c134954ada1efe56453db69" /></li><li><p id="dbb216d4935a4823893d1a4b6e79d668" /></li></ol><p id="df96e996f0884a269550da1c3633024a" /><p id="d249e53cc19247f4be6e39cc4bc2e482">The first kind of test is known as a \xe2\x80\x9ctwo-tailed t-test\xe2\x80\x9d, while the second is known as a \xe2\x80\x9cone-tailed t-test\xe2\x80\x9d. Either way, we\xe2\x80\x99ll end up following the same procedure, so we\xe2\x80\x99ll continue onwards with our next goal: figuring out what sort of test-statistics we wish to compute. Generally, these test-statistics come with their own particular distribution, from which we can calculate a \xe2\x80\x9cp-value\xe2\x80\x9d, or the probability that such a test-statistic can happen given the null-hypothesis.</p><p id="ed7845723c5f47bbb23273b134877819">In our case, we have two averages and want to look at their differences. For the student\xe2\x80\x99s t-test, we shall use the aptly named \xe2\x80\x9ct-test statistic\xe2\x80\x9d:</p><p id="effd3c987eea4cfda7ad991abe619bac" /><p id="e2f576b2394e4b67bcdc2047e7f70f33" /><p id="bda295de447144c7b8e64e27a6d384f7">In particular, we are going to use what is known as \xe2\x80\x9cWelch\xe2\x80\x99s t-test statistic\xe2\x80\x9d, which is used when we have two averages with potentially different variances. This $T$ value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, better when we have less <em>degrees of freedom, i.e. approximately less samples</em>. To calculate the degrees of freedom, we simply need to compute the following:</p><p id="f4009e4fb5f640de9bac331fba8d468a" /><p id="ef91e8b2d90148a28ff392af5877b9f0" /><p id="a44ac69f367f414885cea575db9b34ab">With these values, we can then compute the <em>p-value</em>, or the probability that our null hypothesis holds given our parameters. If this p-value is less than some pre-defined value, then they are different, and we can be more confident that we have different results.</p><p id="d7d9c18c88de42fa8a16110f14b55ff1">While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets, and try this test on two models to see if they have differing performance.</p><p id="ec0ff9e532a24c1a80418c056391411e">However, there are some problems with this testing procedure as-is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.</p><p id="b3491167d7084d388bd510679ba471b6">Additionally, we cannot use the test as-is without heavily segmenting the dataset. If we do not have enough data, or wish to apply something more sensible than simply splitting the dataset three ways and applying k-fold CV to each section, we will need to account for that.</p><p id="add9a2f1bb864dde82f13c30b1bf5c5c" /></body></workbook_page>\n'