b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="cca5fe50ca85452fb8738a54be27af6f"><head><title>OLI Data Structures - Nikhil</title></head><body><section id="f29030eede89479bb8189b0be9bd4b85"><title>Learning Objectives</title><body><p id="e9684f636e164e6bb44e037dccbb5924"> </p></body></section><ol id="edc7ae1e2a874839b9c75a5b4530a01d"><li><p id="f21200a199364a3c8aa07c72a3c29c7e">Analyze the growth rate in runtime of an algorithm in terms of the input size.</p></li><li><p id="c8d9c2ace116420587db360a5c2c64df">Explain the working mechanisms of common data structures: list, tuple, dictionary, set, and tree.</p></li><li><p id="d5603378111b4c489284f06d8533e748">Select the appropriate data structures to fulfill given tasks and constraints.</p></li></ol><p id="f3f5db803c2c418f839b5c108316662f">This module will cover the basics of data structures in Python, with a focus on helping you identify the use cases of different data structure operations. In combination with the Data Structures Primer on Sail(), this module will prepare you to tackle Project 1 effectively.</p><p id="fe60a84c67974793b8e218488480eeb3">We first introduce two overarching concepts that will serve as the basis for characterizing data structures and their operations.</p><section id="d84b44f667c945da889a9c4743e89844"><title>Time and Space Complexity</title><body><p id="cb07f7584ce844a8894a7056d032938c"> </p></body></section><p id="fb0d6426b2e74df4bc775080257a88c9">Algorithms and operations in computer science are often evaluated against two metrics: how much time they take and how much memory they consume. From a systems perspective, there are precise metrics that can capture these dimensions, for example wall time / CPU time for time cost and RAM usage for memory cost. From an algorithms perspective, however, we are more interested in characterizing how the time and memory cost grow with the size of the input, i.e., the time and space <em style="italic">complexity</em>.</p><p id="b1c70d8bb089435fae73e888925d78d9"><em>Example 1</em>. Given a positive integer S, compute the sum 1 + 2 + \xe2\x80\xa6 + S.</p><p id="feaf20d66f154068a021d571de617028">There are two solutions to this problem. The first is to use a for-loop to compute the sum:</p><codeblock id="ee25be7e76b94f8db7d2f2f5b5ac6b52" syntax="text" highlight="" number="false" start=""><![CDATA[def sum(S):\n  t = 0\n\tfor i in range(S):\n\t  t = t + i + 1\n\treturn t]]></codeblock><p id="ceb5e3563cba4b21a2ff8fb388567609"> The second is to apply the math formula 1 + 2 + \xe2\x80\xa6 + S = S*(S+1)/2</p><codeblock id="c815a85f1cb44aa4a5731a2f163ad93b" syntax="text" highlight="" number="false" start=""><![CDATA[def sum(S):\n\treturn S*(S+1)//2]]></codeblock><p id="cc72e606fa1d4d25bbe5c5c43a3cdd10">To characterize the runtime complexity of these solutions, we need to identify (1) what is the input size, and (2) how does runtime scale as the input size grows larger?</p><p id="d285efc500b2444fa261ad06021758b0">In the first solution, we are executing a loop with S steps, so S is the input size and the runtime grows linearly with it (if S is 10 times larger, there are 10 times more steps to perform).</p><p id="b11c9e26cc924d36af9f68157468f403">In the second solution, we are multiplying two numbers, S and S+1; as integers are stored in binary format (with 1\xe2\x80\x99s and 0\xe2\x80\x99s), the runtime of this multiplication is dependent on the number of bits required to represent S and S+1, which are both approximately log<sub>2</sub>(S). As for how the runtime scales with this input size, this depends on the underlying multiplication algorithm; if we assume the standard long multiplication approach taught in grade school, where every digit in the first operand is multiplied by every digit in the second operand, then we can expect a <link href="https://math.stackexchange.com/questions/226394/time-complexity-of-binary-multiplication" target="new" internal="false">quadratic scaling</link>.</p><p id="fd28a84ba477478f9f88b9524a42429f">A compact way to represent these different complexity notions is with the big-O notation. Mathematically speaking, for given functions <em style="italic">f</em> and <em style="italic">g,</em> f(x) = O(g(x)) implies that f(x) is upper bounded by M*g(x) for some constant M, as x <em style="italic">grows large</em>. What this means in our case is that we can characterize the runtime of all algorithms that scale linearly with the input size as O(N), and the runtime of all algorithms that scale quadratically with the input size as O(N<sup>2</sup>), where N is the input size. With the two example solutions above, their runtime complexities are O(S) and O(log(S)<sup>2</sup>) respectively.</p><p id="d3b573dda2a5469dbf1ccdf4f52e4c45">An advantage of the big-O notation is that it allows you to focus on the most dominant term. An algorithm that takes 2N + 1 steps and an algorithm that takes 5N + 700 steps can both be characterized as O(N) because of their linear scale \xe2\x80\x93 the other co-efficients don\xe2\x80\x99t matter for the purpose of complexity analysis. Similarly, an algorithm that takes 100N<sup>2</sup> steps and an algorithm that takes N<sup>2</sup> + 15N + 720 steps can both be characterized as O(N<sup>2</sup>).</p><p id="c2e4b4569b604a23b0cf2b52db147c4c">With this notion in mind, we can refer to this <link href="https://www.freecodecamp.org/news/all-you-need-to-know-about-big-o-notation-to-crack-your-next-coding-interview-9d575e7eec4/" target="new" internal="false">chart</link> to see how different runtime complexities reflect the runtime growth rate. In general you should avoid algorithms with quadratic runtime or above, because those scale up very fast with the input size.</p><p id="cf73a69251974f54a2d9803d730a1cd0">Space complexity behaves in the same manner; if your space consumption is dependent on the input size, you can also characterize its scaling via the big-O notation.</p><p id="fee260df3d6e4398b3863f95f764717e">As a closing note, keep in mind that:</p><ul id="cb3128b28a184d6d8660f3ce3a67d6f4"><li><p id="b863684892ad4a0c83be44df0a1a9d5a">The input size may differ based on the specific algorithm. Both solutions to Example 1 take S as the input, but the input size is S itself in Solution 1 and the number of bits in S (i.e., log<sub>2</sub>(S)) in Solution 2. Identifying the correct input size is crucial to analyzing the runtime and space complexity.</p></li><li><p id="c42ffa9a874c412b99a687536aa3157e">There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime). Dynamic programming, for example, makes heavy use of this technique. There are cases where you can reduce the runtime complexity while maintaining the same space complexity (such as in the two solutions above), but they will often require mathematical insights or shortcuts to simplify certain operations.</p></li></ul><section id="d406bae95b1f40aabd1df27276b4bbb6"><title>Data Structures</title><body><p id="c2cffde14b4c4f13b75741676eed7aee"> </p></body></section><p id="f237de38289849ea9bc694b6c2817e7a">A data structure is a particular way of organizing data in a computer. This organization affects the space required for storage and the time required to process it by different algorithms. Learning to use the right data structure for different use cases is an essential skill for writing efficient programs and building complex applications.</p><p id="c3dc24240fb145b4b5938ab0d095a773">We will now have a look at the most important data structures in Python and understand the complexity of the operations involved.</p><section id="ab666de533be466da6a6e5eeb749d259"><title>List</title><body><p id="d80df272299246628370a967ec67d178"> </p></body></section><p id="e4b52d1335874f29827f8917b8b9a5af">A list is an ordered data structure, similar to an array, which stores items in a sequence. A notable difference between Python lists and arrays is that arrays typically all have the same type of element. For example, in Java you will have an integer array or a boolean array. However, in Python your list can include more than one type.</p><section id="a760f91b5d2049b0a6de529c7cabee59"><title>Element access</title><body><p id="fbe0af220a4840c9aed1bd73df43823a"> </p></body></section><p id="e0c54ef70bfc4b7fbcc40c011bc0f1c7">Each list stores the <em style="italic">pointers</em> to its elements in contiguous memory blocks. Therefore, accessing a list element by indexing takes constant time <em>O(1)</em>.</p><section id="ac4a798d53ee4042b69f5226e40b0f12"><title>Check for existence</title><body><p id="ba6bcd589fef4e1781c0f4da5671f5d3"> </p></body></section><p id="c6f6dbb7018747ebbf1e4ff2d36bdf60">To check if a value exists in a list, we need to, in the worst case, loop through the entire list and compare the target value with each element. This leads to a runtime complexity of <em>O(S)</em>, where S is the current size of the list. If S is sorted, binary search can be performed instead, reducing the runtime to <em>O(log S)</em>.</p><section id="e996ea14cbaa4cd78d4076d2ad3c8344"><title>Append</title><body><p id="a3a22c2755f2414d8cd6c94e4332819d"> </p></body></section><p id="d4264bb372dc42c6ad1f4a0c9738efbf">When Python Lists are created, they typically reserve extra space in the event you might add items into the list later on. If you create a list with 10 elements where each element takes b bytes, then the list might (for example) end up taking 16\xc3\x97b bytes. Here is an example illustrating this. Let&apos;s import the sys module and measure the number of bytes reserved from an empty list. Then we can append items and monitor the byte count after each step.</p><codeblock id="d6474d496d4f46ddaeafdbcb035d0ae8" syntax="text" highlight="" number="false" start=""><![CDATA[]]></codeblock><table id="c597050b701d4916838804393d73c120" summary="" rowstyle="plain"><cite id="i5705e2b2456e46f385d69b2483d6f5d6" /><caption><p id="f355cbfddf1d4db09459890ce88ae7a1" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a5f9971070bb4794b98f885428ac5783"># Output# Empty list takes 56 bytes# List of size 1 takes 88 bytes# List of size 2 takes 88 bytes# List of size 3 takes 88 bytes# List of size 4 takes 88 bytes# List of size 5 takes 120 bytes# List of size 6 takes 120 bytes# List of size 7 takes 120 bytes# List of size 8 takes 120 bytes# List of size 9 takes 184 bytes</p></td></tr></table><p id="cd9af39a70dc48f09456670a6f74555d">As we can see, the size of the list doesn\xe2\x80\x99t grow after each element addition, but only after a certain number of elements have been appended. This avoids the need to re-allocate memory for the list after every addition, at the cost of potentially unused space (e.g., your list will always take 88 bytes even if it has only one element). A popular scheme is to double the size of the list whenever it reaches capacity. If the list has capacity C at a given time, then the list is resized to 2C once the length of the list hits C. Lists will also shrink to avoid using too much extra space when you remove elements from it.</p><p id="b06f5086d398444681b7c57a7706cd3f" /><p id="c761af3c5e8541f58560e4f759d2a338">This makes runtime analysis a bit tricky because element insertion is O(1) most of the time, but it is also accompanied by an occasional list resizing, which is O(S) in runtime, where S is the current size of the list. As your list grows in size, the resizing will be less frequent (because it takes more element insertions to fill the list) so we can say that appending an element to a list has <em>O(1) amortized</em> runtime.</p><section id="e711aa26f3ce48c585088251ea5cbc00"><title>Insertion in the middle</title><body><p id="abf5f6dca5df472c94b0b13b04833e6a"> </p></body></section><p id="bc2dbeb8cc6248ad802e3ce9fd4ec394">Inserting an element to the middle of a list requires shifting all subsequent elements one index forward (see an illustration <link href="https://www.geeksforgeeks.org/how-to-insert-an-element-at-a-specific-position-in-an-array-in-java/" target="new" internal="false">here</link>). In the worst case (inserting to the front of the list) all current elements need to be shifted, so the runtime complexity is <em>O(S)</em>, where S is the current size of the list.</p><section id="f72ca7b6e83f4918a03b76898ab65074"><title>Deletion</title><body><p id="f1e81eb3378045759f29ab013e016ee2"> </p></body></section><p id="fbf4e817496c49ffaac09e20feddff8a">Deleting an element can come in two forms: deleting by index or deleting by value (which entails finding the target index based on the value and then deleting by index). Once an element is deleted, all subsequent elements in the list have to be shifted back by one index, so the runtime complexity is also <em>O(S)</em>, where S is the current size of the list.</p><section id="d8e2287febfb4e2f80e11488c51e0b85"><title>Set</title><body><p id="b624cde479a345b9a2f924f5fcb9a860"> </p></body></section><p id="c7a601f7d4894ded843b31d70563ebc6">A set is an unordered collection of unique (non-duplicate) elements. Similar to Python lists, sets can store elements of different data types. Its most attractive feature is the ability to perform insertion, deletion and checking for existence with <em>O(1)</em> runtime complexity on average. This fast speed is thanks to the mechanisms of hashing. Roughly speaking, hashing involves applying a hash function to an input element (i.e., the value that you want to add, remove or search for) to determine its location in the underlying data structure.</p><p id="c89cce5429f8492abddb845b0ac085c3" /><p id="aa90ac662ed7482f8e0d8341cda5d9ff">Consider the following simplified example where we want to build a set of strings. We can construct our set as a Python list of length <em style="italic">p</em>. Given an input string <em style="italic">s = c</em><em style="italic"><sub>1</sub></em><em style="italic">c</em><em style="italic"><sub>2</sub></em><em style="italic">c</em><em style="italic"><sub>3</sub></em><em style="italic"> \xe2\x80\xa6 c</em><em style="italic"><sub>m</sub></em> (where each c<sub>i</sub> is one character), we apply the following hash function:</p><p id="f91fff3f3d344015b8286cf8700d21ea" /><p id="cb1e3eeef0354717bf58e165d8567c1d">In other words, <em style="italic">h(s)</em> converts every character in s to its ASCII integer value, sum over all these values, and compute the remainder of the sum when divided by p, yielding a non-negative integer. This integer indicates the index within the list of length <em style="italic">p</em> that <em style="italic">s</em> should go into.</p><p id="dbcf24acae704f16adf6272c9d91a0b4" /><p id="b1afdb5d0b914ce0a343aa11e177159e">For example, let\xe2\x80\x99s say our set is a list of size <em style="italic">p</em> = 97. To add the string \xe2\x80\x98xyz\xe2\x80\x99 to the set, we first compute its hash value:</p><p id="d3b538a05c75498eaa52b56a37f3fa22" /><p id="b1d3d481d9dd400e94214a58881702b9">So we place \xe2\x80\x98xyz\xe2\x80\x99 at the index 72. If we later want to check whether \xe2\x80\x98xyz\xe2\x80\x99 is in our set, we just need to see whether it\xe2\x80\x99s at index 72 \xe2\x80\x93 if it\xe2\x80\x99s not there, our set doesn\xe2\x80\x99t contain it (contrast this to a normal Python list where you need to loop through the whole list and compare every element against the input \xe2\x80\x98xyz\xe2\x80\x99).</p><p id="a464de750e964917892231a04192a907" /><p id="cb6096b4af494b7f8f0a7844f1869422">Naturally, you may wonder what happens if different elements have the same hash value. This is called a collision, and there are different ways to address it. For example, you can make your set a 2D list, where the inner list at index <em style="italic">i</em> stores all the elements whose hash value is <em style="italic">i</em> (this method is called <em style="italic">separate chaining</em>). Alternatively, if index i is occupied, you can try putting the element into index i + 1, or i + 2, and so on (this method is called <em style="italic">linear probing</em>). These workarounds do imply that the runtime of set operations will not be exactly constant, but a good selection of the list size and hash function can reduce the frequency of collisions. Similar to Python lists, sets will need to undergo a memory re-allocation to expand its size once it\xe2\x80\x99s almost full, which is an O(S) operation, but its low frequency means we can consider set operations as having <em>O(1) amortized runtime complexity</em>.</p><p id="b04221eb50544fb495429ceb1813fc02" /><p id="a82f7d5625f544ddb32498d6903cb74a">While you don\xe2\x80\x99t need to memorize the above working mechanism, an important implication to keep in mind is that <em style="italic">you can only store hashable elements in a set</em>. Roughly speaking, an element is hashable if its value cannot be changed after creation (e.g., int, float, str, tuple, NoneType). On the other hand, mutable containers (e.g., list, dict, set) are unhashable, so you cannot have a set that contains lists (this makes sense because, if a list is updated, its hash value also needs to be re-computed). In addition, you cannot access set elements by indexing.</p><section id="d26a21b5e12842848d854994c39e2fd7"><title>Dictionary</title><body><p id="d62c16b82eee426c8ed40279babfb1dc"> </p></body></section><p id="ec00445746ff4d1bb34f98265a869927">A dictionary is a collection of key-value mappings, with the ability to quickly retrieve a value based on its corresponding key. In particular, similar to set operations, insertion, deletion and checking for existence in dictionaries have <em>O(1) runtime complexity</em> on average. The working mechanism of dictionaries is similar to that of sets: given a key-value pair <em style="italic">(k, v)</em>, the hash value of <em style="italic">k</em> determines where <em style="italic">v</em> is stored in the underlying list. This also means that <em style="italic">dictionary keys have to be mutable</em>, although dictionary values can be any data type.</p><section id="a56acff1855544edbd5af1ccaf6e9375"><title>Tree</title><body><p id="b056e457c2b341d29500761c8269fa1b"> </p></body></section><p id="f5d4db7117a2479aa7b27c68d73a0fe2">A tree is a special type of directed acyclic graph where each node has at most one incoming edge. There is a single node with no incoming edge called the root. In addition, if there is an edge from node A to B, then A is B\xe2\x80\x99s parent and B is A\xe2\x80\x99s child. If a node has no children, it is called a leaf node. Note that in Mathematics and Graph Theory, trees are undirected graphs; in computer science, however, trees are typically assumed to be rooted and directed, because we are mostly interested in going from the parents to the children and from the root to the leaves.</p><image id="d2d5dfc0a1134bbdba0d234fae4cb350" src="../webcontent/image-cca5fe50ca85452fb8738a54be27af6f-1.png" alt="" style="inline" vertical-align="middle" height="224" width="359"><caption><p id="c776886252bd48539aef53a28af9edd3" /></caption><popout enable="false"></popout></image><p id="ea9e2506ff3e4fa0bb5ae58c52f4d96e">There are different ways to implement the tree structure. A standard object-oriented approach is to define a class Node that captures its properties:</p><table id="ec096543d63d4f109a92d3ccc7a4d479" summary="" rowstyle="plain"><cite id="i3e81024535c244528b5c268d5b27f6cd" /><caption><p id="af1d530a85564f25818ab2d831e9dea1" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="dcaff3314be34c949e6267c4c8fee2e1">class Node:\tdef __init__(self, value):\t\tself.value = value\t\tself.children = []</p></td></tr></table><p id="fa1479ba78b54581a7968625cd73be7b" /><p id="ba268426c67747c8b0306eb0a8f5d8aa">In this way, we can construct the example tree above by creating the nodes and then linking them with each other:</p><p id="f55ed8d4518e44d287464a4a94c6a669" /><table id="fbd7ad3c7c2d41b78236e65deb505671" summary="" rowstyle="plain"><cite id="i9819b23a86d84370bc761f8b0402fcc5" /><caption><p id="bfaa55fee6e048f9bfe1f2019df488e8" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="cdae56903167449b972989983b467311">root = Node(&quot;Root&quot;)child1 = Node(&quot;Child1&quot;)root.children.append(child1)grandchild1 = Node(&quot;Grandchild1&quot;)child1.children.append(grandchild1)</p></td></tr></table><p id="f16fb18a05634048a610f8e49d546fb2" /><p id="d67bfaf17b5d47fcb36fe5191ef1d576">There are several ways to traverse through a tree. One such way is called pre-order traversal, where we visit each node before its children:</p><p id="de388424956a4353ba21f3af925e68e4" /><table id="b15581d924c3461da1906e7a028a9a04" summary="" rowstyle="plain"><cite id="ie0dadc3e595c4cc3b24036a799c82e9a" /><caption><p id="cfcf6297a4b94a7297b10e9f09138d08" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="eadc6b20bedf47fb847645bf49ec6434">def print_node(node):\tprint(node.label)\tfor child in node.children:\t\tprint_node(child)print_node(root)</p></td></tr></table><p id="fb5c55f87e4743bd8fefd81975f15458" /><p id="b5522492412742dfae05902604a191ee">In Project 1 you will see an alternate representation of a tree as a nested dictionary. With this setup, common operations (insertion, deletion, lookup) will have linear runtime complexity O(S), where S is the number of nodes in the tree. While this is fairly intuitive, </p><p id="bb55bb5d83cd4e80b581a5ebc40e56bb" /></body></workbook_page>\n'