b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="a6757e7b3dbf42e5b66e27f7744218bd"><head><title>Manipulating and Measuring Model Interpretability</title></head><body><table id="d6ad0e8940cf468098776cfbbe8cbef6" summary="" rowstyle="plain"><cite id="ic4198b5244c0479c96204267a817c205" /><caption><p id="fcb243e9ad00421195cd3ef75fe2408a" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c41825f1a5ae42a6b0733f8f0d73e1a7"><em>[Required Reading] Paper: </em><link href="https://dl.acm.org/doi/pdf/10.1145/3411764.3445315" target="new" internal="false">Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., &amp; Wallach, H. (2021, May). Manipulating and measuring model interpretability. In <em style="italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (pp. 1-52). </link>(Requires CMU credentials to access)</p></td></tr></table><p id="da866613877e4d108867947bf38e59d6"><em>Who are the papers&apos; authors? Why are they qualified to write about this topic?</em></p><p id="d58d3724f7444fce8564af9399609165">The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.</p><p id="d9972380347e4e8293e87fe92a0e6488"><em>Who is the audience of the paper?</em></p><p id="ddf884ed166549a89cd1fa400e54c4b8">The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.</p><p id="dcc3c68e61784728a10731058a5a6ffc"><em>Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?</em></p><p id="d9162eab82074ece8a8024ac54b40901">Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence people\xe2\x80\x99s everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work. </p><p id="d1ac3557fb6e4dd591b6483d01d8ffc8"><em>What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?</em></p><p id="c204070eada8490bb406c3aa0c8d60c4">The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on people\xe2\x80\x99s behavior.</p><image id="d5175d637eeb45e189e4f0021be8ebf9" src="../webcontent/image-a6757e7b3dbf42e5b66e27f7744218bd-1.png" alt="" style="inline" vertical-align="middle" height="192" width="650"><caption><p id="fb3588a5925e49aebdbcb293fa6a59d0" /></caption><popout enable="false"></popout></image><p id="e6c4b67dd556415a9ab1f333501afc11">This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.</p><p id="d15de5b68b7843f69233b373422d322b"><em>Summary of the paper\xe2\x80\x99s experiments and findings.</em></p><p id="cadf537ff2ce438883267bda12c6380d">The overall experimental procedure is to various factors that may influence a model\xe2\x80\x99s interpretability and measure their effect on people\xe2\x80\x99s behaviors, with a focus on the following aspects:</p><p id="db58984f2b154900acde4211f18ef3b8"><em>RQ1</em>: How well can people simulate a model\xe2\x80\x99s prediction?</p><p id="c843c1fff0b84f1299ce21e0be9abf12"><em>RQ2</em>: To what extent do people follow a model\xe2\x80\x99s prediction when it\xe2\x80\x99s beneficial for them to do so?</p><p id="e36941323e994b7a8bad16f60a2c6c37"><em>RQ3</em>: How well can people detect when a model has made a mistake and correct it?</p><p id="afbde1db387e423ebab46cf458156adb">The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).</p><image id="d633b91f518049c2a14d0ac81afd75a9" src="../webcontent/image-a6757e7b3dbf42e5b66e27f7744218bd-2.png" alt="" style="inline" vertical-align="middle" height="442" width="650"><caption><p id="e582e8521e4d4e77b3124a3baa022102" /></caption><popout enable="false"></popout></image><p id="fda090b3b3214e0aadf51a1e22938fc5">For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartment\xe2\x80\x99s selling price. They were then shown the model\xe2\x80\x99s prediction and asked for their own prediction of the apartment\xe2\x80\x99s selling price. For RQ1, the authors measured the difference between people\xe2\x80\x99s guesses of the model\xe2\x80\x99s prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the model\xe2\x80\x99s prediction in their guess for the apartment\xe2\x80\x99s ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.</p><p id="db69fe302c804c9aa883048eb6654f2e">Based on the paper\xe2\x80\x99s findings:</p><ul id="fac1903f8ea74a52b84b126b7da99958"><li><p id="eae3d6b4d0454f89b93391c67488cfd0">A clear model with a smaller number of features was easiest for participants to simulate.</p></li><li><p id="b2c7595bde8b4b45bfbef1a46edf64ac">There were no significant differences in the participants\xe2\x80\x99 trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.</p></li><li><p id="de4779b124a94044bd0de59609432d41">When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.</p></li></ul><p id="a5c3190049db4bf9a102b2d3580aa5f6"><em>What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?</em></p><p id="e4feeb2128744ed4b272a82c03fda610">There are two important takeaways from the paper:</p><ul id="fde0a311116e4d199ad9f14239280618"><li><p id="f25855714cb4429a8c24a30dbeadf68c">Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.</p></li><li><p id="ce9b77b3812442a5893319cd26e37656">Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.</p></li></ul></body></workbook_page>\n'