b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f59f965701df42e796a110d414580861"><head><title>Accountability</title><objref idref="f074b40cad824beca96ff8fda459aaf6" /><objref idref="d80ef3186b40419cb51d18188c6cc837" /><objref idref="eeb2c243cb7745438173e30094c2fa86" /><objref idref="f3f8412dde92474080cca13b633b0444" /></head><body><p id="fdda7fba884f4a189e54d2f25cf38394">While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: <em>How can we effectively control large automated systems?</em></p><p id="b7010e2c0fa143f28bac04426025e307">When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems we\xe2\x80\x99ve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational. </p><p id="f5026e4d7ff849d2b95d1549bdd7ce81">Madeleine Elish wrote a very fascinating article in 2016 titled <link href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757236" target="new" internal="false">Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction</link>. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the person\xe2\x80\x99s responsibility to intervene and prevent the worst from happening.</p><p id="e3ec3ee1152d45da9e3a8deb961b4c2d">The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesn\xe2\x80\x99t happen again. We care about both.</p><section id="c3ac2d44e2a748cabc28b6265737260a"><title>Transparency</title><body><p id="a9f7ac2b362e42b8bd83a594e925ae1a">In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.</p><p id="babc4601bfb341de8db8d42b07326414">When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place. </p><p id="a9b38ebcfda049d893952be5e012a26f">In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as \xe2\x80\x9cWhy am I seeing this ad?\xe2\x80\x9d (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.</p><image id="a4420776258d45dba29fd3eb724a74fa" src="../webcontent/image-f59f965701df42e796a110d414580861-1.png" alt="" style="inline" vertical-align="middle" height="389" width="650"><caption><p id="afcb16e1c7494df8a3d5c2395a96c010">Figure 1. Why am I seeing this ad? (Source: LinkedIn)</p></caption><popout enable="false"></popout></image><image id="e7c177f73c4247798beb69c3a57680b6" src="../webcontent/image-f59f965701df42e796a110d414580861-2.png" alt="" style="inline" vertical-align="middle" height="389" width="650"><caption><p id="fb39b5a24de141eb8ed2bb3eae9459fd">Figure 2. High-level Design of \xe2\x80\x9cWhy am I seeing this ad?\xe2\x80\x9d Initiative (Source: LinkedIn)</p></caption><popout enable="false"></popout></image><p id="bc718b6d870d4175a48e576bd249360c">Figure 2 shows the flow of control of <link href="https://engineering.linkedin.com/blog/2022/why-am-i-seeing-this-ad-" target="new" internal="false">\xe2\x80\x9cWhy am I seeing this ad?\xe2\x80\x9d on LinkedIn</link>, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIn\xe2\x80\x99s effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary. </p><p id="dfdd58cc091c4153bad82ae25ff576ef"><em>Limitations of the Transparency Ideal</em></p><p id="c754a2ca9bac40638423b0580123ee01">The title of this section is taken from a <link href="http://mike.ananny.org/papers/anannyCrawford_seeingWithoutKnowing_2016.pdf" target="new" internal="false">2016 article by Mike Ananny and Kate Crawford</link>. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:</p><ul id="e475c77e728b497d93c84073d9d5df3c"><li><p id="c68de36bd6af4d04be3ecdd7b79b020c"><em style="italic">Transparency can be disconnected from power</em></p></li><li><p id="bd220364c0c8461ea1e3026aa8f97f15"><em style="italic">Transparency can be harmful</em></p></li><li><p id="fbdbf3d4322c4f6f9d25c116872188d5"><em style="italic">Transparency can intentionally occlude</em></p></li><li><p id="e78836047a8b42daa51470b8645dca95"><em style="italic">Transparency can create false binaries</em></p></li><li><p id="afa63a6ea4ec43fa871fb3eac67f1665"><em style="italic">Transparency can invoke neoliberal models of agency</em></p></li><li><p id="a2108e3ccb8040d98ff20da3c805de3f"><em style="italic">Transparency does not necessarily build trust</em></p></li><li><p id="d340941f0dc04697980cdf53aa1c03d4"><em style="italic">Transparency entails professional boundary work</em></p></li><li><p id="dc55b29094224e98b7cbce6fe22d750d"><em style="italic">Transparency can privilege </em>seeing <em style="italic">over </em>understanding</p></li><li><p id="e7f9a2891e1a4b9cb64bba51de6449bd"><em style="italic">Transparency has technical limitations</em></p></li><li><p id="ba7564832c5c4debb283411e8c5cf55f"><em style="italic">Transparency has temporal limitations</em></p></li></ul><p id="eb965ba38add4220a0dbf26655881d6f">Besides these limitations, other pitfalls of \xe2\x80\x9creverse-engineered\xe2\x80\x9d as a strategy for transparency:</p><ul id="cf923bfc780e4d5386ad2e03cbd2eae1"><li><p id="e1d15c73900d450ab2ec7ccdbcc4b4b8"><link href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/8lb6it/cdi_proquest_journals_2288644371" target="new" internal="false">Set reasonable expectations to disclose what is known.</link> While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really don\xe2\x80\x99t know exactly how it works. We don\xe2\x80\x99t know what we don\xe2\x80\x99t know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.</p></li><li><p id="c9196d32fdcb481ba9302cafdea7453a"><link href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991015299349704436" target="new" internal="false">&quot;Complexity distributes responsibility&quot; - Joseph Weizenbaum (1976).</link> When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of <em style="italic">ex-ante</em> versus <em style="italic">post hoc</em> accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible <em>when</em> something goes wrong. This is a <em style="italic">post hoc</em> accountability matter. In this case, it is much more relevant and sensible to think about <em style="italic">ex-ante </em>accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to <em>avoid</em> mistakes.</p></li><li><p id="d722078df51841c793d9dbbcc4ad0426"><link href="https://doi.org/10.1080/1369118X.2018.1477967" target="new" internal="false">Without a </link><em style="italic"><em>critical </em></em><link href="https://doi.org/10.1080/1369118X.2018.1477967" target="new" internal="false">audience, algorithms cannot be held accountable.</link> Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.</p></li></ul></body></section><section id="e12f3e2179df424a8160ea27b83c443c"><title>Auditing</title><body><p id="dfe0ddae4a594f4fb5612b1cb0853377">Another component of accountability in data science is auditing. Auditing has been used in social science research as <em>an experimental test </em>to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.</p><p id="d4d882a8a44f4dadbf2eb41927ce7e9f">Auditing has been used in the United States to diagnose employment and racial discrimination. A famous <link href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/8lb6it/cdi_crossref_primary_10_1257_0002828042002561" target="new" internal="false">field experiment on labor market discrimination</link> is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.</p><p id="c8b36469c1ec46bdaeeb26c357074d79">In data science applications, when transparency might not be feasible due to the <em>protection of trade secrets</em> or <em>prevention of the system being gamed</em> by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.</p></body></section></body></workbook_page>\n'