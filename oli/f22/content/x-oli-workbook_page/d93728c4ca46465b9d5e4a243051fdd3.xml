b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d93728c4ca46465b9d5e4a243051fdd3"><head><title>Bias-Variance Decomposition and Trade-off</title><objref idref="fb91d7452cca428ebc8f86c4062f67ff" /><objref idref="cc3af471b1ea43abb5a426beefa36779" /></head><body><p id="e55255349da74d1f94f9551a2959ab17">As we explore the different methods, we will dive deeper into calculating the error rate for the models trained using those methods.</p><section id="a66c3f016f2e45729ad7af635a4cd554"><title>Model Bias</title><body><p id="a9db6861839a4ecda804e3943b56ccca">The bias is a measure of how closely the model can capture the mapping function between inputs and outputs and measures the average accuracy of the model arising from erroneous assumptions in the learning algorithm. A high bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting). The bias is always positive.</p><ul id="bc8b97290ff44142a711d64af9d71c39"><li><p id="ecde358a49e644a2820a59ba798d2973"><em style="italic">Low Bias </em>indicates that assumptions regarding the functional form of the mapping of inputs to outputs are weak.</p></li><li><p id="a7b434322bb1428c94e936f4f0e8fde6"><em style="italic">High Bias </em>indicates that assumptions regarding the functional form of the mapping of inputs to outputs are strong.</p></li></ul></body></section><section id="da53e8d5de5b4b07bf3d3aa97ead7df0"><title>Model Variance</title><body><p id="df0e69833f1e4681aab00eb65d0d5cd8">The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.</p><ol id="f0cfe985c8954261b64f041f02b67415"><li><p id="a5059f31212c4c2ca4a9c1ada9e6a7af"><em style="italic">Low Variance</em> indicates that changes to the training dataset cause  small changes to the model.</p></li><li><p id="e828fa2cf39a4d1ca642ae616d525f54"><em style="italic">High Variance </em>indicates that  changes to the training dataset cause large changes to the model.</p></li></ol></body></section><section id="aa3b5138251b4c4396cfe1b157a8e47b"><title>Model Noise</title><body><p id="c3f3fad4dbbd473dbab11b783ef4a29f">How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this. It is a property of the data.</p><p id="f23af853a27440cd9f3da8c1f929cc19">Variance refers to the amount by which \\(\\hat{f}\\) would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of <em style="italic">f</em>.</p><p id="a8b19716af0d4bb2a0ac2776486af182">When you decompose bias-variance, you will analyze an algorithm&apos;s ability to predict outcomes for data that your model has not seen. The bias-variance tradeoff is encountered while working with some supervised learning techniques. The premise is that your model will adequately learn the training data, and it should properly generalize well to new data.</p><p id="a37e9826980a48dda85269a9485474df">As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.</p><image id="d334b59bbdc94d2c9e1944ee327f8226" src="../webcontent/image-d93728c4ca46465b9d5e4a243051fdd3-1.png" alt="" style="inline" vertical-align="middle" height="235" width="262"><caption><p id="aae9120be9664811aa2b79eeb3fcfa07">Bias-Variance Diagram.</p></caption><popout enable="false"></popout></image></body></section><section id="ab78cbed5dc64684b30af02f57f59aa4"><title>The Bias-Variance Decomposition</title><body><p id="e036f646842b4e348a55bfdca6dc1835">Let us look at a very well deconstructed mathematical representation of Bias-Variance by <link href="https://towardsdatascience.com/the-bias-variance-trade-off-explanation-and-demo-8f462f8d6326#:~:text=our%20sample%20set.-,Meaning%20of%20Bias%20and%20Variance,true%20values%20in%20the%20data.&amp;text=This%20will%20usually%20be%20the,the%20inherent%20data%20generating%20function." target="new" internal="false">IBM&apos;s Aditya Prasad</link>:</p><p id="e7c8d79706af459a8ddf9295e51d1089">Note that the bias and variance of an estimator are mathematically related to each other and also to the performance of the estimator. Let us define an estimator\xe2\x80\x99s error at a test point as the \xe2\x80\x9cexpected\xe2\x80\x9d squared difference between the true value and the estimator\xe2\x80\x99s estimate.</p><p id="fb595aaa5d7447e5941bb098bb7268df">Whenever an expected value is referenced, this means the expectation over all the possible models, trained individually over all the possible data samples. For any unseen test point \\(x_{0}\\), you will have:</p><p id="e9d870beacb64134a570440acbda9140">\\[\\operatorname{Err}\\left(x_0\\right)=E\\left[\\left(Y-g\\left(x_0\\right)\\right)^2 \\mid X=x_0\\right]\\]</p><p id="b193cd1fd195481e8383cfd3fe813323">Referring to \\(f\\left(x_0\\right)\\) and \\(g\\left(x_0\\right)\\) as <em style="italic">f</em> and <em style="italic">g</em>, respectively and skipping the conditional on X:</p><p id="d64e464143ba45b7b81baf135c55a1e4">\\[\\begin{aligned}&amp;\\operatorname{Err}\\left(\\mathrm{x}_0\\right)=\\mathrm{E}\\left[\\left(\\mathrm{Y}-\\mathrm{g}\\left(\\mathrm{x}_0\\right)\\right)^2\\right] \\\\&amp;=\\mathrm{E}\\left[(\\mathrm{f}+\\epsilon-\\mathrm{g})^2\\right] \\\\&amp;=\\mathrm{E}\\left[\\epsilon^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{g})^2\\right]+2 \\cdot \\mathrm{E}[(\\mathrm{f}-\\mathrm{g}) \\epsilon] \\\\&amp;=\\mathrm{E}\\left[(\\epsilon-\\mathrm{o})^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\\right]+2 \\cdot \\mathrm{E}[\\mathrm{f} \\epsilon]-2 \\cdot \\mathrm{E}[\\mathrm{g} \\epsilon] \\\\&amp;=\\mathrm{E}\\left[(\\epsilon-\\mathrm{E}[\\epsilon])^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\\right]+\\mathrm{o}-\\mathrm{o} \\\\&amp;=\\operatorname{Var}(\\epsilon)+\\mathrm{E}\\left[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])^2\\right]+\\mathrm{E}\\left[(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})^2\\right]+2 \\cdot \\mathrm{E}[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})] \\\\&amp;=\\operatorname{Var}(\\epsilon)+\\operatorname{Var}(\\mathrm{g})+\\mathrm{Bias}(\\mathrm{g})^2+2 \\cdot\\left\\{\\mathrm{E}[\\mathrm{g}]^2-\\mathrm{E}[\\mathrm{gf}]-\\mathrm{E}[\\mathrm{g}]^2+\\mathrm{E}[\\mathrm{gf}]\\right\\} \\\\&amp;=\\sigma^2+\\operatorname{Var}(\\mathrm{g})+\\operatorname{Bias}(\\mathrm{g})^2\\end{aligned}\\]</p><p id="dcc622b460754a818cefbbc6a1bde1e9">\\[\\textit{Generalization Error} = \\textit{Bias}^2 + \\textit{Variance} + \\textit{Irreducible Error}\\]</p><p id="c3c64396627546a897e9e155e4bf0f4c">So, the error of the estimator at an unseen data sample \\(x_{0}\\) can be decomposed into the variance of the noise in the data, bias, and the variance of the estimator. This implies that both bias and variance are the sources of error in an estimator.</p></body></section><table id="fb996a87e5034e6e9b57217718b9a6d9" summary="" rowstyle="plain"><cite id="i850e3720462c4fa99b8a61425571a169" /><caption><p id="d3ff6a0c8a224741818d06c3e11334a4" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="eb024e700d1b49699f9e7c77c07b4ac0"><em>Reading: </em><link href="http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf" target="new" internal="false">Bias-Variance Tradeoff</link>.</p></td></tr></table><section id="baaa11002e674d57a9786f3e73a855be"><title>The Trade-off!</title><body><p id="f8189a9af5e4457ab896b7f495c5732c">\xe2\x80\x8b\xe2\x80\x8bThe bias and the variance of a model\xe2\x80\x99s performance are connected.</p><p id="f6df78e1658944cdbc59207c7db52e74">Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.</p><image id="e1dd5bd1ea084f6fbad69cf56b18bad0" src="../webcontent/image-d93728c4ca46465b9d5e4a243051fdd3-2.png" alt="" style="inline" vertical-align="middle" height="232" width="369"><caption><p id="a3c977a6381842c4981cb178c951eb8d">The variation of Bias and Variance with the model complexity. </p></caption><popout enable="false"></popout></image><p id="b846cf3c28eb4bc7bf04646d3c53de0f">This is similar to the concept of overfitting and underfitting. More complex models overfit while the simplest models underfit.</p><image id="c7d39770460c420b876c7f44f2ae1a23" src="../webcontent/image-d93728c4ca46465b9d5e4a243051fdd3-3.png" alt="" style="inline" vertical-align="middle" height="270" width="650"><caption><p id="d715a1a05e96435295e4702823774f03" /></caption><popout enable="false"></popout></image><p id="fbd93d9512344db8a9add7229f8a6eff">As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, <em style="italic">Cross-Validation. </em>The figure below gives you a visual representation of bias-variance with the training dataset.</p><image id="dc73676e005448afa63eadb508cfd391" src="../webcontent/image-d93728c4ca46465b9d5e4a243051fdd3-4.png" alt="" style="inline" vertical-align="middle" height="323" width="600"><caption><p id="bb761176466c465bae7a7d7d29051f49">Bias-Variance and Training-Test Data.</p></caption><popout enable="false"></popout></image></body></section><table id="ab946e5eb0e9479ab8249fd18388bafb" summary="" rowstyle="plain"><cite id="i039c300fc48e4f5ab0cc898de4f58ed2" /><caption><p id="f012f49e2e574ea2ab987f243580f279" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a7f95d9d0a234aea884d67ece58950d8">Additional Reading: <link href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html" target="new" internal="false">Bias-Variance Tradeoff</link>.</p></td></tr></table></body></workbook_page>\n'