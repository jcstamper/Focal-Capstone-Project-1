b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bdebfe08ac9049548b99f19762eac40b"><head><title>Cross Validation</title><objref idref="c0047158bffa477cacf9095f445caa08" /></head><body><p id="f9bc08335974476e8ff78e86de86bcaf">When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. <em style="italic">Model Assessment </em>and <em style="italic">Model Selection </em>are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real world data can be introduced to it?</p><p id="bca2273ce54e4055b8959a429ecdc813"><em>Cross Validation</em></p><p id="a05b058b2ee64dd187ad81bcdd9bda89">There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse to not test and validate your model. You can employ a method called <em style="italic">holding out. </em>Here, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses for the observations used to validate the model. This approach is called the <em style="italic">Validation Set Approach</em> and the data that was used during holding out is called the <em style="italic">validation dataset. </em>Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach, using the MSE (it will give an estimate of the test error rate for quantitative outputs).</p><p id="a19ad6afa0e941fca500c9e06c59bf08">Consider that the test error rate for the validation data set will depend on the observations included in the validation data set versus the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large amount of observations.  </p><p id="b6ece187b2784370a93410dc5a51268a"><em style="italic">k-Fold Cross Validation. </em>You should think about <em style="italic">k </em>as a number of groups that are formed as a result of splitting your dataset. Implementing <em style="italic">k-</em>fold cross validation is straight forward. The dataset should be shuffled randomly and split into the groups(according to the chosen value of <em style="italic">k</em>). Each group will be used as the hold out dataset, meanwhile the others will be used as a training dataset. Your model will be fit with the training dataset and then evaluated with the holdout dataset. You will evaluate the models using an evaluation criteria.</p><p id="a68b38ba00fd4a6189b3d12215949cd7">Selecting <em style="italic">k </em>is not a random process, an inappropriate k will lead to a model that has high bias or high variance. Remember, you want a balanced model with low bias and low variance. The techniques used to choose the right value for <em style="italic">k </em>includes:</p><p id="b27885e7a9474e40b693405c12353ce2">Using a fixed value of 10. This number has been used because it has been used in various projects and empirically tested to show that the <em style="italic">k</em>=10 will result in a balanced model (low bias-low variance) and <em style="italic">k</em> at 10 and even 5 yield test error rates that do not suffer from bias-variance issues. k-fold cross validation is not costly to implement as other cross validation techniques. It can be applied to most learning methods. You should assess your model&apos;s bias by calculating the mean of all error estimates.  The model&apos;s variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better...and this means your model is balanced.</p><p id="edf4dd6e9a3349c4bbb7dc2ff590f568">The size of the dataset can also be used as <em style="italic">k, </em>this is known as the <em style="italic">leave one out cross validation technique</em>, where <em style="italic">k</em> is the number of observations in the dataset minus one observation. </p><p id="a2d26f0105294cac84278dae5e91418c"><em style="italic">Leave One Out Cross Validation (LOOCV). </em>This technique involves splitting the dataset to use one observation for validation and the rest of your dataset for training. LOOCV technique presents less bias as it does not overestimate the test error rate, the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split and this makes it costly to implement (think about this technique on a large dataset). A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model. LOOCV can be used with any kind of predictive modeling.</p><p id="cd161684f6304618b79c980edf5c6fcf">LOOCV will have a higher variance than the <em style="italic">k</em>-fold CV because with LOOCV, models are trained on almost identical set of observations and this will mean that the outputs will be positively correlated with each other. With <em style="italic">k</em>-fold CV, when <em style="italic">k </em>is less than <em style="italic">n</em>, the output of your models are not as correlated as is the case with the LOOCV models. </p><p id="d77d0f020324480aa5b91d697ed1da30"><em>Using Cross Validation with Regression and Classification Problems</em></p><p id="e153dfaebf99428287d5e698b045c03b"><em style="italic">Classification Problems. </em>When <em style="italic">Y </em>is qualitative, we use the number of misclassified observations as a measure of the model&apos;s test error. <em style="italic">Regression Problems: </em>When <em style="italic">Y </em>is quantitative, the Mean Square Error(MSE) is used to measure the test error. Cross validation estimates are assessed for accuracy by computing the MSE (for regression problems) or misclassification rate (classification problems). </p><table id="cce2ce913fc04d29ae24730db50a86b2" summary="" rowstyle="plain"><cite id="i7ef44037339940b08b55cc041b6f492a" /><caption><p id="c380436a85194022828d33bc9541ed33" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="f3c01ac143204c1699bfda3eb310c527"><link href="https://scikit-learn.org/stable/modules/cross_validation.html" target="new" internal="false"><em>Cross Validation: Python</em></link></p></td></tr></table><wb:inline idref="newe41aa7be072b4242bd61b5cf249332dc" purpose="didigetthis" /></body></workbook_page>\n'