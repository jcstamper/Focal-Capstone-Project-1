b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f692a0964d6245e48c56a2d31412e253"><head><title>Cross-Validation</title><objref idref="c0047158bffa477cacf9095f445caa08" /></head><body><p id="f8bf4449f5064e4ca607f6dc2d317b69">When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?</p><p id="c5b0789aed1740ad9639065c5d3937d3">There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.</p><p id="e456c0a1e8934c80b3dfb7957d022cd9">Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.</p><section id="fd27c28d5fa04b438f019295f4963ce8"><title>k-Fold Cross-Validation</title><body><image id="af600cb746e64c1dbac8f0bfa6022909" src="../webcontent/image-f692a0964d6245e48c56a2d31412e253-1.png" alt="" style="inline" vertical-align="middle" height="258" width="468"><caption><p id="fa5aebc23657411c854c14ee1f32f678">k-Fold Cross Validation</p></caption><popout enable="false"></popout></image><p id="ec0ece82b7f449268fe6ce48ffa88075">You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model&apos;s bias by calculating the mean of all error estimates. The model&apos;s variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.</p><ul id="bedf6eabfc9b4d348b7f46de0815f3a9"><li><p id="e0e30b4d53c44caaa95fc69cb2bd51b5">Selecting <em style="italic">k</em> is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of<em style="italic"> k</em>=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). <em style="italic">k</em>=10 and even <em style="italic">k</em>=5 yield test error rates that do not suffer from bias-variance issues. </p></li></ul></body></section><section id="e0d65526c1564f038c1fee2f5dd1253a"><title>Leave One Out Cross Validation (LOOCV)</title><body><p id="b09ec6c9324f4b869a2fd73dab03361f">This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.</p><p id="e80e3380396e4306ae0359120c9a14bd">LOOCV can be used with any kind of predictive modeling.</p><image id="d686968a93f942f38989c1e2457882ba" src="../webcontent/image-f692a0964d6245e48c56a2d31412e253-2.png" alt="" style="inline" vertical-align="middle" height="284" width="382"><caption><p id="bc83ecfb2ce14aa491a6fbfae8621995">Leave One Out Cross Validation (LOOCV)</p></caption><popout enable="false"></popout></image><p id="d8c760df086d4613af01d99cbde60e29">LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.</p></body></section><section id="a91b20e5617449c1a932cd277a3c7e60"><title>Using Cross Validation with Regression and Classification Problems</title><body><p id="e32a1f95b4f6430da4c07d8bde8b4dae"><em style="italic">Classification Problems:</em> When Y is qualitative, we use the number of misclassified observations as a measure of the model&apos;s test error.</p><p id="dba514dcc2794f33aa887a180b526d1b"><em style="italic">Regression Problems: </em>When Y is quantitative, the MSE is used to measure the test error. </p><table id="c55ec7a61c644401bb0888f8c6248932" summary="" rowstyle="plain"><cite id="i68fada7e366e41d6aedfcafd86de183e" /><caption><p id="b1e98f159022466e9438b2e84d4ed6a0" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e6416be960d34c5e9d373744ee14466f">Reading: <link href="https://scikit-learn.org/stable/modules/cross_validation.html" target="new" internal="false">Cross-Validation: Python</link></p></td></tr></table></body></section></body></workbook_page>\n'