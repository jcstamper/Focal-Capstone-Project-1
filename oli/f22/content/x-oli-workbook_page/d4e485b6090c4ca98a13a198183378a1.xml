b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="d4e485b6090c4ca98a13a198183378a1"><head><title>Classification Evaluation Metrics</title><objref idref="f884b110ee3f48bfaf5a73e27e20b647" /></head><body><section id="e533917cbd4f47abb0f7e9d336b8f217"><title>Confusion Matrix</title><body><p id="dab67879313e4d3caae9f260cec073c8" /></body></section><p id="bb7f16e38b0f4793868db3f5bc99821f">A Confusion Matrix (contingency table) shows how well a classifier performs compared to the ground truth labels. It is often used in a binary classification setting and has the following components:</p><ul id="a8b14912e9fa4453bb642552569fa839"><li><p id="d447fd602e1d42009302692b5a015da7">True Positives (TP) is the number of positive data points that are correctly predicted as positive.</p></li><li><p id="c07f18a008e24d5ea589c43bd6b2c1cd">True Negatives (TN) is the number of negative data points that are correctly predicted as negative.</p></li><li><p id="f361430caede4b31aa57d78457c9d09d">False Positives (FP) is the number of negative data points that are incorrectly predicted as positive. This is also called the Type I error in statistics.</p></li><li><p id="d71cea4adcb74853b550b94f7f415099">False Negatives (FN) is the number of positive data points that are incorrectly predicted as negative. This is also called the Type II error in statistics.</p></li></ul><p id="b26f58aed7234be19142fd51ab2888c5">A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).</p><table id="fa8672a15cdb4922b5b255ad71edc1be" summary="" rowstyle="plain"><cite id="i07892a5270e549f794e1717c47788489" /><caption><p id="cd0338b9ab024464b0151c3bf95f86b7" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a8e3e544a9bf4f76986b50da69b33540" /></td><td colspan="1" rowspan="1" align="left"><p id="c61b57c2daeb4415ae4f7f968a4cca0a">Ground truth \\(y = 1\\)</p></td><td colspan="1" rowspan="1" align="left"><p id="c24c1074e5cd429684ffe935e4b9841a">Ground truth \\(y = 0\\)</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="bcd23890f71047559172d40061aa0da2">Prediction \\(\\hat y = 1\\)</p></td><td colspan="1" rowspan="1" align="left"><p id="c7df880ba8e348b7b25a8cba127c12d3">TP</p></td><td colspan="1" rowspan="1" align="left"><p id="acea65f6df3242c9ba2aae1cddca3903">FP</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="b38ec82a129346f0adb315537dcfaecb">Prediction \\(\\hat y = 0\\)</p></td><td colspan="1" rowspan="1" align="left"><p id="cb31c98e857446f98fd7d1274dd12cb6">FN</p></td><td colspan="1" rowspan="1" align="left"><p id="de8742a6a9334584b08698a0cf225796">TN</p></td></tr></table><p id="ae73a5ca9d4a4822be47d00def6fa675">In a multi-class classification setting with \\(K\\) categories, a similar confusion matrix with \\(K\\) rows and \\(K\\) columns can be constructed, where the entry at row \\(i\\) and column \\(j\\) denotes the number of instances that have ground-truth label \\(j\\) and are predicted as having label \\(i\\). In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.</p><wb:inline idref="mooclet_activity" purpose="didigetthis" /><section id="e3fe24058efe4afb80a71b743a499b09"><title>Accuracy</title><body><p id="c764d5e7acfd4dc9bc775ff85e18f96b" /></body></section><p id="ccb32257faec4c6398c80eae019c8903">Accuracy is the number of correctly classified instances, divided by the total number of instances in the dataset.</p><p id="a54b78c0fe4a4beb921258b3cb14fb57">\\(\\text{Accuracy} = \\frac{TP+TN}{TP+FP+FN+TN}\\)</p><p id="bb9799772dbf460ba27daf00836e4ae2">\xe2\x80\x8b\xe2\x80\x8bThis is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.</p><section id="ecf2dbc186714afeaed31320f340d037"><title>Recall</title><body><p id="d8f83c0837264757a173a9d59c4044c4" /></body></section><p id="b3dddcfbc6374dda9fd3a66ea239e54c">Recall, also known as sensitivity or true positive rate, denotes the fraction of all positive instances that are correctly classified as such:</p><p id="fab20146015841c6a1ceeadad7764a0d">\\(\\text{Recall} = \\frac{TP}{TP+FN}\\)</p><p id="b49dff90f97d42fbbd082583529072d0">Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).</p><section id="cb1c9521862642259f01ddb8ad22551d"><title>Specificity</title><body><p id="cb98c46594384063bf9948e4dc1a2460" /></body></section><p id="dc52b5b4eb3d43709966332097ae0afc">Specificity, also known as true negative rate, denotes the fraction of all negative instances that are correctly classified as such:</p><p id="e5a7f14f448744f08391979de00bc997">\\(\\text{Specificity} = \\frac{TN}{TN + FP}\\)</p><p id="d61170d258de440fbe76b9887c0e6888">It can be interpreted similarly as recall, but in this case, you prioritize detecting negative instances as best as possible.</p><section id="a2442ad38e774f0397d208f52f49d08c"><title>Precision</title><body><p id="d679cd06f5a84fcabc44dd0b96349659" /></body></section><p id="b4c6c34dec9d4c85874c970156692585">Prediction denotes the fraction of positive predictions whose ground truth label is also positive:</p><p id="a464e1fa1d0a42c789377d7dffbd7880">\\(\\text{Precision} = \\frac{TP}{TP + FP}\\)</p><p id="d69b19d2d7024d89af1a7b949d54b585">Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.</p><section id="f197cb1b083a4cb48888bd4118ddc434"><title>F1 Score</title><body><p id="ed74a5a1ea9d4981a38efa532ed9ca96" /></body></section><p id="e9947c1894d04cd8a8aa6edfd5643613">F1 score is the harmonic mean of precision and recall:</p><p id="ac200f9b512243b3a4cb8371a8ddd78a">\\(\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)</p><p id="f1e489b6bbe740b7a1f98c61e16a929a">Like accuracy, F1  provides a general measure of model performance without a bias for or against a certain type of error. The differences between accuracy and F1 score are as follows:</p><ul id="f9f287b27d7245469295b9668c068862"><li><p id="c38cca74884e4d6c9301b7c6dcd6f86c">Accuracy is good for optimizing true positive and true negative, while F1 is good for optimizing false positive and false negative.</p></li><li><p id="ef3faecd8dce47e6a206874b27112f15">Accuracy is good for balanced datasets (with even distribution of the ground truth labels) while F1 is good for imbalanced datasets.</p></li></ul><section id="d5aaf41d25e4487f846771e51a0275a3"><title>Matthews Correlation Coefficient</title><body><p id="f0ed92a11b0c46d39425a608280f674d" /></body></section><p id="e12e740f2a2e4b24afda71398d58a800">Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: <link href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html" target="new" internal="false">MCC in skikit-learn</link>.</p><p id="ec7ecb8bdce1422cbbc3494441e91c3c">\\[\\textrm {MCC} = \\frac{TP\\times TN - FP\\times FN}{\\sqrt{\\left(TP+FP\\right)\\left(TP+FN\\right)\\left(TN+FP\\right)\\left(TN+FN\\right)}}\\]</p><section id="d500a34dff7a4ad99287eb048e0bb384"><title>Logistic Loss (Log loss)</title><body><p id="c973bdc4f3ba4020b29a3f03a8756d32" /></body></section><p id="cf76ca5a1ede49e6b5f77822680f55db">If your model outputs a probability value \\(\\hat y\\) that an input data point has a positive label, it can be evaluated by the logistic loss</p><p id="b276d75529154608921ff0e93efafc38">\\(L(\\hat y, y) = y \\log(\\hat y) + (1 - y) \\log (1 - \\hat y)\\)</p><p id="b05f10868e6e46c4ac8a853e563c7ac4">where \\(y\\) is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.</p><section id="ac5fce5c37864ff08f39734b18142577"><title>ROC Curve: Receiver Operating Characteristic</title><body><p id="c15517e80677458ea757d54823ee797e" /></body></section><p id="ce9c615a4dbf438e96c78304bd5f1167">The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier&apos;s ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the &quot;cost of different errors,&quot; instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.</p><image id="dc0987419f78446b8235af920b5a8cdd" src="../webcontent/image-bd8deb95dbdf4dabb38ba99e87485f18-2.png" alt="" style="inline" vertical-align="middle" height="330" width="361"><caption /><popout enable="false"></popout></image></body></workbook_page>\n'