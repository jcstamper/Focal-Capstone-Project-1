b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="e843820862824c77903d30f41a295dac"><head><title>Hierarchical Clustering</title></head><body><p id="d041ef6f00c1475e8d0375e0529d9bee">Let us explore the second type of clustering technique called the <em style="italic">Hierarchical Clustering </em>technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a <link href="https://en.wikipedia.org/wiki/Dendrogram" target="new" internal="false"><em style="italic">Dendrogram </em></link><em style="italic">(</em><link href="https://wheatoncollege.edu/wp-content/uploads/2012/08/How-to-Read-a-Dendrogram-Web-Ready.pdf" target="new" internal="false"><em style="italic">reading a Dendrogram</em></link><em style="italic">). </em>There are two techniques used for hierarchical clustering.</p><p id="de3239eeb5c14534819f91f616d85d09"><em>Agglomerative Clustering</em></p><p id="f8f1bd81f26b49b39903f7f1a77c32ad">This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.</p><p id="b339828f2f834a5cb9ab70d761841e97">The technique can take advantage of any distance measure, but you will find that most studies will use the Euclidean distance as a distance metric.</p><image id="cd1065bb899048d586966850df708dff" src="../webcontent/Clusters.svg" alt="" style="inline" vertical-align="middle"><caption><p id="eea04a52023441668774b964b64540b4"><em style="italic">Raw Data-Source</em><em style="italic"><sup>1</sup></em></p></caption><popout enable="false"></popout></image><ul id="a41e488240d74504ac8f1404c7ad0ce2"><li><p id="c089cf32b1564f9886820a5dcba016db">The first round of merges finds clusters with observations/clusters <em style="italic">b </em>and <em style="italic">c</em> merged to form one cluster, and <em style="italic">d </em>and <em style="italic">e </em>are also merged into one. Now we have clusters <em style="italic">a, bc, de </em>and <em style="italic">f. </em></p></li><li><p id="bd0fdf0bcce643bcb574a9f0b6ae16f7">Next, <em style="italic">de </em>and <em style="italic">f</em> are combined to form cluster <em style="italic">a, bc, </em>and <em style="italic">def.</em></p></li><li><p id="ba47f9b17230415d83b0de4f1785e1ea">Clusters are further combined to form <em style="italic">a </em>and <em style="italic">bcdef.</em></p></li><li><p id="e6a359f7caf047e49b808947328972e9">Finally, <em style="italic">abcdef </em>is formed. </p></li></ul><image id="cb1ffd2f9a2c495f97029a28096aed96" src="../webcontent/Hierarchical_clustering_simple_diagram.svg" alt="" style="inline" vertical-align="middle"><caption><p id="e718b81e79a44cef81624c5f98d62519"><em style="italic">Hierarchical Clustering Technique-Source</em><em style="italic"><sup>1</sup></em></p></caption><popout enable="false"></popout></image><p id="ebc1e9029c5d424c9c201bf8e1c31eec"><em>Merging Clusters</em></p><p id="fe5b533045e645e9a370030515791046"><em style="italic">Single Linkage Method </em>is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.</p><p id="cb9a4ad748eb4429b3852d9b95b1288f">Single linkage suffers from <em style="italic">chaining</em>. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.</p><p id="ff02b3697a2d4e7ea750becf68b0f3c1"><em style="italic">Complete Linkage Method </em>uses the maximum distance between data points within each cluster, also known as the <em style="italic">farthest neighbor method</em>. Clusters are combined into larger clusters until all data points are in the same cluster. The distance between clusters is the distance between two data points (i.e., one per cluster) that are farthest from each other. Complete linkage avoids chaining but suffers from <em style="italic">crowding</em><em>; </em>because its score is based on the worst-case dissimilarity between pairs, a point can be closer to points in other clusters than to points in its own cluster. </p><p id="b1d71de84bea435c9e095205a28128cd"><em style="italic">Average Linkage Method </em>uses the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity overall points in opposite groups.</p><p id="e3a07aaa0be34776b79574e3075ce50c"><em style="italic">Centroid Linkage Method </em>is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.</p><p id="cbc1cd163ec2493fb59abc98780357aa"><em>Divisive Clustering. </em>This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.</p><table id="b976700f183d497a9906c380c5b115cd" summary="" rowstyle="plain"><cite id="i1b120a1ec864472da2ffe860aab3f496" /><caption><p id="b91b52793d3b45508d531e39dbff2852" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="fb5cf108b0fd471d925915321f19f720"><em>Reading: </em><link href="https://www.aclweb.org/anthology/W96-0103.pdf" target="new" internal="false">Hierarchical Clustering of Words and Application to NLP Tasks</link></p></td></tr></table></body></workbook_page>\n'