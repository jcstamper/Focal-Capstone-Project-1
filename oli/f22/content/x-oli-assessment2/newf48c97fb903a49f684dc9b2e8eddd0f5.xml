b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newf48c97fb903a49f684dc9b2e8eddd0f5" recommended_attempts="1" max_attempts="1"><title>Quiz 11</title><page id="ee7fb217628649dcb7e3fd95c9fb9741"><title>Assessment Page Title</title><multiple_choice id="newf48c97fb903a49f684dc9b2e8eddd0f5_1a" grading="automatic" select="single"><body><p id="f6e66991c6aa4763afa5522937650488">You are tasked with prototyping a natural language processing task to test the feasibility of the application of BERT. But, you do not have the resources that can fit the entire BERT model due to memory and system limitations. What variation of BERT can you use in this setting to produce results with minimal performance loss?</p></body><input shuffle="true" id="ans" labels="false"><choice value="yes">DistilBERT</choice><choice value="no">RoBERTa</choice><choice value="cacf06b88a5d49c989c2392a133dc358">ELECTRA</choice><choice value="f4715d29b1314cbb9df3ce03e371a59c">We can use BERT</choice></input><part id="c5f33815c68d4008b4bddb645c08122a"><response match="yes" score="10"><feedback><p id="f350584771a042b59d644e900412423d">Correct. DistilBERT has 40% fewer parameters than BERT-Base and runs 60% faster while preserving over 95% of BERT\xe2\x80\x99s performances. It reduced the number of layers in BERT by a factor of two.</p></feedback></response><response match="no" score="0"><feedback><p id="bb5d4bd727fd44db8cab0f0f7712d617">Incorrect. RoBERTa - This has the same number of params as BERT.</p></feedback></response><response match="cacf06b88a5d49c989c2392a133dc358" score="0"><feedback><p id="a16a521413de4a21a2e9f392526835d0">Incorrect. ELECTRA - This has the same number of params as BERT.</p></feedback></response><response match="f4715d29b1314cbb9df3ce03e371a59c" score="0"><feedback><p id="b1293230ebb14119816b4a9f23478083">Incorrect. BERT - The question specifies that it needs to be smaller than BERT.</p></feedback></response></part></multiple_choice></page><page id="e838c54f08b74c6f8bfd2761b63f9c63"><title>Assessment Page Title</title><multiple_choice id="d296530f466849d5bce8402baa72c4c9" grading="automatic" select="multiple"><body><p id="cb8a8b79972c447f85854b5817521908">Which factors could be the possible reasons for Vaswani et al. (2017)\xe2\x80\x99s decision to use a fixed formula (\\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\)) for the positional embedding?</p></body><input shuffle="false" id="b952121c649047c3bf4f5542a82bc491" labels="false"><choice value="A">The authors did not have enough computational resources to learn the positional embedding from data.</choice><choice value="B">The sine and cosine formulas may allow the model to learn to attend to relative positions.</choice><choice value="C">The sine and cosine formulas produced significantly better results than the learned embeddings.</choice><choice value="D">The sine and cosine formulas produced very similar results to the learned embeddings, so there was no advantage in allocating computational resources to learning the embeddings.</choice><choice value="E">The sine and cosine formula may allow the model to deal with sequences longer than the ones from the training data.</choice></input><part id="d832cc6fca2044efb97f5cce6fa7d0eb"><response match="D,C,B,A,E" score="0"><feedback><p id="b9f49aed58d54fb89ad62cd94dfcf256">Incorrect. Not all options are correct. Correct answers are B, D, and E.</p></feedback></response><response match="A" score="0"><feedback><p id="c7e0c31fca994fbdaa7ec39cfa3ad653">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="a2d809eeaeba45f092ecbe5cc405022d">Correct answers are B, D, and E.</p></feedback></response><response match="B" score="4"><feedback><p id="dce0849c90a448aca4cdb5247d9b0f7b">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="b2a71ba9db6648fc989c6ec02dbf292a">Correct answers are B, D, and E.</p></feedback></response><response match="C" score="0"><feedback><p id="e4e4660b79df4049a5be95cf900a62b7">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="c96e3693f1834895a6031d64eaae630a">Correct answers are B, D, and E.</p></feedback></response><response match="D" score="4"><feedback><p id="bb98d0a03d624d5f9e1c093cdcab1d63">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="a08814bec1ac44f2b9d07c4fa902b1c4">Correct answers are B, D, and E.</p></feedback></response><response match="E" score="4"><feedback><p id="a9273389fa5c40adb6e42bc45cf05732">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="cdb40b15e0ad491a936bc884f6f6c0f2">Correct answers are B, D, and E.</p></feedback></response><response match="A,B" score="3"><feedback><p id="e9be47546ede435a804b5ee15d7b62a2">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="cfa0f68be9634f988970e3ef40b7d0e2">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="fccdacc37e9a4550a3668219942f0e3f">Correct answers are B, D, and E.</p></feedback></response><response match="A,C" score="0"><feedback><p id="e6ef2898a4e348008e92890ff839e9aa">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="a516a197fe4d4aedaec5b040020a5890">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="d5829c313cdf4441b4bb521f031a0367">Correct answers are B, D, and E.</p></feedback></response><response match="D,A" score="3"><feedback><p id="d7044f0f7e4944feb7fdb80840ad5220">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="b4c0424a54a6484bbb4124cb9d44aba6">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="d2a84ba391eb47c084d34c3d0d9ce1a9">Correct answers are B, D, and E.</p></feedback></response><response match="A,E" score="3"><feedback><p id="b0f244fcf8bd4126abf4fa4e2b843564">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="fc2123131dbc4fb2836606222fb48c45">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="b58e7c0504324f688b5ae868f2cb2692">Correct answers are B, D, and E.</p></feedback></response><response match="B,C" score="3"><feedback><p id="dc1fe2e579024a38b94e36efe7495723">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="c6aa5adedc4c49649cf36adc6d2bf5cb">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="effc711788024e32b06b74201a40c0f7">Correct answers are B, D, and E.</p></feedback></response><response match="D,B" score="7"><feedback><p id="b7908563f12d4844a9d4f42757a4912b">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="dbeef50ace8745feb2223474960a992f">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="ff34a70a6b4a4abcbb2574359c3ad715">Correct answers are B, D, and E.</p></feedback></response><response match="B,E" score="7"><feedback><p id="b03aacc9241e4ab6a2235f0ab8a1d6e7">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="a254533410a74129a8611b3d4c1c86a1">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="b6e2b6ca7e614cec94436ae880713e6c">Correct answers are B, D, and E.</p></feedback></response><response match="D,C" score="3"><feedback><p id="d48de4f90f2e4bc7bcd77b1f5cefc78d">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="f2ba3c650f7f4931bdc357cefa3f2119">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="cfddb4b7a60e4bcb94644bfea772be87">Correct answers are B, D, and E.</p></feedback></response><response match="C,E" score="3"><feedback><p id="a72f1ab99bd442bfb7501d95936df18f">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="fbcab5ef030f4421be5ecb9c3e960c87">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="a26e668da8684418a8cd86b930c67ef8">Correct answers are B, D, and E.</p></feedback></response><response match="D,E" score="7"><feedback><p id="a352d519962b4cdcaa53d1af734835ec">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="c26b1e19b1904efbb3e25b6e6c4a4fc0">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="bb4e49431ef14bc0a4525ec6f6a23bdd">Correct answers are B, D, and E.</p></feedback></response><response match="A,B,C" score="2"><feedback><p id="d8c8b101c223491793fa8125b79c5110">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="a8a14b79ddc34f7aafd69cf58fe780f9">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="c4aba5b70e2f4d45ab748365750a8cca">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="cae8006bb6f047eeba083473f1ea9c36">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,B" score="6"><feedback><p id="ba5d52d1f0c64263be8e2f788a3f77e9">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="b6558929231e47c0974ed120b4b1645d">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="ef440453c85346258c20efcb04c5e92e">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="e0dfc21e811148a49c6bf5391647a92a">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,B" score="6"><feedback><p id="d5efbbdd4e1242639e24128bbbf442e9">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="c05ac1c706b34166bad7bc217323a57d">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="ecb006c96a864f87ab3566fd8b44e21e">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="ee60bfc6ba824b6892159df60a7aa6f2">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,C" score="2"><feedback><p id="b0121a03e4324aaa85e6c6aba985e7e6">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="ff2eb6436e93460e897115ccdfc25f63">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="ae8169c263b349ad99e0520009ae70de">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="e72c891421314413a3ed8d4f87cebbb3">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,C" score="2"><feedback><p id="adb0ff04887140db89cd44919682969a">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="fb48d219e7584ddd8d967cab893a5220">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="cc82a94a659e408a9ab0adbe107a1f50">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="f1ec20c54a174d95bccf729adec11831">Correct answers are B, D, and E.</p></feedback></response><response match="D,B,C" score="6"><feedback><p id="a42c108d724f42989998be6258d2deda">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="b75fc05028e94765a5888455fa64545d">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="e46863728e6b4beb9196e06704ea4a8e">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="c4084a4f6fbf4307a40031e20d1ae490">Correct answers are B, D, and E.</p></feedback></response><response match="E,B,C" score="6"><feedback><p id="e91ac391d5be44679c390877d8f704d2">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="d43c0b499004444abe86c322a0d7562a">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="ea3b5e8b69574fbb864efa9ce36686cc">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="b446bc5437164e00b5a7a982a6c46768">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,B" score="10"><feedback><p id="d19068a6c18f45f8a5281e49cef77f03">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="d17fd40f411e4acf8fac6541ee16229e">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="bc9552daa826407daacfeb6d8bb172f5">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="eb737199d91741e48a7074e09b8ed256">You got all the correct answers!</p></feedback></response><response match="D,E,C" score="6"><feedback><p id="dd60fb5a41684051856779163f92ba04">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="d9f04975806448abae271938919a778a">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="cf122b87588f4959996a36f006cfa817">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="c05421b03496466c820101e7c5c5a150">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,B,C" score="5"><feedback><p id="ab93dcd6ef3746f0964939b304ac8582">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="ae2eb81960f04f7bbabeb49b7f2b2021">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="f5ed5a2c4979461bbd4099548976ec77">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="d219937555164e21826f88d60f3d5d22">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="c85df6ca6783408bbf77f98e396d2ca6">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,B,C" score="5"><feedback><p id="b3830f596a5d4069b01121c44a145b71">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="ef015e305ca9453086f107c7c11aa656">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="d141a604f3c04d459e6eab0cc7e8ed0c">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="cc7f14fc44654673a06c3310ea6c9b08">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="a66ecfae1cbd4a4d9ecd2812355d4ff0">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,A,B" score="9"><feedback><p id="c779b84e485a4f59a28c07a883e0a014">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="dff6a4376c08439f8e2b6ef232074d83">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="bffe2c84106e447c993ee9743737e3f1">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="be3c588a76b64664a9c73d4d19a9dfcf">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="e96f1ab9cec74a20bd68d17eb2bb6d51">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,A,C" score="5"><feedback><p id="bd66c3ea81f34b12ac05a033abe939c7">A is incorrect. Section 3.5 did not discuss computational resources as a rationale for choosing one form of embedding over the other.</p><p id="eb1d2093f313424b8ff926d07fc8485a">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="f04ac3021bf849d389a9cabe17a3ee39">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="e9cf29579f33483ba07b7bb1c1623f60">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="d3d55d258813492db3bbc44d0c60a9c8">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,B,C" score="9"><feedback><p id="b87dd774371f4170acb894b0fb3da0cb">B is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="a9194b736b174b76a35b304aac1e6057">C is incorrect. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="de89c12ab10e417a8913b6c2ac17f8a7">D is correct. As mentioned in section 3.5, the fixed and learned embeddings produced nearly identical results.</p><p id="ba47cc2119894a7eaa39b0f2029d84de">E is correct. This is one of the reasons mentioned in Section 3.5.</p><p id="a7f9625eb3fc42dcaba6291bc17b1970">Correct answers are B, D, and E.</p></feedback></response><response match="A,B,C,D" name="AUTOGEN_{A,B,C,D}" score="0"><feedback><p id="dce86ce29d764c77bf800863ea8caf3c" /></feedback></response><response match="A,B,C,D,E" name="AUTOGEN_{A,B,C,D,E}" score="0"><feedback><p id="aba20a12af9947fba60ef239c7cc0789" /></feedback></response><response match="A,B,C,E" name="AUTOGEN_{A,B,C,E}" score="0"><feedback><p id="f3b88dc47c5c4899a4b204606529e120" /></feedback></response><response match="A,B,D" name="AUTOGEN_{A,B,D}" score="0"><feedback><p id="dfd3945082584cfa88401e01043bc598" /></feedback></response><response match="A,B,D,E" name="AUTOGEN_{A,B,D,E}" score="0"><feedback><p id="ee0868f2d91c435d9f9802b0bddb269a" /></feedback></response><response match="A,B,E" name="AUTOGEN_{A,B,E}" score="0"><feedback><p id="aaabef666c8f4e9cbe9ebc1ef118916c" /></feedback></response><response match="A,C,D" name="AUTOGEN_{A,C,D}" score="0"><feedback><p id="d482c069141f42639ba6dfbdab9d1a76" /></feedback></response><response match="A,C,D,E" name="AUTOGEN_{A,C,D,E}" score="0"><feedback><p id="c83848eeb5734341b01e8653891b862a" /></feedback></response><response match="A,C,E" name="AUTOGEN_{A,C,E}" score="0"><feedback><p id="c8bb8b4b887f48c1809f42377bcec3ec" /></feedback></response><response match="A,D" name="AUTOGEN_{A,D}" score="0"><feedback><p id="ac55ffc4c41242089654c943fada747b" /></feedback></response><response match="A,D,E" name="AUTOGEN_{A,D,E}" score="0"><feedback><p id="dedae37abbfa4011890d5e3554291e9a" /></feedback></response><response match="B,C,D" name="AUTOGEN_{B,C,D}" score="0"><feedback><p id="b47def805fd7412fab1c92c664ba1d5c" /></feedback></response><response match="B,C,D,E" name="AUTOGEN_{B,C,D,E}" score="0"><feedback><p id="aa0ed6eb0ddc489d9fdaec8536a59b54" /></feedback></response><response match="B,C,E" name="AUTOGEN_{B,C,E}" score="0"><feedback><p id="ddbdf85a6d594bea91dd9e0ce092da9a" /></feedback></response><response match="B,D" name="AUTOGEN_{B,D}" score="0"><feedback><p id="b61e8ddbba6b4c8bb459cea1ea5dd052" /></feedback></response><response match="B,D,E" name="AUTOGEN_{B,D,E}" score="0"><feedback><p id="b23b6757d69c49588aa759478fbbcff2" /></feedback></response><response match="C,D" name="AUTOGEN_{C,D}" score="0"><feedback><p id="d66790a812fe45fc881a1c3b6d38a259" /></feedback></response><response match="C,D,E" name="AUTOGEN_{C,D,E}" score="0"><feedback><p id="d5e31aa364d542bd9069db01ac876bf3" /></feedback></response></part></multiple_choice></page><page id="cb8ec031e5374c3e94980765fbf86665"><title>Assessment Page Title</title><multiple_choice id="a0942a10afb846f597d8f5cd87f4617f" grading="automatic" select="single"><body><p id="efe6f8b4eb6945669dfec9f01d17c1f6">Which of the following is incorrect about Next Sentence Prediction (NSP) in BERT?</p></body><input shuffle="false" id="f3e8f9d7665d44379334fe6413aaa3ab" labels="false"><choice value="aeb473f729a74cf6b6faddb1821a24d5">This step is used to train a model that understands sentence relationships.</choice><choice value="e9d7aa419ab64b4aab1488c9b89b24dd">50% of the time, the sentence pairs A and B are sequential sentences, while the other 50% time, they are chosen at random.</choice><choice value="a6e74c8eb8094db9a935f9c59f7ace46">This is beneficial for many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI).</choice><choice value="dc7c8396671744ea97aa72f7fac36b59">The Masked LM and Next Sentence Prediction techniques are never used together in training BERT.</choice><choice value="d7c209a6699648028502890bfe753be6">All of the above.</choice></input><part id="bfcb4b80ef324a86901b178cc1e56f0e"><response match="aeb473f729a74cf6b6faddb1821a24d5" score="0"><feedback><p id="ab23d2a3f2364ba1a8eea53b567dfb41">Incorrect. This is true. This is the purpose of NSP. The correct answer is D.</p></feedback></response><response match="e9d7aa419ab64b4aab1488c9b89b24dd" score="0"><feedback><p id="ece01dec7b1240128a05fe9cf4495629">Incorrect. This is true. The NSP model is 50% right order and 50% random ordered. The correct answer is D.</p></feedback></response><response match="a6e74c8eb8094db9a935f9c59f7ace46" score="0"><feedback><p id="db306467244748c686a0951cc8415afc">Incorrect. This is true. This is one of the benefits. The correct answer is D.</p></feedback></response><response match="dc7c8396671744ea97aa72f7fac36b59" score="10"><feedback><p id="f25e54824fa54cf0abddddd59f18c0bc">Correct. When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.</p></feedback></response><response match="d7c209a6699648028502890bfe753be6" score="0"><feedback><p id="ad10a586ee4e43598525f1901b26454a">Incorrect. Not all statements are incorrect about Next Sentence Prediction (NSP) in BERT. The correct answer is D.</p></feedback></response></part></multiple_choice></page><page id="f1182b19925c470bb1adb814403f3902"><title>Assessment Page Title</title><multiple_choice id="fe640153cbe945be83ca3ed748b769be" grading="automatic" select="multiple"><body><p id="ab1625f46a754baf9f7af97e986eccac">Based on Vaswani et al. (2017), how is multi-head attention employed in transformer architecture?</p></body><input shuffle="false" id="cdb93e3a88fc43eb9ad31716a4051d40" labels="false"><choice value="A">In the decoder block, multi-headed attention takes input from the word embedding with positional encoding constructed from the output sequence, allowing each position in the decoder to attend to all other positions.</choice><choice value="B">In the decoder block, multi-headed attention takes input from the word embedding with positional encoding constructed from the output sequence, allowing each position in the decoder to attend to all previous positions up to the current position.</choice><choice value="C">In the encoder block, multi-headed attention takes input from both the decoder and the word embedding with positional encoding, allowing every position in the encoder to attend overall positions in the output sequence.</choice><choice value="D">In the decoder block, multi-headed attention takes input from both the encoder and the previous attention layer, allowing every position in the decoder to attend overall positions in the input sequence.</choice><choice value="E">In the encoder block, multi-headed attention takes input from the word embedding with positional encoding constructed from the input sequence, allowing each position in the encoder to attend to all positions in the previous layer of the encoder.</choice></input><part id="c622fd358bb44dc992391eeea3c60c82"><response match="D,C,B,A,E" score="0"><feedback><p id="b19ceb840c2f41f59555d24236ab3e83">Incorrect. Not all options are correct. Correct answers are B, D, and E.</p></feedback></response><response match="A" score="0"><feedback><p id="ddf67c6d3bdd4265b793c7bde07a8906">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="f78eda59e7e34d499cb4f3b27e9950cc">Correct answers are B, D, and E.</p></feedback></response><response match="B" score="4"><feedback><p id="b30efd05374a4d159774dbc77c8c2ef9">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="a35d0b85ab8d4b479d446c49a387f125">Correct answers are B, D, and E.</p></feedback></response><response match="C" score="0"><feedback><p id="bfbc442660854a1bb54a9bd46344c818">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="e49d86800a1449bb9259c5246d8e628c">Correct answers are B, D, and E.</p></feedback></response><response match="D" score="4"><feedback><p id="b143df19a13147e481b7268bbfce60be">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="a884be41c19e447286b8ef0def91b5f4">Correct answers are B, D, and E.</p></feedback></response><response match="E" score="4"><feedback><p id="e1dbcea52de74888876a579b2e990510">E is correct. This is the role of the attention unit in the encoder block.</p><p id="d73ac56422fd4b0bafe762580274d96d">Correct answers are B, D, and E.</p></feedback></response><response match="A,B" score="3"><feedback><p id="d93854c2bf3b485189906a2a80b1d350">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="db402d472ebf40b4a584b146ca3867c8">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="d2d9cf7e7159459392f8cdace061fc84">Correct answers are B, D, and E.</p></feedback></response><response match="A,C" score="0"><feedback><p id="c5469495006245df96d08ec673cbb4b1">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="c7dca6fd67b2407fbddda9b67312a314">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="c1f47c7aef0d4789b2016427355cd08f">Correct answers are B, D, and E.</p></feedback></response><response match="D,A" score="3"><feedback><p id="cfcd4baae51b43e7988126af673108c8">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="eabc1acb5b114d72b922d9006322afe6">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="d4db4cf111fb44b38e3f11a624cd780c">Correct answers are B, D, and E.</p></feedback></response><response match="A,E" score="3"><feedback><p id="f2588a2049b54f9896037849924db941">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="fdddeed0904a47e38cef83514915ee61">E is correct. This is the role of the attention unit in the encoder block.</p><p id="e8ad7a0545ec4811bf84c67d6917a3f1">Correct answers are B, D, and E.</p></feedback></response><response match="B,C" score="3"><feedback><p id="fe0b613035114d9bab06a3fee2afb7a4">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="c37a927957474ad9b7abe4b3c3888b54">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="ed01d7d0316043009a0b1ab56141ab8f">Correct answers are B, D, and E.</p></feedback></response><response match="D,B" score="7"><feedback><p id="da37b9c396754e45bad3ef6d0fd27d36">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="bb393bfeedd543089858369f54349f1f">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="ed23aeba300549898af023ce640b72bb">Correct answers are B, D, and E.</p></feedback></response><response match="B,E" score="7"><feedback><p id="d7a37d1e684548ce8950ab48ab79474a">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="ea4ebed428084f48865a9900e95742da">E is correct. This is the role of the attention unit in the encoder block.</p><p id="d2d17a4c7f85427e83769a3451e2820f">Correct answers are B, D, and E.</p></feedback></response><response match="D,C" score="3"><feedback><p id="aa2812687cbd4ae991ce884a39c43e1b">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="c839e5400d2d4ff180f8037ae338dfd4">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="c4093fe01d0941dfbca54c0f500b3bd0">Correct answers are B, D, and E.</p></feedback></response><response match="C,E" score="3"><feedback><p id="c1321e9d28a649aa9d1316ee3f5be6c1">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="ca649c23f3ae400cb2b2890977510ae2">E is correct. This is the role of the attention unit in the encoder block.</p><p id="b8ab886682424d58a5ed641a40454709">Correct answers are B, D, and E.</p></feedback></response><response match="D,E" score="7"><feedback><p id="ff0fefb4cdab444ea89dc376c419217c">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="dd9174c638d54897b25a8dc5bafdab5c">E is correct. This is the role of the attention unit in the encoder block.</p><p id="adea5ecaa230455b9787d505c4f43d3b">Correct answers are B, D, and E.</p></feedback></response><response match="A,B,C" score="2"><feedback><p id="bf5bbf06986e43e5a48869048cc7d65e">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="a92edc3c46b14c09a8dd46fdbc2be3ff">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="d01836262e5d4a4f9f449ab94006f73d">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="ea7bc5c5362d475c84a8a98db1ac5866">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,B" score="6"><feedback><p id="ffd119f091b147829607e5d4fe411b3e">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="c281af251dd44ba487dcafaca5bafd08">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="ca56c0bbfb0949b8836d9b92099f0f72">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="b01c61d36a654cbbaa8519328a35293d">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,B" score="6"><feedback><p id="f18cac79c4e944cf8415596161e1a44c">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="fd98ef0c2cf14fa4932bf871fcd5e507">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="eb87c518ccd6406c9e2a49f766202adf">E is correct. This is the role of the attention unit in the encoder block.</p><p id="a476fe1fc97d4d2badbfd6f6ee400511">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,C" score="2"><feedback><p id="bf84f69b062c4298ae14b9aaf4db4f56">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="e6f07767aaa3413b810d7d9f8b0fb71c">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="a32e00796d7c4cc09ab1c0b2e6480cd1">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="cca8e3cfd1974a8090d584b294353067">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,C" score="2"><feedback><p id="c0d7fee3850e4176b69a5ce0bbb66e16">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="fa28e095e4454ac4aa8808c75698763c">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="a73facc38d9c4d0287d7d433c44ed549">E is correct. This is the role of the attention unit in the encoder block.</p><p id="f197b28459a742f4a73143473c3a9f1a">Correct answers are B, D, and E.</p></feedback></response><response match="D,B,C" score="6"><feedback><p id="b5b7cbfdcd17433486a6f690dec32876">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="c7e0b42c50a040e0b3a419d59d47e287">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="b09e840721ed4a469e8d5b4e06f82b4b">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="ef1eb5a48ea446e09d2422a367769df5">Correct answers are B, D, and E.</p></feedback></response><response match="E,B,C" score="6"><feedback><p id="dc0b25ef051d424d9cd73bad57110a25">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="d9f5e1036ba145e0a762d4e3af752947">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="df941e65dba8403b85b93938d040ca58">E is correct. This is the role of the attention unit in the encoder block.</p><p id="ddc8fdb12ab2478991210c165b4e0f6a">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,B" score="10"><feedback><p id="a38aa33494eb40e296e7316cc1510150">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="ab0a081d16244a44a2ce5fce67a9025e">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="b1960cc8b9f74b8e8f87ad704b5b7d78">E is correct. This is the role of the attention unit in the encoder block.</p><p id="fc0fc3c27e1d4aa7b2521db31a064e93">You got all the correct answers!</p></feedback></response><response match="D,E,C" score="6"><feedback><p id="f43d0c481bf6418b929dc256325b0cca">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="e852bd78e8cd4c8397c79cb00b1cf5e2">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="d7fbc11109114c919973f7e05bb420d6">E is correct. This is the role of the attention unit in the encoder block.</p><p id="b674b469eb5a48cfa3fd7ba2006c724e">Correct answers are B, D, and E.</p></feedback></response><response match="D,A,B,C" score="5"><feedback><p id="d5d7dc8464af4bb5ae8442eb0e91a064">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="c9ba8d68a2584276b4e6a77597a67b12">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="bc074cacbe85412c901d53e7ff34eea3">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="d0211f378ca247edb9e64215b56ff3ef">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="bc97cd12efb94c9c8ab939bcb65f385a">Correct answers are B, D, and E.</p></feedback></response><response match="E,A,B,C" score="5"><feedback><p id="c012bc09a1924f70acb247216ed510e9">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="e6c34687b55b4bbe9a62b4f781992164">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="a9ab8d00cea245f7b85e2581b142ec6e">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="a65eb9515e244f99b7c036f1a49e9138">E is correct. This is the role of the attention unit in the encoder block.</p><p id="ba189e9acd934a4a83efc3ae3d776124">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,A,B" score="9"><feedback><p id="af5c4608bff0475e98d2db73aa76183a">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="acb661cd5832411dba5f5751ec6692ba">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="e9668a481cb8435ba705b8814fc3726c">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="c773894005fa439ba91ed93f79f231f3">E is correct. This is the role of the attention unit in the encoder block.</p><p id="e536bd3c51414964a31a73d105a3bf1d">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,A,C" score="5"><feedback><p id="d0b0b41bc25744b6aff8e02364a9bb81">A is incorrect. In the decoder block, the attention unit that follows the input + positional embedding is masked, so it only allows each position in the decoder to attend to all previous positions up to the current position.</p><p id="c6adeb7e46d2485d8d38437e15dac011">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="b585d27ccc15446199c9080dc1465732">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="b05293e1770a4fad817455f173fb211e">E is correct. This is the role of the attention unit in the encoder block.</p><p id="da104807bb894b798da334c165823945">Correct answers are B, D, and E.</p></feedback></response><response match="D,E,B,C" score="9"><feedback><p id="cea7ebc242cd491789d9d32439ad9d26">B is correct. This is the role of the first attention unit in the decoder block.</p><p id="dac8154244924f0faafcbbf9aec0e0fb">C is incorrect. The attention unit in the encoder block does not take input from the decoder block.</p><p id="efdc74b9bd6a4e0bb2a6cbc133b2c6d8">D is correct. This is the role of the second attention unit in the decoder block.</p><p id="c5c82247eb964bf49f44d3a4c9039d67">E is correct. This is the role of the attention unit in the encoder block.</p><p id="e4a6b04a28964a759dc148c7c40af527">Correct answers are B, D, and E.</p></feedback></response><response match="A,B,C,D" name="AUTOGEN_{A,B,C,D}" score="0"><feedback><p id="a75a7c8e58944f3ead160688e0b7b8f3" /></feedback></response><response match="A,B,C,D,E" name="AUTOGEN_{A,B,C,D,E}" score="0"><feedback><p id="cb39f40cfd1c4f129a339950a671f8a9" /></feedback></response><response match="A,B,C,E" name="AUTOGEN_{A,B,C,E}" score="0"><feedback><p id="a815bf2c9d774e9ebdba64fed76c5921" /></feedback></response><response match="A,B,D" name="AUTOGEN_{A,B,D}" score="0"><feedback><p id="fde2dc36077a47acaa69b6d6f86fb7ec" /></feedback></response><response match="A,B,D,E" name="AUTOGEN_{A,B,D,E}" score="0"><feedback><p id="ccb234c5acf146939a75f06c16159650" /></feedback></response><response match="A,B,E" name="AUTOGEN_{A,B,E}" score="0"><feedback><p id="aa51c255a5a844b3a70fd52fc7b251d6" /></feedback></response><response match="A,C,D" name="AUTOGEN_{A,C,D}" score="0"><feedback><p id="dd1206a0e9014a00b55d6234b188cf8e" /></feedback></response><response match="A,C,D,E" name="AUTOGEN_{A,C,D,E}" score="0"><feedback><p id="cf147bd4f93b4a42a73052bdd700be99" /></feedback></response><response match="A,C,E" name="AUTOGEN_{A,C,E}" score="0"><feedback><p id="c6285d6383c84de19dd14045ae5ab67e" /></feedback></response><response match="A,D" name="AUTOGEN_{A,D}" score="0"><feedback><p id="b28986bf09fb491a9a1ef28f5c495c89" /></feedback></response><response match="A,D,E" name="AUTOGEN_{A,D,E}" score="0"><feedback><p id="d09a6918e39549b3b3b4ca44a8ae31b2" /></feedback></response><response match="B,C,D" name="AUTOGEN_{B,C,D}" score="0"><feedback><p id="fc7b701ca7ed4d8a94b8ff816d89e9c1" /></feedback></response><response match="B,C,D,E" name="AUTOGEN_{B,C,D,E}" score="0"><feedback><p id="ab9ad8aed42d4220b940cead708bf622" /></feedback></response><response match="B,C,E" name="AUTOGEN_{B,C,E}" score="0"><feedback><p id="d6d727642d774826a91ad045248ea608" /></feedback></response><response match="B,D" name="AUTOGEN_{B,D}" score="0"><feedback><p id="f269214111414dd5bd70d0da0616d461" /></feedback></response><response match="B,D,E" name="AUTOGEN_{B,D,E}" score="0"><feedback><p id="d3dea91ef5d845549d6fb32b5006aa70" /></feedback></response><response match="C,D" name="AUTOGEN_{C,D}" score="0"><feedback><p id="bcba8ee69a6546038dfc645da20c5b19" /></feedback></response><response match="C,D,E" name="AUTOGEN_{C,D,E}" score="0"><feedback><p id="ce42bad105d7429db7c89f611f4e740e" /></feedback></response></part></multiple_choice></page><page id="fd7fc1c4b37e42cc86c0150d32104a8b"><title>Assessment Page Title</title><multiple_choice id="d2daac5cba4a4b609f198e5b4b6468bd" grading="automatic" select="multiple"><body><p id="a6242cda268042a99e37cf81571118da">According to Vaswani et al. (2017), which of the followings is true about the attention formula \\(\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt d_k} \\right)V\\)?</p></body><input shuffle="false" id="b9814c7f7e104539a17e5f0dc1937396" labels="false"><choice value="A">It is similar to the alternative, additive attention, in computational complexity.</choice><choice value="B">It is identical to the dot-product (multiplicative) attention.</choice><choice value="C">The scaling factor \\(\\frac{1}{\\sqrt d_k}\\) helps counteract the vanishing gradient problem when \\(d_k\\) is small.</choice><choice value="D">The scaling factor \\(\\frac{1}{\\sqrt d_k}\\) helps counteract the vanishing gradient problem when \\(d_k\\) is large.</choice><choice value="E">It is faster and consumes less memory than additive attention.</choice></input><part id="fa0fe3526fa5460da1f4dc57acb5f7c2"><response match="D,C,B,A,E" score="0"><feedback><p id="f6f3cc83b63b499895b0081f94c5df3b">Incorrect. Not all options are correct. Correct answers are A, D, and E.</p></feedback></response><response match="A" score="4"><feedback><p id="a5f0d80f176c4b40912e595ab1400813">A is correct. This is mentioned in Section 3.2.1.</p><p id="ee43809a855d46aaab24de65596d63d5">Correct answers are A, D, and E.</p></feedback></response><response match="B" score="0"><feedback><p id="df5b9743261a456cb6f0fb90b3ae1ce6">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="a26d7e770f0f469f9c2fe195f32d4e31">Correct answers are A, D, and E.</p></feedback></response><response match="C" score="0"><feedback><p id="b66b85310b5240b29bb2eeb8d9d7e850">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="f269e10b61b94e90ae0aceb6bcc27064">Correct answers are A, D, and E.</p></feedback></response><response match="D" score="4"><feedback><p id="b935808ccf40438686e925cd9a0cd124">D is correct. This is mentioned in Section 3.2.1.</p><p id="db6b9d628f7449eea65a7295fe57003b">Correct answers are A, D, and E.</p></feedback></response><response match="E" score="4"><feedback><p id="b6cb5180c0f847849265affd9030a42b">E is correct. This is mentioned in Section 3.2.1.</p><p id="c80380c9c9984fb588198d03e156f715">Correct answers are A, D, and E.</p></feedback></response><response match="A,B" score="3"><feedback><p id="c15d52a194334c699ef37d561e0682b8">A is correct. This is mentioned in Section 3.2.1.</p><p id="d5d1e44eded342c48ad06fa2fb34ee1b">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="f4fab241000e433482422663cfde3e2a">Correct answers are A, D, and E.</p></feedback></response><response match="A,C" score="3"><feedback><p id="ecdbae5e27a64fd1a9fa417c16ee1572">A is correct. This is mentioned in Section 3.2.1.</p><p id="fe5a0acb0e7148e8af213bb976e17fce">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="a8e2a6e46f48471f817b02dbad2122a8">Correct answers are A, D, and E.</p></feedback></response><response match="D,A" score="7"><feedback><p id="be185259d0b94ea79d6ca783549d98b5">A is correct. This is mentioned in Section 3.2.1.</p><p id="c5a29c782d19453c8472ae9097ea4651">D is correct. This is mentioned in Section 3.2.1.</p><p id="c8ea500b4ba74b528bb352dae9609425">Correct answers are A, D, and E.</p></feedback></response><response match="A,E" score="7"><feedback><p id="f1046a26c7d34f4089601b756a325b00">A is correct. This is mentioned in Section 3.2.1.</p><p id="b7eead1233a2490f9aa0bb269c034e40">E is correct. This is mentioned in Section 3.2.1.</p><p id="b7b3bdcad030431c9051a4c3e6ff9e20">Correct answers are A, D, and E.</p></feedback></response><response match="B,C" score="0"><feedback><p id="cc85eccb6e1a42d4ad8b150c43c6a8a0">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="d07e85e1d1a040a487fe13b8c02090b4">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="c0d14a3850fc4d58846b7a1a9488bf1f">Correct answers are A, D, and E.</p></feedback></response><response match="D,B" score="3"><feedback><p id="f18e0f383cee43279055e23d33ae02a9">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="aa6af0e0e33d458a846c2bf77c412a02">D is correct. This is mentioned in Section 3.2.1.</p><p id="de8a31723b4e44a890e044d616ab1d94">Correct answers are A, D, and E.</p></feedback></response><response match="B,E" score="3"><feedback><p id="c628893cd60d497bb75deae9056d9779">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="df3279611b7a477dbc2761afb242777c">E is correct. This is mentioned in Section 3.2.1.</p><p id="ad71ecd6745b4d869c1017563deda7d9">Correct answers are A, D, and E.</p></feedback></response><response match="D,C" score="3"><feedback><p id="a780aa7f208b4a17981763eb34480027">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="bf0333e6037b48be8cad6d6c3f5ce5d4">D is correct. This is mentioned in Section 3.2.1.</p><p id="d3dbc85b3d3447f497dd2bacf508db88">Correct answers are A, D, and E.</p></feedback></response><response match="C,E" score="3"><feedback><p id="c7bf0a43e5a0434da36c0dddff25d36c">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="a9b5ffa70b7b4bac8771ba80a733bc32">E is correct. This is mentioned in Section 3.2.1.</p><p id="b67e44eba53245b2bb14d19159303df9">Correct answers are A, D, and E.</p></feedback></response><response match="D,E" score="7"><feedback><p id="c296c4a11f804dbfa1168716db501244">D is correct. This is mentioned in Section 3.2.1.</p><p id="e87aa196fe9f423d981a6565f4b5952d">E is correct. This is mentioned in Section 3.2.1.</p><p id="e8c61b1086f5424ead753f163c975156">Correct answers are A, D, and E.</p></feedback></response><response match="A,B,C" score="2"><feedback><p id="f7b8a46db1d04fa08aa62d6dc86f838c">A is correct. This is mentioned in Section 3.2.1.</p><p id="e8df5c0ddcc04ab4bcbda4c34a53f709">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="b6926ef664cc4150ad966e136e86e272">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="f18c8a85c25b414aada9a4b188491f74">Correct answers are A, D, and E.</p></feedback></response><response match="D,A,B" score="6"><feedback><p id="f12eb4d0a76a41078b698255e958b7c4">A is correct. This is mentioned in Section 3.2.1.</p><p id="f9119a8d2c734655a69bd62c743f64c6">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="bafc2e701bf9408992ea1a147e03d588">D is correct. This is mentioned in Section 3.2.1.</p><p id="e98d760399f34fc8b4d97d03548d305d">Correct answers are A, D, and E.</p></feedback></response><response match="E,A,B" score="6"><feedback><p id="e63be1bdf31a4d5abbd800880033b362">A is correct. This is mentioned in Section 3.2.1.</p><p id="fc62380814284f64895f0f5e9ca4413c">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="fb740a670c5047d487afb4c4fbcf1761">E is correct. This is mentioned in Section 3.2.1.</p><p id="db1ecd5bf6334d0a998eaf655c75deb5">Correct answers are A, D, and E.</p></feedback></response><response match="D,A,C" score="6"><feedback><p id="b49d5438f5f44af79419d9bba140265d">A is correct. This is mentioned in Section 3.2.1.</p><p id="dcf7de2f132d4338bf467b2ac42a474b">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="c59af1f0c55b4936a773eea0d1399809">D is correct. This is mentioned in Section 3.2.1.</p><p id="a31f20ba7c98481ea17edefa7d5064c1">Correct answers are A, D, and E.</p></feedback></response><response match="E,A,C" score="6"><feedback><p id="f11c679b1bf44799803be4e4b397b785">A is correct. This is mentioned in Section 3.2.1.</p><p id="aa5ccd049ace407ab43f39097edd826d">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="ac627a61afe14887a328e80909db8cdd">E is correct. This is mentioned in Section 3.2.1.</p><p id="a6a936a780e3437c93baf6a59d47629a">Correct answers are A, D, and E.</p></feedback></response><response match="D,B,C" score="2"><feedback><p id="db3324a7f4814265900097e23837c48b">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="fb7749d6eeaf47d98cc5b37cf21aab4e">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="c57e281065074601956f021a57b580c2">D is correct. This is mentioned in Section 3.2.1.</p><p id="f865456b113b45fb8145ec37f3e08676">Correct answers are A, D, and E.</p></feedback></response><response match="E,B,C" score="2"><feedback><p id="f6a4d8b122294ba4bfad6d1bf5695e56">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="b0ab40ccf60940c9b908e1d18d0953df">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="d9f17ed940d24280a3c8c94e52d0bb3c">E is correct. This is mentioned in Section 3.2.1.</p><p id="f5ac71d6740a4f2c8bd83e94f53ca83d">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,B" score="6"><feedback><p id="d74f0e2ff25f48d28d2c515a1d970559">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="e58810b9e4cc4479a24033839291b6b5">D is correct. This is mentioned in Section 3.2.1.</p><p id="b9765a24252d4eddba54d20a0a147e3c">E is correct. This is mentioned in Section 3.2.1.</p><p id="bda8e89826df4a908829ff7e68920bd6">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,C" score="6"><feedback><p id="ff445895b0a3478ebee43e693f787662">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="d204f46aa62c4a16af7c6aed2caa1cb1">D is correct. This is mentioned in Section 3.2.1.</p><p id="ca8c6da84797449b85015c776b443dd2">E is correct. This is mentioned in Section 3.2.1.</p><p id="d50f7ad4b659489d881bb1f7d93bd2e8">Correct answers are A, D, and E.</p></feedback></response><response match="D,A,B,C" score="5"><feedback><p id="e548fd28888d475b819f17d1df3b2dd0">A is correct. This is mentioned in Section 3.2.1.</p><p id="f2e59321f8a94e7ba2b4ee6bff25f916">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="c762bf1c39bc4cb1a35ffcaffe2c568b">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="afb49f7d69034060b8397c1cdbb7f8b5">D is correct. This is mentioned in Section 3.2.1.</p><p id="e0b1f28ffb384f37b34b6c93c8830c85">Correct answers are A, D, and E.</p></feedback></response><response match="E,A,B,C" score="5"><feedback><p id="a5d78515f2af4401a8379a7d3e11d891">A is correct. This is mentioned in Section 3.2.1.</p><p id="c27187291eda415e8b52991d5c114fe4">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="c5360895ac5a4086b9a18ffded57eaa8">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="f48c67b8b2284612a23b3d6f6e3f4b48">E is correct. This is mentioned in Section 3.2.1.</p><p id="f562af22e564431ba15308c74c584231">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,A,B" score="9"><feedback><p id="a513bcdb68e94372ab39efce6a1b6546">A is correct. This is mentioned in Section 3.2.1.</p><p id="cc4ef9e9d41242399f813245c457b884">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="c15f9f7a3fd9428b9444aa13b6fcaf8a">D is correct. This is mentioned in Section 3.2.1.</p><p id="f9219ee1d43a412399f1176ca8d45cb7">E is correct. This is mentioned in Section 3.2.1.</p><p id="a056ec5e712f47d1853bcbcc051f4160">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,A,C" score="9"><feedback><p id="f63b0f1d9baf4c4b846c4e48e852bc7d">A is correct. This is mentioned in Section 3.2.1.</p><p id="bdcb33c7d10242be91dc3a6a45f8e3ee">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="ca3ceec6cc914b7ea5e8077ae2593984">D is correct. This is mentioned in Section 3.2.1.</p><p id="d6c1a21d824f4fc18dc25c6ab6ef77ed">E is correct. This is mentioned in Section 3.2.1.</p><p id="f6fe7661e08e446ca648f4c49c047612">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,B,C" score="5"><feedback><p id="b85c5281b77e4e45b60000566a4c279b">B is incorrect. This attention formula introduces an additional scaling factor.</p><p id="bb93abe5493346eb9c3d955e9a65fe68">C is incorrect. The vanishing gradient problem only shows up when \\(d_k\\) is large.</p><p id="a78e6bfc3ab4407da0c019ffa7817ec5">D is correct. This is mentioned in Section 3.2.1.</p><p id="d64162256fba4d17b7164cf04c8aabcc">E is correct. This is mentioned in Section 3.2.1.</p><p id="e0c0e05df2284cbdba278d524197befc">Correct answers are A, D, and E.</p></feedback></response><response match="D,E,A" score="10"><feedback><p id="aa7375dba6f14215aee9d31db4926594">Correct!</p></feedback></response><response match="A,B,C,D" name="AUTOGEN_{A,B,C,D}" score="0"><feedback><p id="aedf16f4f51049dd84293f3d1428e409" /></feedback></response><response match="A,B,C,D,E" name="AUTOGEN_{A,B,C,D,E}" score="0"><feedback><p id="f26497d9c5104670bd1bc37c4dd4559f" /></feedback></response><response match="A,B,C,E" name="AUTOGEN_{A,B,C,E}" score="0"><feedback><p id="b0384156fa794eb6b3b8c0ce47d7f47a" /></feedback></response><response match="A,B,D" name="AUTOGEN_{A,B,D}" score="0"><feedback><p id="eacd9a85a61a4b8dbcd6acc272a482b6" /></feedback></response><response match="A,B,D,E" name="AUTOGEN_{A,B,D,E}" score="0"><feedback><p id="cc5d44c4d0aa4764a20c877e664e8eae" /></feedback></response><response match="A,B,E" name="AUTOGEN_{A,B,E}" score="0"><feedback><p id="b233ac247e1d42bd9ecbfc9ad8f2a0a6" /></feedback></response><response match="A,C,D" name="AUTOGEN_{A,C,D}" score="0"><feedback><p id="ba3244b739684564934eb92d8159af90" /></feedback></response><response match="A,C,D,E" name="AUTOGEN_{A,C,D,E}" score="0"><feedback><p id="c3086a2c306b4306b6a543ecaff74df8" /></feedback></response><response match="A,C,E" name="AUTOGEN_{A,C,E}" score="0"><feedback><p id="d1361263d8674ab1ba5c6005a34e9435" /></feedback></response><response match="A,D" name="AUTOGEN_{A,D}" score="0"><feedback><p id="c1d31c06d1ba4a73a03127e7d75f4053" /></feedback></response><response match="A,D,E" name="AUTOGEN_{A,D,E}" score="0"><feedback><p id="fd48ff5cb26646faa09cf44c46acac8f" /></feedback></response><response match="B,C,D" name="AUTOGEN_{B,C,D}" score="0"><feedback><p id="c5a6bf840418418c965fb06aefc2e650" /></feedback></response><response match="B,C,D,E" name="AUTOGEN_{B,C,D,E}" score="0"><feedback><p id="d30a51f19f514bfe9ed6e9b48aae7ff7" /></feedback></response><response match="B,C,E" name="AUTOGEN_{B,C,E}" score="0"><feedback><p id="a215fa7ab13648e7887d7133a0097414" /></feedback></response><response match="B,D" name="AUTOGEN_{B,D}" score="0"><feedback><p id="b2ea7ad36c0d442e85598b6260538c60" /></feedback></response><response match="B,D,E" name="AUTOGEN_{B,D,E}" score="0"><feedback><p id="bab52bb3a3bb4d66b91381d29492cb8b" /></feedback></response><response match="C,D" name="AUTOGEN_{C,D}" score="0"><feedback><p id="aa774da6a449416c9f7d0f3fbee6c800" /></feedback></response><response match="C,D,E" name="AUTOGEN_{C,D,E}" score="0"><feedback><p id="a908a7cccbc34824b3d06b63ba66010b" /></feedback></response></part></multiple_choice></page><page id="ea79cc4590374e1ebbd2833aedd1aa24"><title>Assessment Page Title</title><multiple_choice id="b937fb71d7b148458a89ed4ac5152683" grading="automatic" select="single"><body><p id="d298aab0fa704fb181d9371baf1462f8">BERT inference is substantially faster when compared to other language models. What is the main reason behind this efficiency?</p></body><input shuffle="true" id="f5c272ac43ff46858b41b1f01e52aa59" labels="false"><choice value="da758b9517d1457fa27025edff82eb61">BERT reads and processes the whole input sequence at once.</choice><choice value="ff2cc7bd983c45779fa5ab9d20511859">BERT\xe2\x80\x99s architecture is unified across different tasks.</choice><choice value="dd98b4fc869e45b38434e20402481344">BERT uses a technique called \xe2\x80\x9cmasked LM\xe2\x80\x9d in training.</choice><choice value="d18b4684f4cd49e4b8f1a00598e7dc75">BERT is trained on a large wikitext and book corpus beforehand.</choice><choice value="a3d415601a6e47e692ebdaf11aab5579">BERT\xe2\x80\x99s attention heads can be configured.</choice></input><part id="b0ed06b07b4e4745a8b4175b8a30b390"><response match="da758b9517d1457fa27025edff82eb61" score="10"><feedback><p id="d1e16f89a91e4dd695fc4604fca5dc02">Correct. BERT reads the whole input sequence at once, making it faster to generate model outputs.</p></feedback></response><response match="ff2cc7bd983c45779fa5ab9d20511859" score="0"><feedback><p id="d2ee70af495540af93045f0f6a5c29eb">Incorrect. This has no relevance to efficient inference. While it does help training, it does not necessarily help inference. The correct answer is BERT reads the whole input sequence at once, making it faster to generate model outputs.</p></feedback></response><response match="dd98b4fc869e45b38434e20402481344" score="0"><feedback><p id="f3f01ed49d434af2ae1f0adcf75ed86d">Incorrect. This has no relevance to efficient inference. The correct answer is BERT reads the whole input sequence at once, making it faster to generate model outputs.</p></feedback></response><response match="d18b4684f4cd49e4b8f1a00598e7dc75" score="0"><feedback><p id="c04c94180305440a8a8773de3b254b2b">Incorrect. This has no relevance to efficient inference. The correct answer is BERT reads the whole input sequence at once, making it faster to generate model outputs.</p></feedback></response><response match="a3d415601a6e47e692ebdaf11aab5579" score="0"><feedback><p id="ca6f8107dcfd4a599c29fd8dc8ceaeba">Incorrect. This has no relevance to efficient inference. The correct answer is BERT reads the whole input sequence at once, making it faster to generate model outputs.</p></feedback></response></part></multiple_choice></page><page id="e9a45d3b0f7f423e863acd3717e89570"><title>Assessment Page Title</title><multiple_choice id="aa355ca8e8ed43df8b4dd047f8ea52db" grading="automatic" select="single"><body><p id="cdd0c001cfe846159ac7101b7dff4f31">When do Transformers use self-attention?</p></body><input shuffle="true" id="b8d69e00760c4fc68a36af9b189da40a" labels="false"><choice value="b5b5fdb8f0d84921a4f101ab41c3f3dc">While encoding only</choice><choice value="e00b19e8581a4c398c606265ee2526f9">While decoding only</choice><choice value="d809e338cd5f4f0ea0fccc05f91f5cff">While encoding and decoding</choice><choice value="f7c5e0a049f340a8a442bae72299cd1e">They do not use self-attention</choice><choice value="c0f09f5c7b9a4375b1e0d16ac40774c8">During another stage</choice></input><part id="d9dd3f5ea58c43d38171afd16f540c10"><response match="b5b5fdb8f0d84921a4f101ab41c3f3dc" score="0"><feedback><p id="c34d6b5256dc4c15b3833a69d3a334bf">Incorrect. Self-attention is not used just while encoding. Self-attention is used during both encoding and decoding.</p></feedback></response><response match="e00b19e8581a4c398c606265ee2526f9" score="0"><feedback><p id="f6828b6a92404600a0c0c300dfd64ac9">Incorrect. Self-attention is not used just while decoding. Self-attention is used during both encoding and decoding.</p></feedback></response><response match="d809e338cd5f4f0ea0fccc05f91f5cff" score="10"><feedback><p id="e06da6486e3a45eb89ff2bc7de158ee7">Correct. Self-attention is used during both encoding and decoding.</p></feedback></response><response match="f7c5e0a049f340a8a442bae72299cd1e" score="0"><feedback><p id="b77a9f04829f4d30b5c313b4730adcb9">Incorrect. Transformers do use self-attention. Self-attention is used during both encoding and decoding.</p></feedback></response><response match="c0f09f5c7b9a4375b1e0d16ac40774c8" score="0"><feedback><p id="c49210a1389348ae914831eed04639d6">Incorrect. Self-attention is used during encoding and decoding.</p></feedback></response></part></multiple_choice></page><page id="bb204f34793244ee946d51d4aa863a39"><title>Assessment Page Title</title><multiple_choice id="dffcce67a386483db934341485f99f7b" grading="automatic" select="single"><body><p id="bfa111adcf8842eaa4a9df74f4065d29">Which of the following issues in natural language processing can be addressed by transformers but not by earlier research mentioned in the introduction section of Vaswani et al. (2017)?</p></body><input shuffle="true" id="de8245883efe439dab0cb93453367d13" labels="false"><choice value="f810df5cacc64267a0dc51d20eadedc7">The vanishing/exploding gradient problem poses difficulty in processing long sequences.</choice><choice value="fe795caa888f478babdfc7c4c47069b6">The sequential computation prevents the efficiency gained from parallelization.</choice><choice value="f0672dd12a1c469aa60c5c7d5fa3b178">The inability to model dependencies without regard to their distance in the input or output sequence.</choice><choice value="c058c136cca64fa895aaa2e3277c3caf">The inability to incorporate attention mechanisms into a recurrent neural network.</choice><choice value="b1e9102ed4b0474aa4134d72def4edb2">The lack of factorization techniques that can improve the computational efficiency of recurrent neural works.</choice></input><part id="fc3dd68839d441029c0adc87c3977c48"><response match="f810df5cacc64267a0dc51d20eadedc7" score="0"><feedback><p id="acf3d51ab5844b199b11cf051ea2b774">Incorrect. Long short-term memory can address the vanishing/exploding gradient problem.</p></feedback></response><response match="fe795caa888f478babdfc7c4c47069b6" score="10"><feedback><p id="a97f3304619249f39251d364045f04cf">Correct. Existing architectures need to process the input tokens sequentially and cannot make use of parallelization capabilities in modern GPUs.</p></feedback></response><response match="f0672dd12a1c469aa60c5c7d5fa3b178" score="0"><feedback><p id="d677a19fbe6d4cbea6ec2b8cf70c71d3">Incorrect. Attention mechanisms can model this dependency and have been used prior to the publication of Vaswani et al. (2017).</p></feedback></response><response match="c058c136cca64fa895aaa2e3277c3caf" score="0"><feedback><p id="d53bf740173344de826fe9456ff86960">Incorrect. Attention mechanisms have been used in conjunction with recurrent neural networks in prior work.</p></feedback></response><response match="b1e9102ed4b0474aa4134d72def4edb2" score="0"><feedback><p id="b0d7b7d1eeb24837817279382df60047">Incorrect. Factorization tricks have been used to improve the computational efficiency of recurrent neural work in prior work.</p></feedback></response></part></multiple_choice></page><page id="c9e7d62756b54e14a4631a2c5b39a371"><title>Assessment Page Title</title><multiple_choice id="ad3403c11db549d9b8538ca4ee1d9e18" grading="automatic" select="single"><body><p id="afcf77954da844f9a8423942d1d8d4d6">Which of these is a typical problem BERT is used for:</p></body><input shuffle="false" id="c3214d08ec624d7b9d611f67117d007e" labels="false"><choice value="c140bd6aa162468c87492c97a0a24d86">Implementing auto-complete feature for search engines.</choice><choice value="a6aa214b3db34ceebed1bd9bf1708c8f">In chatbots for automatic assists on questions users may have.</choice><choice value="c746b977e876420f9a8db521392bcc85">Named Entity Recognition (NER), where the system receives a text sequence and is required to mark the various types of entities.</choice><choice value="faf5801f1ecd4d37a5a0132f07b6e3a7">Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices.</choice><choice value="c4615151335940b7a8587b7f52992122">All of the above.</choice></input><part id="b366a5a8d1e54828b41f3766c057a95f"><response match="c140bd6aa162468c87492c97a0a24d86" score="0"><feedback><p id="c90c2e8f39c145228be51fb439e44523">Incorrect. This is one of the use cases, implementing an auto-complete feature for search engines. All of the above use cases are correct.</p></feedback></response><response match="a6aa214b3db34ceebed1bd9bf1708c8f" score="0"><feedback><p id="eb78b15e253449e9961c015a1d59acbf">Incorrect. This is one of the use cases, chatbots for automatic assistance on questions users may have. All of the above use cases are correct.</p></feedback></response><response match="c746b977e876420f9a8db521392bcc85" score="0"><feedback><p id="b8c98287b6b7416b97b9758ab16a752a">Incorrect. This is one of the use cases, Named Entity Recognition (NER), where the system receives a text sequence and is required to mark the various types of entities. All of the above use cases are correct.</p></feedback></response><response match="faf5801f1ecd4d37a5a0132f07b6e3a7" score="0"><feedback><p id="befb5628e50b4b2e88d3efbca0bb1054">Incorrect. This is one of the use cases, Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. All of the above use cases are correct.</p></feedback></response><response match="c4615151335940b7a8587b7f52992122" score="10"><feedback><p id="b1fd0c459cfb4f2aaa63357ff9d81cec">Correct. All of the above are correct.</p></feedback></response></part></multiple_choice></page><page id="de7dcafedd01441891616dd783679bde"><title>Assessment Page Title</title><multiple_choice id="c897799c70034a1c9f9bbbec5b5367bb" grading="automatic" select="single"><body><p id="a60857fcb703423998047a19b99c4c90">Based on Vaswani et al. (2017), how is masked self-attention different from regular self-attention?</p></body><input shuffle="true" id="e63aa078846746de9a721055cf32063d" labels="false"><choice value="b9ee4e4907044d62bc36601723468503">With masked self-attention, all tokens after the current token in the input sequence (e.g., the English sentence when translating from English to French) are masked, so each token can only attend to itself and the previous tokens.</choice><choice value="a0dc02996229475ea1e13a341ed54b91">With masked self-attention, all values in the input of the softmax layer which correspond to invalid connections (those that involve the tokens after the current position) are set to zero.</choice><choice value="c11158a0ba9d491dab169b736c0b085d">Masked self-attention is used in both the encoder and decoder, while regular self-attention is only used in the decoder.</choice><choice value="cba310c2455a4402929fdc5b51668e49">Regular self-attention prevents leftward information flow, while masked self-attention does not.</choice><choice value="d2b1c30673bf4095892ec6bde6d7798f">With masked self-attention, all tokens after the current token in the output sequence (e.g., the French sentence when translating from English to French) are masked, so each token can only attend to itself and the previous tokens.</choice></input><part id="dc2e4d48061a452cb3bbe18e36e3d8ad"><response match="b9ee4e4907044d62bc36601723468503" score="0"><feedback><p id="aa90ed99c9214e23bfcab4aeddbfbf0c">Incorrect. Only the output sequence needs to be masked, not the input sequence.</p></feedback></response><response match="a0dc02996229475ea1e13a341ed54b91" score="0"><feedback><p id="cbbfafa8cf4143bfbb52bec2c37ff863">Incorrect. The masked inputs are set to minus infinity, not zero.</p></feedback></response><response match="c11158a0ba9d491dab169b736c0b085d" score="0"><feedback><p id="c043285a8003402eb3820f79f36e52d6">Incorrect. Mask self-attention is only used in the decoder, while regular self-attention is used in both the encoder and decoder.</p></feedback></response><response match="cba310c2455a4402929fdc5b51668e49" score="0"><feedback><p id="c9561be03171491d856bcaafb3516475">Incorrect. Masked self-attention prevents leftward information flow.</p></feedback></response><response match="d2b1c30673bf4095892ec6bde6d7798f" score="10"><feedback><p id="e98133d47d4549fc9ae3608b53ca36c0">Correct. When processing the decoder sequence, we have to assume that all tokens after the current position are unknown (because they are the prediction targets), so their attention values need to be masked out.</p></feedback></response></part></multiple_choice></page></assessment>\n'