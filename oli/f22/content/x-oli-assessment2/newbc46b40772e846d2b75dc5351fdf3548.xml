b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newbc46b40772e846d2b75dc5351fdf3548" recommended_attempts="3" max_attempts="3"><title>Quiz 8</title><page id="e9d1d263723a41698d9f7875aad309bc"><content available="always"><p id="b53ddf66dfcb4d7180f3fb57cebfb85d">Answer the following questions to the best of your knowledge. Always pick the solution that you believe is the best answer for the scenario or question. If you have any questions about the test, please post a <em>private</em> note on Piazza. You have the opportunity to include justifications at the end of this quiz.</p></content><multiple_choice id="ee6332dc62464eb08b40661713a6806d" grading="automatic" select="single"><body><p id="f40655a15f10436ca8bb2b9d552748f4">When your dependent variable is categorical, CART will produce this type of tree that will be used to predict the classes of new observations:</p></body><input shuffle="true" id="c201adfc1ccd4dad91d433cce77461f0" labels="false"><choice value="e24ea8b5567d45ea9f05a70a522c8329">Classification Tree.</choice><choice value="b6c208d89275427dacd6a7b5879e16d4">Regression Tree.</choice><choice value="fe6d3f71a007491e84025fa9e37d7899">Either a Classification or Regression Tree.</choice><choice value="ad80457c235844d6bfb5f942442119e8">Pruned Tree.</choice></input><part id="e19084d2278e44e49efc9f6e4211fa4d"><response match="e24ea8b5567d45ea9f05a70a522c8329" score="10"><feedback><p id="c2af524c5a4a4757a32c2cec5f5b5ac3">Correct: When categorical variable is the target or dependent variable, your algorithm will produce a classification tree.</p></feedback></response><response match="b6c208d89275427dacd6a7b5879e16d4" score="0"><feedback><p id="c5a7c4a73e0341bf9e88b27a83ab0686">Incorrect: A Regression Tree is produced from CART using continuous variables.</p></feedback></response><response match="fe6d3f71a007491e84025fa9e37d7899" score="0"><feedback><p id="ec453589bc4e4d4fb543ee57c23c6eda">Incorrect: It will not produce both of the trees. It should produce just one type.</p></feedback></response><response match="ad80457c235844d6bfb5f942442119e8" score="0"><feedback><p id="d5578e981e37443383f11eb0dc6f486f">Incorrect: A pruned tree is produced after a Classification or Regression Tree is built. A tree is pruned to reduce overfitting.</p></feedback></response></part></multiple_choice></page><page id="ef2e853acb68423896272ce190984d97"><multiple_choice id="d09a2ee321b44df38ec052df5ed76b76" grading="automatic" select="single"><body><p id="eb3d7bb5cc264a44b0f4fdeb33329574">The CART algorithm uses certain criteria to measure the impurity of a tree/trees. This impurity measure is the degree to which a variable is incorrectly classified and used to measure impurity for classification trees:</p></body><input shuffle="true" id="a008fb35dc4e4268b5ae4934e687cc8e" labels="false"><choice value="b6fded6ada0b4459bcf106d1675fb0cc">Gini Index.</choice><choice value="eb52a36a93f841709822e76d80ccf4fe">Mean Squared Error.</choice><choice value="deaedc81db38462a990ba80cabb28144">Mean Absolute Deviation.</choice><choice value="e3c7e2962ee142e1a16a660865eed2a6">Root Mean Square Error.</choice></input><part id="ef054ca0ef6e43c49e1a9f817a2792a0"><response match="b6fded6ada0b4459bcf106d1675fb0cc" score="10"><feedback><p id="ac20a43800c34249b5f823845857a7e5">Correct: This measures the degree to which a variable was wrongly classified. It is used to measure how often a randomly chosen data point from the dataset is incorrectly labeled.</p></feedback></response><response match="eb52a36a93f841709822e76d80ccf4fe" score="0"><feedback><p id="ad1fc1d5f49b47a8ba9d7dd6e3e1fc77">Incorrect: The MSE is used to measure the impurity of regression trees and not classification trees.</p></feedback></response><response match="deaedc81db38462a990ba80cabb28144" score="0"><feedback><p id="f22df1724fda4967aa29579bcfc68971">Incorrect: This is used to describe the variation in a dataset and is not the best measure of impurity with a CART.</p></feedback></response><response match="e3c7e2962ee142e1a16a660865eed2a6" score="0"><feedback><p id="c7eac6cc66b84f53a258200b0cbe3c62">Incorrect: This is a criteria that is used to evaluate the performance of CART models. </p></feedback></response></part></multiple_choice></page><page id="c8b4f9032e05424c8cf75ccc0f289cb2"><multiple_choice id="d4817dd506fc4ae49fa48ee93bbecba0" grading="automatic" select="single"><body><p id="a3ce8572c3e846b4ad9b5241cda36ee6">You train a model using this method that will repeat sampling of the training dataset and randomly select features that will be used to construct trees. The method can also show the importance of features and is known as:</p></body><input shuffle="true" id="dfbbc8f78b79475b8a6966040600acce" labels="false"><choice value="d745e75571274193aa04fbe654939e3c">Random Forest.</choice><choice value="a325433d710b42e19555e2a89c6367ce">Boosting Ensemble Method.</choice><choice value="e762446eb84443ac89334ace96e68485">Classification Trees.</choice><choice value="db8c89fb62484006ab17a8fe60a609e5">Naive Bayes.</choice></input><part id="ce3fe404bfd94bd69a8de0e6e282f050"><response match="d745e75571274193aa04fbe654939e3c" score="10"><feedback><p id="c4557fd02037426db8c0bbbedcad6d5b">Correct: RF will take highly correlated trees and conduct a repeated sampling of the training data and a random selection of features to construct trees.</p></feedback></response><response match="a325433d710b42e19555e2a89c6367ce" score="0"><feedback><p id="b1db9da3a5bc495985bfeb70869faa50">Incorrect: Reduces bias and will increase predictive performance of trees.</p></feedback></response><response match="e762446eb84443ac89334ace96e68485" score="0"><feedback><p id="c51817ff5a5242699f342b1241e55a03">Incorrect: This method will represent class labels and branches and is built on the entire dataset as opposed to the scenario described.</p></feedback></response><response match="db8c89fb62484006ab17a8fe60a609e5" score="0"><feedback><p id="e744b71d8ba74c569be9ebb3f8320684">Incorrect: nB will assume that predictor variables are independent. In this scenario, the assumptions of nB will not fit. </p></feedback></response></part></multiple_choice></page><page id="bd856a4114c34a2fb72f44871305e2c7"><multiple_choice id="b3258b1941c748249011753de83c7258" grading="automatic" select="single"><body><p id="f178231e548d4e97a89cb6a1e118e65b">Which option below is known as the tree that is the least complex and contains the smallest classification or prediction error for the validation dataset?\r\r</p></body><input shuffle="true" id="c8b16ccd00c240cbb0abc39bf3cb11db" labels="false"><choice value="c4a01c6e604e415492ec395307d484ed">Minimum Error Tree.</choice><choice value="e84b61904cb84e86b560c178be3a4011">Gini Index</choice><choice value="aab5a0525907463c9362caa352079235">Best-Pruned Tree.</choice><choice value="deb9c333fa5842fe9451f59c1708d956">Entropy.</choice></input><part id="c4d7957b5d0a46cfb86604384ae4896b"><response match="c4a01c6e604e415492ec395307d484ed" score="10"><feedback><p id="b51e9eb0ad79455f99cc7638c4d0264c">Correct: This is the right solution.</p></feedback></response><response match="e84b61904cb84e86b560c178be3a4011" score="0"><feedback><p id="b032a9946c4b473ca850078933a7111f">Incorrect: This is an impurity measure.</p></feedback></response><response match="aab5a0525907463c9362caa352079235" score="0"><feedback><p id="cd79b31feb5d44eca6658ef8db466f11">Incorrect: The best pruned tree is a pruned tree that has the least error that is one standard error from the MET.</p></feedback></response><response match="deb9c333fa5842fe9451f59c1708d956" score="0"><feedback><p id="fe5d4c679d054c9f9af3df06049afce4">Incorrect: This is an impurity measure for classification trees.</p></feedback></response></part></multiple_choice></page><page id="f644d63e0028476b804bf2b927a7f9cc"><multiple_choice id="adca34e5367f45349ee6d88fcee27c37" grading="automatic" select="single"><body><p id="a0fc5d02f4eb4b01b9b3ccd6b8f2ae94">Ensemble methods combine multiple single-tree models to reduce the variation in prediction error. Overall, which of the ensemble methods below may lead to overfitting when there are a large number of trees:</p></body><input shuffle="true" id="ea0ee8f0303140f2a0d38e7bd44e156f" labels="false"><choice value="d30af7e69d2d4f09a3ff9ae365d27f1c">Boosting.</choice><choice value="b0e2f2ed42d54337bed09e778db17ef5">Random Forest.</choice><choice value="aef203b3f3894d2aab29b2ce585c82c6">Bagging.</choice><choice value="af6106dbdb024eeeb85386d089a0831f">Ensemble methods are by design meant to reduce overfitting.</choice></input><part id="c941fb946b224efa90cffcde0b3865ed"><response match="d30af7e69d2d4f09a3ff9ae365d27f1c" score="10"><feedback><p id="eabd0cd3d0844cf5bea08bdf6b020167">Correct: Unlike the other methods, boosting will not develop trees in parallel and this leads to overfitting unlike the other methods.</p></feedback></response><response match="b0e2f2ed42d54337bed09e778db17ef5" score="0"><feedback><p id="cf6d3588840c48fbb0b44e3cf11a80b4">Incorrect: Due to its technique, it can solve for overfitting.</p></feedback></response><response match="aef203b3f3894d2aab29b2ce585c82c6" score="0"><feedback><p id="ea6139c67afd45218636ba24ca1c292a">Incorrect: Due to its technique, it can solve for overfitting.</p></feedback></response><response match="af6106dbdb024eeeb85386d089a0831f" score="0"><feedback><p id="f13fcb1cb0e34b4080a6853ca3534693">Incorrect: The two of the tree methods we have introduced in the course will reduce overfitting.</p></feedback></response></part></multiple_choice></page><page id="bc30d0dcb50a46168171421818e077a9"><multiple_choice id="ee05c6b8162545e6a791ec5147733e67" grading="automatic" select="single"><body><p id="bc07452696ab47c6ad3be952d0f81247">You are training a random forest model, and it needs to select a number of the random features for each tree. If there are 36 predictor variables in the dataset, each tree will randomly select how many features to be included in the tree:</p></body><input shuffle="true" id="f3ecb0fd6c854e9592f1e16034c2173e" labels="false"><choice value="e4d18b46be044b20afe580546d5a9047">6.</choice><choice value="ef92f034045e4226a148bde06ed118d6">12.</choice><choice value="a971d5939eac4fc4a9bbee1b7ee07079">3.</choice><choice value="d07f8c42303a46d0901b8a6c4da5fc26">9.</choice></input><part id="c204bb1790a94482b6fe994e54b240df"><response match="e4d18b46be044b20afe580546d5a9047" score="10"><feedback><p id="c99dd8ab00e84375bfaf134d6fd60e6d">Correct: Best practice is to use the square root of the predictor variable count to find the number of features to be included in the tree. </p></feedback></response><response match="ef92f034045e4226a148bde06ed118d6" score="0"><feedback><p id="f996fcd8fcf84b078c7f95266893d949">Incorrect: Best practice is to use the square root of the predictor variable count to find the number of features to be included in the tree. </p></feedback></response><response match="a971d5939eac4fc4a9bbee1b7ee07079" score="0"><feedback><p id="c67df98db75d49cfb445e8baab35b656">Incorrect: Best practice is to use the square root of the predictor variable count to find the number of features to be included in the tree. </p></feedback></response><response match="d07f8c42303a46d0901b8a6c4da5fc26" score="0"><feedback><p id="a186377970594166ada5f6452bb7e65f">Incorrect: Best practice is to use the square root of the predictor variable count to find the number of features to be included in the tree. </p></feedback></response></part></multiple_choice></page><page id="f742ee9e8baf45ab8f9e0b4e05684706"><multiple_choice id="a762d58897844506916bc240901e6058" grading="automatic" select="single"><body><p id="cf2c357753eb4798adc6d5e6a4d0670a">You are working with a dataset containing 400 observations with the response variable <em style="italic">y</em> along with the predictor variables <em style="italic">x</em><sub><em style="italic">1</em></sub><em style="italic">, x</em><sub><em style="italic">2</em></sub><em style="italic">, x</em><sub><em style="italic">3</em></sub> and<em style="italic"> x</em><sub><em style="italic">4</em></sub>.\r You have trained a <em style="italic">k</em>NN on this dataset and have received the results. Those results have been compiled in the image attached. Interpret the Accuracy rate:</p><p id="f97db7c910fe47b9afc35c85795852fd" /><image id="c5541ff7327f40cd8a5450e5d8000637" src="../webcontent/Accuracy.png" alt="" style="inline" vertical-align="middle"><caption><p id="b7a9d552e3274f86b1452d855f16fd6f">Interpret the accuracy rate.</p></caption><popout enable="false"></popout></image></body><input shuffle="true" id="e2dec7980297489e895a652bf032dd50" labels="false"><choice value="f590f2779cf94f66b250274b05d4c46d">The <em style="italic">k</em>NN model was able to classify 76% of all cases.</choice><choice value="cf9a47ec86a94a7f98da9b887fe3d325">The <em style="italic">k</em>NN model was able to classify 24% of all cases.</choice><choice value="be0a34cb893e4bed8deea7f1ae5101f9">The <em style="italic">k</em>NN model was able to classify 76% of Class 1 cases.</choice><choice value="fe73179589484d49a0324b0ab6d8fd61">The <em style="italic">k</em>NN model was able to classify 76% of Class 0 cases.</choice></input><part id="ed4ba4a1deff4d839721709698cb230b"><response match="f590f2779cf94f66b250274b05d4c46d" score="10"><feedback><p id="eccea6fb682e4175b4232f34448a640e">Correct: Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points. </p></feedback></response><response match="cf9a47ec86a94a7f98da9b887fe3d325" score="0"><feedback><p id="d1a44034147d41ec817e13df0183de9f">Incorrect: Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points. This figure gives the percentage that were not accurately classified.</p></feedback></response><response match="be0a34cb893e4bed8deea7f1ae5101f9" score="0"><feedback><p id="d74a13e42b844dc985a5bd941883bde7">Incorrect: Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points. </p></feedback></response><response match="fe73179589484d49a0324b0ab6d8fd61" score="0"><feedback><p id="fd434c05ea814f139f272226fc36438c">Incorrect: Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points. </p></feedback></response></part></multiple_choice></page><page id="e4f7883dd9fa4049b699834a6671d23b"><multiple_choice id="fc091c3ed52441c0b97d19cd82940f94" grading="automatic" select="single"><body><p id="d61fc44a9183453f996d21a1ce5cc778">You are working with a dataset containing 6000 observations with the response variable y along with the predictor variables x<sub>1</sub>, x<sub>2</sub>, and x<sub>3</sub>.\r You have trained a model with the <em style="italic">k</em>NN algorithm on this dataset. The attached image shows a compilation of misclassification rates for all possible <em style="italic">k</em>. What <em style="italic">k </em>should be considered the optimal:</p><p id="afaa032029f94cddb8088e07f1e0a09d" /><image id="c260877eebb84be6b93898620235c47b" src="../webcontent/Optimal_K.png" alt="" style="inline" vertical-align="middle"><caption><p id="bbb4fa2f46534bf4a9e44a2b47fec562">Which is the best <em style="italic">K</em></p></caption><popout enable="false"></popout></image></body><input shuffle="true" id="bc5605b155394454853759e06cce1a1f" labels="false"><choice value="d43c758a95904b01ae451ac3579a29c0">9</choice><choice value="a1a261976d004614a09d754ba77e6f98">7</choice><choice value="d6718254c0414bc19caefdd5aa97bedf">2</choice><choice value="e94824be436341a89b2ef72a3959f7ed">5</choice></input><part id="adf63ddb658140139a4274994ed3cf63"><response match="d43c758a95904b01ae451ac3579a29c0" score="10"><feedback><p id="ee9c2e9980fc420d89be7c813179fae0">Correct: <em style="italic">k</em>=9 has the lowest misclassification rate and you want to select that <em style="italic">k </em>as your optimal.</p></feedback></response><response match="a1a261976d004614a09d754ba77e6f98" score="0"><feedback><p id="faee05cdd83e432c881672af93a182c5">Incorrect: You want to select the <em style="italic">k</em> with the lowest misclassification rate.</p></feedback></response><response match="d6718254c0414bc19caefdd5aa97bedf" score="0"><feedback><p id="ad13731a2a9241ca8edce8de94c8c03e">Incorrect: You want to select the <em style="italic">k</em> with the lowest misclassification rate.</p></feedback></response><response match="e94824be436341a89b2ef72a3959f7ed" score="0"><feedback><p id="a53dabc390b048c087a6ca6dddc0272b">Incorrect: You want to select the <em style="italic">k</em> with the lowest misclassification rate.</p></feedback></response></part></multiple_choice></page><page id="cde39e622bd847d98ff23a42e7b84767"><multiple_choice id="f58f2c39581344fe8e46849b0ab6f84c" grading="automatic" select="single"><body><p id="e2d9a9b7f47b48b1b6db84863995c55b">You have been presented with a <em style="italic">k</em>NN problem with independent variables that are categorical. Since <em style="italic">k</em>NN requires the use of distance measures. Which set of the following distance measures would best be used for this problem:</p></body><input shuffle="true" id="ab8751419cc74455acac9dc471847320" labels="false"><choice value="d67b98cc9abc44d881b3e4e85dd03e66">Hamming and Euclidean Distance.</choice><choice value="b950a9214bc940108e8da8ec393bc0f7">Euclidean and Minkwoski.</choice><choice value="e556d54674d14ff28a580be8c243f3c9">Solely Minkwoski Distance.</choice><choice value="d66ac9ac67bd41598b62f9a376fffb9b">None of the above.</choice></input><part id="db3d69760de64bf89cb9cdbb88e8c3a3"><response match="d67b98cc9abc44d881b3e4e85dd03e66" score="10"><feedback><p id="cf42c5f773b7426a95551acc34567eee">Correct: Hamming Distance is the best distance measure to use when categorical variables are present. You can also use the Jaccard distance as well. You can also use euclidean as long as you transform the categorical variables.</p></feedback></response><response match="b950a9214bc940108e8da8ec393bc0f7" score="0"><feedback><p id="e4af574c7a194c75b4a18f6f801665db">Incorrect: If you use euclidean distance for categorical variables, you would need to transform those variables into real values. Minkwoski is not the best measure for categorical variables.</p></feedback></response><response match="e556d54674d14ff28a580be8c243f3c9" score="0"><feedback><p id="c30aba87bf4c479da5c6eca4ed3ebd74">Incorrect: You can consider this measure as a hybrid of euclidean and manhattan. It is not the best measure for categorical variables.</p></feedback></response><response match="d66ac9ac67bd41598b62f9a376fffb9b" score="0"><feedback><p id="ead32737f988402f8272b552e7bfde80">Incorrect: You can use hamming distance for categorical variables. You can also use euclidean as long as you transform the categorical variables.</p></feedback></response></part></multiple_choice></page><page id="f32f188e785349e0b46c0bbc4a1f3b2a"><multiple_choice id="ee076870c6624087b9f2f5c2bcd3f728" grading="automatic" select="single"><body><p id="d4f3cc00982f4587b9da43cd8d5ddae9">Which of the following will be the Euclidean Distance between the two data point A(1,3) and B(2,3)?</p></body><input shuffle="true" id="de57768c09474987bf9f4a105e9ec0a7" labels="false"><choice value="b2184972e4b4477dbff50aa523f0007c">1</choice><choice value="e2788bccd9664124aa530b55867d1981">2</choice><choice value="b01bbc16670947ebab86d0e175f6c456">4</choice><choice value="b6e51232b85e4f43a00aab940cfe8af9">8</choice></input><part id="b77db210054f4bd297512f2a894ff5fd"><response match="b2184972e4b4477dbff50aa523f0007c" score="10"><feedback><p id="d36602d7786d46709a0daeb99fe4fca7">Correct: sqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) </p></feedback></response><response match="e2788bccd9664124aa530b55867d1981" score="0"><feedback><p id="b9db65f8f3804830aaca5321cb38205a">Incorrect: sqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) </p></feedback></response><response match="b01bbc16670947ebab86d0e175f6c456" score="0"><feedback><p id="e104116e7ce646df9176c96c671fd882">Incorrect: sqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) </p></feedback></response><response match="b6e51232b85e4f43a00aab940cfe8af9" score="0"><feedback><p id="ba80f9bf079d46c586c05d7281f7b5e0">Incorrect: sqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) </p></feedback></response></part></multiple_choice></page><page id="d4a1916a607c43f4b37e265cb296f718"><essay id="a6392290f07949cd8e401c4912548f91" grading="automatic"><body><p id="fc700bbb5ae8443aa6962c4fb3227d7d">Use this space to include notes about any questions about. Notes could include justification. If you have nothing to include, just type <em>n/a</em></p></body><part id="e974e60a931d4a76838144ab21bf16f2"><response match="*" score="1"><feedback><p id="a050022f9fea41c88dfc833bda8f2613" /></feedback></response></part></essay></page></assessment>\n'