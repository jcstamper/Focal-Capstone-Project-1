b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c6b8b1164cf04cb7a119d08ffcf91511"><head><title>Clustering Evaluation Metrics</title><objref idref="f884b110ee3f48bfaf5a73e27e20b647" /></head><body><p id="e4b6a8190b2c449c977038dab8595fb7">The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model&apos;s performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The <em style="italic">Internal </em>and <em style="italic">External </em>evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.</p><section id="b4c5b90073be46a5a432c6ec73445ca5"><title>Internal Evaluation</title><body><p id="c29090c45deb4f98801b1715d37b5162" /></body></section><p id="f06bc2418a4749a594c5157a6cde911b">Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let&apos;s look at internal evaluation techniques that are used to assess the quality of clustering methods:</p><section id="d83f2eed02954beda7378cc17dadba1b"><title>Silhouette Coefficient</title><body><p id="ee7cfc8f5b334c4495878b30ccc30be7" /></body></section><p id="c7515bb14f474d189d5c54dcf7181e43"><em style="italic">The Silhouette Coefficient </em>shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see <link href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" target="new" internal="false">https://en.wikipedia.org/wiki/Silhouette_(clustering).</link></p><section id="e45a60dd56f34232bc9cc3ebd2d7a23d"><title>Dunn Index</title><body><p id="b88117aab62f4cc0af0234b9bdf5cfe0" /></body></section><p id="dad66a3beec049308f18b163b956487d"><em style="italic">Dunn Index</em> is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter</p><p id="b4269d7652c64bf4a41d02731338a3a2">If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.  </p><p id="ee774c1983c84718bc544f199ec947bf">See <link href="https://en.wikipedia.org/wiki/Dunn_index" target="new" internal="false">https://en.wikipedia.org/wiki/Dunn_index</link> for more details.</p><section id="a5054a650e4c45e0935e5eb0c38625e3"><title>External Evaluation</title><body><p id="f139553d7634451b9e305abcc7cd1504" /></body></section><p id="ef8acc9fe2684169be4089587aec7139">External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.</p><section id="b6b48e78f9f34125ab33d48cd82b1662"><title>Rand Index</title><body><p id="bedd70d6640944bcafac9c2735a9c081" /></body></section><p id="acb07613d698401fbe95ac239e971288"><em style="italic">Rand Index </em>tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:</p><p id="a14235ebda374d33935cfb56ebb6db3e">(TP + TN)/(TP+FP+FN+TN)</p><section id="b4f6b3aadf174ebc9995f62c707eb5c2"><title>Purity</title><body><p id="cd54dbcdd2a746ef96eb1e65fa8e5439" /></body></section><p id="cebc709c395f4ac8be62884570526a5c"><em style="italic">Purity </em>is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The <em style="italic"><em>normalized mutual information </em></em>(NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.</p><section id="e26c532c26a34e1d97134a6160884c65"><title>Jaccard Index</title><body><p id="f931544880ff4906bdee764a28ba5d9e" /></body></section><p id="e9503eba42534260b8174848546089b3"><em style="italic">Jaccard Index </em>is used in cluster analysis evaluation. It is defined as &quot;the size of the intersection divided by the size of the union of the sample sets.&quot; The Jaccard distance measures dissimilarity between sample sets.</p><section id="ec0cd4be57e84f2daed804d5b1cd4678"><title>F-Measure</title><body><p id="b60b293284aa4080bfe524afade4d48d" /></body></section><p id="b5fff7318c2f4b11ad7f7b0b84d3cfcd"><em style="italic">F-Measure</em> is simply computed as the \\(\\frac{2*Precision*Recall}{Precision+Recall}\\). You might remember it from the classification metrics, it is also known as the F1 score.</p><section id="e36d57f39d9f4d34acc6f71c00b83f10"><title>Dice Index</title><body><p id="a3d6587b45ed4fd3bbf81fb58e13a896" /></body></section><p id="f2816c145acc42fb989cfb683d253bf4"><em style="italic">The Dice Index</em>, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.</p></body></workbook_page>\n'