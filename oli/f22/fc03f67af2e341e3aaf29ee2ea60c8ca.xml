b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="fc03f67af2e341e3aaf29ee2ea60c8ca"><head><title>Transformers</title></head><body><p id="f09a2f91ac7b4eceb9fc55e2d1898faf">The Transformer model was introduced in the famous paper <link href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="new" internal="false">Attention is All You Need</link> in 2017.</p><p id="eeab5ac0f16a4ff38ef3023ec10c1770">A good way to understand Transformers is to think about the fact that in the <em>Sequence2Sequence</em> models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldn\xe2\x80\x99t using the input embeddings directly make more sense? </p><p id="d72e87ddbac84ac49259d3cb922d0085">Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.</p><image id="f0ef67e88c72414eba79032f844c4171" src="../webcontent/image-fc03f67af2e341e3aaf29ee2ea60c8ca-1.png" alt="" style="inline" vertical-align="middle" height="223" width="650"><caption><p id="dc5fad2fd9ff49cba3829f4bd35300de">Figure 5: Interactions within components in different architectures.</p></caption><popout enable="false"></popout></image><p id="dbda889fdadf466fbf774645250b12b9">Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.</p><p id="b8c978101e714431a93e7ed2033984f0"><em>Self-attention</em>, also known as <em>intra-attention</em>, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.</p><p id="b8596390f5754042852bc605527731db">On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.</p><p id="e47c395db10a42338ff76c5d0bfd9930">On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.</p></body></workbook_page>\n'