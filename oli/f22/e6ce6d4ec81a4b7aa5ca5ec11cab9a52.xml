b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="e6ce6d4ec81a4b7aa5ca5ec11cab9a52"><head><title>Activation Functions and Objective / Loss Functions</title></head><body><section id="b174bad98be242bebcb5b70bf47d2091"><title>Activation Functions</title><body><p id="ea94580f69eb49aaaa3c6070ffc46c9e"> </p></body></section><p id="d9e3a62d977f482b861d41f6a0048d60" /><p id="e0548aa8f11e4b12afd8894d0379a59c">The nonlinearity introduced in neural networks is key to its learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as \xe2\x80\x9cactivated\xe2\x80\x9d or \xe2\x80\x9cinhibited\xe2\x80\x9d, thus affecting the transformations in the subsequent layers.</p><p id="cf71b2cea813436aae50d9ece1311bd1" /><p id="afc7a3ac5ac44e25830c3b060e556d0d">Following are some common activation functions:</p><p id="fe72ed56412d4c99bda8beb40ee44745" /><p id="c250d96e11414fa89b954cabb23b2613"><em>Sigmoid</em>: The sigmoid function transforms an input to a value between 0 and 1. It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network, and is hence mostly used in the output or the final layer. One of the disadvantages of the Sigmoid is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.</p><p id="d741589b05694194a957b406eff4ec28" /><p id="c2449c9825ee492a91369c6a4aabaddf"><em>ReLU</em>: ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions, and does not get saturated at high input values as opposed to the logistic function which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.</p><p id="ac5e2e3ff93b479b941ed59388bb26cf" /><p id="cc9711283bee40008f2331d7f8a57a2e"><em>LeakyReLU</em>: Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, and hence is considered as a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.</p><p id="f05b269285094f619c1835f10ba73961" /><p id="d6f906ed8c9f4521922b6e52870a7f3f"><em>Tanh</em>: Tanh is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the hidden layer are closer to having a zero mean therefore data is more centered which makes the learning for the next layer easier and faster. The Tanh function shares the same disadvantage as that of the Sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.</p><p id="f5fd1a3ee5894f08ac0337157e770302" /><p id="bbdcbec75dec420d939d31200c512db0"><em>Softmax</em>: The Softmax activation function is used in neural networks when we want to build a multi-class classifier which solves the problem of assigning an instance to one class when the number of possible classes is larger than two, as opposed to the Sigmoid, where the number of classes is two. The Softmax ensures that the sum of outputs for each class is equal to 1 for a given input, giving it a sense of a probability distribution over the output classes. </p><section id="efe1a8f2c4654b42a0071da4671e9585"><title>Objective / Loss Functions</title><body><p id="dfdc8ca27aef435680ff282aee341dd2"> </p></body></section><p id="f582ff2ec2764aa48ee58e17bbb28957">An objective function quantifies how well, or how badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called as loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and often depend on the choice of the activation function in the final layer of the model. A function with a smooth and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.</p><p id="b41c0edad2f7402baa2a6d1a6bf0c096" /><p id="df9b576cfe814549ace84df363b744f7">Following are some of the commonly used loss functions based on the problem at hand:</p><p id="db6e4c83a39045d5b8ff524e300bf372" /><p id="ada8b7b5cd6242bca5f81c362f45f30b"><em>Regression Problem</em>: A problem where the model predicts a real-value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function. </p><p id="baaffc9aeac148a0972c3d25ab2931f2" /><p id="ce9f4a848ecc4590acf3fb2060ca094f"><em>Binary Classification Problem</em>: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.</p><p id="e8952fad791a4316b902217aba4cae82" /><p id="bb7d1ee8d9684dc5b48d217fae5a9a15"><em>Multi-Class Classification Problem</em>: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. Cross Entropy can be used as a loss function in this case.</p></body></workbook_page>\n'