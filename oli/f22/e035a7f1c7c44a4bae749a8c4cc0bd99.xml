b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="e035a7f1c7c44a4bae749a8c4cc0bd99"><head><title>Model Architectures</title></head><body><section id="f1accf0f426448d5bf2881d1be47cd5b"><title>Model Architectures</title><body><p id="d35818bc191f472080fe86646dc35489"> </p></body></section><p id="f4fe002b609c48038a97b176868aa98f">Following are some of the key deep learning architecture/algorithms:</p><p id="dfd2f914612d48089c9c7ec9a84b2fe6" /><p id="e6f4a8a1ab5b47308518f1cf02cdbcfd"><em>Artificial Neural Networks (ANN)</em>: </p><p id="c9228f301c5845e483b97599284496f0" /><p id="ca512c592798457a98b0c635c35bd245">Artificial neural networks (ANNs) are composed of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.</p><p id="d6fe2f3dbd0742d992aa529e79ca2ba4" /><image id="a2832867c47245ceb9d61755bb9044d3" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-1.png" alt="" style="inline" vertical-align="middle" height="198" width="279"><caption><p id="d5079446c0f54152b88c7bbcd3326413" /></caption><popout enable="false"></popout></image><p id="d7ee941ddae74a29876ed98ce48bd09a"><em style="italic">(</em><em style="italic"><em>https://www.ibm.com/cloud/learn/neural-networks</em></em><em style="italic">) </em></p><p id="cb540b4e48c944f68305bc0966b7d279" /><p id="c4319b9b6480406c8f6db6cf4f94d291"><em>Convolutional Neural Networks (CNN)</em>: </p><p id="a7c5fe4b1e594577a7ae0e4ea23a58d4" /><p id="d1b8ec06e18243ef8c47ca02c9256118">A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun; at the time, the architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You train the network by using back-propagation.</p><p id="c289c1564b2443a7bc3b77414182a762" /><image id="b9c7ee4c5069473e8fbad69f5b13c173" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-2.png" alt="" style="inline" vertical-align="middle" height="120" width="468"><caption><p id="e73671d60df14562b8a5877088b5d388" /></caption><popout enable="false"></popout></image><p id="abce29299e5c45bf9fc8eb1ed2de85ac" /><p id="c9ff87212261409fab46330022fe737a"><em style="italic">(</em><em style="italic"><em>https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/</em></em><em style="italic">) </em></p><p id="e66cba3b49ec48e7affff6f2f6c91ad9" /><p id="ebfe9017f50440cbb9099b3691285255">The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, the CNN has been successfully applied to video recognition and various tasks within natural language processing.</p><p id="eaeef2229b614447b5e99f23cd0e8bc7" /><p id="ac6b56cef0f34ebba7a1213aebcf3368"><em>Recurrent Neural Networks (RNN)</em>: </p><p id="e3b14a2154ec43129f4f79b5ef7fac57" /><p id="d10eb38e2e624e7db4dbe9785a64a88f">The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multilayer network and a recurrent network is that rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model problems in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof.<em style="italic"> </em>RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).</p><p id="ad316b2e302b4a069e00a8a57c0d8bb6" /><p id="c3da47748f2c472d8087a4aa988f6871">RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures were developed, and are described below:</p><p id="ec7519eddcdd4b9095b10addab003c63" /><p id="d8e531df62394943a20db5efa87fe257"><em><em style="italic">Long Short Term Memory (LSTM) Networks</em></em>: The LSTM was created in 1997 by Hochreiter and Schimdhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what&apos;s important and not just its last computed value.</p><p id="fbb32b275e9348049176eae9d8fe626f" /><p id="deb173df5222436cb3be1f598c18d050">The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.</p><p id="c6c1c10fa0a245a1a714f27e4c82b4d9" /><p id="d2f516454efe404b94c3f36b22cec039"><em><em style="italic">Gated Recurrent Unit (GRU) Networks</em></em>: In 2014, a simplification of the LSTM was introduced called the gated recurrent unit. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.</p><p id="fb308dcb61b1458b858a07765cd0f78e" /><p id="ef80023118604daf82bb8dd629296a7f">The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and with more data can lead to better results.</p><image id="f5b5737b4e6a427287ceff87a64afdc3" src="../webcontent/image-e035a7f1c7c44a4bae749a8c4cc0bd99-3.png" alt="" style="inline" vertical-align="middle" height="132" width="440"><caption><p id="a99df39260fd43cca55595072f05eee4" /></caption><popout enable="false"></popout></image><p id="f7059323c09f457d8aa489582c96efc7"><em style="italic">(</em><em style="italic"><em>https://sh-tsang.medium.com/review-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling-gru-2adb86559257</em></em><em style="italic">)</em></p></body></workbook_page>\n'