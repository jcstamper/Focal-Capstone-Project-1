b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="abc1353e91484c4c826f24ec08c0c0d0"><head><title>Introduction to Computer Vision</title></head><body><section id="dddfed0fb36f471e99268d8621b6d8d3"><title>Introduction to Computer Vision</title><body><p id="e632d7617fe74efeaeabf204240b4b9d">Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.</p><p id="bfb86c1d32d141a1a1d6df204c4845b3">Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.</p><image id="fafd32346d284d2a9d5df48e5930dc39" src="../webcontent/image-fa4af4b47a6b4c43af857ae981212dc6-1.png" alt="" style="inline" vertical-align="middle" height="341" width="650"><caption><p id="f238610dd6744dd5b9f059980dd21c4b">Figure 1. Rivian Pickup Truck.</p></caption><popout enable="false"></popout></image><p id="c6e1efcebf884b719beb09bf32da37c9">Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of <em style="italic">image understanding</em> in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer&apos;s view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).</p><image id="aa885124a8eb4263b8f60ef41609f3dc" src="../webcontent/Screen_Shot_2022-07-15_at_32852_pm.png" alt="" style="inline" vertical-align="middle" height="440" width="650"><caption><p id="f0e34c370e8f48dc8e341a591134ce9d">Figure 2. What a person sees.</p></caption><popout enable="false"></popout></image><image id="ac80b0682be44687b57fcb0083898eb3" src="../webcontent/Screen_Shot_2022-07-15_at_32902_pm.png" alt="" style="inline" vertical-align="middle" height="394" width="650"><caption><p id="a8a839763e6a43429f9225195c8ea792">Figure 3. What a computer sees.</p></caption><popout enable="false"></popout></image><p id="f09d4e0cdeb64418ac6ee8002ed8e0b0">A straightforward approach would be to flatten the image&apos;s pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).</p><p id="e3c8067322c74f929a0a243f93ff202e">In this module, we will introduce the basic CNN architecture and classic architectures such as LeNet, AlexNet, VGGNet, and ResNet, which you will be implementing during the Computer Vision task in one of the Projects during the course. We will also brief the current state of CNN research and a few contemporary applications of computer vision.</p></body></section></body></workbook_page>\n'