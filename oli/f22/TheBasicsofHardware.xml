b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="TheBasicsofHardware"><head><title>The Basics of Hardware: Memory</title></head><body><p id="fe701680d2b04a8380125b84f49e4955">For our purposes, before we get into the differences between CPUs and GPUs, it is important to take some time to first understand memory and its role in computation.</p><p id="b4b7c0365bb74e15a5a8bbdd4b10ce8e">While it is incredibly difficult, if not impossible, to know the relative speeds of different operations on a computer, accessing and loading data from memory tends to be one of the slowest if poorly done. To better use memory, you need to understand that memory is not a singular shelf from which you can pull and place data. Instead, it operates as a series of increasingly higher shelves, with those shelves at the bottom being much more expensive but way faster to reach than the shelves at the top. This is known as a <em>memory hierarchy;</em> it serves as a great basic model of memory for performance-intensive applications like those found in data science.</p><p id="c83aabb8f8274610a2838be040d85f1d">The memory hierarchy can frequently be pictured as a pyramid, where at the top lie the bits of memory that are the fastest to access but the most expensive to get, while those at the bottom are the slowest to access but the least expensive to buy:</p><image id="ebf5e8ef1d2e44bdbac38988d98d4794" src="../webcontent/image-The_Basics_of_Hardware-1.png" alt="" style="inline" vertical-align="middle" height="461" width="650"><caption><p id="b8dbba3cb4e8422896570d1a99273cd9" /></caption><popout enable="false"></popout></image><p id="b145eafff3d74a19abe141916caba163">With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datum\xe2\x80\x99s location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. You\xe2\x80\x99ve implemented this partially in P1.</p><p id="b082bb203f744e199b393e667f627b70">As most of this management is done outside of your control, it can be frustrating to try to figure out how to make data access code faster. However, while the systems managing the data are opaque, it pays to instead focus on two simple facts that underlie all attempts at managing this hierarchy:</p><ol id="e1876bc90ec249cfb9cfad74db2e63f9"><li><p id="c9cb9298cf124cc29eaa763d4ccfbe8f"><em>Temporal Locality:</em> When you access some data on your computer once, you\xe2\x80\x99re probably going to access it again in the near future. Thus, if you can keep the data you\xe2\x80\x99re accessing to one small section at a time, you can receive better performance.</p></li><li><p id="d88201cba4c44464b74785266dbf3f8f"><em>Spatial Locality:</em> When you access one element in an array or another complex data structure, you are probably going to access data next to it. Thus, the management system will not just store the data at your location, but also the data near your requested location for future quick access. Thus, if you can access data sequentially in memory, you can receive better performance.</p></li></ol><p id="cc2482f8418d491c9d2980c893b51545">Following these two principles can help guide you towards faster code in general. For example, consider the following two pseudocode sections:</p><section id="e33379301fd646b997d066c2c4bf6705"><title>Section 1</title><body><codeblock id="ef5124be2fe04edd9508c26b1cccbd00" syntax="text" highlight="" number="false" start=""><![CDATA[sum = 0\nfor index from 1 to 100:\n    sum = sum + array[index]\nreturn sum]]></codeblock></body></section><section id="cf315b7ba1bd4e6b95d8153ad06ceb1d"><title>Section 2</title><body><codeblock id="e6a3359024f64958b086dc8e9cbaab6e" syntax="text" highlight="" number="false" start=""><![CDATA[sum = 0\nlist_of_numbers = [1,2,...,100]\nfor index from 1 to 100:\n    random_index = random number from list_of_numbers not chosen yet\n    sum = sum + array[random_index]\nreturn sum]]></codeblock></body></section><p id="ec179c0bf5bf497a98a45c96943ea974">The first code section accesses the elements of <code><![CDATA[array]]></code> in order, from 1 to 100, while the second section accesses the elements of <code><![CDATA[array]]></code> randomly. While both tasks perform essentially the same task, namely that of summing the elements of an array, the code in section 1 will be much faster than that of section 2, as when we access <code><![CDATA[array[1]]]></code> The computer stores the elements near it in the cache, allowing for their rapid access compared to the access pattern in section 2. A good general rule of thumb here is <em>simple and direct</em> access patterns beat <em>complex and potentially insufficient</em> algorithms here. In fact, it is for this reason that the traditional \\(O(n^3)\\) matrix-multiplication algorithm tends to beat fancier algorithms in benchmark tests. While there are analytically faster algorithms for the problem, the simple access pattern, combined with ease of tuning, ensures that competitive performance is maintained for longer than more complicated algorithms.</p></body></workbook_page>\n'