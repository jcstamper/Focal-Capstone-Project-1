b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="aa224e7d28ee42f4bf5a6162ff092f1b"><head><title>Introduction and Evolution of Language Representations</title></head><body><p id="dd5ccedd5b694ff49d5820469bb1879f">Machines don\xe2\x80\x99t understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like <em style="italic">bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf)</em>. However, these techniques had the following two significant limitations:</p><ol id="c04165f532524cccb2f96a977e732576"><li><p id="f7f12af389f448dc838282423cda1f92">The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.</p></li><li><p id="f2c72c7df4a54e478a78d25a7d621326">No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.</p></li></ol><p id="bd4f3752be6640ac9df67ecbff1aad43">These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, \xe2\x80\x98learn,\xe2\x80\x99 and \xe2\x80\x98study\xe2\x80\x99 will be closer than the pair \xe2\x80\x98learn\xe2\x80\x99 and \xe2\x80\x98eat.\xe2\x80\x99 Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris. </p><p id="a4cb28f1f8514b98a6283c9940a63a92">These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, it\xe2\x80\x99s a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.</p><p id="eb7653573ad9494c9616d45f5807d5ef"><em>Word2Vec (2013)</em> and <em>GloVe (\xe2\x80\x9dGlobal Vectors for Word Representation\xe2\x80\x9d)</em> (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.</p><p id="cf3dcc02b40742dcbaff1a9e2870c6b1">Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word \xe2\x80\x9ccell\xe2\x80\x9d in the sentence, \xe2\x80\x9cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,\xe2\x80\x9d would be the same.</p><p id="fbd141964b6542aeb9c19bf8f11bd2b9">This problem was solved by the <em>ELMo (&quot;Embeddings from Language Model&quot;)</em> model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.</p><p id="e14e3419963a4d55ab2aba495d049c4c">Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results. </p><p id="e86fefd9d09b47da87228e2d47ffe17e"><em>ULM-FiT (\xe2\x80\x9cUniversal Language Model Fine-tuning\xe2\x80\x9d)</em>, also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:</p><ol id="afa60eef5b164465818e04a0fac3e304"><li><p id="ded22484c16141279d907ffaceab1629"><em style="italic">General Domain LM Pre-Training,</em> where the language model is trained on a general-domain corpus to capture general features of language in different network layers.</p></li><li><p id="a41b9a92a703466289b9bc2202fd4648"><em style="italic">Target task Discriminative Fine-Tuning</em> where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.</p></li><li><p id="d7a91edb4d7c490a92a8365138c2ec8d"><em style="italic">Target task Classifier Fine-Tuning</em> where the classifier is fine-tuned on the target task using gradual unfreezing (unfreezing weights from the last to the first layer in different learning epochs) and repeating stage 2. This helps the network to preserve low-level representations and adapt to high-level ones.</p></li></ol><p id="bb12845a4dfb41b6a0df7b27964331f2">Finally, the <em>Transformer</em> architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. We\xe2\x80\x99ll discuss the  Transformer model and the motivation behind its architecture in detail next.</p></body></workbook_page>\n'