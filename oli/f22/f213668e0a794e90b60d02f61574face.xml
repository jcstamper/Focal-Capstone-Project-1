b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f213668e0a794e90b60d02f61574face"><head><title>BERT Training</title></head><body><section id="dde71d1d1a9a4bba83b53c148eb37a33"><title>BERT Training</title><body><p id="cb967cd99b5c497dbbba8ca4445123ef"> </p></body></section><p id="e284e9e8718c48b6ab97f22ef53c8daf">For any NLP task, BERT is generally trained in two steps:</p><ol id="df99132218cc4f3b975abc4a8f0df614"><li><p id="dce5cc248e8e4794ab2f9619c6db6999">First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.</p></li><li><p id="a43c36b45e7248c3b5f2891c94a65e71">Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.</p></li></ol><p id="e7ce05b4efca44a398019d70eb56ac23">The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.</p><section id="cff6a40d565b4c72a42680b8e0814087"><title>Masked Language Modeling (MLM)</title><body><p id="f3d47931f680442b8c8c8965ce429570"> </p></body></section><p id="b7390c0e89df48c1a9581e1d6ba7077a">Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERT\xe2\x80\x99s prediction of the masked token.</p></body></workbook_page>\n'