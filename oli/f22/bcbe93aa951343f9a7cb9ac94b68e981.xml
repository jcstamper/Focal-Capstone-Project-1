b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="bcbe93aa951343f9a7cb9ac94b68e981"><head><title>OLI BERT - Nikhil</title></head><body><section id="e2431809c72845c69773316fab9e423e"><title>Learning Objectives</title><body><p id="a3ea384b3281486d89ffb82cb007b149"> </p></body></section><ul id="c9ff15b943eb4203ac5bb61410c7992b"><li><p id="a257d0ca377d4b43a7713ae80c7d3903">Present encoder and decoder models derived from Transformers.</p></li><li><p id="a9b944195188494c9e297ffa6d8971f3">Illustrate the general BERT architecture and how it relates to Transformers.</p></li><li><p id="fc1651b232f2437496fd2d455be150d1">Explain BERT pre-training using Masked Language Modeling and Next Sentence Prediction and discuss fine-tuning.</p></li><li><p id="efdc16fef26847fca8fc10aad545cf4f">Show how BERT is used in various Natural Language Processing tasks.</p></li><li><p id="f6984d331ddf45bbaf32a21507083b00">Discuss the latest variants of BERT.</p></li></ul><p id="e617f3541e79498e96ef18f39c78c2b1" /><p id="e111e5ef77604938b90ef68ffd2ca7bf" /><p id="b31185f6ccca4e88b240b1fe4be34b94" /><p id="dcb870ada8f9426aac1e31617be212e6">Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.</p><section id="d301d34a5b6e405eb7dd289dd28c45fd"><title>Encoder-Only Models</title><body><p id="c86c1592b990485d99221d8272050ca4"> </p></body></section><ol id="e947413e89a64afba2db7de5e835ece8"><li><p id="fe2ebc592e99400cbb9105fb2a3c9bb4">Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \xe2\x80\x9cbi-directional\xe2\x80\x9d attention and are often called <em style="italic">auto-encoding models</em>.</p></li><li><p id="bde037b38ad44355b2381a1ab04a4fe3">The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p></li><li><p id="afbc7f2c38184cea8f9624417fdd9b9e">Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.</p></li><li><p id="b74bcf7123f74702aad97b44e241cdb7">Representatives of this family of models include BERT, ALBERT, RoBERTa.</p></li></ol><section id="fc090a61c5d24f09aab8083e66a04da8"><title>Decoder-Only Models</title><body><p id="ecc90b945f5a439d9d7932abc77a14d5"> </p></body></section><ol id="feb2c5e630974ae885276d46567f2dd0"><li><p id="d09f51bb8090430ab742c5707ab85bb4">Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em style="italic">auto-regressive models</em>.</p></li><li><p id="e36dcf91e47e45f2bec0575915d4a667">The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p></li><li><p id="c454a11ca2bb4b8c90f14a75b995ebc2">These models are best suited for tasks involving text generation.</p></li><li><p id="cf1325d0ae634d3f8122c47968f65d84">Representatives of this family of models include CTRL, GPT, GPT-2.</p></li></ol><p id="ef78b1deaa89400abfb4e1f7893854f6">We are now finally ready to study arguably the most famous Encoder Model, BERT and its variants in detail.</p><section id="af866886bdbd40dbb227873e6814a6e8"><title>BERT</title><body><p id="a472ed33a9e743ceacc2972153ef485f"> </p></body></section><p id="f1974024f13c4bacad0271bfd82664e3">One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.</p><p id="ad29012f63164cbba8f5a470cd14b58e">BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms \xe2\x80\x94 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\xe2\x80\x99s goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained Transformer Encoder stack.</p><p id="a01cb1a0a34240938059733ec2f157f7">As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\xe2\x80\x99s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p><p id="c4c7c1e8aaeb496691a5fa9de1813061">The original BERT<link href="https://arxiv.org/abs/1810.04805" target="new" internal="false"> paper</link> presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.</p><ol id="a81a5dfb43954c5c87d488a599fc6ce4"><li><p id="f4137acee073418bb8960f2d27175ad1">BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.</p></li><li><p id="da0b46654bdf453ab66b02cf1cfcb922">BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.</p></li></ol><section id="b505f89a212541ec84244886d462d008"><title>BERT Training</title><body><p id="da59fece0e534e1ebd292a922cf06551"> </p></body></section><p id="a95a9f78f2044a2c8180cf511fcac8eb">For any NLP task, BERT is generally trained in two steps:</p><ol id="c6218d1ff1604e98a85c4ea3e1ce02a5"><li><p id="b5a98f6807a447c1a45ebacd4bed68ab">First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.</p></li><li><p id="a047c8357816440690bfacaac428dd91">Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.</p></li></ol><p id="a49225b4783143db984624075bee19f9">The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.</p><section id="b3ee8a71ad4c429a91355877d32d5522"><title>Masked Language Modeling (MLM)</title><body><p id="d9bad77a1b914d489522e5c85c4ff391"> </p></body></section><p id="cfc61fdb1b39432984f913212e02c31a">Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERT\xe2\x80\x99s prediction of the masked token.</p><image id="ca5c88ecd7cf4639bc84c2bd7fb53d9c" src="../webcontent/image-bcbe93aa951343f9a7cb9ac94b68e981-1.png" alt="" style="inline" vertical-align="middle" height="315" width="468"><caption><p id="a73960b2587846d68dac7767b7d82824" /></caption><popout enable="false"></popout></image><section id="a59058d927e84691ad18e76d5ec3c3d6"><title>Next Sentence Prediction (NSP)</title><body><p id="ace98d7b86a2419f96d68d00b4800bfb"> </p></body></section><p id="c8a06571e1384d92840f81c3a66e2e3e">While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.</p><p id="e7bd8541d5c54bf493533f8e54e6ce5c">Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.</p></body></workbook_page>\n'