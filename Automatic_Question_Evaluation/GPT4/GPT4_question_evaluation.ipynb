{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Question_Rubric_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('expert_fcds_eval.csv')\n",
    "df = df.drop(columns=['File_name', 'Unnamed: 0', 'Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concepts</th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Active Learning: A data science pattern that ...</td>\n",
       "      <td>. Expected Model Change: How does the expected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI consulting firm, EVP framework, Automotive...</td>\n",
       "      <td>. What are some measurable metrics used to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI consulting firm, EVP framework, Automotive...</td>\n",
       "      <td>. What is the significance of completed transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Interpretability: The importance of interpret...</td>\n",
       "      <td>What are the effects of trade-offs within a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Assessment and Model Selection are key ...</td>\n",
       "      <td>What is the process of testing and validating...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Concepts  \\\n",
       "0   Active Learning: A data science pattern that ...   \n",
       "1   AI consulting firm, EVP framework, Automotive...   \n",
       "2   AI consulting firm, EVP framework, Automotive...   \n",
       "3   Interpretability: The importance of interpret...   \n",
       "4   Model Assessment and Model Selection are key ...   \n",
       "\n",
       "                                           Questions  \n",
       "0  . Expected Model Change: How does the expected...  \n",
       "1  . What are some measurable metrics used to ass...  \n",
       "2  . What is the significance of completed transa...  \n",
       "3   What are the effects of trade-offs within a m...  \n",
       "4   What is the process of testing and validating...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "questions = df['Questions'].to_list()\n",
    "concepts = df['Concepts'].to_list()\n",
    "iwfs = [{\"criteria\": \"gramatical accuracy\",\n",
    "    \"definition\": \"question text is grammatically accurate and logical to reader\"},\n",
    "    {\"criteria\": \"ambiguous or unclear information \",\n",
    "    \"definition\": \"questions is written in clear, unambiguous language. It is clear what is being asked and what is expected in the answer\"}, \n",
    "    {\"criteria\": \"gratuious information\",\n",
    "     \"definition\": \"avoids unnecessary information in the stem that is not required to answer the question\"}, \n",
    "    {\"criteria\": \"pedagogical value\",\n",
    "     \"definition\": \"question is of educational value to students in a data science course\"}, #update to reflect course you are interested in\n",
    "    {\"criteria\": \"covers key concept\",\n",
    "     \"definition\": \"question relates closely to an identified key concept for the given block of text\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop over each question, then for each question, call the IWF criteria one at a time on it.\n",
    "done = False\n",
    "counter = 0\n",
    "error_counter = 0\n",
    "for q in questions:\n",
    "    print(counter)\n",
    "    results.append(q)\n",
    "    for i in iwfs:\n",
    "        #Run this as a while loop with error handling code, as sometimes the GPT-4 API goes down, returning an error, in which \n",
    "        #we'll need to wait and retry our call\n",
    "        while(done == False):\n",
    "            try:\n",
    "                o = openai.ChatCompletion.create(\n",
    "                  model=\"gpt-4\", \n",
    "                  messages=[\n",
    "                    {\"role\": \"user\", \"content\": f'Begin your response with yes or no, does this question satisfy the criteria relating to {i[\"criteria\"]}: {i[\"definition\"]}? Explain why. {q}'},\n",
    "                  ],\n",
    "                  max_tokens=100\n",
    "                 )\n",
    "                time.sleep(1)\n",
    "                done = True \n",
    "            except Exception as e:\n",
    "                error_counter += 1\n",
    "                print(f'Error: {error_counter}, Message: {str(e)}')\n",
    "                time.sleep(15)\n",
    "        done = False\n",
    "        results.append(o)\n",
    "    while(done == False):\n",
    "        try:\n",
    "            o = openai.ChatCompletion.create(\n",
    "              model=\"gpt-4\", \n",
    "              messages=[\n",
    "                {\"role\": \"user\", \"content\": f'Start your answer with the concept. Given this list of concepts: {concepts[counter]}, which is most closely related, if any, to this question: {q}'},\n",
    "                ],\n",
    "                max_tokens=100\n",
    "              )\n",
    "            time.sleep(1)\n",
    "            done = True \n",
    "        except Exception as e:\n",
    "            error_counter += 1\n",
    "            print(f'Error: {error_counter}, Message: {str(e)}')\n",
    "            time.sleep(15)\n",
    "    done = False\n",
    "    results.append(o)\n",
    "    while(done == False):\n",
    "        try:\n",
    "            o = openai.ChatCompletion.create(\n",
    "              model=\"gpt-4\", \n",
    "              #update with current course selection\n",
    "              messages=[\n",
    "                {\"role\": \"user\", \"content\": f'Begin your response with either good, fair, or poor, how well is this question written for testing a students understanding in a data science course. Explain why. {q}'},\n",
    "                ],\n",
    "                max_tokens=100\n",
    "              )\n",
    "            time.sleep(1)\n",
    "            done = True \n",
    "        except Exception as e:\n",
    "            error_counter += 1\n",
    "            print(f'Error: {error_counter}, Message: {str(e)}')\n",
    "            time.sleep(15)\n",
    "    done = False\n",
    "    results.append(o)\n",
    "    counter += 1\n",
    "rows = []\n",
    "r = []\n",
    "indz = 0\n",
    "for res in results :\n",
    "    try:\n",
    "        r.append(res.choices[0].message.content)\n",
    "    except:\n",
    "        r.append(res)\n",
    "        \n",
    "    #Once we've created a row, r, that contains the question text and 19 criteria, append it to our greater rows list\n",
    "    if indz == 1:\n",
    "        rows.append(r)\n",
    "        r = []\n",
    "        indz = 0\n",
    "    else:\n",
    "        indz = indz + 1\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'question',\n",
    "    'gramatical_accuracy',\n",
    "    'ambiguous_or_unclear',\n",
    "    'gratuitous_information',\n",
    "    'pedagogical_value',\n",
    "    'covers_key_concept',\n",
    "    'concept_covered',\n",
    "    'question_grade'\n",
    "]\n",
    "\n",
    "pd_results = pd.DataFrame(rows, columns=columns)\n",
    "pd_results.to_csv(\"gpt-4_fcds_pedagogical_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
