File_name,Questions,Concepts
/content/data/f92219fb485c4e96bbb649d63e54a7c5.xml,Expected Model Change: How does the expected model change approach select an observation for labeling?," Active Learning: A data science pattern that involves an algorithm or learner choosing its own data to learn from, resulting in better performance with less training., Scenarios: Presentations that allow a learner to query the labels of observations in a dataset., Membership Query Synthesis: A scenario where a learner generates an observation similar to those in the dataset, which can then be labeled by an oracle., Stream-based Selective Sampling: A scenario where unlabeled data points are evaluated by an algorithm to determine if they should be labeled for training or discarded., Pool Based Sampling: A scenario where observations are collected from a pool of unlabeled data based on an informativeness measure, and the selected observations are labeled., Informative data points: Data points that are difficult for the algorithm to classify, improving the algorithm's abilities., Query Strategies: Strategies used to evaluate the informativeness of unlabeled data., Uncertainty Sampling: An approach that allows the active learner to query observations it is unable to label., Query-by-committee: Involves using a group of models with competing hypotheses, where the most informative query is identified based on disagreement among the models.,. Expected Model Change: Selecting the observation that would"
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml,What are some measurable metrics used to assess the success of a customer retention strategy?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml,What is the significance of completed transactions in evaluating the success of a customer retention strategy?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/ff16c81c152740aa9d2d3e312b8cc4f7.xml, What are the effects of trade-offs within a model?," Interpretability: The importance of interpretability in data science and machine learning, and its role in fixing issues with models and explaining their results., Explainability: The concept of explaining why a model produces certain results and the effects of changes within a model., Accuracy: The measurement used to determine the best model for a task and its role in producing better results and predictions., Trade-offs: The ability of a data scientist to measure the effects of any trade-offs within a model., Restrictions: Certain sectors, such as banking and education, are restricted by laws and standards in their use of certain techniques to protect consumer data and prevent bias in decision-making processes., Retaining interpretability: The challenge of retaining interpretability while improving accuracy in data science., Recommended steps: Hall (2016) recommends several steps to strike a balance between accuracy and interpretability, including training black box models, using different regression techniques, and creating small interpretable ensemble models., Variable importance measures: The use of variable importance measures to explain black box models better., Metrics: The discussion of different metrics to evaluate model accuracy in the next module."
/content/data/f692a0964d6245e48c56a2d31412e253.xml, What is the process of testing and validating a model to ensure that real-world data can be introduced to it?," Model Assessment and Model Selection are key concepts of importance to every data scientist., Testing and validating a model to ensure that real-world data can be introduced to it., The Validation Set Approach and the use of a Validation Dataset., The mean squared error (MSE) as a measure of error rate for quantitative outputs., k-Fold Cross-Validation and its implementation., Assessing model bias and variance., The importance of selecting an appropriate value for k in k-Fold Cross-Validation., Leave One Out Cross Validation (LOOCV) and its characteristics., LOOCV having higher variance compared to k-Fold Cross-Validation.,. Using Cross Validation with Regression and Classification Problems.,. The use of misclassified observations for measuring test error in classification problems.,. The use of mean squared error (MSE) for measuring test error in regression problems."
/content/data/fc03f67af2e341e3aaf29ee2ea60c8ca.xml,What happens on the encoder side of a Transformer model?," Transformer model, Sequence2Sequence models with attention, Hidden states, Input embeddings, RNNs (Recurrent Neural Networks), Attention mechanism, Encoder and decoder blocks, Self-attention, Intra-attention,. Machine reading,. Abstractive summarization,. Image description generation,. Encoder side,. Decoder side,. Context vector,. Parallel processing,. Time-step,. Encoder output"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,What is the Minimum Description Length (MDL) in model selection?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. Bayesian Information Criterion (BIC),. Bayesian statistics,. BIC score,. Minimum Description Length (MDL),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f77eeffd875947d69e59a08b5d4076ec.xml,Accuracy rate: What does the accuracy rate indicate in model performance?," Feature engineering: Creating features from raw data to facilitate model building., Principal Component Analysis (PCA): A technique used in the data science lifecycle for modeling., Model understanding phase: Exploring terminology and concepts related to model building., Analytic objectives: Goals set to meet the needs of the client and obtain actionable insights., Model training: Partitioning or splitting data into training and test sets to build and validate models., Supervised learning techniques: Predicting outcomes based on independent variables., Unsupervised learning techniques: Uncovering patterns in data without a known response variable., Goodness of fit measures: Assessing the fit of a regression model to the data., Overfitting: When a model learns all the details in the training set and fails to generalize to new data.,. Underfitting: When a model does not suit the dataset.,. Confusion matrix: Assessing the classification performance of models.,. Misclassification rate: Proportion of observations classified incorrectly.,. Accuracy rate: Proportion of observations classified correctly.,. Sensitivity or recall: Proportion of the target class classified correctly.,. Precision: Proportion of predicted target class observations that belong to the target"
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml, What strategies can be used for effective customer acquisition in the automotive services industry?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/ec0e8fe23815494b830d5d25f22e55a2.xml, - When should data science knowledge be applied in project planning and execution?," Vision document: The starting point of documentation set, provides an overview of the project, removes ambiguities, and aligns collaborators towards the same goal., Collaborators: The importance of having clear vision and documentation when working with multiple collaborators., Virtual Case File study: Example of a project failure due to an unclear vision., FBI: The organization that wasted over $100 million on a case-management software project., Data science knowledge: The text provides information related to project documentation and the impact of unclear vision on project success."
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml, How does the EVP framework contribute to meeting business needs?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/ebc62268ac6c4c188a6719231f3c3f54.xml, How does the project team Evaluate Requirements?," Requirements Gathering Process, Stakeholders, Data Science Projects, Gather Information, Define and Prioritize Requirements, Evaluate Requirements, Receive Sign-Off, Requirements Management Plan, Project Description,. Team Responsibilities,. Tools,. Change Control and Requirements Gathering"
/content/data/eb2f3840a8104780912ccad53392a130.xml, When should a data science project be considered to succeed in principle?," Solving the problem must be part of a possible solution vision toward the business objective., Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data., The problem must be specific and realistic so that a corresponding data science project can succeed in principle., The business objective should be clearly stated to propose a solution vision., The data science team is responsible for leveraging data to help solve sub-problems and produce data-derived insights., Using data to support business objectives may be the main business objective., Higher-level business needs should be fulfilled to progress beyond ""we want to leverage data somehow"" and arrive at a proper project formulation., An example of a data science project involving social media data for understanding the market., There must be a sound presumption that the problem's solution must benefit from the use of data.,. Collaboration between domain experts and data scientists to discuss available data sources and their suitability for the solution.,. Lack of readiness in the organization, unsuitability of the data, or difficulty in collecting data are domain-specific problems around data availability.,. Technical objections such as too little data, fragmentation"
/content/data/f73b5ad409304e1c8a4ec689bab7463d.xml, What is the role of the Bayes theorem in accurately assessing the risk of a person developing macular degeneration based on a certain age range?," The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event., The Bayes theorem supports accurately assessing the risk of a person developing macular degeneration based on a certain age range instead of making assumptions., Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available., Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability., Naive Bayes (NB) Method is a simple classifier that can be applied to categorical predictors., Naive Bayes performs quite well for real-world applications, despite its naive assumption of independence between predictors., Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task., Naive Bayes is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results., Naive Bayes is useful for ranking and classification tasks.,. Other Bayesian Methods can be used in Data Science, which are explored in machine learning and applied to statistics courses."
/content/data/f4e02624db114a8a9c8cd78840a1f707.xml,What methods can be used to calculate the error rate for trained models?," Errors At All Phases: Errors can occur during the business understanding phase and the data understanding phase, leading to bloated costs, scope creep, and inadequate solutions., Errors in Model Understanding: Errors in this phase are used to validate models and determine their expected performance for deployment., Training Error: Derived by calculating the classification error of a model on the exact data used for training., Test Error: Gives insight into the amount of errors to expect when making future predictions and is used for model selection., Irreducible Error: The noise term in the true relationship that cannot be reduced by any model., Reducible Error: The error resulting from a mismatch between the estimated relationship and the true relationship between variables., Calculating Error Rate: Exploring different methods and diving deeper into calculating the error rate for trained models."
/content/data/f92219fb485c4e96bbb649d63e54a7c5.xml, Pool Based Sampling: What is the role of an informativeness measure in pool based sampling?," Active Learning: A data science pattern that involves an algorithm or learner choosing its own data to learn from, resulting in better performance with less training., Scenarios: Presentations that allow a learner to query the labels of observations in a dataset., Membership Query Synthesis: A scenario where a learner generates an observation similar to those in the dataset, which can then be labeled by an oracle., Stream-based Selective Sampling: A scenario where unlabeled data points are evaluated by an algorithm to determine if they should be labeled for training or discarded., Pool Based Sampling: A scenario where observations are collected from a pool of unlabeled data based on an informativeness measure, and the selected observations are labeled., Informative data points: Data points that are difficult for the algorithm to classify, improving the algorithm's abilities., Query Strategies: Strategies used to evaluate the informativeness of unlabeled data., Uncertainty Sampling: An approach that allows the active learner to query observations it is unable to label., Query-by-committee: Involves using a group of models with competing hypotheses, where the most informative query is identified based on disagreement among the models.,. Expected Model Change: Selecting the observation that would"
/content/data/f9851381ff604681bddf45844c5dcee1.xml,"\n<!DOCTYPE workbook_page PUBLIC...""?"," XML version: 1.0, Encoding: UTF-8, Workbook page doctype: ""-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN"", Workbook page doctype URL: ""http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"", Workbook page namespace: bib=""http://bibtexml.sf.net/"", Workbook page namespace: cmd=""http://oli.web.cmu.edu/content/metadata/2.1/"", Workbook page namespace: m=""http://www.w3.org/1998/Math/MathML"", Workbook page namespace: pref=""http://oli.web.cmu.edu/preferences/"", Workbook page namespace: theme=""http://oli.web.cmu.edu/presentation/"",. Workbook page namespace: wb=""http://oli.web.cmu.edu/activity/workbook/"",. Workbook page ID: f9851381ff604681bddf45844c5dcee1,. Page title: Data Sampling,. Paragraph ID: a518e4253f0c49fcaeb0f7c3a428d714,. Paragraph content: ""This is"
/content/data/eeda06a41a3a45fe86aee400b55d7b69.xml, When did Kaushik become a fellow and alumni mentor at the Insights Data Science program?," Kaushik Shakkari is a senior data scientist at Cognistx., Kaushik proposed and leads SQUARE, an end-end question answering product at Cognistx., Kaushik is a fellow and alumni mentor at the Insights Data Science program., Kaushik's research and areas of interest include user behavioral analysis, semantic search, and deep learning., Kaushik can be reached on LinkedIn and his articles can be read on Medium."
/content/data/f213668e0a794e90b60d02f61574face.xml,How is the pre-trained BERT model fine-tuned for a specific task?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/f9851381ff604681bddf45844c5dcee1.xml,. What does the Workbook page ID f9851381ff604681bddf45844c5dcee1 represent?," XML version: 1.0, Encoding: UTF-8, Workbook page doctype: ""-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN"", Workbook page doctype URL: ""http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"", Workbook page namespace: bib=""http://bibtexml.sf.net/"", Workbook page namespace: cmd=""http://oli.web.cmu.edu/content/metadata/2.1/"", Workbook page namespace: m=""http://www.w3.org/1998/Math/MathML"", Workbook page namespace: pref=""http://oli.web.cmu.edu/preferences/"", Workbook page namespace: theme=""http://oli.web.cmu.edu/presentation/"",. Workbook page namespace: wb=""http://oli.web.cmu.edu/activity/workbook/"",. Workbook page ID: f9851381ff604681bddf45844c5dcee1,. Page title: Data Sampling,. Paragraph ID: a518e4253f0c49fcaeb0f7c3a428d714,. Paragraph content: ""This is"
/content/data/f59f965701df42e796a110d414580861.xml, What are the technical and temporal limitations of transparency?," Data science often includes descriptive analysis and prescriptive analytics., Large automated systems raise questions of responsibility and accountability., Responsibility can be personal or organizational., The concept of the moral crumple zone in automated systems., Accountability involves all the little decisions made by a group of people., Transparency and auditability are important in data governance., Transparency involves disclosing human involvement, data sources, algorithms, and the presence of AI solutions., Transparency can be disconnected from power, harmful, intentionally occlude, create false binaries, invoke neoliberal models of agency, and not necessarily build trust., Transparency has technical and temporal limitations.,. Auditing is an experimental test to ensure systems are doing what they were intended to do.,. Auditing can be used to investigate discrimination and security vulnerabilities in data science applications."
/content/data/ed6c530b62b0449484448126c11e6613.xml," What is the significance of the title ""Module 15 Summary"" in this XML document?"," XML version: 1.0, Encoding: UTF-8, Document type: Workbook Page MathML 3.8, Public identifier: -//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN, Document type definition (DTD) URL: http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd, Root element: workbook_page, XML namespaces: bib, cmd, m, pref, theme, wb, Workbook page ID: ed6c530b62b0449484448126c11e6613, Title: Module 15 Summary,. Body content: Empty paragraph"
/content/data/f73b5ad409304e1c8a4ec689bab7463d.xml, How can Naive Bayes perform well with a small training dataset for estimating the right parameters for a classification task?," The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event., The Bayes theorem supports accurately assessing the risk of a person developing macular degeneration based on a certain age range instead of making assumptions., Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available., Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability., Naive Bayes (NB) Method is a simple classifier that can be applied to categorical predictors., Naive Bayes performs quite well for real-world applications, despite its naive assumption of independence between predictors., Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task., Naive Bayes is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results., Naive Bayes is useful for ranking and classification tasks.,. Other Bayesian Methods can be used in Data Science, which are explored in machine learning and applied to statistics courses."
/content/data/e843820862824c77903d30f41a295dac.xml, Agglomerative Clustering: Why is Agglomerative Clustering considered a bottom-up approach in Hierarchical Clustering?," Hierarchical Clustering, Dendrogram, Agglomerative Clustering, Euclidean distance, Merging Clusters, Single Linkage Method, Complete Linkage Method, Average Linkage Method, Centroid Linkage Method,. Divisive Clustering"
/content/data/f9469fffd5d24f85a8982f29b9fb7a38.xml, Why is the encoding set to UTF-8 in this XML content?," XML version: 1.0, Encoding: UTF-8, DTD: Workbook Page MathML 3.8, DTD URL: http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd, Root element: workbook_page, Namespaces: bib, cmd, m, pref, theme, wb, ID attribute: f9469fffd5d24f85a8982f29b9fb7a38, Title: New Page, Body content: Empty,. Paragraph ID: e46f4057d95e45cfb0f8567cb65fdb90,. Paragraph content: This is a new page with empty contents."
/content/data/ed91e3be86334b32883983125941046e.xml, What is the main difference between lazy learning and eager learning?," Lazy learning: A method in which training data is generalized and is most useful for large datasets that will be updated continuously., Eager learning: The opposite of lazy learning, where the learner learns immediately and takes a shorter time to classify data., k-Nearest Neighbor (k-NN) method: A well-known lazy learner that can be used to solve both classification and regression problems., Curse of dimensionality: A challenge faced by k-NN where the method may not perform well when data is not rescaled., Normalization and standardization: Best practices for preparing data for k-NN by rescaling applicable data to the range of 0,1 and standardizing data with a Gaussian distribution., Euclidean distance: A distance measure used in k-NN to determine the distance between observations., Manhattan and Minkowski distances: Alternative distance measures used in k-NN., Hamming Distance: An alternative distance measure for categorical variables in k-NN., Similarity function: k-NN is considered a similarity function as it assigns a new observation to a class based on the classes of its neighbors.,. Determining the number of neighbors (k): Choosing an appropriate value of k to"
/content/data/e18993e9d0f9473e943d7817917b6586.xml," What is the significance of the title ""New Page"" in this XML document?"," XML version: 1.0, Encoding: UTF-8, Document type: Workbook Page MathML 3.8, Workbook page ID: e18993e9d0f9473e943d7817917b6586, Title: New Page, Body content: This is a new page with empty contents."
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. How is the MDL score calculated?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f92219fb485c4e96bbb649d63e54a7c5.xml, Uncertainty Sampling: How does uncertainty sampling assist an active learner in labeling observations?," Active Learning: A data science pattern that involves an algorithm or learner choosing its own data to learn from, resulting in better performance with less training., Scenarios: Presentations that allow a learner to query the labels of observations in a dataset., Membership Query Synthesis: A scenario where a learner generates an observation similar to those in the dataset, which can then be labeled by an oracle., Stream-based Selective Sampling: A scenario where unlabeled data points are evaluated by an algorithm to determine if they should be labeled for training or discarded., Pool Based Sampling: A scenario where observations are collected from a pool of unlabeled data based on an informativeness measure, and the selected observations are labeled., Informative data points: Data points that are difficult for the algorithm to classify, improving the algorithm's abilities., Query Strategies: Strategies used to evaluate the informativeness of unlabeled data., Uncertainty Sampling: An approach that allows the active learner to query observations it is unable to label., Query-by-committee: Involves using a group of models with competing hypotheses, where the most informative query is identified based on disagreement among the models.,. Expected Model Change: Selecting the observation that would"
/content/data/eb2f3840a8104780912ccad53392a130.xml, How should the business objective be stated to propose a solution vision?," Solving the problem must be part of a possible solution vision toward the business objective., Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data., The problem must be specific and realistic so that a corresponding data science project can succeed in principle., The business objective should be clearly stated to propose a solution vision., The data science team is responsible for leveraging data to help solve sub-problems and produce data-derived insights., Using data to support business objectives may be the main business objective., Higher-level business needs should be fulfilled to progress beyond ""we want to leverage data somehow"" and arrive at a proper project formulation., An example of a data science project involving social media data for understanding the market., There must be a sound presumption that the problem's solution must benefit from the use of data.,. Collaboration between domain experts and data scientists to discuss available data sources and their suitability for the solution.,. Lack of readiness in the organization, unsuitability of the data, or difficulty in collecting data are domain-specific problems around data availability.,. Technical objections such as too little data, fragmentation"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml, How are models trained on the entire dataset for inference?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. Bayesian Information Criterion (BIC),. Bayesian statistics,. BIC score,. Minimum Description Length (MDL),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/ff7cba66391a4fb392fae7dd1f8d98db.xml, - What are the components of system requirements in the context of software development?," Business Requirements: Describing the context, scope, and background of a business need, including reasons for proposed changes., System and User Requirements: Detailed description of the system and its operational and development constraints, as well as functions or tasks that a user must perform within the system., Use cases and user stories: Representing user requirements and providing a big picture of what the user will be able to do within a system., Solution Requirements: Tailoring solution requirements to the data science process, including functional and non-functional requirements, as well as requirements for data and models in an analytic solution and reports and dashboards in a business intelligence solution."
/content/data/ed91e3be86334b32883983125941046e.xml, Why is the k-NN method considered a similarity function?," Lazy learning: A method in which training data is generalized and is most useful for large datasets that will be updated continuously., Eager learning: The opposite of lazy learning, where the learner learns immediately and takes a shorter time to classify data., k-Nearest Neighbor (k-NN) method: A well-known lazy learner that can be used to solve both classification and regression problems., Curse of dimensionality: A challenge faced by k-NN where the method may not perform well when data is not rescaled., Normalization and standardization: Best practices for preparing data for k-NN by rescaling applicable data to the range of 0,1 and standardizing data with a Gaussian distribution., Euclidean distance: A distance measure used in k-NN to determine the distance between observations., Manhattan and Minkowski distances: Alternative distance measures used in k-NN., Hamming Distance: An alternative distance measure for categorical variables in k-NN., Similarity function: k-NN is considered a similarity function as it assigns a new observation to a class based on the classes of its neighbors.,. Determining the number of neighbors (k): Choosing an appropriate value of k to"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. How is the BIC score calculated?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f213668e0a794e90b60d02f61574face.xml, Semi-supervised training: Why is semi-supervised training used in the first step of BERT training?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml, What is the role of inference in model selection?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f782d472cff14cc3a7ef287ecba87596.xml,. What is meant by general language understanding in the context of BERT training?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/fc03f67af2e341e3aaf29ee2ea60c8ca.xml,. What is the role of the context vector in Sequence2Sequence models?," Transformer model, Sequence2Sequence models with attention, Hidden states, Input embeddings, RNNs (Recurrent Neural Networks), Attention mechanism, Encoder and decoder blocks, Self-attention, Intra-attention,. Machine reading,. Abstractive summarization,. Image description generation,. Encoder side,. Decoder side,. Context vector,. Parallel processing,. Time-step,. Encoder output"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml, Why is there no train-test split in inference models?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/fe97bc80ef0f46e79cf72378227d95a2.xml, How does model selection with hyperparameter tuning differ from regular model selection?," Model Selection for Prediction: This section describes the process of model selection for prediction, including the input, procedure, and output., Input: The input for model selection includes candidate models M1, M2, ..., Ml., Procedure: The procedure involves splitting the dataset into a train set and a test set. For each candidate model Mi, the model is trained on the train set and evaluated on the test set., Output: The output is the model Mj with the best performance on the test set., Model Selection with Hyperparameter Tuning: This section extends the model selection process to include hyperparameter tuning., Input: The input includes candidate models M1, M2, ..., Ml and a hyperparameter space S., Procedure: The procedure involves splitting the dataset into a train subset, a validation subset, and a test set. For each candidate model Mi, the best hyperparameters are selected based on their performance on the validation subset. A new model Mi is then trained using the best hyperparameters on the combined data from the train subset and validation subset. The model's performance is evaluated on the test set., Output: The output is the model Mj and the associated best hyperparameters with the"
/content/data/ef9a898102e44a628fc073032c6a11ff.xml, Why is the first step in gathering information to identify the stakeholders within the business?," Similar to traditional software development projects, data science projects are guided by requirements gathering principles., The steps followed during the requirements gathering process are listed in Figure 1., Requirements gathering for a data science project involves eliciting the needs of stakeholders and defining the requirements for the analytic solution(s)., The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project., The first step in gathering information is to identify the stakeholders within the business., The business analyst elicits information to determine what the solution should do to meet the defined business and analytic objectives., Stakeholders provide information according to their view of the business needs, and it is the job of the business analyst to define and prioritize requirements., It is important to document ""complete"" requirements that capture the needs of the stakeholders., The project team must verify and validate all documented requirements to ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.,. Sign-off from the client indicates that the requirements have been approved and agreed upon.,. A requirements management plan can be used to document the requirements-gathering process.,. The requirements management plan includes sections such as"
/content/data/ec21750cc96c4744ac20259c45ac474b.xml, Data preprocessing: How is data prepared for use during the EDA process and beyond?," Data integration: Involves ingesting, transforming, and integrating data for access., Analytic solution development: Modeling and analysis of integrated data., Data warehouse: A centralized repository for integrated data, accessible by OLAP servers, DSS systems, and other analytic tools., Data marts: Segments of the data architecture where integrated data can be accessed by different parts of the enterprise., ETL mechanism: Extract, transform, and load mechanism used for data integration in a data warehouse., Feature engineering: Extension of the transformation process during data wrangling, performed after enriching and integrating data., Exploratory Data Analysis (EDA): In-depth data exploration techniques that provide insights to a project., Data understanding process: Starting with data wrangling, it is important for the data science lifecycle., Data preprocessing: Preparing data for use during the EDA process and beyond.,. Data quality: The extensiveness of the data understanding phase can impact the success of an analytic solution.,. Iterative process: The data understanding phase, including data wrangling, is repeated if new data is sourced."
/content/data/f73b5ad409304e1c8a4ec689bab7463d.xml, Why is Naive Bayes not considered the go-to algorithm for estimating the probability of an observation's class?," The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event., The Bayes theorem supports accurately assessing the risk of a person developing macular degeneration based on a certain age range instead of making assumptions., Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available., Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability., Naive Bayes (NB) Method is a simple classifier that can be applied to categorical predictors., Naive Bayes performs quite well for real-world applications, despite its naive assumption of independence between predictors., Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task., Naive Bayes is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results., Naive Bayes is useful for ranking and classification tasks.,. Other Bayesian Methods can be used in Data Science, which are explored in machine learning and applied to statistics courses."
/content/data/f6a6e752b7e24948a450dd7d36f338dd.xml, Discrimination: How can data science be discriminatory due to the nature of making predictions and classifications?," Data science: The primary goal of data science is to generate specific conclusions based on evidence or data., Data-driven: Data science relies on information about the past to make inferences and predictions about the future., Algorithmic Bias: Data scientists must consider the directionality of time and the implications of historical discrimination when analyzing data about the past., Discrimination: Data science can be discriminatory due to the nature of making predictions and classifications, which can be skewed towards certain groups., Data imbalance: Data scientists need to be aware of minority subgroups in datasets and take steps to account for data imbalance to ensure unbiased results., Case Study: The case study discusses the implementation of a school assignment algorithm in Boston and its impact on minority families due to limited access to high-quality schools in certain neighborhoods., Proper consideration and design: The negative impact on minority families could have been avoided with proper consideration and design of the algorithm."
/content/data/f2e81bd7cb7242d5b291b029288123c8.xml,. What is the role of high-level components in a system architecture?," Requirements document, Design for the project, Low-level design, Implementation details, Key design considerations, Assumptions, Constraints, System environment, Design methodology,. System architecture,. Low-level components,. High-level components,. Design document,. Data science project,. System architecture diagram,. Solution"
/content/data/f77eeffd875947d69e59a08b5d4076ec.xml,. Misclassification rate: How is the misclassification rate calculated?," Feature engineering: Creating features from raw data to facilitate model building., Principal Component Analysis (PCA): A technique used in the data science lifecycle for modeling., Model understanding phase: Exploring terminology and concepts related to model building., Analytic objectives: Goals set to meet the needs of the client and obtain actionable insights., Model training: Partitioning or splitting data into training and test sets to build and validate models., Supervised learning techniques: Predicting outcomes based on independent variables., Unsupervised learning techniques: Uncovering patterns in data without a known response variable., Goodness of fit measures: Assessing the fit of a regression model to the data., Overfitting: When a model learns all the details in the training set and fails to generalize to new data.,. Underfitting: When a model does not suit the dataset.,. Confusion matrix: Assessing the classification performance of models.,. Misclassification rate: Proportion of observations classified incorrectly.,. Accuracy rate: Proportion of observations classified correctly.,. Sensitivity or recall: Proportion of the target class classified correctly.,. Precision: Proportion of predicted target class observations that belong to the target"
/content/data/e148406a387545d7ac7e91f1ab4ae2e3.xml, How does inference help in making predictions based on data?," Statistics: The science of using data to learn about the world around us., Design: Planning how to gather data for research studies., Description: Summarizing the data., Inference: Making predictions based on the data., Descriptive statistics: Graphs, tables, and numerical summaries used to summarize data., Statistical inferences: Predictions made using data., Quantitative data: Data with numerical values representing different magnitudes of a variable., Categorical data: Data with a set of categories., Structured data: Organized facts presented in fixed formats.,. Unstructured data: Data that does not neatly fit in a row and column structure.,. Internal data: Data collected and/or controlled by an organization.,. External data: Data collected from sources outside of an organization.,. Primary data sources: Data collected and processed by an organization.,. Secondary data sources: Data gathered from sources external to an organization."
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. What is the role of frequentist statistics in model selection for inference?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. Bayesian Information Criterion (BIC),. Bayesian statistics,. BIC score,. Minimum Description Length (MDL),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. How is the AIC score calculated?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f471cfbbd151446d825497987a3eeeb1.xml, Why are Long Short-Term Memory (LSTM) Networks introduced in RNN architectures?," Artificial Neural Networks (ANN): Composed of node layers with input, hidden, and output layers. Nodes compute weighted sums and pass through activation functions., Multi-layer perceptron: General name for ANN architecture., Convolutional Neural Networks (CNN): Useful in image-processing applications. Recognize features and recombine them into higher-level attributes. LeNet CNN architecture implements feature extraction and classification., Receptive fields: Divisions of the input image in CNN that feed into a convolutional layer., Pooling: Reduces dimensionality of extracted features while retaining important information., Recurrent Neural Network (RNN): Maintains memory of past inputs and models relationships in time. Can be unfolded and trained with back-propagation or back-propagation in time (BPTT)., Long Short-Term Memory (LSTM) Networks: Introduces memory cell to retain values for short or long time. Contains input, forget, and output gates., Gated Recurrent Unit (GRU) Networks: Simplification of LSTM with update and reset gates., Vanishing and exploding gradient problems in RNN architectures.,. LSTM vs GRU: LSTM is more expressive but slower, while GRU is simpler and"
/content/data/f782d472cff14cc3a7ef287ecba87596.xml, What is the role of a labeled dataset in the fine-tuning process of BERT training?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/f782d472cff14cc3a7ef287ecba87596.xml,. What is the role of output embedding in the prediction of masked words in BERT's pre-training process?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/e148406a387545d7ac7e91f1ab4ae2e3.xml,. How does secondary data sources differ from primary data sources?," Statistics: The science of using data to learn about the world around us., Design: Planning how to gather data for research studies., Description: Summarizing the data., Inference: Making predictions based on the data., Descriptive statistics: Graphs, tables, and numerical summaries used to summarize data., Statistical inferences: Predictions made using data., Quantitative data: Data with numerical values representing different magnitudes of a variable., Categorical data: Data with a set of categories., Structured data: Organized facts presented in fixed formats.,. Unstructured data: Data that does not neatly fit in a row and column structure.,. Internal data: Data collected and/or controlled by an organization.,. External data: Data collected from sources outside of an organization.,. Primary data sources: Data collected and processed by an organization.,. Secondary data sources: Data gathered from sources external to an organization."
/content/data/ef9a898102e44a628fc073032c6a11ff.xml," Why is it important to document ""complete"" requirements that capture the needs of the stakeholders?"," Similar to traditional software development projects, data science projects are guided by requirements gathering principles., The steps followed during the requirements gathering process are listed in Figure 1., Requirements gathering for a data science project involves eliciting the needs of stakeholders and defining the requirements for the analytic solution(s)., The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project., The first step in gathering information is to identify the stakeholders within the business., The business analyst elicits information to determine what the solution should do to meet the defined business and analytic objectives., Stakeholders provide information according to their view of the business needs, and it is the job of the business analyst to define and prioritize requirements., It is important to document ""complete"" requirements that capture the needs of the stakeholders., The project team must verify and validate all documented requirements to ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.,. Sign-off from the client indicates that the requirements have been approved and agreed upon.,. A requirements management plan can be used to document the requirements-gathering process.,. The requirements management plan includes sections such as"
/content/data/f59f965701df42e796a110d414580861.xml, What does transparency involve in the context of data science?," Data science often includes descriptive analysis and prescriptive analytics., Large automated systems raise questions of responsibility and accountability., Responsibility can be personal or organizational., The concept of the moral crumple zone in automated systems., Accountability involves all the little decisions made by a group of people., Transparency and auditability are important in data governance., Transparency involves disclosing human involvement, data sources, algorithms, and the presence of AI solutions., Transparency can be disconnected from power, harmful, intentionally occlude, create false binaries, invoke neoliberal models of agency, and not necessarily build trust., Transparency has technical and temporal limitations.,. Auditing is an experimental test to ensure systems are doing what they were intended to do.,. Auditing can be used to investigate discrimination and security vulnerabilities in data science applications."
/content/data/fc03f67af2e341e3aaf29ee2ea60c8ca.xml, Why is the attention mechanism important in Sequence2Sequence models?," Transformer model, Sequence2Sequence models with attention, Hidden states, Input embeddings, RNNs (Recurrent Neural Networks), Attention mechanism, Encoder and decoder blocks, Self-attention, Intra-attention,. Machine reading,. Abstractive summarization,. Image description generation,. Encoder side,. Decoder side,. Context vector,. Parallel processing,. Time-step,. Encoder output"
/content/data/f782d472cff14cc3a7ef287ecba87596.xml,. How does BERT go about predicting masked words in its pre-training process?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/f73b5ad409304e1c8a4ec689bab7463d.xml, When is Bayesian Inference applied in the context of the Bayes theorem?," The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event., The Bayes theorem supports accurately assessing the risk of a person developing macular degeneration based on a certain age range instead of making assumptions., Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available., Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability., Naive Bayes (NB) Method is a simple classifier that can be applied to categorical predictors., Naive Bayes performs quite well for real-world applications, despite its naive assumption of independence between predictors., Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task., Naive Bayes is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results., Naive Bayes is useful for ranking and classification tasks.,. Other Bayesian Methods can be used in Data Science, which are explored in machine learning and applied to statistics courses."
/content/data/ff7cba66391a4fb392fae7dd1f8d98db.xml, - How do business requirements influence the proposed changes in a business?," Business Requirements: Describing the context, scope, and background of a business need, including reasons for proposed changes., System and User Requirements: Detailed description of the system and its operational and development constraints, as well as functions or tasks that a user must perform within the system., Use cases and user stories: Representing user requirements and providing a big picture of what the user will be able to do within a system., Solution Requirements: Tailoring solution requirements to the data science process, including functional and non-functional requirements, as well as requirements for data and models in an analytic solution and reports and dashboards in a business intelligence solution."
/content/data/ff16c81c152740aa9d2d3e312b8cc4f7.xml,9. What is the role of variable importance measures in explaining black box models?," Interpretability: The importance of interpretability in data science and machine learning, and its role in fixing issues with models and explaining their results., Explainability: The concept of explaining why a model produces certain results and the effects of changes within a model., Accuracy: The measurement used to determine the best model for a task and its role in producing better results and predictions., Trade-offs: The ability of a data scientist to measure the effects of any trade-offs within a model., Restrictions: Certain sectors, such as banking and education, are restricted by laws and standards in their use of certain techniques to protect consumer data and prevent bias in decision-making processes., Retaining interpretability: The challenge of retaining interpretability while improving accuracy in data science., Recommended steps: Hall (2016) recommends several steps to strike a balance between accuracy and interpretability, including training black box models, using different regression techniques, and creating small interpretable ensemble models., Variable importance measures: The use of variable importance measures to explain black box models better., Metrics: The discussion of different metrics to evaluate model accuracy in the next module."
/content/data/ec21750cc96c4744ac20259c45ac474b.xml,". Iterative process: When is the data understanding phase, including data wrangling, repeated in the data science lifecycle?"," Data integration: Involves ingesting, transforming, and integrating data for access., Analytic solution development: Modeling and analysis of integrated data., Data warehouse: A centralized repository for integrated data, accessible by OLAP servers, DSS systems, and other analytic tools., Data marts: Segments of the data architecture where integrated data can be accessed by different parts of the enterprise., ETL mechanism: Extract, transform, and load mechanism used for data integration in a data warehouse., Feature engineering: Extension of the transformation process during data wrangling, performed after enriching and integrating data., Exploratory Data Analysis (EDA): In-depth data exploration techniques that provide insights to a project., Data understanding process: Starting with data wrangling, it is important for the data science lifecycle., Data preprocessing: Preparing data for use during the EDA process and beyond.,. Data quality: The extensiveness of the data understanding phase can impact the success of an analytic solution.,. Iterative process: The data understanding phase, including data wrangling, is repeated if new data is sourced."
/content/data/f213668e0a794e90b60d02f61574face.xml,. Predicting masked words: How does BERT predict the original value of masked words?," BERT Training, NLP task, Semi-supervised training, Textual data, Language understanding, Resource-intensive training, Fine-tuning, Labeled dataset, Additional layers,. General language understanding,. Pre-training process,. Masked Language Modeling (MLM),. Word sequences,. [MASK] token,. Predicting masked words,. Output embedding,. Classification layer,. Probability vector,. Language vocabulary,. BERT's prediction"
/content/data/e843820862824c77903d30f41a295dac.xml, Average Linkage Method: Why might one choose to use the Average Linkage Method over the Single or Complete Linkage Methods in Hierarchical Clustering?," Hierarchical Clustering, Dendrogram, Agglomerative Clustering, Euclidean distance, Merging Clusters, Single Linkage Method, Complete Linkage Method, Average Linkage Method, Centroid Linkage Method,. Divisive Clustering"
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml,. How can hardware and software gaps affect the implementation of a data-driven solution?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/ff7cba66391a4fb392fae7dd1f8d98db.xml,. System and User Requirements: - Why is it necessary to identify stakeholders and systems that support the business requirement(s)?," Business Requirements: Describing the context, scope, and background of a business need, including reasons for proposed changes., System and User Requirements: Detailed description of the system and its operational and development constraints, as well as functions or tasks that a user must perform within the system., Use cases and user stories: Representing user requirements and providing a big picture of what the user will be able to do within a system., Solution Requirements: Tailoring solution requirements to the data science process, including functional and non-functional requirements, as well as requirements for data and models in an analytic solution and reports and dashboards in a business intelligence solution."
/content/data/f77eeffd875947d69e59a08b5d4076ec.xml, Supervised learning techniques: How do supervised learning techniques predict outcomes?," Feature engineering: Creating features from raw data to facilitate model building., Principal Component Analysis (PCA): A technique used in the data science lifecycle for modeling., Model understanding phase: Exploring terminology and concepts related to model building., Analytic objectives: Goals set to meet the needs of the client and obtain actionable insights., Model training: Partitioning or splitting data into training and test sets to build and validate models., Supervised learning techniques: Predicting outcomes based on independent variables., Unsupervised learning techniques: Uncovering patterns in data without a known response variable., Goodness of fit measures: Assessing the fit of a regression model to the data., Overfitting: When a model learns all the details in the training set and fails to generalize to new data.,. Underfitting: When a model does not suit the dataset.,. Confusion matrix: Assessing the classification performance of models.,. Misclassification rate: Proportion of observations classified incorrectly.,. Accuracy rate: Proportion of observations classified correctly.,. Sensitivity or recall: Proportion of the target class classified correctly.,. Precision: Proportion of predicted target class observations that belong to the target"
/content/data/e82200b224094720a962d7c4bba44d7b.xml, When should team members be assigned to specific tasks in a project?," Project design, Sub-tasks, Team members, Deliverables, Deliverable dates, Budget plans, Temporal resources, Human resources, Monetary resources,. Milestone plan,. Table of tasks,. Flow diagram,. Gantt chart,. Understanding process,. Future researchers,. Developers,. Domain,. Comprehensive collection of documentation,. Data science project,. Diagrams,. Requirements,. System design"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. What does K_h represent in the AIC formula?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. Bayesian Information Criterion (BIC),. Bayesian statistics,. BIC score,. Minimum Description Length (MDL),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/fa6250d2e56a418db1cf98a4827de1a4.xml, Compatibility: What might cause tools used in data science to not be compatible with accelerators like DSAs and GPUs?," Processors: CPUs, GPUs, and DSAs are the main categories of processors used in data science., CPU: The Central Processing Unit is the primary processor used for complex calculations in most systems., GPU: The Graphics Processing Unit is an additional processor used for graphics and calculation-intensive operations., DSA: Domain-Specific Architectures are purpose-built systems for computationally expensive modeling problems., Performance: DSAs and GPUs can provide significant performance gains in data science tasks., Cost and Time: Budgeting cost and time is important when deciding whether to use DSAs or GPUs., Compatibility: Tools used in data science might not be compatible with accelerators like DSAs and GPUs., Usage Patterns: Consider the usage patterns of the code to determine if using an accelerator chip is beneficial., Memory Usage: Check if the memory usage aligns with the memory limits of the GPU/DSA.,. Profiling Tools: Use profiling tools or timers to identify potential slowdowns and optimize code."
/content/data/f48ee0338e814a2c90e1294d14052375.xml, What are the phases included in the data science lifecycle?," Data Science projects can be complex and require the input of many stakeholders., A well-defined workflow is important in data science projects., The data science lifecycle is not linear and requires iteration., The data science lifecycle gives structure to the process and keeps the data scientist on task., The CRISP-DM lifecycle is similar to the data science lifecycle., The data science lifecycle includes phases such as business understanding, data acquisition, data preparation, data exploration and cleaning, modeling, feature engineering, model training, model evaluation, and deployment."
/content/data/fe97bc80ef0f46e79cf72378227d95a2.xml, What is the output of the model selection process?," Model Selection for Prediction: This section describes the process of model selection for prediction, including the input, procedure, and output., Input: The input for model selection includes candidate models M1, M2, ..., Ml., Procedure: The procedure involves splitting the dataset into a train set and a test set. For each candidate model Mi, the model is trained on the train set and evaluated on the test set., Output: The output is the model Mj with the best performance on the test set., Model Selection with Hyperparameter Tuning: This section extends the model selection process to include hyperparameter tuning., Input: The input includes candidate models M1, M2, ..., Ml and a hyperparameter space S., Procedure: The procedure involves splitting the dataset into a train subset, a validation subset, and a test set. For each candidate model Mi, the best hyperparameters are selected based on their performance on the validation subset. A new model Mi is then trained using the best hyperparameters on the combined data from the train subset and validation subset. The model's performance is evaluated on the test set., Output: The output is the model Mj and the associated best hyperparameters with the"
/content/data/f1fd5eae72a14a8485c16fb93f4d15dc.xml, Who are the stakeholders involved in the development of a data-driven solution?," AI consulting firm, EVP framework, Automotive services provider, Customer retention, Customer acquisition, Data-driven solution, Data science team, Business needs, Stakeholders,. Data management,. Analytical solution,. Service managers,. Hardware and software gaps,. Business objectives,. Customized service experience,. Mobile app,. Customer profiles,. VIP customers,. Loyalty program,. Measurable metrics,. App installation,. Store visits,. Completed transactions,. Revenue generation,. Repeat customer prediction,. AI-enabled application,. Customer engagement,. App-based transactions"
/content/data/ef9a898102e44a628fc073032c6a11ff.xml,. When does sign-off from the client indicate that the requirements have been approved and agreed upon?," Similar to traditional software development projects, data science projects are guided by requirements gathering principles., The steps followed during the requirements gathering process are listed in Figure 1., Requirements gathering for a data science project involves eliciting the needs of stakeholders and defining the requirements for the analytic solution(s)., The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project., The first step in gathering information is to identify the stakeholders within the business., The business analyst elicits information to determine what the solution should do to meet the defined business and analytic objectives., Stakeholders provide information according to their view of the business needs, and it is the job of the business analyst to define and prioritize requirements., It is important to document ""complete"" requirements that capture the needs of the stakeholders., The project team must verify and validate all documented requirements to ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.,. Sign-off from the client indicates that the requirements have been approved and agreed upon.,. A requirements management plan can be used to document the requirements-gathering process.,. The requirements management plan includes sections such as"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml, Why does model complexity need to be penalized in model selection for inference?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. What is the role of logistic loss in the calculation of the AIC score?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. BIC (Bayesian Information Criterion),. Bayesian statistics,. BIC score,. MDL (Minimum Description Length),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/fa6250d2e56a418db1cf98a4827de1a4.xml, DSA: How are Domain-Specific Architectures used for computationally expensive modeling problems?," Processors: CPUs, GPUs, and DSAs are the main categories of processors used in data science., CPU: The Central Processing Unit is the primary processor used for complex calculations in most systems., GPU: The Graphics Processing Unit is an additional processor used for graphics and calculation-intensive operations., DSA: Domain-Specific Architectures are purpose-built systems for computationally expensive modeling problems., Performance: DSAs and GPUs can provide significant performance gains in data science tasks., Cost and Time: Budgeting cost and time is important when deciding whether to use DSAs or GPUs., Compatibility: Tools used in data science might not be compatible with accelerators like DSAs and GPUs., Usage Patterns: Consider the usage patterns of the code to determine if using an accelerator chip is beneficial., Memory Usage: Check if the memory usage aligns with the memory limits of the GPU/DSA.,. Profiling Tools: Use profiling tools or timers to identify potential slowdowns and optimize code."
/content/data/f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,. Why is a simple/interpretable model preferred in inference?," Model Selection for Inference, Inference, Models, Dataset, Independent and dependent variables, Train-test split, Probabilistic metrics, Goodness of fit, Model complexity,. Reasonable model,. Simple/interpretable model,. Akaike Information Criterion (AIC),. Frequentist statistics,. AIC score,. Model M,. K_h,. LL(M),. N,. Mean squared error,. Logistic loss,. Bayesian Information Criterion (BIC),. Bayesian statistics,. BIC score,. Minimum Description Length (MDL),. Information theory,. MDL score,. L(M),. L(D|M),. Model predictions"
/content/data/f4e02624db114a8a9c8cd78840a1f707.xml, Reducible Error: What causes a reducible error in a model?," Errors At All Phases: Errors can occur during the business understanding phase and the data understanding phase, leading to bloated costs, scope creep, and inadequate solutions., Errors in Model Understanding: Errors in this phase are used to validate models and determine their expected performance for deployment., Training Error: Derived by calculating the classification error of a model on the exact data used for training., Test Error: Gives insight into the amount of errors to expect when making future predictions and is used for model selection., Irreducible Error: The noise term in the true relationship that cannot be reduced by any model., Reducible Error: The error resulting from a mismatch between the estimated relationship and the true relationship between variables., Calculating Error Rate: Exploring different methods and diving deeper into calculating the error rate for trained models."
/content/data/ec5450c0005241cdb5f7d2b80331aab2.xml, Ideal and balanced model: How can a model with low bias and low variance be achieved?," Bias error: An error from erroneous assumptions in the learning algorithm that can cause underfitting., Variance: An error from sensitivity to small fluctuations in the training set that can cause overfitting., Bias-Variance tradeoff: The tradeoff encountered while working with some supervised learning techniques, where high bias leads to underfitting and high variance leads to overfitting., Decomposition of bias-variance: The mathematical representation of bias-variance decomposition, which includes the variance of the noise in the data, bias, and the variance of the estimator., Complementary relationship between bias and variance: When bias is increased, variance is decreased and vice versa, indicating a tradeoff between them., Ideal and balanced model: A model with low bias and low variance, which can be achieved through dimensionality reduction techniques., Cross Validation: A technique discussed on the next page to optimize the bias-variance tradeoff., Visual representation of bias-variance with training dataset: A figure that shows the relationship between bias-variance and training-test data., Additional reading: Links to additional resources on the bias-variance tradeoff."
