,Unnamed: 0.1,Unnamed: 0,Unit,Module,Title,Subheaders,Paragraph,Question,predicted_label,human_eval,Information Score,Perplexity
0,168,168,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.","In addition to the p-value, what else is the p-value?",1,0,0.0,21.294580459594727
1,643,643,Collecting and Understanding Data,Data Collection,Study Design,,Tests an existing theory,"Tests an existing theory of the theory of relativity, which is based on what theory?",0,0,0.0,79.03993225097656
2,564,564,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.",A sampling distribution is a type of what?,1,1,1.7142857142857135,332.7987670898437
3,533,533,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.",cbirdsd is related to cbirdsd as a prepositional object which itself is related to what?,1,0,0.5454545454545455,213.0848693847656
4,113,113,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 1. Execution Graph of a Completed Run of the Continuous Training Pipeline (ACAI, MCDS Capstone Project, 2020)",The execution of a continuous training pipeline is called what?,1,1,0.0,470.7225036621094
5,105,105,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules And Their Relationships - How code is structured,Modules And Their Relationships - How code is structured in the code?,1,0,0.0,265.7673034667969
6,625,625,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Reduces the need to manually modify learning rate,Reduces the need to manually modify learning rate settings in a student's computer to make sure they're doing what?,0,0,0.1111111111111111,73.65406799316406
7,364,364,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.","When events occur, what must the system respond to?",1,0,0.0,58.5093002319336
8,468,468,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Reading: Cross-Validation: Python,Reading: Cross-Validation: Python is a cross-validation system that uses cross validation to determine what?,1,0,0.1428571428571428,36.35392761230469
9,309,309,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.",The tool is based on what?,0,0,0.0,123.62580871582033
10,619,619,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Converges in lesser time because of frequent updates,Converges in lesser time because of frequent updates and what else?,0,0,0.0,371.7347106933594
11,82,82,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.","In educational experiments, what is impossible to have?",1,1,0.0,242.6163330078125
12,310,310,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).","Instead, we model how the growth rate of two functions compares to large input?",0,0,0.0,224.706787109375
13,118,118,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,How does data progress through the system?,How does data progress through the system?,1,1,0.0,124.18952178955078
14,130,130,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Optical character recognition (OCR),Optical character recognition (OCR) is a term that refers to the recognition of a character that is recognized by a computer as a physical object that is recognized by a computer as what?,1,0,0.1052631578947368,13.958077430725098
15,500,500,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.",Kahneman's book is a great example of what type of bias?,0,1,1.4,62.52066040039063
16,198,198,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.",kNN is also known as what?,0,1,0.0,233.42044067382807
17,245,245,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.,Sequences2Sequence  models replace RNNs with what?,1,1,0.0,780.5552978515625
18,508,508,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.",The new variables are called what?,0,0,0.0,275.5058898925781
19,47,47,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.,The probability of missing data is the same for all observations except for what?,1,1,0.0,57.59121322631836
20,5,5,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Representatives of this family of models include CTRL, GPT, GPT-2.",GPT is a family of models that are modeled after what?,1,1,0.1818181818181818,138.19300842285156
21,613,613,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.",Backpropagation is used heavily in what?,1,1,0.0,215.9085693359375
22,691,691,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Noun (NN) cthey fill the comforter with downd,"Noun (NN) cthey fill the comforter with downds and downds, and what else?",0,0,0.0,129.04144287109375
23,27,27,Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,A matrix is a data structure that encodes the relationship between rows and columns. The disadvantage of this format is that matrices can be very sparse in certain domains. Sparsity refers to the fact that the majority of entries are unknown or missing. Sparsity leads to a waste of space and computational resources.,Sparsity leads to a waste of what?,1,1,2.1428571428571423,888.388916015625
24,528,528,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.","Semantically, what is semantically meaningless?",1,0,0.0,103.19766998291016
25,18,18,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.","In RTD, what is replaced by a model?",1,1,0.75,187.3277282714844
26,396,396,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?,1,1,0.0,49.51056671142578
27,175,175,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?",How do we test our hypothesis statistically?,1,1,0.0,99.3636474609375
28,149,149,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.",The interview protocol is designed to provide instructions for what?,0,0,0.0,207.021240234375
29,156,156,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.,A vector space is a collection of what?,1,1,0.4285714285714285,189.84323120117188
30,203,203,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.,"Instead, it adds information that is what?",0,0,0.0,192.94430541992188
31,437,437,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,None of the above are examples of what?,0,0,0.0,32.201454162597656
32,76,76,Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Consider this requirement for a report-generating solution:,Consider this requirement for a report-generating solution: What is the name of the solution that is required for a report-generating solution?,1,0,0.0625,18.0318546295166
33,546,546,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client is a logistics company that wants to speed up its automatic package sorting\r (Business objective)\r,The client is a logistics company that wants to speed up its automatic package sorting\r (Business objective)\r (business objective) and what else?,0,0,0.1904761904761904,169.30897521972656
34,509,509,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",A log level that is critical is called what?,1,0,1.0,266.7073669433594
35,311,311,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.",The first is called what?,0,0,0.0,228.37168884277344
36,491,491,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Managing the Data Quality,"Managing the Data Quality of the Data Science and Data Science departments at the University of Southampton, Southampton, and Southampton Universities is a part of what type of research?",0,0,0.0526315789473684,33.44900131225586
37,708,708,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.",The toolkit provides options for what?,0,0,0.0,360.9049682617188
38,228,228,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Using human language to give commands to the operating system,Using human language to give commands to the operating system?,1,0,0.0,74.63433074951172
39,423,423,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.","It is slow to train, but it tends to deal with what?",0,0,0.0,83.72993469238281
40,327,327,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,Procedure: What is the name of the procedure that involves the use of a metal detector?,1,0,0.0,18.815332412719727
41,414,414,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.",What is an example of external data?,1,1,0.0,67.49166870117188
42,436,436,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Post-processing,Post-processing is a term that refers to what?,1,1,0.0,55.706390380859375
43,4,4,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,These models are best suited for tasks involving text generation.,Text generation is a part of what type of work?,0,1,0.2222222222222222,155.06475830078125
44,305,305,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.",The sigmoid function is used in intermediate layers of what?,1,1,0.0,148.40969848632812
45,512,512,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.",The production environment is a phase where the models are scaled and served in real-time by what?,1,0,0.3125,97.7269287109375
46,704,704,Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",What is a good solution to an organization?,1,0,1.375,57.25117492675781
47,206,206,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","There are instances that require the use of scaled data, including algorithms that use gradient descent","There are instances that require the use of scaled data, including algorithms that use gradient descent, which involves using what kind of data?",1,0,0.0,62.22339630126953
48,346,346,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix,<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix?,1,0,0.0,25.87687110900879
49,164,164,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this test, our test metric is actually much simpler:","For this test, our test metric is actually much simpler: what is the test metric?",1,0,0.0,41.17227554321289
50,284,284,Collecting and Understanding Data,Ethics of Data Science,Governance,,Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.d,Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.dcts and dcts are examples of what?,1,0,0.0,111.91397857666016
51,660,660,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ3: How well can people detect when a model has made a mistake and correct it?,RQ3: How well can people detect when a model has made a mistake and correct it?,1,1,0.0666666666666666,49.21749114990234
52,583,583,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.,What is the purpose of Sapley Value?,1,1,0.0,182.0938415527344
53,131,131,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.",OCR is a technology to convert images of text into what?,1,1,1.3636363636363635,128.8497314453125
54,165,165,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.","In addition to Welchs t-test, what other test is used to test for more accuracy?",1,0,0.0,121.96944427490234
55,513,513,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.",The model health can be measured by how many different metrics?,1,0,0.0,170.1630096435547
56,140,140,Collecting and Understanding Data,Data Collection,Where do data come from?,,Data collected from a designed study,Data collected from a designed study to determine the relationship between the two groups of people in a study?,0,0,0.0588235294117647,49.955509185791016
57,569,569,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.",The act of random sampling means that what is the result?,1,0,0.0,83.64038848876953
58,682,682,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",Auditing has been used in social science research to discover if a system is doing what?,1,1,1.3125,61.95938873291016
59,268,268,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Elastic Net, a convex combination of Ridge and Lasso.",The convex combination is similar to what other type of combination?,0,0,0.0,198.12893676757807
60,439,439,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.,Model interpretability is important because it allows a politician to know which factors have the largest influence on what?,1,1,0.9473684210526312,65.2719955444336
61,554,554,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client \r(Methods and Data)\r,The client's data is used to conduct a qualitative survey on what?,0,0,0.6666666666666666,89.60399627685547
62,574,574,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.,A 95% CI means that what percentage of the 40 confidence intervals contain the true mean value?,1,1,0.25,122.04146575927734
63,81,81,Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.","In addition to the covariate, what else is a potential problem with using data to determine the relationship between the covariate and outcome?",1,0,1.1000000000000003,33.62228012084961
64,538,538,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.", The model is known as what?,0,0,0.0,131.4796905517578
65,457,457,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y3,"Y3, and what else?",0,0,0.0,111.9992904663086
66,604,604,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to change the destination so that I can reach a new destination.",I want to be able to do what?,0,0,0.1428571428571428,19.247785568237305
67,358,358,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.","NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, and what else?",1,1,0.5652173913043477,42.98421859741211
68,137,137,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Vision-based Biometrics,Vision-based Biometrics is a type of biometrics that involves the use of what kind of biometrics?,1,0,0.0769230769230769,23.612625122070312
69,29,29,Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,"The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","In addition to model parameters, what else is used to store model parameters?",1,0,0.0,59.35566711425781
70,334,334,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.",The focus of a model is on what?,0,1,1.125,161.10304260253906
71,431,431,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Model 1,Model 1 of the Model 1 was built on what type of building material?,1,0,0.0,79.90019989013672
72,435,435,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,None of the above are examples of what?,0,0,0.0,32.201454162597656
73,200,200,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",The RMS is used to determine the best k for a classification task in which the classification task is performed by the person who performed the classification task?,1,0,1.1052631578947365,40.112091064453125
74,562,562,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.,The model would use what type of model change?,0,0,0.0,489.65765380859375
75,549,549,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",To improve performance beyond the current model based on standard convolutional neural networks without language information\r (Valuable functionality),"To improve performance beyond the current model based on standard convolutional neural networks without language information\r (Valuable functionality) and without the need for language information, what is the goal of the current model?",1,0,0.0,66.1720962524414
76,51,51,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",An exampleof a popular quantitative transformation is converting the date of birth to age.,"In addition to age, what else is a popular transformation?",1,0,0.6,208.15431213378903
77,686,686,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.",What would have to be taken into account in order to analyze words?,1,1,0.0,36.09900665283203
78,487,487,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",public (from government websites and open data),public (from government websites and open data) to open source software and open source software?,0,0,0.0,72.6928482055664
79,479,479,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the types of data items that are input to each operation?,What are the types of data items that are input to each operation?,1,1,0.0,37.25467300415039
80,611,611,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.",The model is modeled after what?,0,0,0.0,203.07981872558597
81,189,189,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Jack,"Jack, the leader of the Free World, was a leader of what?",1,0,0.0,24.36885070800781
82,270,270,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.,LambdaRank is an implementation of what?,0,1,0.0,196.81002807617188
83,526,526,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found to fly to Tokyo,"I found to fly to Tokyo in the summer of 2008, where I flew to see what was happening?",0,0,0.0625,43.9224853515625
84,417,417,Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are different types of requirements including business and solution requirements. Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.","Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe what?",1,0,0.3846153846153846,313.32574462890625
85,88,88,Collecting and Understanding Data,Data Collection,Validity and Bias,,Diffusion of treatments (also called cross-contamination of groups),Diffusion of treatments (also called cross-contamination of groups) is a term used to describe what happens when a treatment is combined with a treatment?,1,0,0.15,21.78841590881348
86,62,62,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.",The model is said to be linear because the output is expected to be what?,0,0,0.0,76.87177276611328
87,388,388,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.",The goal of a well-studied dataset is to be able to be used for what?,1,0,1.538461538461538,31.173511505126957
88,49,49,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.","For example, if the missing values are small, what can you do to mitigate the loss of data?",1,1,0.0,22.64656639099121
89,345,345,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:",The columns are compressed in a similar manner to what?,0,0,0.4,111.64278411865234
90,631,631,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Rapid convergence,"Rapid convergence is a term that refers to the convergence of two or more things at once, in which case the convergence is called what?",1,0,0.0476190476190476,16.07095718383789
91,565,565,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Suppose a student decides to record her commuting times on various days. She selects these days at random from the school year, and her daily commuting time has the cumulative distribution function in Figure 1.",The curve is in the form of a curve with a line that points to where the student is going to go to where?,0,0,0.2941176470588235,30.4626407623291
92,96,96,Collecting and Understanding Data,Data Collection,Validity and Bias,,"The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.",The pre-test is a test of what?,0,0,0.2857142857142857,55.76642990112305
93,510,510,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.","DVC, Delta Lake, and LakeFS are examples of what kind of data versioning?",1,1,0.0,287.3000793457031
94,563,563,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).,The model is called what?,0,0,0.0,206.36846923828125
95,669,669,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.",What is the purpose of PCA?,1,1,0.0,38.73583984375
96,674,674,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",How can data be used to meet the business and analytic objectives?,How can data be used to meet the business and analytic objectives?,1,1,0.0,115.69606018066406
97,395,395,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?,1,1,0.0,87.75947570800781
98,147,147,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.",Organic data is collected from what type of study?,1,1,0.0,166.84510803222656
99,24,24,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 14. ResNet-18 architecture.,ResNet-18 architecture is a type of architecture that is based on what type of architecture?,1,0,0.0909090909090909,116.07894897460938
