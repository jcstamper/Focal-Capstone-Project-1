,Unnamed: 0.1,Unnamed: 0,index,Unit,Module,Title,Text,Subheaders,answer,questions,predicted_label,human_eval,Information Score,Perplexity
0,3634,3657,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",finnish,what language is the language of the language that uses a closed class of language?,1,0,16.272727272727334,54.44371032714844
1,699,705,51,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Reminder: The data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client. The requirements gathering process will involve identifying the stakeholders, eliciting needs, and defining requirements. The difference between the requirements of a traditional IT project and those of a data science project is the focus on the requirements for the analytic solution.
Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.
Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.
Traditional functional requirements considered in the software and application development process include Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Requirements, Administration Functions, Authorization Requirements, and Data Management.
Reading: IEEE ANSI 830 Documentation on Functional Requirements.
Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.
Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.
The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:
Figure 1. Use Case Specification
A business analyst (BA) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers. The BA analyzed the information from the questionnaire and defined some user requirements. One of the user requirements is customers ability to update their billing address in the new system. This requirement describes what customers can do with the solution, but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement. We will illustrate how we can simply decompose that user requirement into functional requirements.
Use Case/User Requirement: Update billing address.
(1) The user shall be able to view the billing addresses in the system.
(2) The user shall be able to update a billing address in the system.
(2) The system shall display updated customer service and billing addresses.
Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.
Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.
Availability
Refers to a property of software that is there and ready to carry out its task when you need it to.
Interoperability
Interoperability refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context.
Modifiability
Modifiability refers to the ease of modifying the system with minimal changes to the architecture.
Performance
Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.
Security
Security refers to the systems ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized.
Testability
Software testability refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing. Specifically, testability refers to the probability that the software will fail on its next test execution, assuming that it has at least one fault.
Usability
Usability is concerned with how easy it is for the user to accomplish the desired task and the kind of user support the system provides. Over the years, a focus on usability has shown itself to be one of the cheapest and easiest ways to improve a systems quality (or, more precisely, the users perception of quality).
Scalability
Scalability refers to the ease of adding new resources to a system to cope with increasing demands on its use.
Observability / Monitorability
These refer to the ability of the operations staff to monitor the system while it is in operation.
Portability and Compatibility
These refer to the compatibility of the hardware, systems, application software, and solution with other applications and processes within the existing environment.
Modules and Architecture
These refer to the technical considerations for the system, including operating system compatibility and the programming development environment employed by the developers.
Non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules. Framing a non-functional requirement for a data science project will include the above-mentioned attributes and additional requirements that are related to machine learning models and AI analytic solution(s).","Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",include,what is one of the requirements that is used in the software process?,1,0,0.0,44.74017333984375
2,1935,1949,125,Problem Identification and Solution Vision,Problem Identification,Problem Identification,"Just like other business processes, a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influences the stakeholders, existing system environment, and support structure. The data whose analysis supports the proposed solution may have to be gathered both from inside and outside the organization. Overall, this process of initiating and executing a data science project can be challenging, and it requires technical expertise as well as domain guidance. A Gartner study showed that 85% of data science projects fall short of expectations. Project expectations are defined by your client, and the data science team creates a solution vision that will meet their expectations. The path to defining those project expectations must be tread carefully to avoid project failure.
According to the Chaos Report, a group that tracks IT project failure, reports that data science projects might fail due to multiple reasons, including those that occur in general IT projects. However, there are some unique issues that data science teams must consider to ensure the success of their projects. Some of the reasons for data science project failure include:
Insufficient or inappropriate data,
Lack of technical data science skills,
Issues with project management,
Inaccurate interpretation of results, and
Mismanaged client expectations.
The above-mentioned issues are related to a misunderstanding regarding the clients business needs and to inadequate communication between the data science project team and the business stakeholders. It is important to identify and understand your clients business needs, environment, and current solution. This will increase the chances of meeting the business objectives and providing the right analytical solution.
In this unit, we will discuss how you derive analytic objectives from a clients or organizations business needs. In doing so, you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset. While these aspects are certainly important parts of many data science projects, being a successful data scientist involves being mindful of the big picture to envision, design, implement and ultimately deploy creative solutions for real-world problems. You will find that this course introduces a data science approach that is grounded in scientific research, software engineering principles, and experimentation.
The remainder of this unit focuses on the ability to identify problems and envision a solution, which is among the most important skills a data scientist must possess.",,data science project,what is the name of the project that is trying to track and execute the business?,1,0,0.0,36.086761474609375
3,3959,3982,233,Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?
For example, lets say you have a longitudinal dataset about income and education and you want to make some inferences from it. If the data go back far enough, you will get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didnt take that into account or consider it when creating your framing questions, you could end up with conclusions like cgirls arent smart enough to go to colleged or cwomen dont like high-paying jobs.d
The importance of recognizing that these conclusions, while technically possible but maybe problematic to assume, is that in history, there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.
One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.
As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most cnormal,d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.
Reading: Introduction section of ""Raw Data"" Is An Oxymoron by Lisa Gitleman and Virginia Jackson.
The location, tier level, and capacity of all 6th grades in Boston for the 2016-17 academic year. Illustration by the Boston Area Research Initiative.
The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.",,impact,what must data do when analyzing data about the past?,1,0,0.0,119.72154998779295
4,1264,1274,95,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.
We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.
A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.
Additional Reading: Decision Trees for Decision Making
Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.
Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.
Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.
Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.
Reading: Gini Index and Impurity Measures
Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.
Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.
Reduced error pruning is done by replacing each node with the node's most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.
Application: Decision Trees and NLP: A Case Study in POS Tagging.
When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.
The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.
We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).
Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.
Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.
Additional Reading: History of Random Forest Algorithm
Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).
Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.
Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.
Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.
Reading: Ensemble Methods-General Use","Building Classification Trees,Building Regression Trees",feature,what can be used to map a tree's tree?,1,0,13.555555555555523,89.05690002441406
5,1894,1908,121,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.
Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:
Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.
Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.
Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.
Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:
RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.
LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.
LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.
Learning to Rank Algorithms (Source: Lucidworks)
Additional Reading: Application of LTR - Bayesian Product Ranking at Wayfair
Additional Reading: From RankNet to LambdaRank to LambdaMART",,results,what does the learning - to - rank algorithm apply to?,1,1,0.0,379.1914367675781
6,113,114,15,Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,"A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a users movie rating history, which movie would they want to watch next?)
However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).
Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources:
The placeholder values still consume actual memory. For example, storing a 10000 x 10000 sparse matrix of integers takes about 381MB, even if most entries are 0 and do not carry actual meaning.  There is no point in representing data that does not exist!
Many computations on sparse matrices yield trivial results due to addition/subtraction or multiplication with 0s; however, they still need to be carried out by the computer.
Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.",,computational,what resources can be used to store information in a movie?,1,1,5.454545454545452,60.10105514526367
7,132,134,19,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"[Required Reading] Paper: Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-52). (Requires CMU credentials to access)
The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.
The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.
Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence peoples everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.
The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on peoples behavior.
This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.
The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects:
RQ1: How well can people simulate a models prediction?
RQ2: To what extent do people follow a models prediction when its beneficial for them to do so?
RQ3: How well can people detect when a model has made a mistake and correct it?
The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).
For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartments selling price. They were then shown the models prediction and asked for their own prediction of the apartments selling price. For RQ1, the authors measured the difference between peoples guesses of the models prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the models prediction in their guess for the apartments ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.
Based on the papers findings:
A clear model with a smaller number of features was easiest for participants to simulate.
There were no significant differences in the participants trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.
When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.
There are two important takeaways from the paper:
Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.
Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.","Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",prediction,what do people follow when looking at a better model for a model?,0,0,6.749999999999993,104.79925537109376
8,2475,2490,151,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.
Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.
One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.
Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.
The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.
Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.
Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.
The are several measures to describe the spread, variability, or dispersion of a dataset
Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.
Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is
\\[\\sum_i\\frac{\\mid x_i - \\mu\\mid}{N}\\]
where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.
This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is
\\[\\sigma^2 = \\sum_i\\frac{(x_i - \\mu)^2}{N}\\]
Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.
A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.
Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.
Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:
Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.
Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC)
Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.
Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.
Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \ight |\\leq 1\\)
The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).
More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.
A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.
Figure 2. Scatterplots for Four Hypothetical Datasets.
Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.
One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.","Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",sum,what is the purpose of descriptive statistics?,1,1,0.0,47.56364440917969
9,1684,1696,112,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Let us assume that you have a medical dataset that contains observations with features including patient age, weight, height, sex, and race, and you have been tasked with identifying whether a patient is diabetic or not). It would take a long time for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes. Using a data science approach, you can assign a diagnosis to each observation based on the historical data for that diagnosis. You would be a classification problem. Classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new, previously unseen dataset. Below, you will find the different types of classification problems. Later in the course, we will explore methods that can be used to solve classification problems.
Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).
Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.
Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.
A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?
Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document  though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.
You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.
Multi-label classification does not have constraints on the labels that observation can have, and this makes it difficult to learn. Using the OneVsRest Technique, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes.
Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.
The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.
Example: scikit-multilearn for MLkNN.
You can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier.
X
Y1
Y2
Y3
X1
0
1
1
X2
1
1
0
X3
1
1
0
X4
1
0
1
Here we see that observations X2 and X3 belong to the same classes. This technique will transform our task into a single multi-class task and give a unique class to all possible combinations in your training data set.
X
Y1
X1
1
X2
2
X3
2
X4
3
So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.
We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.","Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",pineapple,"along with peach and mango, what is another example of a tree that can be categorized as a tree?",1,0,5.111111111111101,43.45343399047852
10,468,473,39,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Throughout this section, we will make use of the following matrix as an example:
We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.
COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):
Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.
Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.
DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,
its DOK representation is
This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.
The compressed sparse row format stores a matrix as three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of the column indices of the non-zero values
<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.
<![CDATA[row[i]]]> denotes the number of non-zero entries that appear in the rows above the i-th row in the original matrix, where row indexes start from 0, and <![CDATA[row[m]]]> denotes the number of non-zero entries in the entire matrix.
With the example matrix A,
its CSR representation is
While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:
0 at index 0, as there are no non-zero entries above the first row.
2 at index 1, as there are 2 non-zero entries above the second row ( 7 and 5 ).
2 at index 2, as there are still only 2 non-zero entries above the third row.
4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).
6 at index 4, as there are 6 non-zero entries in the matrix.
With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.
This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).
This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:
\\[M x=\\left(M_{1} \\cdot x M_{2} \\cdot x \\quad \\ldots M_{m} \\cdot x\ight)^{\op}\\]
We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.
At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?
The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.
<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix
<![CDATA[row]]>: a list of the row indices of the non-zero values.
With the example matrix A,
its CSC representation is
Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.
This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:
\\[ M x=x_{1} M_{(1)}+x_{2} M_{(2)}+\\ldots+x_{n} M_{(n)} \\]
We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.
At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?
Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.
A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.
At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \imes n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \imes k}, V \\in R^{k \imes n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.
The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","Natural Language Processing,Recommender Systems,Sparse Modeling",compresses,what can be used to store a matrix and column indices?,1,0,7.5454545454545405,160.61328125
11,3934,3957,232,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?
There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.
Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.
k-Fold Cross Validation
You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.
Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of k=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.
This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.
LOOCV can be used with any kind of predictive modeling.
Leave One Out Cross Validation (LOOCV)
LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.
Classification Problems: When Y is qualitative, we use the number of misclassified observations as a measure of the model's test error.
Regression Problems: When Y is quantitative, the MSE is used to measure the test error.
Reading: Cross-Validation: Python",,value,what should the dataset be based on?,1,0,0.0,59.91057586669922
12,1229,1239,95,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.
We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.
A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.
Additional Reading: Decision Trees for Decision Making
Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.
Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.
Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.
Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.
Reading: Gini Index and Impurity Measures
Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.
Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.
Reduced error pruning is done by replacing each node with the node's most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.
Application: Decision Trees and NLP: A Case Study in POS Tagging.
When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.
The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.
We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).
Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.
Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.
Additional Reading: History of Random Forest Algorithm
Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).
Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.
Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.
Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.
Reading: Ensemble Methods-General Use","Building Classification Trees,Building Regression Trees",measured,what can be used to map a tree's tree?,1,0,13.555555555555523,89.05690002441406
13,4018,4042,240,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.
Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.
Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).
Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.
Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1
How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.
Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.
Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.
Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.
Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).
Additional Reading: Survey of Active Learning. This report gives an in depth review of active learning in machine learning and artificial intelligence.","Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",measures,what must the selected observations be selected for?,1,0,0.0,325.14794921875
14,3378,3397,201,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.
This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.
The technique can take advantage of any distance measure, but you will find that most studies will use the Euclidean distance as a distance metric.
Raw Data-Source1
The first round of merges finds clusters with observations/clusters b and c merged to form one cluster, and d and e are also merged into one. Now we have clusters a, bc, de and f.
Next, de and f are combined to form cluster a, bc, and def.
Clusters are further combined to form a and bcdef.
Finally, abcdef is formed.
Hierarchical Clustering Technique-Source1
Single Linkage Method is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.
Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.
Complete Linkage Method uses the maximum distance between data points within each cluster, also known as the farthest neighbor method. Clusters are combined into larger clusters until all data points are in the same cluster. The distance between clusters is the distance between two data points (i.e., one per cluster) that are farthest from each other. Complete linkage avoids chaining but suffers from crowding; because its score is based on the worst-case dissimilarity between pairs, a point can be closer to points in other clusters than to points in its own cluster.
Average Linkage Method uses the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity overall points in opposite groups.
Centroid Linkage Method is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.
Divisive Clustering. This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.
Reading: Hierarchical Clustering of Words and Application to NLP Tasks","Agglomerative Clustering,Merging Clusters",distances,what does the clustering method produce clusters that have small clusters?,1,0,0.0,111.32723236083984
15,3878,3901,229,Collecting and Understanding Data,Ethics of Data Science,Accountability,"While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?
When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.
Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.
The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesnt happen again. We care about both.
In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.
When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place.
In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as cWhy am I seeing this ad?d (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.
Figure 1. Why am I seeing this ad? (Source: LinkedIn)
Figure 2. High-level Design of cWhy am I seeing this ad?d Initiative (Source: LinkedIn)
Figure 2 shows the flow of control of cWhy am I seeing this ad?d on LinkedIn, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIns effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.
The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:
Transparency can privilege seeing over understanding
Besides these limitations, other pitfalls of creverse-engineeredd as a strategy for transparency:
Set reasonable expectations to disclose what is known. While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really dont know exactly how it works. We dont know what we dont know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.
""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.
Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.
Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.
Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.
In data science applications, when transparency might not be feasible due to the protection of trade secrets or prevention of the system being gamed by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.","Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",crumple,what section of the moral crumpling zone is designed to protect and protect the passengers?,1,0,0.0,115.51248931884766
16,2783,2798,163,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,"Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.
Figure 6: Transformer architecture.
Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.
Figure 7: Transformer Encoder Inputs.
Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.
The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.
The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.
The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.
Figure 8: Transformer Encoder Architecture.
Figure 9: Transformer Encoder-Decoder.
Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.
In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.
After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.
At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.
Here are some additional sources for more details on Transformers.
The Annotated Transformer
The Illustrated Transformer",,word,what does the input code encode in input sentence?,1,0,0.0,462.6108093261719
17,508,513,40,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.
So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.
Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.
You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.
PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a csmalld number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.
The following steps are used when performing PCA:
Standardize the data.
Compute the Covariance matrix of dimensions in the data.
Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.
Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.
Construct the projection matrix W from the selected k Eigenvectors.
Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.
PCA in Python Example: Principal Component Analysis in three (3) steps.
Reading: A Brief Article-Principal Component Analysis (Lever, Krzywinski, and Altman, 2017)","Dimensionality,Feature Extraction",perform,what is one possible scenario that can be used to study the effects of variables?,1,0,0.0,38.954925537109375
18,404,409,38,Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,"Throughout the course of this module, you have learned a lot about the underlying hardware that makes or breaks data science operations. While choosing underlying hardware goes a long way to ensuring that your system works within the budgets you allocated for yourself, you should also be prepared also to optimize your code accordingly. Given that times for jobs can range from overnight to multiple days, even a meager 10% speedup can provide more time to tune your model and extract bigger insights into your problem.
Even if you have experience optimizing C or C++ code, data science code can be much trickier to optimize. To better understand why, lets consider a couple of notable factors at play.
Most languages in which data science is done are interpreted rather than compiled. When you interpret code, you do not have the same ability to statically optimize the code as a compiled language. As a result, some of the automatic optimizations you might expect to happen will not, and this will result in slowdowns compared to C++.
For most data science stacks, you will need to use matrix-manipulation libraries instead of implementing every bit of an algorithm from scratch. These libraries provide fast implementations of specific operations in a pre-compiled binary, ensuring that the actual code is much faster than what is possible in the interpreted language as is.
Sometimes, the code you are looking at will have layers upon layers of code underneath it that could be the source of your performance issues. Even simple data science projects will have libraries for linear algebra operations linked in, along with the tree of libraries your project requires. This added complexity can make it difficult to understand where the potential problems are and make the relationship between code and performance harder to reason with.
Despite these factors, however, there are principles that can be followed to improve performance actively. These are not going to be surefire ways to optimize code, but tend to lead to better performance more often than not.
Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.
In particular, your goal should be to do at least the following:
Remove for-loops and replace them with maps: For-loops in interpreted code are much slower than for-loops in pre-compiled code. If you can replace a for-loop with a map function or a function without any side-effects that take in each element of the matrix as input, applies some operation without any side-effect, and returns that element, then your code will speed up accordingly.
Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to make some function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.
You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.
If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments which share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.
Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the cMap-Reduced framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:
Mapping: Here, you separately process each line of a data file, and apply some operation to turn it into a key, value pair.
Reducing: Here, you take all of the data for a given key, and process the values together, producing some output to then map again.
While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets, later on, youll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.
These frameworks can automatically parallelize the job with those functions given, allowing you to process more data faster.
If the above steps are not enough, you could write sections of your code in a compiled language and then create functions that use that code in your interpreted language. While this is generally not advised unless you know the code is the main bottleneck, it can provide large speedups at the cost of technical complexity.",,like,is it possible that a computer can optimize the code?,1,0,9.199999999999983,59.23939895629883
19,2819,2834,163,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,"Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.
Figure 6: Transformer architecture.
Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.
Figure 7: Transformer Encoder Inputs.
Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.
The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.
The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.
The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.
Figure 8: Transformer Encoder Architecture.
Figure 9: Transformer Encoder-Decoder.
Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.
In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.
After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.
At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.
Here are some additional sources for more details on Transformers.
The Annotated Transformer
The Illustrated Transformer",,weighted,how are the products added to input input values?,1,0,0.0,295.6541442871094
20,1171,1181,94,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:
A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search
The model parameters are the numeric values or structures that are derived from the learning process.
The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.
The learning algorithm specifies the way in which model parameters are updated or derived from the input data.
With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.
Component
Model 1
Model 2
Hyperparameters
Learning rate \\(\\alpha =0.1\\), regularizer \\(\\lambda =1\\), number of iterations \\(N = 1000\\)
Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)
Learning algorithm
Gradient descent over the logistic loss function with L2 regularization
Gradient descent over the logistic loss function with L2 regularization
Pre-processing
None
Normalize the data to have zero mean and unit variance
Post-processing
None
None
Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.
Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.
Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.
Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).
Optional Reading: Integrating explanation and prediction in computational social science",,information,what does the learning algorithm specify?,1,0,0.0,389.1381530761719
21,2897,2913,173,Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,"For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there werent many rules to follow, so people did and took what they could and werent exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome  that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.
We have virtually unlimited access to data as data scientists today, and we have unprecedented analytical techniques with which to analyze that data. So the question we should be thinking about is whether we should do something just because it is technically possible. Are there things that are possible to do but which we can agree would not be right to do? This may sound strange, but it is a question that modern science has already begun considering and continues to do so.
Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences peoples lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.
The difficult thing about being an ethical data scientist is not about understanding ethics. It is the connection to how one applies ethical principles to data science practice; it is about doing cgoodd or ethical data science.
By Rijksdienst voor het Cultureel Erfgoed, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=23391637
This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.
The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.
By Design: Willem Sandberg. Photographer: Frans Willemsen - Own work, CC BY-SA 3.0 nl, https://commons.wikimedia.org/w/index.php?curid=32468279
We hope this is a good place to start the conversation about doing good data science and the motivation to do so. In the rest of the module, we hope to convince you, if youre not already convinced, that the story of the 1943 bombing of the Amsterdam civil registry office is highly relevant to the work you do as a data scientist. While the data science work you do may be mundane by comparison and not involve a life-or-death decision, ethical data science practices matter to everyday things that affect all of us. The goal of this module is to help you participate in the ethical debates that you will face as a data scientist, infuse your data science work with ethical principles, and inform you to be thoughtful, deliberate, and ethical  the kind of data scientists that we all hope that youre going to be.
[Required Reading]
Please read chapter 3 from Loukides, Mason, H., & Patil, D. (2018). Ethics and Data Science (1st edition). OReilly Media, Inc.
Note: When prompted to select institution, select ""Not listed? Click here"" and enter your CMU email address to access content.",,data,what do we have unlimited access to in the past?,1,0,0.0,59.14800262451172
22,4067,4091,249,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,"The Transformer model was introduced in the famous paper Attention is All You Need in 2017.
A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?
Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.
Figure 5: Interactions within components in different architectures.
Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.
Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.
On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.
On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.",,processing,what does the transformer model replace?,1,1,0.0,758.3145751953125
23,1409,1419,99,Data Science Project Planning,Design and Plan Overview,Overview,"Following the requirements document, it is vital to develop a design for the project. This is the most important documentation of the project as it provides not only a low-level design of the system but also dives deep into the implementation details of the system. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, the system environment, and the design methodology.
The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.
Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)
While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.
Figure 2. Use Case Diagram (Source: https://venngage.com/blog/use-case-diagram/)
Apart from employing diagrams, it is also a good practice to make a list of tools and dependencies along with the suitable versions that the project may require.
This documentation also helps the developers think about various risks and challenges involved in the data science project. These risks could be domain, technical or business-related risks that are a part of the solution proposed for this project.",,documentation,what is the most important about the design of the project?,1,1,0.0,26.05032539367676
24,2627,2642,158,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.
A typical classical NLP pipeline uses at least the representation levels, as shown in the following figure.
Phonetics is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while phonology is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc.
For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.
Morphology is the study of word structures, especially how morphemes, which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morphemes).
In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).
A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.
As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:
I want a flight to Tokyo
I want to fly to Tokyo
I found a flight to Tokyo
I found to fly to Tokyo
The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.
Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.
The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.
For example, the following tree representation captures the structure of the sentence, cA boy with a flower sees a girl with a telecope.d The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.
Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.
The sentence can also have the following tree representation:
This is for the interpretation where the girl is carrying a telescope!
This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of cbooksd above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.
A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.
Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.
Loosely speaking, this level represents the cmeaningd of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a cworld modeld and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as cEverybody has something they  like.d would be represented by a logical form such as \\(\\forall x \\exists y\\  likes(x, y)\\).  The true value of such a sentence can then be computed based on the description of the world model.
A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event.  For example, all these sentences:
Warren bought the stock.
Someone sold the stock to Warren.
The stock was bought by Warren.
are describing the same csellingd event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent.  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.
Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.
Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual information also includes intonation, physical gestures, and social identity.  For example, an utterance such as cCan you pass the salt? c in a dinner set is really not a question of someones ability to pass the salt but is rather interpreted as a gentle request.
Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.
A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.
Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:
Eric is a pathetic programmer. He only knows Java. Worse still, he always optimizes the outermost loop first. However, the incompetence of his managers ensures him a steady, six-figure income.
Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income.  He only knows Java.
Clearly, only the first of these cmakes sensed; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not cmake sense.d
A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have cmeaningful relationships.d
Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics.  The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s.  This hypothesis basically states that cWords that occur in similar contexts tend to have similar meanings.d
Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as cbook,d cbank,d or cdown,d get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input.  These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.","Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",compute,how can a morphological representation of a language be described?,1,1,17.888888888888843,60.631961822509766
25,1407,1417,99,Data Science Project Planning,Design and Plan Overview,Overview,"Following the requirements document, it is vital to develop a design for the project. This is the most important documentation of the project as it provides not only a low-level design of the system but also dives deep into the implementation details of the system. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, the system environment, and the design methodology.
The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.
Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)
While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.
Figure 2. Use Case Diagram (Source: https://venngage.com/blog/use-case-diagram/)
Apart from employing diagrams, it is also a good practice to make a list of tools and dependencies along with the suitable versions that the project may require.
This documentation also helps the developers think about various risks and challenges involved in the data science project. These risks could be domain, technical or business-related risks that are a part of the solution proposed for this project.",,design,what is the main consideration for the user interactions of the system?,1,1,0.0,56.48831558227539
26,3511,3533,217,Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,"In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.
Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as
\\[ AIC(M)=(2K_{h}-2LL(M))/N \\]
where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.
Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as
\\[ BIC(M)=K_{M}\imes logN-2LL(M) \\]
where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.
Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as
\\[ MDL =L(M)+L(D|H) \\]
Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",,information,"what does the acronym "" akaiker "" derive from?",1,0,0.0,339.7032775878906
27,1619,1631,110,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,"While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they dont take the context of relative positivity between words into consideration. For example, cgood food and terrible serviced and cterrible food and good serviced mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.
Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).
\\[ P(W_{n}|W_{n-1})=\\frac{P(W_{n-1},W_{n})}{P(W_{n-1})} \\]
Where the probability \\(P()\\) of a token \\(W_{n}\\) given the preceding token \\(W_{n-1}\\) is equal to the probability of their bigram \\(P(W_{n-1},W_{n})\\), divided by the probability of the preceding token.
Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences cBig blue skies look appealing.d and cColorless green ideas sleep furiously.d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one  a sentence she can say someone could use.
Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.
These days people build (classical) language models using well-established toolkits:
SRILM Toolkit: https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit
CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html
KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/
Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.",,build,what does a language model say would be used to measure the second word?,1,0,3.500000000000003,209.58981323242188
28,1496,1506,100,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.
At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.
Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.
Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.
Despite its importance, data wrangling presents some challenges that are common in data science projects.
So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.
Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.
A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.
The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.
Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.
Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:
Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.
The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.
Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.
In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.
Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.
In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.
Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.
\\(z = (x-\\mu )/\\sigma\\)
The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!
The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task
One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.
In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.
Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.
Missing at random (MAR) is the probability that the missing data is the same within certain groups.
Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.
The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.
Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.
Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.
Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.
You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.
Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.
When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.
As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.
Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.
Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.
Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.
Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.
Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:
For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).
One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.
Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.
Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.
An exampleof a popular quantitative transformation is converting the date of birth to age.
Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.
Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.
Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.
Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.
Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.
In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.
The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.","Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",decisions,what is a direct effect on the quality of your data?,1,0,20.54545454545456,29.070295333862305
29,3009,3025,175,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Statistics is the science of using data to learn about the world around us. In this course, we use the term ""statistics"" in the broad sense to refer to methods for obtaining and analyzing data. Specifically, statistics provides methods for:
Design: Planning how to gather data for research studies,
Description: Summarizing the data, and
Inference: Making predictions based on the data.
Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.
Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, lets say 3, or the most common response, lets say 2. Graphs, tables, and numerical summaries are called descriptive statistics.
Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.
Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, ""Is changing the website layout associated with an increase in visitors?"" or ""Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?""
Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.
Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.
When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.
Discrete: I have one sibling.
Ordinal: I can rate my customer service experience at the grocery store as Good.
Continuous: It takes 1 hour and 20 minutes to get to school.
Nominal: What is your hair color? Brown.
Data is quantitative when the measurement scale has numerical values. The values represent different magnitudes of the variable. Examples of quantitative variables are income, the number of siblings, age, the number of years of education completed, etc.
Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.
For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.
Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.
Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.
The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.","Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",text multimedia,what type of information can be used to measure data?,1,0,0.0,36.25758743286133
30,932,941,67,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,"Above, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.
In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.
Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.
The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).
The most important tool is based on the big-O notation. We say a function \\(f(n)\\) is \\(O(g(n)\\) if there is are positive constants \\(c\\) and \\(n_0\\) such that for \\(n \\geq n_0\\) \\(f(n) \\leq c\\cdot g(n)\\). That is, beyond \\(n_0\\), \\(f(n)\\) grows at most as fast as \\(c\\cdot g(n)\\), that is, \\(c\\cdot g(n)\\) always dominates \\(f(n)\\) in growth, as shown below.
So when analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of the input. Based on the definition above, we express the number of steps with a function \\(f(n)\\), which captures the size of the \\(n\\). We then try to find the function \\(g(n)\\) such that \\(f(n)\\) is \\(O(g(n)\\).
Let's give a very simple example to explain this. Suppose we have an unsorted array of \\(n\\) integers, and we want to search if a given integer \\(x\\) appears in the array or not. A simple algorithm for this would a simple loop that searches if the next entry in the array is equal to \\(x\\). So in the worst case, we will need to look at each of the \\(n\\) integers until we know what the result is. Assume of these steps take, say, 3 operations; our function \\(f(n)\\) will be \\(3n\\). Now with the choice of \\(c=4\\) and \\(n_0 = 0\\) you can easily convince that \\(f(n)\\) is \\(O(n)\\). That is, the number of steps of our algorithm grows linearly as \\(n\\). While this is a very simple example, not every algorithm analysis will be simple as that. You can try and come up with an algorithm that searches \\(x\\) in a sorted array whose number of steps \\(f(n)\\) is \\(O(\\log n)\\). So ignoring the cost of initial sorting (which can be amortized over many searches if necessary), you can see that this latter algorithm uses much less time than the former one, as the function \\(\\log n\\) grows much much slower than the function \\(n\\).
The following are some additional important concepts regarding asymptotic analysis:
Upper bound is the asymptotically maximum time that a given algorithm needs for all inputs of size \\(n\\). Algorithms have upper bounds. For example, we say, ""mergesort has an upper bound time complexity of \\(f(n) = c_1 n \\log n\\)"" or ""selection sort has an upper bound time complexity of \\(f(n) = c_2 n^2\\)"".
Lower bound is the minimum asymptotically the minimum time that any algorithm for a problem needs for all inputs of size \\(n\\). For example, it can be shown that no sorting algorithm that works by comparing elements can take less than \\(c_3n \\log n\\) steps.
For more details on asymptotic analysis one can refer to any book on algorithm analysis, or to https://en.wikipedia.org/wiki/Asymptotic_analysis",,sorted,what is the best algorithm to solve a set of numbers?,1,0,5.545454545454542,30.327373504638672
31,2279,2294,140,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.
Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.
[Required Reading] Paper: Briscoe, E., & Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff. Cognition, 118(1), 2-16. (Requires CMU credentials to access)
The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.
The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.
The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.
There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.
While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).
The paper presents a number of conceptual and empirical contributions:
The characterization of exemplar theory as the low bias, high variance extreme, and prototype theory as the high bias, low variance extreme on the bias-variance continuum.
A class of experiments to evaluate human learners position on this continuum when the complexity of the training data varies has not been systematically attempted before.
The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.
The experiment reported in the paper has the following phases:
Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.
Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.
Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.
Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?
Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?
The important findings from the experiment are as follows:
Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.
When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.
When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.
The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.
Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.
From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.
From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.","Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",flags,"what does the term "" concept "" represent in a field of study?",1,0,14.083333333333364,77.05534362792969
32,4038,4062,245,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,"Besides memory, the other main aspect of any hardware system you will need to assess in order to understand if its important for computing are the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.
The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.
Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.
Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.
While it is tempting to use the most-efficient DSAs and try to squeeze as much performance as possible out of the newest systems, remember that, in the data science process, you will need to budget cost as well as time. It might be useful to use a DSA or a GPU, but you should remember to do the following when deciding whether to use either chip for a project or not:
If you have access to trial usage of the DSA/GPU for your project, check that the tools you are using utilize the accelerators at all. As these chips require separate programming APIs to utilize, the tools which you use them might not be compatible out of the box or at all.
Additionally, before setting and forgetting your system, check the usage patterns of the code you are able to run.
If the code is relatively I/O, Network, or Memory intensive, it might not make sense to use an accelerator chip, i.e., a GPU or a DSA, in your system. Instead, it might pay to try to use multiple processors or multiple computers together to solve the work in question.
If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.
Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.",,separately,how are data paralleled in data processing?,1,1,0.0,221.8419342041016
33,3602,3625,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",downd,what is cdr?,1,0,0.0,377.2213745117188
34,3308,3327,198,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.
Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.
Figure 1. Scenes from the Tuskegee Syphilis Study. (Source: https://www.rmpbs.org/)
The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.
By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.
In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.
The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.
Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.
There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.
In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.",,ethical practices,what kind of matter matter is subject to knowledge and society at large?,1,0,0.0,99.7439422607422
35,2543,2558,155,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Now that you have studied the elements of a properly framed analytical objective, we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science. They are provided here as purely illustrative example instances of the general template on which you can base your own formulations.
Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.
A constructive analytical objective states that it is, in principle, possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet. One can think of it as a proof-of-concept or prototyping endeavor.
In order to increase sales from the companys online store (Business objective)
...we work towards increasing the click-through rate of its advertising through targeted content (Problem)
...by classifying website visitors into youth, middle-age, and senior demographics (Task)
...using supervised learning models on curated internal datasets (Method)
(Business objective omitted due to project being primarily research)
In order to enable more effective search of audio collections (Problem)
We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds (Task)
using neural models on a dataset of short clips of classical music and their descriptions (Method)
towards developing suitable multi-modal audio-textual encoding (valuable functionality)
In scenarios where the feasibility of an analytical task has been established, projects may be targeted toward improvement over the state-of-the-art in some performance metrics by using innovative methods/features/data. This is typically the case if one works on leaderboard-type datasets where there are models.
The client is a logistics company that wants to speed up its automatic package sorting (Business objective)
We focus on the problem of handwritten address recognition from shipping label scans (Problem and Task)
We want to combine neural image recognition with language models on company-internal data (Method and Data)
To improve performance beyond the current model based on standard convolutional neural networks without language information (Valuable functionality)
(Business objective omitted due to project being primarily research)
For the task of span-based question answering from text (problem and task merged because span-based question answering is a common leaderboard task) 
We want to combine graph-based knowledge bases with neural attention models (Method)
To improve over state of the art performance on realistic news text (valuable functionality and data)
Exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives.
The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient (Business objective)
Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified (Problem and Task)
We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client (Methods and Data)
Towards identifying correlating events across the production process that can be used for process optimization.
(Business objective omitted due to project being primarily research)
The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bots replies (Problem)
We want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols (Methods and Data)
In order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset. (Task/Valuable Insight)","Constructive,Benchmarking,Exploratory\r",performance,what metric does a constructive analytical objective use to improve?,1,1,10.799999999999978,558.2789306640625
36,487,492,39,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Throughout this section, we will make use of the following matrix as an example:
We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.
COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):
Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.
Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.
DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,
its DOK representation is
This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.
The compressed sparse row format stores a matrix as three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of the column indices of the non-zero values
<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.
<![CDATA[row[i]]]> denotes the number of non-zero entries that appear in the rows above the i-th row in the original matrix, where row indexes start from 0, and <![CDATA[row[m]]]> denotes the number of non-zero entries in the entire matrix.
With the example matrix A,
its CSR representation is
While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:
0 at index 0, as there are no non-zero entries above the first row.
2 at index 1, as there are 2 non-zero entries above the second row ( 7 and 5 ).
2 at index 2, as there are still only 2 non-zero entries above the third row.
4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).
6 at index 4, as there are 6 non-zero entries in the matrix.
With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.
This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).
This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:
\\[M x=\\left(M_{1} \\cdot x M_{2} \\cdot x \\quad \\ldots M_{m} \\cdot x\ight)^{\op}\\]
We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.
At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?
The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.
<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix
<![CDATA[row]]>: a list of the row indices of the non-zero values.
With the example matrix A,
its CSC representation is
Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.
This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:
\\[ M x=x_{1} M_{(1)}+x_{2} M_{(2)}+\\ldots+x_{n} M_{(n)} \\]
We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.
At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?
Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.
A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.
At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \imes n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \imes k}, V \\in R^{k \imes n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.
The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","Natural Language Processing,Recommender Systems,Sparse Modeling",computational,what type of computation can be used to store a matrix?,1,0,7.5454545454545405,46.82714080810547
37,1859,1873,121,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.
Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:
Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.
Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.
Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.
Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:
RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.
LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.
LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.
Learning to Rank Algorithms (Source: Lucidworks)
Additional Reading: Application of LTR - Bayesian Product Ranking at Wayfair
Additional Reading: From RankNet to LambdaRank to LambdaMART",,rank,what does learning - to - rank algorithms apply to?,1,1,0.0,536.322509765625
38,2183,2197,135,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.
This chapter introduces three different environments for AI professionals and focuses on different tasks and skills required for each environment. In general, a good AI professional needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment. Based on her interest and expertise in tasks of environments, she can further pursue skills in depth and expand their skill set. Awareness of these environments can help individuals avoid confusion while making career decisions in the dynamic data world.
AI professionals first collaborate with stakeholders and domain experts to understand and define the business problem. They might also present and validate assumptions related to the problem. Once the problem is defined, and assumptions are validated, AI professionals can start working in the research environment.
A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.
The different steps in the research environment include:
Define Experiments: Different experiments can be defined based on the definition of the problem. For example, suppose we have a classification problem. In that case, experiments might be defined based on different approaches like conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training, etc. Experiments are prioritized based on the type, project timeline, and quantity and quality of data.
Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.
Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.
Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.
Feature Engineering: The knowledge from domain experts and EDA patterns help AI professionals create new features that might increase the performance of models in the experiment. Remember that generating relevant new features from existing features is called feature engineering.
Data Modeling: The goal of a model is to try to replicate domain experts decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.
Tuning and Evaluation: Optimal hyperparameters can be found to maximize the model performance by comparing the metrics of each version of the model in an experiment on evaluation data.
Experiments Tracking and Evaluation:  Steps 2 to 7 are repeated for each experiment and evaluated at the end. Experiment tracking tools like Neptune AI and Weights and Biases can efficiently track experiment information with a good user interface.
The model in an experiment with the highest performance is selected for working further in the development environment.
A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.
In applied machine learning, the AI/ML Pipeline automates performing a sequence of steps in components and interaction between the components defined by the AI/ML system design. The components include data collection, data preprocessing, model development and fine-tuning, post-processing on predictions, model evaluation, model deployment, maintenance, and monitoring.
Use a version control tool. Version control plays a crucial role in the development environment. Version control tools like perforce and assembla make the processes like creating a GIT repository, defining the code repository structure, and branching strategy easy.
Install IDE like PyCharm to automatically create virtual environments for projects and allow easy integration with GIT.
Convert Jupyter Notebook code into object-oriented code and save in .py files. Have appropriate variable names, add comments, and organize different files into components with proper hierarchy.
Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.
Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).
Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.
Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.
Often based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.
The packaged code is further used in the production environment.
Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.
Design Optimization: In general, there is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment. Hence, if required, the AI/ML system design created before in the development environment needs to be optimized and redesigned for production.
Containerization using Docker: Developers might use multiple components like Data Extractor, Elastic Search, Rest API, Messaging Queues, etc. Each component has its respective dependency libraries. Having components with different versions of a library in the same environment might lead to conflict. With the help of Docker, AI Professionals can standardize environments and run different containers for different components in isolation, where each container has dependent libraries for the respective component. An environment can be created by Docker using a DockerFile. DockerFile contains instructions like navigating to a respective folder, installing dependencies, setting environment variables, loading configuration parameters for the model, etc. Scaling is easy with containers because AI professionals can spin up new containers for the same component in seconds to satisfy the scaling requirements.
Continuous Integration and Continuous Delivery (CI/CD): CI/CD enables AI professionals to work together in a shared code repository where updates to a part of code by an individual are automatically pushed, built, tested, delivered, and deployed to the shared code repository, and code issues can be tracked and resolved respectively
Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Googles Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Googles Kubeflow and Netflixs Metaflow, which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.
Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.
Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintains good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).
Data Metrics: It is essential to check if the input data format is correct first instead of debugging the entire pipeline.
Anomaly Checks: Simple checks like having max and minimum values for each feature (age cannot be negative or 100000) can identify and validate extreme or anomalous data points in input data. Later, the team can brainstorm and find root causes for receiving these anomalies from users.
Data Quality Issues: Users might give synonyms (cGirld for cFemaled) or incorrect values (cMaild instead of cMaled) as input to the pipeline. In these cases, the model might fail to recognize the value in the feature cGenderd (data might be absent while training the model) and assign NaN (not a number) for the feature. Even though the model doesnt break, the predictions produced by the model might be wrong. Hence, testing new data that the model hasnt seen before is essential.
Data Drift: When we train a model with some static data, it assumes specific patterns based on the distribution of provided data. However, real-world data is dynamic. Because of these changes, the assumptions made by the model might no longer be valid, and the model might get biased, which leads to bad performance in real-time model evaluation. For example, water consumption in hospitals during COVID-19 was very high compared to historical data. Hence, we cannot use a model built on historical water consumption data during COVID-19. This phenomenon is called cData Drift.d Periodically detecting changes in the distribution of data using statistical tests can help to detect data drift.
Model Metrics: It is crucial to estimate the expected performance of models before deploying them into production and periodically check if expected KPIs are met. If model predictions or expected KPI values are bad compared to benchmarks, AI professionals might consider the Model Drift issue. Model Drift is a phenomenon where the relationship between features changes, and the model no longer gives accurate predictions. For example, the relationship between the births and deaths ratio changed during COVID-19 causing model drift. Model drift can be detected by periodically analyzing feedback from feedback loops and correlating it with what is affecting the business.
Monitoring: Based on Model Metrics and Data Metrics, if re-training is required, the AI professional might start repeating the research, development, and production environment to deploy and monitor the new model. Often, retraining is also a way to improve the model's performance to reflect the change in data over time. Some of the Data Monitoring tools include SuperwiseAI, ArtherAI , and VertaAL.
It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",professionals,who does ai stand for?,1,0,0.0,625.1419677734375
39,446,451,39,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Throughout this section, we will make use of the following matrix as an example:
We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.
COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):
Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.
Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.
DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,
its DOK representation is
This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.
The compressed sparse row format stores a matrix as three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of the column indices of the non-zero values
<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.
<![CDATA[row[i]]]> denotes the number of non-zero entries that appear in the rows above the i-th row in the original matrix, where row indexes start from 0, and <![CDATA[row[m]]]> denotes the number of non-zero entries in the entire matrix.
With the example matrix A,
its CSR representation is
While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:
0 at index 0, as there are no non-zero entries above the first row.
2 at index 1, as there are 2 non-zero entries above the second row ( 7 and 5 ).
2 at index 2, as there are still only 2 non-zero entries above the third row.
4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).
6 at index 4, as there are 6 non-zero entries in the matrix.
With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.
This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).
This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:
\\[M x=\\left(M_{1} \\cdot x M_{2} \\cdot x \\quad \\ldots M_{m} \\cdot x\ight)^{\op}\\]
We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.
At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?
The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.
<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix
<![CDATA[row]]>: a list of the row indices of the non-zero values.
With the example matrix A,
its CSC representation is
Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.
This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:
\\[ M x=x_{1} M_{(1)}+x_{2} M_{(2)}+\\ldots+x_{n} M_{(n)} \\]
We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.
At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?
Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.
A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.
At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \imes n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \imes k}, V \\in R^{k \imes n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.
The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","Natural Language Processing,Recommender Systems,Sparse Modeling",rating,what can be quickly converted to?,0,0,0.0,260.44403076171875
40,3522,3544,217,Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,"In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.
Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as
\\[ AIC(M)=(2K_{h}-2LL(M))/N \\]
where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.
Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as
\\[ BIC(M)=K_{M}\imes logN-2LL(M) \\]
where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.
Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as
\\[ MDL =L(M)+L(D|H) \\]
Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",,bayesian,what type of information is the bic score?,1,0,0.0,211.30528259277344
41,2425,2440,151,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.
Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.
One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.
Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.
The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.
Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.
Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.
The are several measures to describe the spread, variability, or dispersion of a dataset
Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.
Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is
\\[\\sum_i\\frac{\\mid x_i - \\mu\\mid}{N}\\]
where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.
This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is
\\[\\sigma^2 = \\sum_i\\frac{(x_i - \\mu)^2}{N}\\]
Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.
A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.
Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.
Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:
Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.
Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC)
Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.
Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.
Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \ight |\\leq 1\\)
The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).
More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.
A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.
Figure 2. Scatterplots for Four Hypothetical Datasets.
Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.
One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.","Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",mean,what does the arithmetic average mean?,1,0,0.0,174.8638458251953
42,3368,3387,200,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?
A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.
When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.
Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.
Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018))
Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.
During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.
Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.
Scalar: Single numeric feature, e.g., mass.
Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.
Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.
In machine learning, the input to a model is represented as a numeric vector.","Raw Data to Features,Features",life,what is the main task of transforming features from data to processing?,1,1,0.0,131.04148864746094
43,2345,2360,142,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.
The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.
The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.
Figure 1: Traditional Sequence2Sequence model architecture.
A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.
Figure 2: Limitation of the traditional Sequence2Sequence model architecture.
The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this cbottleneck problemd.
Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.
In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.
Figure 3: Attention between words in a sequence.
In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of cattentiond to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.
In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.
Figure 4: Attention in Seq2Seq RNN architectures.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",,term,what is the term used to describe the food that is related to the food?,1,0,0.0,24.182653427124023
44,552,557,45,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.
Following are some of the commonly used loss functions based on the problem at hand:
Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.
Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.
Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.
For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.
Following are some common optimization algorithms along with their advantages and disadvantages:
Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.
\\[ w:=w-\\eta \abla Q_{i}(w) \\]
Easy to compute
Easy to implement
Susceptible to getting stuck in a local minima
Convergence is slow as updates are calculated after calculating the gradient for the entire dataset
Computation for entire dataset requires a large memory
Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.
\\[ w:=w-\\eta \abla Q_{i}(w) \\]
Converges in lesser time because of frequent updates
Lesser memory requirements for calculating updates
Less likely than Gradient Descent to get stuck in a local minima
High variance in parameter updates due to high frequency in updates
Learning rate needs to be correctly chosen and adjusted for effective training
To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given batch. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.
Frequent updates and lesser variance as compared to SGD
Moderate amount of memory requirements
Learning rate needs to be correctly chosen and adjusted for effective training
Learning rate is constant for all parameters which might not be desirable
Susceptible to getting trapped in a local minima
The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.
Reduces the high variance in model updates by SGD
Faster convergence than Gradient Descent
The momentum hyperparameter needs to be additionally tuned
Learning rate needs to be correctly chosen and adjusted for effective training
The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.
\\[ \\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\eta_{\\mathrm{t}}^{\\prime} \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}(\\mathrm{t}-1)} \\]
\\[ \\eta_{\\mathrm{t}}^{\\prime}=\\frac{\\eta}{\\operatorname{sqrt}\\left(\\alpha_{\\mathrm{t}}+\\epsilon\ight)} \\]
Reduces the need to manually modify learning rate
Tends to have faster convergence than Gradient Descent and SGD
The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate
RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.
\\[ v(w, t):=\\gamma v(w, t-1)+(1-\\gamma)\\left(\abla Q_{i}(w)\ight)^{2} \\]
\\[ w:=w-\\frac{\\eta}{\\sqrt{v(w, t)}} \abla Q_{i}(w) \\]
Requires lesser tuning than other optimization algorithms
Faster convergence
The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks
AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.
\\[ \\mathbf{s}_{t}=\ho \\mathbf{s}_{t-1}+(1-\ho) \\mathbf{g}_{t}^{2} \\]
\\[ {x}_{t} = {x}_{t-1} - {g}_{t}^{\\prime} \\]
\\[ \\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} \\]
\\[ \\Delta \\mathbf{x}_{t}=\ho \\Delta \\mathbf{x}_{t-1}+(1-\ho) \\mathbf{g}_{t}^{\\prime 2} \\]
Elevates the learning rate decay problem in AdaGrad
Computationally expensive
Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.
\\[ m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\ight)\\left[\\frac{\\delta L}{\\delta w_{t}}\ight] v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\ight)\\left[\\frac{\\delta L}{\\delta w_{t}}\ight]^{2} \\]
Rapid convergence
Rectifies vanishing learning rate and high variance
Computationally expensive
Equations Reference: Link
(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)
(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)","Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",rate,what does an optimization algorithm use to scale the next point?,1,0,0.0,205.94036865234372
45,3644,3667,220,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,"MotoManager Case-in-Point
Here is an example of an AI consulting firm that used the EVP framework to meet the business needs of a popular automotive services provider.
Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer cstickinessd leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.
Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.
The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the companys business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the companys data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.
Data Management in the Enterprise
Finally, service managers were interviewed on customer service difficulties that could be addressed by the proposed solution. The service managers also identified the hardware and software gaps at various stores around the country. Once the interviews were completed, the Cognistx data science team formulated business objectives that would meet Monro Inc.s business needs. The business objectives included:
Creating an application that provides customers with a customized service experience for their automotive needs.
Offering customers 50 coupon to download Monro Inc mobile app.
Onboarding customers to the application with the creation of customer profiles.
Providing tailored customer service management to very important (VIP) customers.
Classifying customers as VIP customers based on defined characteristics.
Creating a loyalty program to increase repeat customer transactions.
Business objectives should be measurable to ensure that business needs are met. The metrics used to assess the success of the project were:
The number of people who installed the app and on-boarded upon receiving a 50 e-coupon.
The number of times an on-boarded customer visited a store close to them.
The number of transactions completed with the app.
The total amount of revenue generated via the app compared to total cost of  maintaining the app.
A model that can predict a repeat customer from among on-boarded customers with an accuracy of 85%.
The metrics were both technical (precision and accuracy of the model) and business-related (calculate return-on-investment).
Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a 50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customers local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over 14 million US dollars from app-based transactions.",,data,what did the information management team provide to customers at this enterprise?,1,0,0.0,164.35635375976562
46,390,395,37,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,"When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.
There are different types of clustering techniques. We will briefly define the general idea at this point and fully explore it in an upcoming module.
K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.
It is not uncommon to begin by randomly selecting a number of observations from the data as the initial cluster center, the remaining observations will then be assigned to the nearest cluster center. The algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids (the middle of a cluster). The reassignment is done to minimize dispersion within clusters.
Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.
There is another approach of hierarchical clustering that puts all data points in one cluster and then separates them based on dissimilarity until different clusters are formed. This is called divisive clustering.
Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.
Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the kind of clustering technique used to meet your analytic objective. The next sections will focus on the different types of clustering techniques and the evaluation techniques that apply to each technique.
Reading: Evaluating Clustering Results",Example of a Dendrogram. (Source: Mathlab).,uses,what does the cluster analysis technique do?,1,1,0.0,251.1094512939453
47,3032,3050,181,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,"The ability to study and understand a projects business objective in sufficient depth in order to maximize the benefit from leveraging data is a critical skill for data scientists. This skill can be thought of as having three aspects:
Engagement with the client and research about the specific circumstances of the clients business.
First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).
Engagement with the client and research around the general domain in which the clients business operates .
Second, preparation will often require additional research around the clients general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the clients experts in a serious conversation about the topic without frequently stumbling over misunderstandings.
Spending continuous efforts to maintain alignment of the ongoing project with changes or new information in both the general domain and specific circumstances.
Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science projects success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.
Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",,general,what domain of the art is involved in the business of the company?,1,0,0.0,47.93196868896485
48,3834,3857,227,Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,"Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.
A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.
Figure 2. Data Science Life Cycle (courtesy: Microsoft )
You can draw similarities between the CRISP-DM lifecycle and the data science lifecycle. Perusing the Internet, you will also find that different data science solution vendors have adapted the lifecycle to fit their products and the solutions supported by their tools. The data science lifecycle is briefly explained below:
Business Understanding. cWe fail more often because we solve the wrong problem than because we get the wrong solution to the right problem.d  Professor Russell L. Ackoff.
Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.
Data Acquisition. This process involves obtaining data from various sources and may also require setting up a data collection task and infrastructure.
Subsequently, you will perform data preparation to ensure the data is ready for analysis.
Data Preparation. This is the process of cleaning and transforming raw data prior to processing and analysis. This needs to be done carefully as assumptions made here may influence, or even limit, the use of the data during analysis.
Data Exploration and Cleaning. The quality of your dataset will determine your success in meeting your business objectives. Data exploration includes identifying variables, conducting univariate and multivariate analyses, identifying outliers, anomalies, and missing values, as well as feature creation and selection. We will cover these topics in future units.
Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.
Feature Engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features.
Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.
Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.
Model Evaluation is an essential step in the lifecycle. Typically, analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced. Depending on the nature of the task (as stated in the analytic objective), model evaluation will follow corresponding metrics and techniques that will be explored in this course.
Deployment. Once you have evaluated your models to ensure accuracy and performance, you will deploy the model to an environment for application consumption.",,life cycle,what does sdlc stand for?,1,1,0.0,140.79483032226562
49,3630,3653,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",orthographical,what type of language can be used to separate words from a single language?,1,0,12.78571428571426,27.458026885986328
50,1758,1771,115,Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.
Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).
Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter
If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.
See https://en.wikipedia.org/wiki/Dunn_index for more details.
External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.
Rand Index tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:
(TP + TN)/(TP+FP+FN+TN)
Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.
Jaccard Index is used in cluster analysis evaluation. It is defined as ""the size of the intersection divided by the size of the union of the sample sets."" The Jaccard distance measures dissimilarity between sample sets.
F-Measure is simply computed as the \\(\\frac{2*Precision*Recall}{Precision+Recall}\\). You might remember it from the classification metrics, it is also known as the F1 score.
The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",,classification,what does the external approach compare to?,0,0,0.0,292.52935791015625
51,1891,1905,121,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.
Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:
Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.
Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.
Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.
Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:
RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.
LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.
LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.
Learning to Rank Algorithms (Source: Lucidworks)
Additional Reading: Application of LTR - Bayesian Product Ranking at Wayfair
Additional Reading: From RankNet to LambdaRank to LambdaMART",,task,what is a data science pattern used to solve?,1,0,6.88888888888888,148.69998168945312
52,3370,3389,200,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?
A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.
When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.
Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.
Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018))
Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.
During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.
Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.
Scalar: Single numeric feature, e.g., mass.
Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.
Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.
In machine learning, the input to a model is represented as a numeric vector.","Raw Data to Features,Features",space,what can be transformed into a data science process?,1,1,7.777777777777765,101.90457153320312
53,3988,4012,235,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,"The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.
Bayes Rule (Source: https://www.psychologyinaction.org)
Additional Reading: Bayes Theorem
Additional Reading: Overview of Bayesian Statistics
Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.
Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.
NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.
There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.",Naive Bayes (NB) Method,task,what does the bayesian dataset have to do?,1,0,0.0,88.94413757324219
54,2058,2072,130,Collecting and Understanding Data,Data Collection,Where do data come from?,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.
As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:
Organic or process data
Data collected from a designed study
Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.
Examples of organic or process data:
Financial or stock market exchange transactions
Web browser history
Web or mobile application activity history
Netflix viewing history
Surveillance camera video recordings
The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.
Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.
Here are examples of data that can be collected from a designed study:
Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.
Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.
Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.
Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.
As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.
Figure 1: Drawing a representative sample from the population. (Source: https://www.voxco.com/)
In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.
You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.",,answer,what is the main difference between data and process data?,1,0,0.0,36.43783950805664
55,2832,2847,167,Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,"A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.
Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.
A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.
This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.
When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",,identification,what does a data science team look at?,1,0,4.75,61.31736755371094
56,753,759,52,Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]
The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.
The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Googles services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.
The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.
Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.
There are two primary innovations from the paper.
Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.
Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.
The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):
Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., \\(PE_{(pos, 2i)}\\) or \\(PE_{(pos, 2i+1)}\\) to yield the word vector with positional information for each token.
Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.
Step 3 is similar to Step 1 but carried out on the output sequence tokens.
Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).
Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).
Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.
During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.
The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.
The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.","Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",different positions,what is the main component of the paper that has been similar to previous research?,0,1,0.0,39.177188873291016
57,835,844,62,Collecting and Understanding Data,Data Collection,Module 7 Summary,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.
Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.
Validity is the development of sound evidence to demonstrate the intended test interpretation.  In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.
Threats to internal validity are problems in drawing correct inferences about whether the covariation between the presumed treatment variable and the outcome reflects a causal relationship.
Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.
Statistical bias results from violations of external validity or internal validity of a study. Common statistical biases that you need to be aware of and take into account during the data understanding process are:
Selection Bias: a threat to internal validity occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about.
Self-selection Bias: If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.
Confirmation Bias: This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.
Information Bias: The data collected has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.
Confounding Bias: occurs when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable.",,information,what has a misclassification effect on?,1,0,9.999999999999998,138.80474853515625
58,3789,3812,226,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,"Following are some of the key deep learning architecture/algorithms:
Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a multi-layer perceptron.
Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks
A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.
Figure 2. LeNet CNN. Source: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures
The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.
The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).
RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been  developed and are described below:
Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.
The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.
Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.
The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.
Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/",,typical,what kind of layers does the annunciation look like?,1,0,0.0,95.75358581542967
59,3993,4017,235,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,"The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.
Bayes Rule (Source: https://www.psychologyinaction.org)
Additional Reading: Bayes Theorem
Additional Reading: Overview of Bayesian Statistics
Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.
Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.
NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.
There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.",Naive Bayes (NB) Method,age range,what does the bayes theorem support to accurately assess risk based on?,1,0,0.0,853.107177734375
60,3534,3557,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",entities,what can be used to separate words from a single language?,1,1,16.272727272727334,50.45629501342773
61,2928,2944,174,Collecting and Understanding Data,Data Collection,Study Design,"To connect the studys objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.
Exploratory
Confirmatory
Bottom-up (without a pre-specified question)
Can lead to knowledge discovery or new theory
Uses inductive logic and the logic of discovery
Top-down (with a specified falsifiable hypothesis)
Tests an existing theory
Use deductive logic, the logic of justification, and reconstructed logic
Comparative
Non-Comparative
Contrasts one subject with another based on certain measures.
Estimates or predicts absolute outcomes of the certain subject matter without explicitly making a comparison with its counterpart.
Experimental
Observational
The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.
To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.
Good experimental designs use randomization to determine which treatment a subject receives.
The purpose of observational studies is to draw inferences about the effect of an cexposured or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.
Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.
It is therefore important for readers of observational research to consider if alternative explanations for study results exist.
Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.
In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.",,experimental,what type of designs are used to measure responses to subjects?,1,0,0.0,131.0995635986328
62,3106,3124,184,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,"Natural Language Processing (NLP hereafter) is a subfield of computer science aiming to  build  systems that:
Enable human-computer communication
Enable/enrich human-human communication
Perform tasks requiring the use of human language faculty, more efficiently and more correctly.
Here are some applications of NLP in these three areas:
Human-computer communication
Dictating email messages
Listening to email messages
With proper pronunciation, tone, and stress!
Using human language to give commands to the operating system
Human-human communication
Real-time text and speech translation
Summarizing meeting conversations
Enabling communication with people with hearing/vision impairments
Improving the efficiency of tasks requiring human language faculty
Correcting typing/grammatical errors
Question answering
Answering e-mails automatically
Searching large document databases
Here are some (and definitely not a comprehensive set of) applications of  NLP in a diverse set of areas:
Information extraction (IE), especially copend IE:  Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.
Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.
Question answering beyond finding the documents relevant to the question and instead delivering the actual answer.
Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.
Image (and eventually video) understanding:  Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).
Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.
Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).
Summarization:  Generating a short summary of the salient points of a document or a set of documents.
Opinion and sentiment analysis:  Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.
Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.
Fake news or fake review detection:  Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.
The following figure gives a birds eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.
From Eduard Hovys cThe Past and 3\xbd Futures of NLPd presentation.
There are three high-level functions that NLP tries to automate:
Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.
Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).
Acquisition: obtaining the representation and necessary algorithms from data (e.g., learning to translate from aligned translated sentences).
The representation that these employs depends on the actual task being solved:  they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.
NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.
Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",,conversations,what is the function of interaction between a user and a user?,1,1,10.000000000000012,29.23209571838379
63,200,203,23,Collecting and Understanding Data,Data Collection,Validity and Bias,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.
Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.
Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.
There are four types of validity:
Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.
Construct validity refers to the validity of inferences about the constructs (or variables) in the study.
Internal validity relates to the validity of inferences drawn about the cause-and-effect relationship between the independent and dependent variables.
External validity refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures.
In this module, we will discuss external and internal validity in more detail.
As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.
Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.
As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.
Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.
Type of Threat to Internal Validity
Description
Suggested response by Data Scientist
History
Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.
The data scientist can have the control and experimental groups experience the same activities (except for the treatment) during the experiment.
Maturation
Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.
A careful selection of participants who mature or develop in a similar way for both the control and experimental groups helps guard against this problem.
Regression to the mean
Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.
The data scientist can select participants who do not have extreme scores as entering characteristics for the experiment.
Selection
Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)
Random selection may partly address this threat.
Mortality (also called study attrition)
Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.
The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.
Diffusion of treatments (also called cross-contamination of groups)
Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.
The data scientist must keep the two groups as separate as possible during the experiment.
Compensatory equalization
When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.
The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.
Compensatory rivalry
Participants in the control group feel that they are being devalued, as compared to the experimental group, because they do not experience the treatment.
The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.
Resentful demoralization
When a control group is used, individuals in this group may become resentful and demoralized because they perceive that they receive a less desirable treatment than other groups.
The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.
Testing
Participants become familiar with the outcome measure and remember responses for later testing
To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.
Instrumentation
The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.
The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.
Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.
Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.
Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.
Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.
Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.
Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.
Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.",,statistics,what is restricted to inferredness of a given range?,1,0,19.33333333333328,275.09381103515625
64,4130,4154,259,Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,"At the beginning of a  data science process on any task, it is important not just to understand the dataset and the tools available but also the hardware resources available to you for the task at hand. Most data science tasks rely on using statistical or numerical techniques to process the data and/or optimize some discrete/continuous optimization task. In doing so, youll need to assess if a task is feasible with the hardware you have and what hardware to use to make the most of the computational budget youre given to solve the analytic objective.
In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:
First, you will want to spend time to make sure that the tools you use actively use the computational resources you are given. Taking the time to understand if and when you can scale down any cloud computing resources you are using can save you time and money in performing any computational task, which then can be a boon as you assess different statistical techniques and their performance in your analytic objective.
Second, you will want to assess how you are using the various libraries common to data science. Compared to more traditional single-threaded and multi-threaded applications, data science libraries tend to require users to focus on operations, unlike those more traditionally used, such as conditional indexing and matrix multiplication. In this chapter, youll find motivation for learning these libraries to the depth necessary to perform these tasks, as they form the basis for highly performant and highly readable data science code.",,resources,what does the dataset have to use to calculate?,1,0,0.0,119.52464294433594
65,292,295,25,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:
The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.
No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.
These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, learn, and study will be closer than the pair learn and eat. Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.
These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, its a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.
Word2Vec (2013) and GloVe (dGlobal Vectors for Word Representationd) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.
Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.
This problem was solved by the ELMo (""Embeddings from Language Model"") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.
Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.
ULM-FiT (cUniversal Language Model Fine-tuningd), also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:
General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.
Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.
Target task Classifier Fine-Tuning where the classifier is fine-tuned on the target task using gradual unfreezing (unfreezing weights from the last to the first layer in different learning epochs) and repeating stage 2. This helps the network to preserve low-level representations and adapt to high-level ones.
Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. Well discuss the  Transformer model and the motivation behind its architecture in detail next.",,techniques like,what technique did the nlp module introduce?,1,0,0.0,931.8208618164062
66,910,919,66,Data Science Project Planning,Developing a Vision,The Vision Document,"The important components of a vision document are described as follows:
Problem Description. First, you need to determine the real-world problem that you are trying to address. You should also look into the literature to identify existing solutions and their limitations. Your goal is to propose an improvement to these solutions in some ways so as to yield tangible social or business impacts.
Proposed Solution. Next, come up with an overarching framework for your solution. An example solution formulation could be cA framework that supports a declarative description of a configuration space and automatically evaluates all the options, finding the best given some measure and labeled dataset.d Compare your solution with the existing solutions identified previously. What makes your solution better?
Scientific Hypotheses. Now you need to formalize the scientific hypotheses underlying your proposed solution. Keep in mind that hypotheses need to be testable assumptions. For example, if you make the assumption that the majority of your users are teenagers, but your platform doesnt collect users ages, then your assumption is not testable. Then, develop a plan to validate your hypothesis as you develop your solution. With respect to a technical data science solution, you can formulate your hypothesis along with one of the following themes:
cConstructived: It is possible to build such a framework.
cFormatived: The proposed solution is cfast /good enoughd
cEmpiricald: The proposed solution will significantly outperform the state-of-the-art baseline measured by a certain metric.
Major Features. For the next step, you should describe in more specific terms how the system features you plan to implement form the proposed solution to the problem (e.g., a list of major features or components of your solution). This is really important from a ""traceability"" perspective as it's easy to miss major features as you dive deeper into the project. Here are the important requirements for the proposed features:
Features should not be technical constraints. For example, cthe system will use an Amazon AWS load balancer to manage trafficd is a bad example, while cthe system will use a cloud-based load balancer to manage trafficd is a good example.
Features should be distinct in the sense that no two features should overlap.
Features should be traceable, with a unique name or identifier, so that they can be traced across different documents and project stages.
Scope. At this point, you will want to outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline. If there is something your project will specifically not do, you should note it here. For example, be clear about how much (or how little) data will be covered, whether your solution will meet certain run-time requirements in terms of responsiveness, whether your solution will be deployed as a web service or application or exist only as a code repository/Jupyter notebook, etc.",,documents,what can the components of a vision be traced back to?,1,0,5.363636363636361,65.56922149658203
67,1462,1472,100,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.
At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.
Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.
Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.
Despite its importance, data wrangling presents some challenges that are common in data science projects.
So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.
Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.
A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.
The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.
Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.
Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:
Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.
The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.
Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.
In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.
Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.
In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.
Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.
\\(z = (x-\\mu )/\\sigma\\)
The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!
The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task
One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.
In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.
Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.
Missing at random (MAR) is the probability that the missing data is the same within certain groups.
Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.
The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.
Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.
Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.
Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.
You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.
Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.
When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.
As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.
Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.
Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.
Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.
Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.
Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:
For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).
One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.
Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.
Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.
An exampleof a popular quantitative transformation is converting the date of birth to age.
Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.
Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.
Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.
Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.
Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.
In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.
The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.","Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",analysis,data wrangling data is used for what?,1,1,0.0,444.9342956542969
68,1076,1086,88,Data Science Project Planning,Design and Plan Overview,Module 6 Summary,"It is necessary to establish a design for the project as it can help identify various bottlenecks of the project. This documentation makes the developers begin thinking about the various risks and challenges involved in the data science project. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, and the system environment. A clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams. The developers are also prompted to consider the many risks and difficulties that the data science project may present.
Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:
User stories: A high-level definition of a work request.
Sprints: Short spans of work where teams work on tasks determined in the sprint planning meeting. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service.
Stand-up meetings:  These are daily stand-up meetings where participants are required to stay standing, helping to keep the meetings short and to the point.
Agile board: This is a tracking system that helps your team track the progress of your project.
Backlog: This is the set of outstanding project requests that are added through your intake system. Managing your backlog is a vital role for project managers in an Agile environment.
Sprint retrospective: This is a meeting where the team gathers to discuss what went well and what didn't.
Demonstration: This is when the team demonstrates a working product to the stakeholder.",,standing,what kind of meetings are participants required to stay in the project?,1,0,0.0,69.47277069091797
69,1170,1180,94,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:
A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search
The model parameters are the numeric values or structures that are derived from the learning process.
The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.
The learning algorithm specifies the way in which model parameters are updated or derived from the input data.
With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.
Component
Model 1
Model 2
Hyperparameters
Learning rate \\(\\alpha =0.1\\), regularizer \\(\\lambda =1\\), number of iterations \\(N = 1000\\)
Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)
Learning algorithm
Gradient descent over the logistic loss function with L2 regularization
Gradient descent over the logistic loss function with L2 regularization
Pre-processing
None
Normalize the data to have zero mean and unit variance
Post-processing
None
None
Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.
Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.
Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.
Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).
Optional Reading: Integrating explanation and prediction in computational social science",,inform,"according to machine learning, what does machine learning depend on?",1,0,0.0,53.36227035522461
70,2027,2041,130,Collecting and Understanding Data,Data Collection,Where do data come from?,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.
As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:
Organic or process data
Data collected from a designed study
Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.
Examples of organic or process data:
Financial or stock market exchange transactions
Web browser history
Web or mobile application activity history
Netflix viewing history
Surveillance camera video recordings
The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.
Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.
Here are examples of data that can be collected from a designed study:
Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.
Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.
Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.
Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.
As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.
Figure 1: Drawing a representative sample from the population. (Source: https://www.voxco.com/)
In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.
You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.",,information,what is used to collect data from a given source of data?,1,0,7.916666666666655,35.3231315612793
71,3615,3638,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",computational,what type of processing can be used to separate words from a single language?,1,1,12.78571428571426,42.027587890625
72,1384,1394,97,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.
To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.
Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.
In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.
A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.
 Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.
Performing a hypothesis test involves three major steps:
Deciding what your Null Hypothesis, \\(H_{0}\\) and what your Alternative Hypothesis are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to cdisprove,d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.
Computing some sort of ctest-statisticd. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.
Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.
 It is important to note that crejecting the nulld does NOT mean caccepting the alternatived. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.
When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.
That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.
Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.
Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.
If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.
We could try to test if they are just different from each other with the following hypotheses:
\\(H_{0}: \\mu _{\\alpha}=\\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}\eq \\mu _{\\beta}\\)
Or we could test that one is strictly larger than the other:
\\(H_{0}: \\mu _{\\alpha}< \\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}> \\mu _{\\beta}\\)
The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.
In our case, we have two averages and want to look at their differences. For the students t-test, we shall use the aptly named ct-test statisticd:
\\[ T = \\frac{\\mu_\\alpha - \\mu_\\beta}{\\sqrt{\\frac{\\sigma^2_\\alpha}{n_\\alpha} + \\frac{\\sigma^2_\\beta}{n_\\beta}}} \\]
In particular, we are going to use what is known as cWelchs t-test statistic,d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:
\\[ \u = \\frac{(\\sigma^2_\\alpha + \\sigma^2_\\beta)^2}{\\frac{\\sigma^4_\\alpha}{n^2_\\alpha(n_\\alpha-1)} + \\frac{\\sigma^4_\\beta}{n^2_\\beta(n_\\beta-1)}} \\]
With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.
While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.
However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.
Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.
To help combat some of the issues with Welchs t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\ext{accuracy}\\).
If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:
\\(H_{0}: E _{\\alpha}=E _{\\beta}\\)
\\(H_{A}: E _{\\alpha}\eq E _{\\beta}\\)
For this test, our test metric is actually much simpler:
\\[ \\frac{(|E_\\alpha - E_\\beta|-1)^2}{E_\\alpha + E_\\beta} \\]
 In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.
Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.
While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.
On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welchs or another paired t-test variety.
However, both of these tests do suffer a single, incredibly critical flaw.
Namely, they suffer from cp-hacking.d
From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.
One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.
Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.
For our problem, this is especially grave. Consider that, for n models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.
Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?
To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.
 If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.
With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.
The hypotheses are the following:
\\(H_{0}: r_{1}=r_{i}=...=r_{k}\\)
\\(H_{1}:\\) They are not all equal.
and the associated statistic is:
\\[ \\frac{12n}{k(k+1)} \\sum_{i}^{k}(r_i - \\frac{k+1}{2})^2 \\]
For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.
With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.
There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:
Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.
Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.
Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.
According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:
Identify the population parameter of interest.
Determine whether you will be conducting a one-tailed or two-tailed test.
Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.
...then define an alternative hypothesis, denoted as \\(H_{A}\\). This would be the opposite of the null hypothesis.
The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?
Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.
Type I error occurs when you reject the null hypothesis when it should be accepted.
Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.
Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.
As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.
Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)
Here, given a z-score of -2.878, we can calculate the p-value as,
p-value = \\(2\imes P(z< -2.878)\\)
 Note: Since were conducting a two-tailed test, we can then multiply this value by 2.
If you locate -2.878 in a z-score table, you get a value of 0.002.
p-value = \\(2\imes 0.002\\)
p-value = 0.004
So, we have our p-value <  \\(\\alpha_z\\)
Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.","McNemar Test,P-Hacking",manufacturer,"according to the text, who else should have a better model for a model?",1,0,13.999999999999968,65.41585540771484
73,2525,2540,152,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.
Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was madeunderstanding the why will make the problem clearer and affect problem-solving for future challenges.
Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.
Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).
The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.
Reading: Should you trust that model?
The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.
Local Surrogate Models""explain individual predictions of black box models.""
Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.
Additional Resource: Sara Hooker: The Myth of the Perfect Model
Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.
Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.
Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).
There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""
Hall (2016) has recommended the following steps:
Train black-box models and use them as benchmarks.
Use different regression techniques.
Use black-box models in the deployment process.
Train small interpretable ensemble models.
Create nonlinear predictors using black-box techniques.
Explain black box models better using variable importance measures.
Figure 1. Accuracy versus Interpretability (Source: Rane-20181)",,additional,what can be used to explain how a model should be used?,1,1,8.545454545454545,33.659149169921875
74,3695,3718,222,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),"Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.
Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having cbi-directionald attention and are often called auto-encoding models.
The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.
Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.
Representatives of this family of models include BERT, ALBERT, RoBERTa.
Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.
The pretraining of decoder models usually revolves around predicting the next word in the sentence.
These models are best suited for tasks involving text generation.
Representatives of this family of models include CTRL, GPT, GPT-2.
We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.
One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.
BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms  an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERTs goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.
As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that its non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.
The original BERT paper presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.
BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.
BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.
For any NLP task, BERT is generally trained in two steps:
First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.
Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.
The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.
Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERTs prediction of the masked token.
Figure 1. Illustration of masking and input flow across the model in BERT.
While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.
Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.
Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training.
When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.
Figure 3: Overview of fine-tuning BERT and usage of BERT in various downstream tasks.
Pre-trained BERT models can be used for a wide variety of language tasks by fine-tuning. Some examples are:
Classification tasks such as sentiment analysis are done like Next Sentence Prediction classification, by adding a classification layer on top of the Transformer output for the [CLS] token.
Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.
Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.
Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.
The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.
ALBERT (A Lite BERT)
ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.
To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.
In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.
DistilBERT (Distilled BERT)
DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERTs performances. It reduced the number of layers in BERT by a factor of two.
DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.
RoBERTa (Robustly Optimized BERT pre-training Approach)
Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.
To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERTs pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.
ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.
This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.",,task,the encoder model is best suited for what?,1,1,0.0,237.7286529541016
75,2445,2460,151,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.
Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.
One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.
Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.
The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.
Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.
Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.
The are several measures to describe the spread, variability, or dispersion of a dataset
Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.
Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is
\\[\\sum_i\\frac{\\mid x_i - \\mu\\mid}{N}\\]
where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.
This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is
\\[\\sigma^2 = \\sum_i\\frac{(x_i - \\mu)^2}{N}\\]
Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.
A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.
Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.
Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:
Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.
Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC)
Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.
Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.
Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \ight |\\leq 1\\)
The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).
More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.
A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.
Figure 2. Scatterplots for Four Hypothetical Datasets.
Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.
One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.","Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",dataset,what is the term used to describe a dataset?,1,0,12.222222222222197,38.60511016845703
76,1318,1328,97,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.
To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.
Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.
In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.
A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.
 Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.
Performing a hypothesis test involves three major steps:
Deciding what your Null Hypothesis, \\(H_{0}\\) and what your Alternative Hypothesis are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to cdisprove,d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.
Computing some sort of ctest-statisticd. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.
Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.
 It is important to note that crejecting the nulld does NOT mean caccepting the alternatived. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.
When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.
That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.
Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.
Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.
If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.
We could try to test if they are just different from each other with the following hypotheses:
\\(H_{0}: \\mu _{\\alpha}=\\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}\eq \\mu _{\\beta}\\)
Or we could test that one is strictly larger than the other:
\\(H_{0}: \\mu _{\\alpha}< \\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}> \\mu _{\\beta}\\)
The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.
In our case, we have two averages and want to look at their differences. For the students t-test, we shall use the aptly named ct-test statisticd:
\\[ T = \\frac{\\mu_\\alpha - \\mu_\\beta}{\\sqrt{\\frac{\\sigma^2_\\alpha}{n_\\alpha} + \\frac{\\sigma^2_\\beta}{n_\\beta}}} \\]
In particular, we are going to use what is known as cWelchs t-test statistic,d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:
\\[ \u = \\frac{(\\sigma^2_\\alpha + \\sigma^2_\\beta)^2}{\\frac{\\sigma^4_\\alpha}{n^2_\\alpha(n_\\alpha-1)} + \\frac{\\sigma^4_\\beta}{n^2_\\beta(n_\\beta-1)}} \\]
With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.
While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.
However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.
Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.
To help combat some of the issues with Welchs t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\ext{accuracy}\\).
If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:
\\(H_{0}: E _{\\alpha}=E _{\\beta}\\)
\\(H_{A}: E _{\\alpha}\eq E _{\\beta}\\)
For this test, our test metric is actually much simpler:
\\[ \\frac{(|E_\\alpha - E_\\beta|-1)^2}{E_\\alpha + E_\\beta} \\]
 In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.
Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.
While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.
On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welchs or another paired t-test variety.
However, both of these tests do suffer a single, incredibly critical flaw.
Namely, they suffer from cp-hacking.d
From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.
One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.
Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.
For our problem, this is especially grave. Consider that, for n models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.
Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?
To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.
 If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.
With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.
The hypotheses are the following:
\\(H_{0}: r_{1}=r_{i}=...=r_{k}\\)
\\(H_{1}:\\) They are not all equal.
and the associated statistic is:
\\[ \\frac{12n}{k(k+1)} \\sum_{i}^{k}(r_i - \\frac{k+1}{2})^2 \\]
For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.
With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.
There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:
Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.
Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.
Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.
According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:
Identify the population parameter of interest.
Determine whether you will be conducting a one-tailed or two-tailed test.
Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.
...then define an alternative hypothesis, denoted as \\(H_{A}\\). This would be the opposite of the null hypothesis.
The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?
Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.
Type I error occurs when you reject the null hypothesis when it should be accepted.
Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.
Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.
As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.
Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)
Here, given a z-score of -2.878, we can calculate the p-value as,
p-value = \\(2\imes P(z< -2.878)\\)
 Note: Since were conducting a two-tailed test, we can then multiply this value by 2.
If you locate -2.878 in a z-score table, you get a value of 0.002.
p-value = \\(2\imes 0.002\\)
p-value = 0.004
So, we have our p-value <  \\(\\alpha_z\\)
Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.","McNemar Test,P-Hacking",statistics,what is a hypothesis that must take a page from a different model to determine your performance?,1,0,12.13333333333331,102.424072265625
77,1599,1611,109,Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,"As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.
Data Scientist. This role involves solving business tasks using machine learning model development and statistical techniques. This individual identifies trends and patterns within the data and makes predictions based on trends. The data scientist will write code to support the data analysis and model-building process.
Data Engineer. The Data Engineer specializes in data structures and algorithms, as well as in working with data in databases and other large repositories.
Solutions Architect. This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.
Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.
Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.
Software Engineer. The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.
Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",,machine,what kind of learning is computer science?,1,0,0.0,95.58074188232422
78,1622,1634,110,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,"While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they dont take the context of relative positivity between words into consideration. For example, cgood food and terrible serviced and cterrible food and good serviced mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.
Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).
\\[ P(W_{n}|W_{n-1})=\\frac{P(W_{n-1},W_{n})}{P(W_{n-1})} \\]
Where the probability \\(P()\\) of a token \\(W_{n}\\) given the preceding token \\(W_{n-1}\\) is equal to the probability of their bigram \\(P(W_{n-1},W_{n})\\), divided by the probability of the preceding token.
Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences cBig blue skies look appealing.d and cColorless green ideas sleep furiously.d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one  a sentence she can say someone could use.
Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.
These days people build (classical) language models using well-established toolkits:
SRILM Toolkit: https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit
CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html
KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/
Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.",,sleep,d. c. and ccolorless green proposed how to dwhelm a sentence?,1,0,4.454545454545452,1067.8555908203125
79,2503,2518,152,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.
Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was madeunderstanding the why will make the problem clearer and affect problem-solving for future challenges.
Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.
Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).
The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.
Reading: Should you trust that model?
The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.
Local Surrogate Models""explain individual predictions of black box models.""
Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.
Additional Resource: Sara Hooker: The Myth of the Perfect Model
Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.
Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.
Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).
There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""
Hall (2016) has recommended the following steps:
Train black-box models and use them as benchmarks.
Use different regression techniques.
Use black-box models in the deployment process.
Train small interpretable ensemble models.
Create nonlinear predictors using black-box techniques.
Explain black box models better using variable importance measures.
Figure 1. Accuracy versus Interpretability (Source: Rane-20181)",,important,what is the most important thing that interprets interpretability of a model?,1,0,7.833333333333322,55.20365905761719
80,1058,1068,87,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Feature engineering is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering.
Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course)
Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.
Numerical Data Imputation. If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with "" - "" or ""NA"" can be replaced with a ""0"". Other methods used include using the median or mean values of that variable.
Categorical Data imputation. In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the cmost frequently  occurring value,d and you can impute cotherd in a situation where there is no dominant value in the categorical column.
Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.
In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.
You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the ""Exploratory Data Analysis"" module.
Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.
One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called dummy variables.
One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.
Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable chigh school,d which becomes one if the person has high school or 0 if the person has less than high school.
When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information.
When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.
Highly Satisfied
1
Satisfied
2
Somewhat Satisfied
3
Not Applicable
4
Dissatisfied
5
Highly Dissatisfied
6
When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.
Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.
There are instances that require the use of scaled data, including algorithms that use gradient descent
Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.
Quick thought: How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.
Normalization. This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled.
Standardization. Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks.
Reading: Data Preprocessing in scikit-learn.","Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",scaling,what categorization can be used to help categorizing the model?,1,0,0.0,75.65003967285156
81,3418,3437,207,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,"The problem underlying an analytic objective should satisfy the following criteria:
Solving the problem must be part of a possible solution vision toward the business objective.
Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.
The problem must be specific and realistic so that a corresponding data science project can succeed in principle.
First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.
As data-driven methods are still in the phase of being adopted across many fields, the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement. In such a case, it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond cwe want to leverage data somehowd and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.
An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.
Second, there must be a sound presumption that the problems solution must benefit from the use of data. A collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution, including data that would need to be collected as part of the project. In order to be suitable, the data should exhibit informative patterns relating to the problem. It takes a combination of substantive expertise and data science skills to assess this criterion.
Domain-specific problems around the availability of data include:
Lack of readiness in the organization (e.g., the organization's data is not readily processable), unsuitability of the data for the objective (e.g., data is old/stale, out of domain, incomplete, or suffers from an obvious bias), or difficulty in collecting data because the expert labor involved is too expensive.
Various technical objections, such as too little data for the required methods, fragmentation of data across multiple units with no suitable way of joining, or overwhelming imprecision/noise in the data. In cases where data is available, but the existence of a csignald for the problem is uncertain, exploring whether the signal exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself.
Typically, data science projects/consultations involve a preliminary data survey that informs or even precedes a longer substantive discussion. It is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client.
Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.",,processable,what is the difficulty in determining the application of a problem?,1,0,11.099999999999977,52.47465133666992
82,2292,2307,140,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.
Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.
[Required Reading] Paper: Briscoe, E., & Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff. Cognition, 118(1), 2-16. (Requires CMU credentials to access)
The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.
The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.
The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.
There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.
While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).
The paper presents a number of conceptual and empirical contributions:
The characterization of exemplar theory as the low bias, high variance extreme, and prototype theory as the high bias, low variance extreme on the bias-variance continuum.
A class of experiments to evaluate human learners position on this continuum when the complexity of the training data varies has not been systematically attempted before.
The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.
The experiment reported in the paper has the following phases:
Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.
Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.
Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.
Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?
Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?
The important findings from the experiment are as follows:
Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.
When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.
When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.
The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.
Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.
From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.
From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.","Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",set,what kind of questions does the concept of human learners have to learn about?,0,0,0.0,107.9697265625
83,2777,2792,163,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,"Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.
Figure 6: Transformer architecture.
Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.
Figure 7: Transformer Encoder Inputs.
Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.
The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.
The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.
The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.
Figure 8: Transformer Encoder Architecture.
Figure 9: Transformer Encoder-Decoder.
Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.
In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.
After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.
At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.
Here are some additional sources for more details on Transformers.
The Annotated Transformer
The Illustrated Transformer",,decoder,the encoder and decoders are in a transformer or what?,1,0,7.19999999999999,102.25985717773438
84,1987,2001,129,Data Science Project Planning,Requirements Gathering,What is a Requirement?,"The IEEE defines a requirement as a documented condition or capability needed by a user or system to meet a business need or achieve a business objective. Requirements become useful to the solution development process when they have been converted into specifications. Requirements must meet certain criteria to be useful for achieving business objectives. Let us define the general characteristics of a good requirement.
Reading: IEEE Guidelines on Software Requirements Gathering.
A good requirement should be complete and correct. You must identify the relevant stakeholders and define a set of needs, goals, and objectives for your project. As there is no perfect scenario, you must define the constraints that are applicable to a project. Those constraints might include cost, scope, existing systems and processes, time, and technology. Defined scenarios can also result in identifying all the stakeholders and their needs within the business context. A requirement should also be traceable. This refers to tracking the life cycle of a requirement from its development to its specification and deployment in various versions of the solution. Traceability can be supported in a straightforward way via a matrix which associates a unique identifier, a description, and a design specification element to each requirement, as shown below.
Requirement
Design Specification
1.0.0 Consumption reports should be integrated with the Dashboard API.
Data from the hourly consumption report will use data_integrate_trail in the Update event procedure.
A requirement should be unambiguous. Given that the requirements gathering process involves contributions from multiple stakeholders, the requirement should be explicit and clear to all. It should have the same meaning to everyone involved and not be open to interpretation. Unambiguous requirements must have defined acceptance criteria, metrics for success, expected outcomes, and acceptable values. The use of an active voice in its description will make a requirement clearer. Finally, a requirement should be verifiable. Testers should be able to verify that the requirement is implemented correctly. A requirement is considered complete when it is verifiable, unambiguous, and traceable.
Consider this requirement for a report-generating solution:
This requirement leaves room for interpretation, and this can lead to not meeting client expectations. A better representation of the requirement is to include a time frame, a responsible party, and a deliverable.
Reading: Traceability in Requirements and Other Artifacts.","Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",defined,unambiguous requirements must be included in the requirements for a mission to a project?,1,0,3.4166666666666683,92.7027816772461
85,3633,3656,219,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",vowel,what part of a language can be written without a single syllable?,1,0,16.272727272727334,25.53314971923828
86,629,634,46,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"The CNN architecture incorporates a number of layer types as follows:
The input layer accepts a 3D matrix of size W1
The convolutional layer accepts a 3D matrix of size W1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K \\]
The pooling layer accepts a 3D matrix of size W1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F}{S}+1, \\quad H_{2}=\\frac{H_{1}-F}{S}+1, \\quad D_{2}=D_{1} \\]
The fully connected layer is identical to a fully connected network layer. It accepts a K-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).
LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.
Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.
Figure 10. Data Flow of AlexNet
AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.
Figure 11. Image filters learned by the first layer of AlexNet.
Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.
Figure 12. Comparison of AlexNet and VGGNet.
VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).
He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.
Figure 13. Comparison of Normal CNN layer and Residual layer.
Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.
Figure 14. ResNet-18 architecture.
Now that we have seen several milestone architectures for CNNs, let's now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a 2017 paper by Canziani et al., shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.
Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017)","LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",vggnet,what was the first successful application of the q1 system?,1,1,0.0,114.693603515625
87,710,716,51,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Reminder: The data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client. The requirements gathering process will involve identifying the stakeholders, eliciting needs, and defining requirements. The difference between the requirements of a traditional IT project and those of a data science project is the focus on the requirements for the analytic solution.
Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.
Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.
Traditional functional requirements considered in the software and application development process include Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Requirements, Administration Functions, Authorization Requirements, and Data Management.
Reading: IEEE ANSI 830 Documentation on Functional Requirements.
Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.
Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.
The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:
Figure 1. Use Case Specification
A business analyst (BA) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers. The BA analyzed the information from the questionnaire and defined some user requirements. One of the user requirements is customers ability to update their billing address in the new system. This requirement describes what customers can do with the solution, but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement. We will illustrate how we can simply decompose that user requirement into functional requirements.
Use Case/User Requirement: Update billing address.
(1) The user shall be able to view the billing addresses in the system.
(2) The user shall be able to update a billing address in the system.
(2) The system shall display updated customer service and billing addresses.
Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.
Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.
Availability
Refers to a property of software that is there and ready to carry out its task when you need it to.
Interoperability
Interoperability refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context.
Modifiability
Modifiability refers to the ease of modifying the system with minimal changes to the architecture.
Performance
Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.
Security
Security refers to the systems ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized.
Testability
Software testability refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing. Specifically, testability refers to the probability that the software will fail on its next test execution, assuming that it has at least one fault.
Usability
Usability is concerned with how easy it is for the user to accomplish the desired task and the kind of user support the system provides. Over the years, a focus on usability has shown itself to be one of the cheapest and easiest ways to improve a systems quality (or, more precisely, the users perception of quality).
Scalability
Scalability refers to the ease of adding new resources to a system to cope with increasing demands on its use.
Observability / Monitorability
These refer to the ability of the operations staff to monitor the system while it is in operation.
Portability and Compatibility
These refer to the compatibility of the hardware, systems, application software, and solution with other applications and processes within the existing environment.
Modules and Architecture
These refer to the technical considerations for the system, including operating system compatibility and the programming development environment employed by the developers.
Non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules. Framing a non-functional requirement for a data science project will include the above-mentioned attributes and additional requirements that are related to machine learning models and AI analytic solution(s).","Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",operating,what type of requirements are used to define the requirements for the application of a computer?,1,0,8.615384615384603,38.3936653137207
88,3288,3307,198,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.
Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.
Figure 1. Scenes from the Tuskegee Syphilis Study. (Source: https://www.rmpbs.org/)
The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.
By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.
In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.
The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.
Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.
There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.
In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.",,information,what does the research on the genetics of the human subjects subject to?,1,0,0.0,114.2897720336914
89,790,796,54,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,"It would be helpful to think about what you would like to showcase in your diagrams. The following classifications of diagrams can help you get started.
Modules And Their Relationships - How code is structured
Runtime Components and Connections - How data flow during runtime
Deployment Diagrams - What is Infrastructure for the architecture
You can use these types of diagrams at any layer of abstraction, i.e., you can use these types of diagrams to describe  a high-level view of the system or a sub-system, as is apparent in the exemplar documentation.
These diagrams are useful if you would like to show modules and their relationships with each other. Modules represent a static way of structuring the system. In these diagrams, we do not care much about how the system behaves at runtime.
Modules allow us to answer questions like:
What are the different business functions in your system?
What other modules does each business function depend on?
Are there any external dependencies for each module?
Does any module inherit behavior from another module?
Figure 1. Execution Graph of a Completed Run of the Continuous Training Pipeline (ACAI, MCDS Capstone Project, 2020)
Notice how model dependency is represented in this execution graph, as it is important to this project.
When you want to showcase how a system functions during runtime, you can use Components and Connectors. Each runtime element is related to another element via a Connector, which should be adequately described in your writeup as well as in the legend.
Examples of runtime elements, i.e., components are:
Services
Peers
Clients
Servers
Filter Systems
Examples of connectors also called vehicles of communication are:
Call-return style connector
Data pipelines
Process synchronization operators
These diagrams help us answer the following type of questions:
What are the major components of your system at runtime?
Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?
How does data progress through the system?
Does anything run in parallel?
This is a good example of using components and connectors to provide a high-level view of the flow of data. We can see that connection types have been labeled separately to represent the different types of data transfer that can happen in the system.
Figure 2. A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020)
Representing deployment models can be confusing because they look very similar to runtime diagrams.
The important distinction between the two is that deployment diagrams are meant to display the interaction of the solution with non-software structures like CPUs, file systems, networks, development teams, etc.
In a deployment model, you will need to make infrastructural considerations for your solution. Your deployment model will help answer questions like:
What cloud instance type does your solution execute on?
What type of cloud data store are you expected to use?
Who deploys the solution?
What type of queuing system are you expected to use?
In the following example, we know that we have docker containers that interact with a Job Monitor, a Log Server, and a Launcher.
A Job Registry requires the use of an SQL server. In your diagrams, you can be specific about particular SQL servers like MySQL/Postgres if the requirement is clear.
Figure 3. Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020)",Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,data,what does the system use to tell the difference between different system systems?,1,0,0.0,42.38520812988281
90,4025,4049,240,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.
Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.
Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).
Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.
Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1
How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.
Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.
Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.
Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.
Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).
Additional Reading: Survey of Active Learning. This report gives an in depth review of active learning in machine learning and artificial intelligence.","Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",expected,what kind of model change would be used to identify the current model?,1,0,0.0,56.66844940185547
91,188,191,23,Collecting and Understanding Data,Data Collection,Validity and Bias,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.
Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.
Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.
There are four types of validity:
Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.
Construct validity refers to the validity of inferences about the constructs (or variables) in the study.
Internal validity relates to the validity of inferences drawn about the cause-and-effect relationship between the independent and dependent variables.
External validity refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures.
In this module, we will discuss external and internal validity in more detail.
As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.
Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.
As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.
Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.
Type of Threat to Internal Validity
Description
Suggested response by Data Scientist
History
Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.
The data scientist can have the control and experimental groups experience the same activities (except for the treatment) during the experiment.
Maturation
Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.
A careful selection of participants who mature or develop in a similar way for both the control and experimental groups helps guard against this problem.
Regression to the mean
Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.
The data scientist can select participants who do not have extreme scores as entering characteristics for the experiment.
Selection
Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)
Random selection may partly address this threat.
Mortality (also called study attrition)
Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.
The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.
Diffusion of treatments (also called cross-contamination of groups)
Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.
The data scientist must keep the two groups as separate as possible during the experiment.
Compensatory equalization
When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.
The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.
Compensatory rivalry
Participants in the control group feel that they are being devalued, as compared to the experimental group, because they do not experience the treatment.
The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.
Resentful demoralization
When a control group is used, individuals in this group may become resentful and demoralized because they perceive that they receive a less desirable treatment than other groups.
The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.
Testing
Participants become familiar with the outcome measure and remember responses for later testing
To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.
Instrumentation
The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.
The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.
Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.
Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.
Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.
Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.
Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.
Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.
Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.",,group,what type of group is used to determine the validity of the test?,1,1,0.0,22.389841079711918
92,778,784,52,Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]
The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.
The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Googles services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.
The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.
Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.
There are two primary innovations from the paper.
Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.
Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.
The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):
Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., \\(PE_{(pos, 2i)}\\) or \\(PE_{(pos, 2i+1)}\\) to yield the word vector with positional information for each token.
Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.
Step 3 is similar to Step 1 but carried out on the output sequence tokens.
Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).
Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).
Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.
During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.
The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.
The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.","Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",inputs,what part of the paper does dmor construct before cliker?,1,0,0.0,853.700439453125
93,2715,2730,159,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.
Lets dive into how Agile is usually implemented in the industry and how to apply it to a data science project.
Lets first understand the roles in the Agile Scrum framework:
Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.
Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.
Team members. The people on this team are the ones who execute the work in each sprint. These teams, usually of three to seven people, can be composed of different specialties and strengths, or they can be teams of people with the same job roles.
Stakeholders. This is an informational role only. The stakeholders should be kept up-to-date on the product and sprint goals, have the opportunity to review and approve work during a sprint and provide feedback during the sprint retrospective.
User stories: A user story is simply a high-level definition of a work request. It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request. This short, simple description is written from the users perspective and focuses on outlining what the stakeholder wants (their goals) and why.
For example, if you are building an application that guides a user with directions, here is an example of a user story:
As a user, I want to be able to navigate my way using the directions so that I can reach my destination.
As a user, I want to be able to change the destination so that I can reach a new destination.
Sprints:  Sprints are a short span of work, usually taking between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to repeat these sprints until your product is feature ready continuously. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service. This feature helps you build products that generate value for the stakeholder.
Stand-up meetings: Daily stand-up meetings (typically under 10 minutes), also known as cdaily Scrum meetings,d are a great way to ensure everyone is on track and informed. These daily interactions are known as cstand upd because the participants are required to stay standing, helping to keep the meetings short and to the point. Regular sync-up helps in mitigating any new challenges early on.
Agile board: An Agile board helps your team track the progress of your project. This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software (like JIRA). If you love sticky notes, you can, for instance, consider MURAL to build your board virtually, and all pending activities can go onto this board.
Backlog: As project requests are added through your intake system, they become outstanding stories in the backlog. During Agile planning sessions, your team will estimate story points for each task. During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration. Managing your backlog is a vital role for project managers in an Agile environment. In a project team, this will be the teams collective effort.
Sprint retrospective:  This is a meeting where the team comes together to recognize what has worked and what has not worked. The team makes efforts to make sure the concerns are dealt with to ensure the smooth completion of the project.
Demonstration: This is when the team demonstrates a working product to the stakeholders in order to show them the value generated from the project and to keep them happy.",,usually,how are agile scrumps different from other companies?,0,0,0.0,538.5184326171875
94,3352,3371,200,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?
A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.
When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.
Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.
Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018))
Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.
During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.
Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.
Scalar: Single numeric feature, e.g., mass.
Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.
Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.
In machine learning, the input to a model is represented as a numeric vector.","Raw Data to Features,Features",customer,who does a solution to the problem of identifying segments of a customer base?,1,0,5.8333333333333295,116.25775909423828
95,2217,2232,139,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,"When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied  Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.
You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.
Clustering (Source: www.pyarmy.com)
Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.
Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.
Overlapping Clustering allows data to belong to more than one cluster.
Hierarchical Clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.
Reading: An Expansion on Clustering.
This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:
A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.
Each data point is then assigned to the cluster with the nearest mean.
After a pass, using the points assigned to a cluster, new means are computed.
The last two steps are repeated.
The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.
How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.
k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.
Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.
When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the ""elbow"" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.
Elbow Method
Reading: k-Means Clustering-sklearn.
Additional Reading: K-Means Clustering Algorithm
K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.
K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.",,wcss,what method is used to identify cluster analysis?,1,0,0.0,136.63401794433594
96,1531,1541,100,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.
At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.
Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.
Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.
Despite its importance, data wrangling presents some challenges that are common in data science projects.
So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.
Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.
A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.
The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.
Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.
Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:
Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.
The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.
Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.
In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.
Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.
In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.
Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.
\\(z = (x-\\mu )/\\sigma\\)
The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!
The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task
One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.
In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.
Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.
Missing at random (MAR) is the probability that the missing data is the same within certain groups.
Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.
The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.
Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.
Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.
Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.
You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.
Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.
When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.
As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.
Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.
Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.
Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.
Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.
Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:
For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).
One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.
Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.
Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.
An exampleof a popular quantitative transformation is converting the date of birth to age.
Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.
Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.
Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.
Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.
Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.
In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.
The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.","Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",usage,data from what can be used for enterprise - wide access?,0,0,0.0909090909090909,367.18267822265625
97,1090,1100,88,Data Science Project Planning,Design and Plan Overview,Module 6 Summary,"It is necessary to establish a design for the project as it can help identify various bottlenecks of the project. This documentation makes the developers begin thinking about the various risks and challenges involved in the data science project. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, and the system environment. A clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams. The developers are also prompted to consider the many risks and difficulties that the data science project may present.
Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:
User stories: A high-level definition of a work request.
Sprints: Short spans of work where teams work on tasks determined in the sprint planning meeting. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service.
Stand-up meetings:  These are daily stand-up meetings where participants are required to stay standing, helping to keep the meetings short and to the point.
Agile board: This is a tracking system that helps your team track the progress of your project.
Backlog: This is the set of outstanding project requests that are added through your intake system. Managing your backlog is a vital role for project managers in an Agile environment.
Sprint retrospective: This is a meeting where the team gathers to discuss what went well and what didn't.
Demonstration: This is when the team demonstrates a working product to the stakeholder.",,deliverable,what kind of dates were used to classifying the software?,1,0,0.0,140.3325653076172
98,2319,2334,142,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.
The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.
The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.
Figure 1: Traditional Sequence2Sequence model architecture.
A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.
Figure 2: Limitation of the traditional Sequence2Sequence model architecture.
The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this cbottleneck problemd.
Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.
In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.
Figure 3: Attention between words in a sequence.
In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of cattentiond to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.
In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.
Figure 4: Attention in Seq2Seq RNN architectures.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",,decoding,what does the encoder take its word embedding?,1,0,0.0,266.64404296875
99,3098,3116,184,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,"Natural Language Processing (NLP hereafter) is a subfield of computer science aiming to  build  systems that:
Enable human-computer communication
Enable/enrich human-human communication
Perform tasks requiring the use of human language faculty, more efficiently and more correctly.
Here are some applications of NLP in these three areas:
Human-computer communication
Dictating email messages
Listening to email messages
With proper pronunciation, tone, and stress!
Using human language to give commands to the operating system
Human-human communication
Real-time text and speech translation
Summarizing meeting conversations
Enabling communication with people with hearing/vision impairments
Improving the efficiency of tasks requiring human language faculty
Correcting typing/grammatical errors
Question answering
Answering e-mails automatically
Searching large document databases
Here are some (and definitely not a comprehensive set of) applications of  NLP in a diverse set of areas:
Information extraction (IE), especially copend IE:  Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.
Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.
Question answering beyond finding the documents relevant to the question and instead delivering the actual answer.
Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.
Image (and eventually video) understanding:  Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).
Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.
Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).
Summarization:  Generating a short summary of the salient points of a document or a set of documents.
Opinion and sentiment analysis:  Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.
Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.
Fake news or fake review detection:  Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.
The following figure gives a birds eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.
From Eduard Hovys cThe Past and 3\xbd Futures of NLPd presentation.
There are three high-level functions that NLP tries to automate:
Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.
Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).
Acquisition: obtaining the representation and necessary algorithms from data (e.g., learning to translate from aligned translated sentences).
The representation that these employs depends on the actual task being solved:  they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.
NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.
Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",,sciences,what field of study does the work of natural language processing belong to?,1,1,0.0,40.03981018066406
