{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-3 Question Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import All Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input User's OpenAI API key to enable querying fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '' #API Key Here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read In CSV File to Pandas Dataframe and Prepare for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filepath): #file path to a csv file where the last column is model generated questions\n",
    "    \"\"\"\n",
    "    Takes in filepath leading to csv file containing automatically generated questions and prepares a dataframe for GPT-3 Prediction\n",
    "\n",
    "    filepath: path to a csv file where the final column contains generated questions. \n",
    "    \n",
    "    return: tuple(data, question_column) data is a dataframe generated from the input csv file with rows dropped that do not contain a question. question_column\n",
    "    returns key name for column containing generated questions for use later in prediction task. \n",
    "    \"\"\"    \n",
    "    data = pd.read_csv(filepath) #reading in file to a pandas dataframe\n",
    "    question_column = data.keys()[-1] #getting the key name of the column contatining questions\n",
    "    data = data.dropna(axis=0, subset=[question_column]) #removes any row from the dataframe that doesn't contain a generated question\n",
    "    return data, question_column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, question_column = prepare_data('filepath.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run predictions using fine-tuned GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, question_column, model, delay=False):\n",
    "    \"\"\"\n",
    "    Takes a single row from a dataframe which contains a generated question and returns the prediction of a fine-tuned GPT-3 model\n",
    "\n",
    "    row: row of dataframe containing generated question\n",
    "    question_column (string): key name of dataframe column containing questions\n",
    "    model (string): OpenAI model identifier \n",
    "    delay (boolean): Enables sleep function. Only necessary if utilizing free tier of OpenAI API access as it limits requests per minute\n",
    "\n",
    "    return: Prediction from GPT-3 Model. 1 corresponds to a prediction of 'sound', 0 corresponds to a prediction of 'not sound' \n",
    "    \"\"\"  \n",
    "    question = row[question_column]\n",
    "    ft_model = model\n",
    "    res = openai.Completion.create(model=ft_model, prompt=question, max_tokens=1, temperature=0, logprobs=2)\n",
    "    if delay:\n",
    "        time.sleep(1)\n",
    "    return res['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data, question_column, model, delay=False):\n",
    "    \"\"\"\n",
    "    Takes in dataframe returns the prediction of a fine-tuned GPT-3 model\n",
    "\n",
    "    row: row of dataframe containing generated question\n",
    "    question_column (string): key name of dataframe column containing questions\n",
    "    model (string): OpenAI model identifier \n",
    "    delay (boolean): Enables sleep function. Only necessary if utilizing free tier of OpenAI API access as it limits requests per minute\n",
    "\n",
    "    return: Dataframe with predictions in new column named 'predicted_label' \n",
    "    \"\"\"  \n",
    "    data['predicted_label'] = data.apply(lambda row: predict(row, question_column, model))\n",
    "    index_names = data[ (data['predicted_label'] == 1) or (data['predicted_label'] == 0)].index #removes rows where prediction model makes an incorrect predicition (not a 1 or 0)\n",
    "    data.drop(index_names, inplace = True)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example implementation and write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ada:ft-personal-2023-04-24-01-17-14'\n",
    "df_with_preds = make_predictions(data, question_column, model)\n",
    "df_with_preds.to_csv('filepath_with_predictions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Counts of Sound and Not Sound Predictions by GPT-3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_count = len(df_with_preds.query(\"predicted_label == 1\"))\n",
    "not_sound_count = len(df_with_preds.query(\"predicted_label == 0\"))\n",
    "percentage_sound  = sound_count / (sound_count + not_sound_count)\n",
    "print('Count of sound predictions: ' + str(sound_count))\n",
    "print('Count of not sound predictions: ' + str(not_sound_count))\n",
    "print('Percentage rated sound: ' + str(percentage_sound) + '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly Sample N Rows from Dataframe and Write to new CSV File for Human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "rand_samples = df_with_preds.sample(N)\n",
    "rand_samples.to_csv('filepath_with_human_eval')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
