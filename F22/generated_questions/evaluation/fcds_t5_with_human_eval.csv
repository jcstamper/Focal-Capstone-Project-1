,Unnamed: 0,Unit,Module,Title,Subheaders,Cleaned Text,Paragraph,Generated Question,predicted_label,human_eval
909,911,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"just as the methods proposed must be suitable to achieve the analytic goal so must the data with which the methods are combined the most important criterion in this regard is of course that the data is presumed to contain patterns that are informative for the analytic objective and allows one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way beyond this main requirement and depending on the project context and proposed methods specific data sources can be more or less suitable for the project possible considerations include:
is the dataset of the appropriate size for the proposed method how many data points and how many features does it contain
what distribution do the individual phenomena in the data follow
is the dataset raw or readily processable what preprocessing is necessary and how will it influence the relevant patterns
is the data complete or are some parts of it missing
is the dataset clean or noisy does measurement error play a role
are there access or disclosure restrictions to the data is it confidential private sensitive partially redacted or classified is it subject to licensing restrictions
well-studied benchmark datasets are available to the data science community for many tasks while prior work on a dataset is a good indicator of its utility for a task it cannot replace the process of familiarizing yourself with it before commencing serious experimentation work it is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal this is typically done through research and preliminary data surveys possibly in collaboration with domain experts
while statistical analysis and machine learning are of course core pillars of data science effective collection curation and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects as such the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis
when proposing data collection as part of an analytical objective the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource data collection especially involving human annotators is its own research field relevant considerations include:
how well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem
does the collection require human annotators can it be done using crowdworkers
what qualifications do the human annotators need to possess how can they be effectively trained for the task
do the human annotators need to be examined/tested before and/or after collection
how should the annotation task be designed to ensure productive and correct annotation how will agreement between annotators be measured
how much data should be gathered to enable progress towards the analytic objective how much data can be gathered given the budget
once the data has been gathered what cleaning and curation needs to happen
are there any approvals/permissions that need to be obtained before the collection can proceed (eg human subject research)
this course includes an introduction to data collection methods for purposes of this unit the main thing to take away is that collecting good datasets requires an amount of skill care and attention to detail comparable to those needed for doing good data analysis like core machine learning efforts data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template",Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?,How many data points does the dataset contain?,1,1
1362,1365,Analytic Requirements Gathering,Requirements Overview,Overview,Requirements Management Plan,"as you have learned in a previous unit gaining an understanding of the business need and formulating project objectives (business and analytic) are best practices for successfully meeting client expectations once the project objectives have been defined the project team will identify the needs and constraints of stakeholders and the current system or environment related to the business need; this is referred to as the requirements gathering process
requirements gathering establishes communication between the solutions development team (referred to as the project team in this unit) and the business stakeholders the objective of the requirements gathering process is to define the system inputs processes outputs and interfaces when properly done requirements gathering will positively influence the outcome of a project requirements gathering can also help the project team to visualize the intended solution
the requirements gathering process involves input from the system users and system owners a system user interacts with the solution and a system owner (who can also be a user) is an official who is responsible for decisions made about systems within their organization in this unit we will refer to these roles as stakeholders
similar to traditional software development projects data science projects are also guided by requirements gathering principles figure 3 lists the steps that are followed during the process requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s)
figure 3: requirements gathering process
the requirements gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data related project
gather information: the first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business need as well as decision makers within the business once stakeholders are identified the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives
 later on in this unit we will discuss the techniques used to gather information
define and prioritize requirements: stakeholders will provide information according to their view of the business needs it is the job of the business analyst to lead the effort in defining and prioritizing requirements it is important to document ""complete"" requirements that capture the needs of the stakeholders as this will guide the project team in developing the right solution(s) stakeholders might provide information that can be used for future projects related to the proposed solution that information should not be discarded it is prioritized as a low priority requirement and considered for future implementation
evaluate requirements: the project team must verify and validate all documented requirements this additional step in the requirements gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders
receive sign-off: this is an indication that the requirements have been approved and agreed upon by the client requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution
a requirements management plan can be used to document the requirements gathering process this document is made available to the client and the project team as it contains information that affects both parties there is no standard template for this document but it is in good practice to include the following sections:
project description is an overview of your project this section describes the purpose of your project
team responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements gathering process data science project team members might take on duties outside of their normal roles eg a data analyst on the data science team might serve in the role of scribe during joint application development sessions
tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture manage and track requirements through the requirements gathering process and throughout the project lifecycle
requirements gathering process should be defined in this plan this section will describe the techniques used in eliciting user and system needs defining the requirements and evaluating the success of the requirements gathering process (these techniques are covered later in this unit)
change control and requirements gathering: modern day collaboration tools will support change control however a change control process should be documented to ensure that changes to requirements are formally managed","Define and Prioritize Requirements: Stakeholders will provide information according to their view of the business needs, it is the job of the business analyst to lead the effort in defining and prioritizing requirements. It is important to document ""complete"" requirements that capture the needs of the stakeholders, as this will guide the project team in developing the right solution(s). Stakeholders might provide information that can be used for future projects related to the proposed solution, that information should not be discarded, it is prioritized as a low priority requirement and considered for future implementation.",When should information be used for future projects related to the proposed solution?,1,1
1972,1975,Model Evaluation,Evaluation Metrics,Clustering Evaluation Metrics,,"the previous page focused on the metrics for evaluating supervised learning problems  the presence of labeled data makes it somewhat straightforward to train and test the model's performance now we will focus on metrics that can be used when labeled data is not present  there are two approaches to evaluating clustering the internal and external evaluation approaches the internal approach involves summarizing the clustering task to a single quality score while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable clustering can also be evaluated by an expert
internal evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score the cluster with the best score is seen to be the best internal evaluation although useful can have its drawbacks it gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters  a sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters and many evaluation indexes assume convex clusters on a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity is sound
let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
silhouette coefficient shows how similar a data point is to its cluster compared to other clusters it is calculated using the mean intra cluster distance and the mean nearest cluster distance for each data point a silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster when the silhouette coefficient is close to 0 there is a presence of overlapping clusters
dunn index is also used to evaluate clustering techniques and similar to the silhouette coefficient it is dependent on the data within the clusters a good clustering is one with a higher dunn index when using this evaluation technique you want to be aware of a high computational cost when you have a large number of clusters the dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters the minimum of the pairwise distance is used to determine minimum separation (minseparation) the compactness of a cluster is measured by computing the distance between the data in the same cluster (maxdiameter) finally the dunn index will be:
minseparation/maxdiameter
external evaluation measures the results from a clustering task based on data not used for the clustering task benchmarks are set from a set of pre-classified data external evaluation techniques need ground truth data to evaluate clustering
rand index tells you how similar a cluster or clusters are to a set benchmark this is similar a classification evaluation technique you can calculate the rand index thus:
(tp + tn)/(tp+fp+fn+tn)
purity is considered a no frills technique that assigns each cluster to a class (usually one that occurs often in the cluster) the number of correctly assigned observations is divided by the overall number of observations to determine accuracy purity close to 1 is best and close to 0 is not optimal a large number of clusters can lead to a higher purity there is a tradeoff between quality of clustering and number of clusters when using purity as a metric the normalized mutual information (nmi) can be used to measure and compare the quality of clustering between different clusterings with varying number of clusters
jaccard index is used in cluster analysis evaluation and convolutional neural networks among others it is defined as ""the size of the intersection divided by the size of the union of the sample sets"" the jaccard distance measures dissimilarity between sample sets
f-measure is simply computed as the 2 * ((precision * recall)/(precision + recall)) you might remember it from the classification metrics it is also known as the f1 score
dice index also known as the sorensen-dice indexor dice coefficient can assess the similarity of two samples it ranges from 0 to 1 dice index is a semimetric version of jaccard index and gives less weight to outliers in a dataset it is used to measure the lexical association score of two words
reading: clustering evaluation techniques (20min read)
you should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis this is sometimes done by comparing a dataset against another (one without clusters) the hopkins statistic is used to measure cluster tendency a resulting value of 1 or close enough will show that the data is clustered data that is uniformly distributed will be closer to 0 hopkins statistic is quite good at estimating randomness in a dataset",min.separation/max.diameter,What does min.separation/max.diameter do?,1,1
1254,1257,Analytic Algorithms and Model Building,Data Science Patterns,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","let us assume that you have a medical dataset that contains observations with features including patient age weight height sex and race; you have been presented with the task of identifying the category of diagnosis for these observations in your dataset (the category would be diabetic or not) it would take a long time for you to research each observation and compare their features and symptoms to classical symptoms of diabetes using a data science approach you can assign a diagnosis to each observation based on the historical data for that diagnosis you would be answering a classification problem classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new dataset below you will find the different types of classification problems then later in the course we will explore methods that can be used to solve classification problems
classification tasks that are binary will classify observations in a dataset into two defined categories the observations are grouped based on the presence of characteristics unique to one of the two categories examples can include a decision on a credit card application (ie approve/deny)
also referred to as multinomial classification  classes in a classification task are three or more; observations can be classified into one of the three or more classes each observation can only be classified as one of the multiple classes an observation can not be labeled to multiple classes at once if a fruit image dataset is presented as a classification task a valid assumption would be that the observations will be labeled as one type of fruit from the multiple classes if the labels are orange pineapple peach and mango then each observation can only be classified as one type of fruit
you want to be careful not to confuse multi-label classification and multi-class classification to have the same meaning unlike multi class classification that has an assumption of observations having one class out of the multiple classes; the multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification
a quick thought: can you think about a scenario were observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)
multi-label classification can be applied to classifying textual data if you watch movies you know that some movies can belong to multiple genres eg romantic comedy romantic drama thriller comedy
let us stick with this example and conceptually define how a multi-label classification task would pan out
you are tasked to classify movies based on their plot we can assume that we have defined our analytic objective defined our requirements and we have gathered and prepared our data when you classify the observations in this dataset you might find movie a will belong to romance and comedy let us see the different multi-label classification techniques and how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardize the performance of your model
multi-label classification does not have constraints on the labels that an observation can have and this makes it difficult to learn using the onevsrest technique your classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes
similar to onevsrest the binary relevance technique trains a single label binary classifier for each class ie for each class an observation will either be predicted as belonging to that class or not this technique ignores any correlation between classes
the algorithm for your classification task can also adapt the algorithm to perform multi-label classification a popular example is using a multi-label version of the k-nearest neighbors (a supervised learning technique that makes the assumption that similar data points are always close together)
example: scikit-multilearn for mlknn
you can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier
x
y1
y2
y3
x1
0
1
1
x2
1
1
0
x3
1
1
0
x4
1
0
1
here we see that observations x2 and x3 belong to the same classes this technique will transform our task into a single multi class task and give a unique class to all possible combinations in your training data set
x
y1
x1
1
x2
2
x3
2
x4
3
so far we have seen that onevsrest binary relevance and label powerset techniques do not consider correlations between classes the classifier chains technique will build a chain of binary classifiers to take into account any correlations between classes the classifiers that are constructed equal the number of classes ie if we have classes: comedy drama and romance we will have three classifiers as well c1:c3
we should mention logistic regression in this section because it is an important classification technique you will learn more about it in the next module logistic regression uses a logistic function to model the probability of a class or event will you pass or fail a course will you develop high blood pressure based on certain attributes will 18-35 year old college educated men from georgia vote for the democratic or republican presidential candidate in the 2020 presidential elections the logistic regression model can have independent variables that are of diverse data types but the response is categorical",1,How many questions do you have?,0,0
1839,1842,Model Evaluation,Evaluation Metrics,Classification and Regression Metrics,"Classification Metrics,MCC Formula,ROC-AUC Source,Regression Metrics","the metrics used to evaluate the results of your task is of great importance the performance of your algorithms need to be measured and compared to ensure that you select the right algorithm
confusion matrix is quite easy to interpret and it is straightforward in its duties this metric simply shows how well a classifier has performed it is a simple visualization of the task's performance you might also find that it is referred to as a contingency table confusion matrix can present the prediction results of a binary or multi class classification problem as shown in the example below the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category those categories are defined as follows:
true positives (tp) is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values
true negatives (tn) indicates that the classifier correctly predicted no-event values an observation in the negative class is correctly classified as being in said class
false positives (fp) is an error in which a classifier improperly indicates classifies as observation in the positive class when in reality it belongs to the negative class this is considered a type i error and is considered a false alarm the conditional probability of a positive test result given an event that was not present is called the false positive rate
false negatives (fn) is an error in which a classifier improperly classifies an observation in the negative class when it belongs to the positive class this is considered a type ii error the conditional probability of a negative test result given an event that was present is called the false negative rate  this error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society the premise of blackstone's formulation supports the above claim it states that ""it is better that 10 guilty persons escape than that one innocent person should suffer"" it can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease they could possibly die (or spread said disease) because of this type ii error
let us use the example of predicting the diagnosis classes for a dataset with 300 observations the matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes the table below shows a perfect classification exercise it goes without saying that this can not be the case when real data is introduced to a trained classifier the confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model
diabetic
not-diabetic
diabetic
110
0
not-diabetic
0
90
accuracy is simply a measure of how accurately the classifier performed with classifying data  accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier's performance
recall sometimes referred to as sensitivity is the true positive rate and is calculated as:
# of true positives/ (# of true positives + # of false negatives) it is not holistic because it does not account for the false positive and true negative
recall = tp/(tp+fn)
specificity is the true negative rate and similar to the recall it can result in biased results it is calculated as # of true negatives/(# of true negatives + # of false positives)
specificity = tn/(tn + fp)
precision is the positive predictive value and calculated as:
# of true positives/ (# of true positives/# of false positives)
precision = tp/ (tp + fp)
f1 score: all the metrics above were highlighted to introduce biased results because they do not account for all four rates the f1 score will is calculated as:
2 * (precision * recall) / (precision + recall)
matthews correlation coefficient will solve for the issue of biased results it is often used to assess the quality of a binary classification model it is a correlation coefficient between the observed and predicted binary classification a value of +1 means a perfect prediction 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events and finally -1 means the classifier misclassified observations mcc is considers symmetric meaning that no class is more important than another (switch negatives and positives and the result will remain the same)
resource: mcc-scikitlearn
logistic loss (log loss) is a metric that evaluates the predictions of probabilities of an observation's membership to a specific class the prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value log loss will ""take into account the uncertainty of a prediction based on how much it varies from the actual label while accuracy is the count of predictions where the predicted value equals the actual value ""
resource: the math behind log loss
receiver operating characteristic (roc) curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds it is the comparison of two operating characteristics the roc curve has been useful for many years according to research it was developed and used by engineers during the world war ii it was used for battlefield detection analysis and has been used in many fields its most prominent use in recent days is for machine learning and model performance assessment
consider the roc curve to be ""sensitivity as a function of false positive rate"" it can be used to select the optimal models
area under the roc curve otherwise known as auc measures the entire area underneath the roc curve and it is the measure of the classifier's ability to distinguish between classes it also provides a measure of performance across different thresholds the auc measures how well predictions are ranked and the quality of the prediction auc might not be useful for certain scenarios such as it does not tell you much about the ""cost of different errors"" it gives similar weight to errors the general interpretation of the chart is that the higher the auc the better the model is at its task of distinguishing between classes eg the model has predicted observations that are apples as apples and observations that are not apples as not apples when the auc is close to 1 it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability
so far we have talked about the auc roc for binary classification it can be used in a multi class model as well note that there will be multiple auc's plotted for each class
auc-roc for multiclass
icml presentation on multiclass roc analysis
regression tasks will predict the state of a target variable based on other correlated input variables as a quick reminder target variables in these tasks are continuous values let us discuss the metrics that are used to evaluate the outcome of regression tasks:
r-squared also known as the coefficient of determination is the proportion of the variance in the outcome variable that can be predicted using the predictor variables it tells you how well ""observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model"" when interpreting r-squared in a simple linear regression model it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2) if r2 is 05 this would mean that 50% of the variation in the dependent variable is explained by the predictor variables a good model has a high r2
when there are multiple regressors then r2 is the square of the coefficient of multiple correlation (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"")
this metric will provide an indication of how well new data will be predicted by the model
r2 does not come without some issues as a metric it cannot determine whether the coefficient estimates and predictions might be biased the r2 will typically increase when a predictor is added to a model and as you add more predictors your model will likely overfit and result in a high r2  adjusted r-squared will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected it tells you the percentage of variation explained by predictors that will have an effect on the outcome basically adjusted r2 will calculate r2 from the predictors that have a significant impact on the model adjusted r2 is best used to compare models with different number of predictors
mean squared error (mse) is measure of the quality of an estimator or a predictor (depending on context) a value closer to zero is always best mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom
mean absolute error (mae) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value) this metric is used to measure accuracy you use the mean absolute percentage error (mape) to compare predictions and interpret whether the size of an error is small or large the mape is a model evaluation technique that clearly interprets the relative error
root mean squared error (rmse) gives weight to large errors since it squares the errors before computing the mean the rmse is computed by first determining the residuals (difference between the actual and predicted y values) residuals are squared and the squares are averaged finally the square root of the averaged squares will result in the rmse an easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y
resource: regression metrics-scikit-learn",0,What is the name of the number of questions that are asked?,0,0
1896,1899,Model Evaluation,Evaluation Metrics,Classification and Regression Metrics,"Classification Metrics,MCC Formula,ROC-AUC Source,Regression Metrics","the metrics used to evaluate the results of your task is of great importance the performance of your algorithms need to be measured and compared to ensure that you select the right algorithm
confusion matrix is quite easy to interpret and it is straightforward in its duties this metric simply shows how well a classifier has performed it is a simple visualization of the task's performance you might also find that it is referred to as a contingency table confusion matrix can present the prediction results of a binary or multi class classification problem as shown in the example below the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category those categories are defined as follows:
true positives (tp) is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values
true negatives (tn) indicates that the classifier correctly predicted no-event values an observation in the negative class is correctly classified as being in said class
false positives (fp) is an error in which a classifier improperly indicates classifies as observation in the positive class when in reality it belongs to the negative class this is considered a type i error and is considered a false alarm the conditional probability of a positive test result given an event that was not present is called the false positive rate
false negatives (fn) is an error in which a classifier improperly classifies an observation in the negative class when it belongs to the positive class this is considered a type ii error the conditional probability of a negative test result given an event that was present is called the false negative rate  this error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society the premise of blackstone's formulation supports the above claim it states that ""it is better that 10 guilty persons escape than that one innocent person should suffer"" it can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease they could possibly die (or spread said disease) because of this type ii error
let us use the example of predicting the diagnosis classes for a dataset with 300 observations the matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes the table below shows a perfect classification exercise it goes without saying that this can not be the case when real data is introduced to a trained classifier the confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model
diabetic
not-diabetic
diabetic
110
0
not-diabetic
0
90
accuracy is simply a measure of how accurately the classifier performed with classifying data  accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier's performance
recall sometimes referred to as sensitivity is the true positive rate and is calculated as:
# of true positives/ (# of true positives + # of false negatives) it is not holistic because it does not account for the false positive and true negative
recall = tp/(tp+fn)
specificity is the true negative rate and similar to the recall it can result in biased results it is calculated as # of true negatives/(# of true negatives + # of false positives)
specificity = tn/(tn + fp)
precision is the positive predictive value and calculated as:
# of true positives/ (# of true positives/# of false positives)
precision = tp/ (tp + fp)
f1 score: all the metrics above were highlighted to introduce biased results because they do not account for all four rates the f1 score will is calculated as:
2 * (precision * recall) / (precision + recall)
matthews correlation coefficient will solve for the issue of biased results it is often used to assess the quality of a binary classification model it is a correlation coefficient between the observed and predicted binary classification a value of +1 means a perfect prediction 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events and finally -1 means the classifier misclassified observations mcc is considers symmetric meaning that no class is more important than another (switch negatives and positives and the result will remain the same)
resource: mcc-scikitlearn
logistic loss (log loss) is a metric that evaluates the predictions of probabilities of an observation's membership to a specific class the prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value log loss will ""take into account the uncertainty of a prediction based on how much it varies from the actual label while accuracy is the count of predictions where the predicted value equals the actual value ""
resource: the math behind log loss
receiver operating characteristic (roc) curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds it is the comparison of two operating characteristics the roc curve has been useful for many years according to research it was developed and used by engineers during the world war ii it was used for battlefield detection analysis and has been used in many fields its most prominent use in recent days is for machine learning and model performance assessment
consider the roc curve to be ""sensitivity as a function of false positive rate"" it can be used to select the optimal models
area under the roc curve otherwise known as auc measures the entire area underneath the roc curve and it is the measure of the classifier's ability to distinguish between classes it also provides a measure of performance across different thresholds the auc measures how well predictions are ranked and the quality of the prediction auc might not be useful for certain scenarios such as it does not tell you much about the ""cost of different errors"" it gives similar weight to errors the general interpretation of the chart is that the higher the auc the better the model is at its task of distinguishing between classes eg the model has predicted observations that are apples as apples and observations that are not apples as not apples when the auc is close to 1 it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability
so far we have talked about the auc roc for binary classification it can be used in a multi class model as well note that there will be multiple auc's plotted for each class
auc-roc for multiclass
icml presentation on multiclass roc analysis
regression tasks will predict the state of a target variable based on other correlated input variables as a quick reminder target variables in these tasks are continuous values let us discuss the metrics that are used to evaluate the outcome of regression tasks:
r-squared also known as the coefficient of determination is the proportion of the variance in the outcome variable that can be predicted using the predictor variables it tells you how well ""observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model"" when interpreting r-squared in a simple linear regression model it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2) if r2 is 05 this would mean that 50% of the variation in the dependent variable is explained by the predictor variables a good model has a high r2
when there are multiple regressors then r2 is the square of the coefficient of multiple correlation (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"")
this metric will provide an indication of how well new data will be predicted by the model
r2 does not come without some issues as a metric it cannot determine whether the coefficient estimates and predictions might be biased the r2 will typically increase when a predictor is added to a model and as you add more predictors your model will likely overfit and result in a high r2  adjusted r-squared will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected it tells you the percentage of variation explained by predictors that will have an effect on the outcome basically adjusted r2 will calculate r2 from the predictors that have a significant impact on the model adjusted r2 is best used to compare models with different number of predictors
mean squared error (mse) is measure of the quality of an estimator or a predictor (depending on context) a value closer to zero is always best mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom
mean absolute error (mae) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value) this metric is used to measure accuracy you use the mean absolute percentage error (mape) to compare predictions and interpret whether the size of an error is small or large the mape is a model evaluation technique that clearly interprets the relative error
root mean squared error (rmse) gives weight to large errors since it squares the errors before computing the mean the rmse is computed by first determining the residuals (difference between the actual and predicted y values) residuals are squared and the squares are averaged finally the square root of the averaged squares will result in the rmse an easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y
resource: regression metrics-scikit-learn",Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.,What is the measure of errors between paired observations?,1,1
1872,1875,Model Evaluation,Evaluation Metrics,Classification and Regression Metrics,"Classification Metrics,MCC Formula,ROC-AUC Source,Regression Metrics","the metrics used to evaluate the results of your task is of great importance the performance of your algorithms need to be measured and compared to ensure that you select the right algorithm
confusion matrix is quite easy to interpret and it is straightforward in its duties this metric simply shows how well a classifier has performed it is a simple visualization of the task's performance you might also find that it is referred to as a contingency table confusion matrix can present the prediction results of a binary or multi class classification problem as shown in the example below the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category those categories are defined as follows:
true positives (tp) is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values
true negatives (tn) indicates that the classifier correctly predicted no-event values an observation in the negative class is correctly classified as being in said class
false positives (fp) is an error in which a classifier improperly indicates classifies as observation in the positive class when in reality it belongs to the negative class this is considered a type i error and is considered a false alarm the conditional probability of a positive test result given an event that was not present is called the false positive rate
false negatives (fn) is an error in which a classifier improperly classifies an observation in the negative class when it belongs to the positive class this is considered a type ii error the conditional probability of a negative test result given an event that was present is called the false negative rate  this error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society the premise of blackstone's formulation supports the above claim it states that ""it is better that 10 guilty persons escape than that one innocent person should suffer"" it can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease they could possibly die (or spread said disease) because of this type ii error
let us use the example of predicting the diagnosis classes for a dataset with 300 observations the matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes the table below shows a perfect classification exercise it goes without saying that this can not be the case when real data is introduced to a trained classifier the confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model
diabetic
not-diabetic
diabetic
110
0
not-diabetic
0
90
accuracy is simply a measure of how accurately the classifier performed with classifying data  accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier's performance
recall sometimes referred to as sensitivity is the true positive rate and is calculated as:
# of true positives/ (# of true positives + # of false negatives) it is not holistic because it does not account for the false positive and true negative
recall = tp/(tp+fn)
specificity is the true negative rate and similar to the recall it can result in biased results it is calculated as # of true negatives/(# of true negatives + # of false positives)
specificity = tn/(tn + fp)
precision is the positive predictive value and calculated as:
# of true positives/ (# of true positives/# of false positives)
precision = tp/ (tp + fp)
f1 score: all the metrics above were highlighted to introduce biased results because they do not account for all four rates the f1 score will is calculated as:
2 * (precision * recall) / (precision + recall)
matthews correlation coefficient will solve for the issue of biased results it is often used to assess the quality of a binary classification model it is a correlation coefficient between the observed and predicted binary classification a value of +1 means a perfect prediction 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events and finally -1 means the classifier misclassified observations mcc is considers symmetric meaning that no class is more important than another (switch negatives and positives and the result will remain the same)
resource: mcc-scikitlearn
logistic loss (log loss) is a metric that evaluates the predictions of probabilities of an observation's membership to a specific class the prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value log loss will ""take into account the uncertainty of a prediction based on how much it varies from the actual label while accuracy is the count of predictions where the predicted value equals the actual value ""
resource: the math behind log loss
receiver operating characteristic (roc) curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds it is the comparison of two operating characteristics the roc curve has been useful for many years according to research it was developed and used by engineers during the world war ii it was used for battlefield detection analysis and has been used in many fields its most prominent use in recent days is for machine learning and model performance assessment
consider the roc curve to be ""sensitivity as a function of false positive rate"" it can be used to select the optimal models
area under the roc curve otherwise known as auc measures the entire area underneath the roc curve and it is the measure of the classifier's ability to distinguish between classes it also provides a measure of performance across different thresholds the auc measures how well predictions are ranked and the quality of the prediction auc might not be useful for certain scenarios such as it does not tell you much about the ""cost of different errors"" it gives similar weight to errors the general interpretation of the chart is that the higher the auc the better the model is at its task of distinguishing between classes eg the model has predicted observations that are apples as apples and observations that are not apples as not apples when the auc is close to 1 it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability
so far we have talked about the auc roc for binary classification it can be used in a multi class model as well note that there will be multiple auc's plotted for each class
auc-roc for multiclass
icml presentation on multiclass roc analysis
regression tasks will predict the state of a target variable based on other correlated input variables as a quick reminder target variables in these tasks are continuous values let us discuss the metrics that are used to evaluate the outcome of regression tasks:
r-squared also known as the coefficient of determination is the proportion of the variance in the outcome variable that can be predicted using the predictor variables it tells you how well ""observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model"" when interpreting r-squared in a simple linear regression model it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2) if r2 is 05 this would mean that 50% of the variation in the dependent variable is explained by the predictor variables a good model has a high r2
when there are multiple regressors then r2 is the square of the coefficient of multiple correlation (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"")
this metric will provide an indication of how well new data will be predicted by the model
r2 does not come without some issues as a metric it cannot determine whether the coefficient estimates and predictions might be biased the r2 will typically increase when a predictor is added to a model and as you add more predictors your model will likely overfit and result in a high r2  adjusted r-squared will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected it tells you the percentage of variation explained by predictors that will have an effect on the outcome basically adjusted r2 will calculate r2 from the predictors that have a significant impact on the model adjusted r2 is best used to compare models with different number of predictors
mean squared error (mse) is measure of the quality of an estimator or a predictor (depending on context) a value closer to zero is always best mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom
mean absolute error (mae) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value) this metric is used to measure accuracy you use the mean absolute percentage error (mape) to compare predictions and interpret whether the size of an error is small or large the mape is a model evaluation technique that clearly interprets the relative error
root mean squared error (rmse) gives weight to large errors since it squares the errors before computing the mean the rmse is computed by first determining the residuals (difference between the actual and predicted y values) residuals are squared and the squares are averaged finally the square root of the averaged squares will result in the rmse an easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y
resource: regression metrics-scikit-learn","Consider the ROC curve to be ""sensitivity as a function of false positive rate."" It can be used to select the optimal models.",What can be used to select the optimal models?,1,1
1411,1414,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory","now that you have studied the elements of a properly framed analytical objective we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science they are provided here as purely illustrative example instances of the general template on which you can base your own formulations
not all framings of analytic objectives will include every individual elements as some of them may not be necessary depending on the situation in industry settings the problem and task may be merged and the added valuable functionality may be evident from a model that performs its function well in academic settings the overarching interest may be that of advancing the state of the art in research and hence the statement may either not include an explicit business objective or state it as a problem solution vision
a constructive analytical objective states that it is in principle possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet one can think of it as a proof-of-concept or prototyping endeavor
in order to increase sales from the company’s online store
 (business objective)
we work towards increasing the click-through rate of its advertising through targeted content
 (problem)
by classifying website visitors into youth middle-age and senior demographics (task)
using supervised learning models on curated internal datasets
 (method)
(business objective omitted due to project being primarily research)
in order to enable more effective search of audio collections
 (problem)
we demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds
 (task)
using neural models on a dataset of short clips of classical music and their descriptions
 (method)
towards developing suitable multi-modal audio-textual encoding
 (valuable functionality)
in scenarios where the feasibility of an analytical task has been established projects may be targeted towards improving over the state of the art in some performance metric by using innovative methods/features/data this is typically the case if one works on leaderboard-type datasets where there are models
the client is a logistics company that wants to speed up its automatic package sorting
 (business objective)
we focus on the problem of handwritten address recognition from shipping label scans 
(problem and task)
we want to combine neural image recognition with language models on company-internal data
 (method and data)
to improve performance beyond the current model based on standard convolutional neural networks without language information
 (valuable functionality)
(business objective omitted due to project being primarily research)
for the task of span-based question answering from text
 (problem and task merged because span-based question answering is a common leaderboard task)
we want to combine graph-based knowledge bases with neural attention models
 (method)
to improve over state of the art performance on realistic news text
 (valuable functionality and data)
exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives
the client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient
 (business objective)
specifically he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified
 (problem and task)
we want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client 
(methods and data)
towards identifying correlating events across the production process that can be used for process optimization

(business objective omitted due to project being primarily research)
the development of ai dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bot’s replies
 (problem)
we want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols
 (methods and data)
in order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset 
(task / valuable insight)",We focus on the problem of handwritten address recognition from shipping label scans ,What is the problem with handwritten address recognition from shipping label scans?,1,1
276,276,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","so far we have discussed data as a entity in the data science process and how it is transformed during the cleansing/wrangling process used for exploratory data analysis and used to draw conclusions with inferential statistics now we will focus on the parts of data that can be useful in the model building process parts of data that will assist in performing the tasks that you have defined in earlier stages of the data  science process; those tasks that are done to meet our analytic objective developing your analytic solution will involve the use of statistical modeling we must understand that those models consist of formulae that only relates numerical quantities to each other1 how then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service how can a mathematical model understand variables that are not numeric
a feature is a numeric representation of a part of the raw data the wikipedia definition of a feature best describes it as ""an individual measurable property or characteristic of an observation"" 
features are the parts of an observation that are represented in a way that a machine learning model can use consider an image classification task to properly represent the features of your image they are processed into a numerical format that allows the mathematical model to use it
when raw data is transformed into features a data scientist must consider the right features that are useful for the data science task a good feature is one that is appropriate to the statistical modeling technique and data science task features should also provide information ie if you are performing a predictive task your features should have predictive values
transforming or processing features from data is an important task in the data science project life cycle but often glossed over the price for badly selected features is a costly one that rears its head when you are training your model as shown in the figure below features will directly affect the models that you develop and the insights gleaned from your models the snowball effect of badly selected features will end up leading decision makers down a wrong path as efficiency and accuracy are key in the data science process it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling note that feature engineering requires both domain and technical expertise
feature engineering and analytic solution building source: zheng & casari (2018)1
feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model
 feature engineering leads to higher quality models and better insights for decision makers when you think about the diverse machine learning techniques data science tasks and contexts in which we apply machine learning you will see that feature engineering can not be generalized it is not a one size fits all process it is dependent on your analytic objective and your data feature engineering requires domain knowledge and intuition during the feature engineering process the data scientist will remove features from the data that do not provide task specific information (eg the feature has no predictive value) and also features that introduce redundancy this is called feature selection
numeric data type: even though we defined a feature as a numeric representation of data raw data that is in numeric form should also undergo feature engineering this is because the data must meet the assumptions of the chosen model
scalar: single numeric feature eg mass
vector: ordered list of scalars
; also defined as an object that has both a magnitude and direction
spaces: vectors exist within a vector space
 and are also a collection of vectors that can be added or multiplied by scalars
in machine learning the input to a model is represented as a numeric vector","Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task, to properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use it.",What is an image classification task?,1,1
800,802,Analytic Algorithms and Model Building,Unsupervised Learning Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","let us explore the second type of clustering technique called the hierarchical clustering technique here you will begin clustering to form hierarchies of clusters and those hierarchies are presented using a dendrogram (reading a dendrogram) there are two techniques used for hierarchical clustering
this technique involves starting the clustering process with one observation forming its own cluster clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left essentially at each step of agglomerating clusters the clusters with the smallest distance from each other will be combined as shown in the figure below the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left
the technique can take advantage of any distance measure but you will find that most studies will use the euclidean distance as a distance metric
raw data-source1
the first round of merges finds clusters with observations/clusters b and c merged to form one cluster and d and e are also merged into one now we have cluster a bc de and f
next de and f are combined to form cluster a bc and def
clusters are further combined to form a and bcdef
finally abcdef is formed
hierarchical clustering technique-source1
single linkage method is based on grouping clusters using the agglomerative method with two clusters merged at each step those clusters contain the closest observations that are not yet part of the same cluster the distance between the nearest pair of observations in the two clusters are used to determine the best clusters to combine this method will produce clusters that have small distances while ignoring observations in clusters that are further from it as clusters are merged the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters
single linkage suffers from chaining in order to merge two
 groups only need one pair of points to be close irrespective
 of all others therefore clusters can be too spread out and
 not compact enough
complete linkage method uses the maximum distance between data points within each cluster also known as the farthest neighbor method clusters are combined into larger clusters until all data points are in the same cluster the distance between clusters is the distance between two data points (ie one per each cluster) that are farthest from each other complete linkage avoids chaining but suffers from crowding;  because its score is based on the worst-case dissimilarity
 between pairs a point can be closer to points in other clusters
 than to points in its own cluster
average linkage method is the average distance between data points within each cluster you can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity over all points in opposite groups
centroid linkage method is based on maximum distance and uses the centroid distance between clusters the mean for data points in a cluster is the centroid in complete linkage dissimilarity
 between clusters is the largest dissimilarity between two points in
 opposite groups
divisive clusteringthis is the opposite of the agglomerative method data is clustered using a top-down approach all data will belong to one cluster and then the largest clustered is split until each observation is in its own cluster this method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder this method is great at identifying large clusters and the agglomerative method is great at identifying small clusters
reading: hierarchical clustering of words and application to nlp tasks",Single linkage suffers from chaining. In order to merge two,"In order to merge two a single linkage, how many a unified linkage must be merged?",1,0
79,79,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning","Stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample. You can check the extent to which a model fits the data with the residual standard error(RSE is standard deviation of error e) i.e ""the average amount that the response will deviate from the true regression line"". A large RSE means the model was not a good fit to the data. and the R2 is independent of your response variable, unlike the RSE.",What is a stepwise regression prone to?,1,1
1369,1372,Analytic Requirements Gathering,Requirements Overview,Overview,Requirements Management Plan,"as you have learned in a previous unit gaining an understanding of the business need and formulating project objectives (business and analytic) are best practices for successfully meeting client expectations once the project objectives have been defined the project team will identify the needs and constraints of stakeholders and the current system or environment related to the business need; this is referred to as the requirements gathering process
requirements gathering establishes communication between the solutions development team (referred to as the project team in this unit) and the business stakeholders the objective of the requirements gathering process is to define the system inputs processes outputs and interfaces when properly done requirements gathering will positively influence the outcome of a project requirements gathering can also help the project team to visualize the intended solution
the requirements gathering process involves input from the system users and system owners a system user interacts with the solution and a system owner (who can also be a user) is an official who is responsible for decisions made about systems within their organization in this unit we will refer to these roles as stakeholders
similar to traditional software development projects data science projects are also guided by requirements gathering principles figure 3 lists the steps that are followed during the process requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s)
figure 3: requirements gathering process
the requirements gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data related project
gather information: the first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business need as well as decision makers within the business once stakeholders are identified the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives
 later on in this unit we will discuss the techniques used to gather information
define and prioritize requirements: stakeholders will provide information according to their view of the business needs it is the job of the business analyst to lead the effort in defining and prioritizing requirements it is important to document ""complete"" requirements that capture the needs of the stakeholders as this will guide the project team in developing the right solution(s) stakeholders might provide information that can be used for future projects related to the proposed solution that information should not be discarded it is prioritized as a low priority requirement and considered for future implementation
evaluate requirements: the project team must verify and validate all documented requirements this additional step in the requirements gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders
receive sign-off: this is an indication that the requirements have been approved and agreed upon by the client requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution
a requirements management plan can be used to document the requirements gathering process this document is made available to the client and the project team as it contains information that affects both parties there is no standard template for this document but it is in good practice to include the following sections:
project description is an overview of your project this section describes the purpose of your project
team responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements gathering process data science project team members might take on duties outside of their normal roles eg a data analyst on the data science team might serve in the role of scribe during joint application development sessions
tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture manage and track requirements through the requirements gathering process and throughout the project lifecycle
requirements gathering process should be defined in this plan this section will describe the techniques used in eliciting user and system needs defining the requirements and evaluating the success of the requirements gathering process (these techniques are covered later in this unit)
change control and requirements gathering: modern day collaboration tools will support change control however a change control process should be documented to ensure that changes to requirements are formally managed","Project description is an overview of your project, this section describes the purpose of your project.",What is an overview of your project?,0,1
366,367,Data Gathering and Wrangling,Data Gathering,Overview-Data Governance,,"data governance defines how data is accessed and managed within an organization it is important because it facilitates effective data management and has positive implications for the quality security and integrity of data used for analysis an organization that handles data efficiently understands that data governance impacts data quality and the decisions that can be made from the data available to the organization this unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and decision making any organization that stores and utilizes data should have a data governance strategy for internal data or data stored within the organization and external data
data governance is beneficial because it provides a reliable and consistent view of enterprise wide data it ensures that there is a plan for improved quality of data maps the location of data in the enterprise reducing the scourge of data silos and improves data management overall
reading: data governance in the cloud 
data governance is the responsibility of an entire organization although it is administered by the data management team; all users of data in an organization are considered stakeholders of the organization's data the data governance institute defines a stakeholder as an individual or group that make or is affected by data driven decisions within an organization in addition to data governance policies data stakeholders will have an influence on the state and use of data as a quick reminder the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle as well as determining the requirements for the analytic solution so why is data governance important to a member of a data science project team
data governance is not just policy making for data it is more than that; it influences business strategy because data is now (more than ever) considered as an asset to an organization an organization that has embedded data governance principles into its data infrastructure or data management framework will abide by data standards (industry set or company defined)
data governance best practices for organizations are met when data has integrity and data related decisions and controls are transparent and can be audited another best practice that is gaining ground in industry is the push for organizations to collect and store data that is unbiased unbiased data looks different to each organization and industry but the general idea is that the data represents all members of a population that could be served by an organization this best practice will positively influence the development of ethical models and algorithms for analytic solutions
reading: data governance and its implications for ethical models","Data governance is the responsibility of an entire organization although it is administered by the data management team; all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that make or is affected by data driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as determining the requirements for the analytic solution. So why is data governance important to a member of a data science project team?",What does the Data Governance Institute define an individual or group that makes or is affected by data driven decisions?,1,0
1819,1822,Model Evaluation,Evaluation Metrics,Classification and Regression Metrics,"Classification Metrics,MCC Formula,ROC-AUC Source,Regression Metrics","the metrics used to evaluate the results of your task is of great importance the performance of your algorithms need to be measured and compared to ensure that you select the right algorithm
confusion matrix is quite easy to interpret and it is straightforward in its duties this metric simply shows how well a classifier has performed it is a simple visualization of the task's performance you might also find that it is referred to as a contingency table confusion matrix can present the prediction results of a binary or multi class classification problem as shown in the example below the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category those categories are defined as follows:
true positives (tp) is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values
true negatives (tn) indicates that the classifier correctly predicted no-event values an observation in the negative class is correctly classified as being in said class
false positives (fp) is an error in which a classifier improperly indicates classifies as observation in the positive class when in reality it belongs to the negative class this is considered a type i error and is considered a false alarm the conditional probability of a positive test result given an event that was not present is called the false positive rate
false negatives (fn) is an error in which a classifier improperly classifies an observation in the negative class when it belongs to the positive class this is considered a type ii error the conditional probability of a negative test result given an event that was present is called the false negative rate  this error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society the premise of blackstone's formulation supports the above claim it states that ""it is better that 10 guilty persons escape than that one innocent person should suffer"" it can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease they could possibly die (or spread said disease) because of this type ii error
let us use the example of predicting the diagnosis classes for a dataset with 300 observations the matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes the table below shows a perfect classification exercise it goes without saying that this can not be the case when real data is introduced to a trained classifier the confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model
diabetic
not-diabetic
diabetic
110
0
not-diabetic
0
90
accuracy is simply a measure of how accurately the classifier performed with classifying data  accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier's performance
recall sometimes referred to as sensitivity is the true positive rate and is calculated as:
# of true positives/ (# of true positives + # of false negatives) it is not holistic because it does not account for the false positive and true negative
recall = tp/(tp+fn)
specificity is the true negative rate and similar to the recall it can result in biased results it is calculated as # of true negatives/(# of true negatives + # of false positives)
specificity = tn/(tn + fp)
precision is the positive predictive value and calculated as:
# of true positives/ (# of true positives/# of false positives)
precision = tp/ (tp + fp)
f1 score: all the metrics above were highlighted to introduce biased results because they do not account for all four rates the f1 score will is calculated as:
2 * (precision * recall) / (precision + recall)
matthews correlation coefficient will solve for the issue of biased results it is often used to assess the quality of a binary classification model it is a correlation coefficient between the observed and predicted binary classification a value of +1 means a perfect prediction 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events and finally -1 means the classifier misclassified observations mcc is considers symmetric meaning that no class is more important than another (switch negatives and positives and the result will remain the same)
resource: mcc-scikitlearn
logistic loss (log loss) is a metric that evaluates the predictions of probabilities of an observation's membership to a specific class the prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value log loss will ""take into account the uncertainty of a prediction based on how much it varies from the actual label while accuracy is the count of predictions where the predicted value equals the actual value ""
resource: the math behind log loss
receiver operating characteristic (roc) curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds it is the comparison of two operating characteristics the roc curve has been useful for many years according to research it was developed and used by engineers during the world war ii it was used for battlefield detection analysis and has been used in many fields its most prominent use in recent days is for machine learning and model performance assessment
consider the roc curve to be ""sensitivity as a function of false positive rate"" it can be used to select the optimal models
area under the roc curve otherwise known as auc measures the entire area underneath the roc curve and it is the measure of the classifier's ability to distinguish between classes it also provides a measure of performance across different thresholds the auc measures how well predictions are ranked and the quality of the prediction auc might not be useful for certain scenarios such as it does not tell you much about the ""cost of different errors"" it gives similar weight to errors the general interpretation of the chart is that the higher the auc the better the model is at its task of distinguishing between classes eg the model has predicted observations that are apples as apples and observations that are not apples as not apples when the auc is close to 1 it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability
so far we have talked about the auc roc for binary classification it can be used in a multi class model as well note that there will be multiple auc's plotted for each class
auc-roc for multiclass
icml presentation on multiclass roc analysis
regression tasks will predict the state of a target variable based on other correlated input variables as a quick reminder target variables in these tasks are continuous values let us discuss the metrics that are used to evaluate the outcome of regression tasks:
r-squared also known as the coefficient of determination is the proportion of the variance in the outcome variable that can be predicted using the predictor variables it tells you how well ""observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model"" when interpreting r-squared in a simple linear regression model it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2) if r2 is 05 this would mean that 50% of the variation in the dependent variable is explained by the predictor variables a good model has a high r2
when there are multiple regressors then r2 is the square of the coefficient of multiple correlation (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"")
this metric will provide an indication of how well new data will be predicted by the model
r2 does not come without some issues as a metric it cannot determine whether the coefficient estimates and predictions might be biased the r2 will typically increase when a predictor is added to a model and as you add more predictors your model will likely overfit and result in a high r2  adjusted r-squared will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected it tells you the percentage of variation explained by predictors that will have an effect on the outcome basically adjusted r2 will calculate r2 from the predictors that have a significant impact on the model adjusted r2 is best used to compare models with different number of predictors
mean squared error (mse) is measure of the quality of an estimator or a predictor (depending on context) a value closer to zero is always best mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom
mean absolute error (mae) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value) this metric is used to measure accuracy you use the mean absolute percentage error (mape) to compare predictions and interpret whether the size of an error is small or large the mape is a model evaluation technique that clearly interprets the relative error
root mean squared error (rmse) gives weight to large errors since it squares the errors before computing the mean the rmse is computed by first determining the residuals (difference between the actual and predicted y values) residuals are squared and the squares are averaged finally the square root of the averaged squares will result in the rmse an easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y
resource: regression metrics-scikit-learn","Confusion Matrix is quite easy to interpret and it is straightforward in its duties. This metric simply shows how well a classifier has performed, it is a simple visualization of the task's performance. You might also find that it is referred to as a Contingency Table. Confusion Matrix can present the prediction results of a binary or multi class classification problem. As shown in the example below, the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category. Those categories are defined as follows:",What is Confusion Matrix easy to interpret?,1,1
512,513,Exploratory Data Analysis,Feature Engineering,Summary and Quiz 5,,"in this module we explored a technique used to convert raw data from its numeric or categorical format to a format that can be used when performing modeling
mathematical models mostly ""understand"" data in numeric form hence the need for transforming raw data
during the transformation process features should be normalized as needed to meet the assumptions of the mathematical models
categorical variables can be transformed to numeric format using feature engineering techniques including categorical variable encoding
bias can be introduced to the data during the feature engineering process this bias could be conscious or unconscious there are different kinds of biases and ways to control for these biases
principal component analysis is the first introduction to modeling it is a way of using modeling techniques for dimensionality reduction in the dataset
now that we have completed the data understanding phase we will transition to modeling",Bias can be introduced to the data during the feature engineering process. This bias could be conscious or unconscious. There are different kinds of biases and ways to control for these biases.,What is a bias that can be conscious or unconscious?,1,1
1323,1326,Analytic Requirements Gathering,Requirements Gathering Techniques,Summary and Quiz 1,,"poorly documented requirements lead to assumptions and miscommunications between the project team and client stakeholders communication issues can lead to defects in solutions and unmet expectations these are issues that a project team should avoid this can be done by defining requirements in the early stages of solution development requirements are well documented statements that define the needs of the users and systems that should be implemented to solve a business need the requirements gathering process can become complex if it is not managed properly this process is managed by the business analyst
there are different types of requirements including business and solution requirements business requirements describe the why behind the implementation of a solution the  user requirements describe the tasks that users will be able to perform with the system and the solution requirements specify the behavior of the system and its characteristics
similar to a traditional it project it is best practice to collect requirements for analytic projects the requirements for an analytic project include determining how data models and results or outputs of the models will support meeting the analytic and business objectives
there are techniques that are used to gather requirements including conducting interviews brainstorming sessions and facilitated workshops other techniques used in addition to the above listed include document analysis and observations once requirements are elicited they are analyzed and translated into written requirements for understanding finally requirements are validated by the project and client team to signal the commencement of solution development","There are different types of requirements including business and solution requirements. Business requirements describe the why behind the implementation of a solution, the  user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.",How do the user requirements describe the tasks that users will be able to perform with the system?,1,1
1964,1967,Model Evaluation,Evaluation Metrics,Clustering Evaluation Metrics,,"the previous page focused on the metrics for evaluating supervised learning problems  the presence of labeled data makes it somewhat straightforward to train and test the model's performance now we will focus on metrics that can be used when labeled data is not present  there are two approaches to evaluating clustering the internal and external evaluation approaches the internal approach involves summarizing the clustering task to a single quality score while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable clustering can also be evaluated by an expert
internal evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score the cluster with the best score is seen to be the best internal evaluation although useful can have its drawbacks it gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters  a sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters and many evaluation indexes assume convex clusters on a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity is sound
let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
silhouette coefficient shows how similar a data point is to its cluster compared to other clusters it is calculated using the mean intra cluster distance and the mean nearest cluster distance for each data point a silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster when the silhouette coefficient is close to 0 there is a presence of overlapping clusters
dunn index is also used to evaluate clustering techniques and similar to the silhouette coefficient it is dependent on the data within the clusters a good clustering is one with a higher dunn index when using this evaluation technique you want to be aware of a high computational cost when you have a large number of clusters the dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters the minimum of the pairwise distance is used to determine minimum separation (minseparation) the compactness of a cluster is measured by computing the distance between the data in the same cluster (maxdiameter) finally the dunn index will be:
minseparation/maxdiameter
external evaluation measures the results from a clustering task based on data not used for the clustering task benchmarks are set from a set of pre-classified data external evaluation techniques need ground truth data to evaluate clustering
rand index tells you how similar a cluster or clusters are to a set benchmark this is similar a classification evaluation technique you can calculate the rand index thus:
(tp + tn)/(tp+fp+fn+tn)
purity is considered a no frills technique that assigns each cluster to a class (usually one that occurs often in the cluster) the number of correctly assigned observations is divided by the overall number of observations to determine accuracy purity close to 1 is best and close to 0 is not optimal a large number of clusters can lead to a higher purity there is a tradeoff between quality of clustering and number of clusters when using purity as a metric the normalized mutual information (nmi) can be used to measure and compare the quality of clustering between different clusterings with varying number of clusters
jaccard index is used in cluster analysis evaluation and convolutional neural networks among others it is defined as ""the size of the intersection divided by the size of the union of the sample sets"" the jaccard distance measures dissimilarity between sample sets
f-measure is simply computed as the 2 * ((precision * recall)/(precision + recall)) you might remember it from the classification metrics it is also known as the f1 score
dice index also known as the sorensen-dice indexor dice coefficient can assess the similarity of two samples it ranges from 0 to 1 dice index is a semimetric version of jaccard index and gives less weight to outliers in a dataset it is used to measure the lexical association score of two words
reading: clustering evaluation techniques (20min read)
you should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis this is sometimes done by comparing a dataset against another (one without clusters) the hopkins statistic is used to measure cluster tendency a resulting value of 1 or close enough will show that the data is clustered data that is uniformly distributed will be closer to 0 hopkins statistic is quite good at estimating randomness in a dataset",Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:,What are internal evaluation techniques used to assess?,1,1
1002,1004,Analytic Algorithms and Model Building,Supervised Techniques,Summary and Quiz 8,,"supervised learning involves training models with labeled input and output data
knn is a similarity function and a weak learner this is a method in which training data is generalized and most useful for large datasets that will be updated continuously in this case a model typically depends on (or queries) a small number of attributes in the dataset there are three steps involved in making predictions with k-nn: calculate the euclidean distance identify nearest neighbor(s) and then perform task knn had been described for the purposes of predicting a categorical response but it is also effective for predicting continuous value responses just like you would with a linear regression model
bayes theorem describes the probability of an event based on prior knowledge of conditions related to that event bayesian inference is applied when bayes theorem seeks to update the probability for a hypothesis as more information becomes available used in sports medicine and law among other fields naive bayes is a simple classifier that can be applied to categorical predictors when classifying observations using nb the classifier locates all observations that have similar predictor values to the observation that is to be classified and then assigns it to the class that it belongs to; when the problem calls for predicting the probability that an observation belongs to a class we can use this method
tree based methods are considered to be among the simpler methods for prediction and classification in this module we discuss classification and regression trees (cart)
cart methods do not have good predictive performance; there are methods that can be used to compensate for this deficiency called ensemble models ensemble models combine other models to produce an optimal predictive model ensemble models also solve the problem of overfitting as faced by single tree models bagging reduces variance in a decision tree method random forest method is an extension of bagging and makes needed changes to bagged trees when there is overfitting with decision trees random forests will remedy this issue similar to bagging boosting can be used to improve the predictive accuracy of certain methods including decision trees it differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it)
now you are able to take the quiz please be sure that you post your quiz questions as private on piazza or send an email and you will receive a response","Now, you are able to take the quiz. Please be sure that you post your quiz questions as private on Piazza or send an email and you will receive a response.",What is the name of the place where you are able to take the quiz?,0,0
525,526,Analytic Algorithms and Model Building,Data Science Patterns,Ranking,Learning to Rank Algorithms Source-Lucidworks,"a data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is ranking learning to rank is a technique used to train a model for ranking tasks we do not always want to predict the probability of scenarios we might want to rank things ranking is used to solve information retrieval problems including collaborative filtering sentiment analysis and document retrieval the learning to rank technique is applied to supervised learning techniques to rank results according to relevancy when you are building a model using this approach you must decide on the features used but also the adequate relevance criteria
ranking algorithms or learning to rank algorithms are used in recommender systems fault localization computational biology among others
ranking approaches are applied to information retrieval problems assume that you have posted videos on a channel for subscribers to watch you want to know whether subscribers will click on that video and you might want to also
pointwise approach is used under the assumption that each query-document pair in a dataset has an ordinal or numerical score this approach can be used to predict the score of a single query-document pair the ranking task using the pointwise approach will become a classification or regression task
listwise approach reviews the list of documents and produces an optimal ordering
pairwise approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth) ranking using the pairwise approach becomes a classification or regression task
microsoft research developed the three known learning to rank algorithms that all use pairwise ranking:
ranknet uses the gradient descent to update the weights or model parameters for a learning to rank task this algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list
lambdarank uses a cost function to train a ranknet which results in speed and accuracy improvements
lambdamart uses multiple additive regression trees(mart-"" an implementation of the gradient tree boosting methods for regression and classification"") and lambdarank to solve a ranking task
wayfair: application of ltr
additional reading: from ranknet to lambdarank to lambdamart",Pointwise Approach is used under the assumption that each query-document pair in a dataset has an ordinal or numerical score. This approach can be used to predict the score of a single query-document pair. The ranking task using the pointwise approach will become a classification or regression task.,The ranking task using the pointwise approach becomes what?,1,1
1639,1642,Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"similar to other business processes a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influence the stakeholders existing system environment and support structure the data whose analysis supports the proposed solution may have to be gathered both from inside  and outside the organization overall this process of initiating and executing a data science project can be challenging and it requires technical expertise as well as domain guidance a gartner study showed that 85% of data science projects fall short of expectations project expectations are defined by your client and the data science team creates a solution vision that will meet their expectations the path to defining those project expectations must be tread carefully to avoid project failure
according to the chaos report a group that tracks it project failure reports that data science projects might fail due to multiple reasons including those that occur in general it projects however there are some unique issues that data science teams must consider to ensure the success of their projects some of the reasons for data science project failure include:
insufficient or inappropriate data
lack of technical data science skills
issues with project management

inaccurate interpretation of results
mismanaged client expectations

the above-mentioned issues are related to a misunderstanding regarding the client’s business needs and to inadequate communication between the data science project team and the business stakeholders it is important to identify and understand your client’s business needs environment and current solution this will increase the chances of meeting the business objectives and providing the right analytical solution
in this unit we will discuss how to derive analytic objectives from a client’s or organization’s business needs in doing so you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset while these aspects are certainly important parts of many data science projects being a successful data scientist involves being mindful of the big picture to envision design implement and ultimately deploy creative solutions for real world problems you will find that this course introduces a data science approach that is grounded in scientific research software engineering principles and experimentation
the remainder of this unit focuses on the ability to identify problems and envision a solution which is among the most important skills a data scientist must possess",Mismanaged client expectations,What does a poorly managed client expect?,1,0
672,673,Exploratory Data Analysis,Performing Exploratory Data Analysis,Information Design Overview,"Summarizing Data,**Do you know that if the mean and median of your data set differ greatly, you should check that variable for outliers!**,Source: BPI Consulting LLC,Relationships and Association,Correlation Coefficient,Meaning,Outliers,Box Plot. Source. University of Manchester,Graphical Summaries,One of the first Cartesian Coordinates Graphs,Visualizing Variables","the exploratory data analysis (eda) process is comprised of visualizing data to allow a data scientist or a data analyst explore datasets to gain insights from the data there are some non-graphical techniques that are used to explore data and as well as graphical techniques non-graphical techniques include using summary statistics to describe the data and the graphical techniques are used to describe the frequency distribution of the dataset both techniques can be used to show the skewness and extreme outliers in a dataset
summarizing data is dependent on the types of data present in your dataset when you want to understand the observed data aka your sample you will key descriptors of a dataset when you have a large data set it is difficult to describe it in its raw form you will use specific techniques to summarize and describe your sample some of those techniques include describing central tendency and assessing measures of spread and relationships
you can use the location shape and spread of the data in a dataset to understand the data you might find some of the concepts below as a review of your first statistics course but pay attention to the reason for using these techniques in exploring your data also these concepts are important for when you learn about using statistical inference to draw conclusions on an unknown population parameter
location during the eda process we are looking to describe our data using a central value we will explore the mean which is sometimes called the average this is the sum total of all observations divided by the number of observations you will calculate the mean for your population (population mean = μ)  and for the sample that you have drawn (sample mean = x̄) there are different types of mean including the typical arithmetic mean geometric mean and the harmonic mean
reading: central tendency with different types of means
median is the mid value of a dataset to calculate a median value you will sort your data in ascending order the median value in a data set with odd number of observations is the middle value while you can find the median of a dataset with even observations by calculating the average of the two middle values
mode is the variable of an observation that most frequently occurs in your dataset a uni-modal variable is one that has just one mode and a bimodal variable has two modes if your variable has more than two modes it can be referred to as multi-modal do not think it is useless in the eda process the mode is quite useful when summarizing categorical variables
percentile you will remember this nifty word from your gre scores or height and weight data from your health records it tells you the position of a value in your dataset  if you are 175cm in height and you are in the 10th percentile of height measurement for your gender it means that among all the height data collected for your gender you are taller than 10% of those observations the 50th percentile is considered the median quartiles are values that split the data into quarters
spread how can you describe the spread or variability of your dataset you use the measures of dispersion although used for descriptive statistics you must be careful in relying on this measure to describe variability
range of a set of values can be calculated by subtracting the minimum value in your dataset from the maximum value notice that range only considers two values and ignores all other values of a variable
mean absolute deviation is the ""average distance between each value and the mean of a dataset"" this measure of dispersion can tell you how values are spread out in a dataset and determines whether the mean is a useful indicator of the values within the data the larger the mean absolute deviation the more spread out the data when you work with time series forecasting methods you will use the mean absolute deviation to measure the performance of a forecasting model you will find that the measures used in eda are often used in model selection criteria
reading: mean deviation =  (σ|x − μ|)/n this is the sum of the absolute values minus the mean and divide this by the number of values in the dataset
variance s2 / σ2 is the average of the squared difference between the observations in a dataset and the mean so far we know that you can have measures for both your overall population and your sample drawn from the population keep in mind that you will sometimes have a sample variance s2  or a population variance σ2 the difference between both is that you are looking at the spread of data from the sample mean versus the population mean
standard deviation σ or s is the most common measure of dispersion it tells you the distance of the values from the mean in your dataset a low standard deviation tells you that the values are close to the mean and a high standard deviation means there is a spread the σ is derived by calculating the square root of the variance as you perform exploratory data analysis and even while developing models the importance of the standard deviation can not be overstated despite its mention as a way to summarize data standard deviation is used to ""measure the confidence in statistical conclusions"" and if you remember from the overview of this unit you will be conducting statistical inference to begin to draw some conclusions on your data and hypotheses
interquartile range (iqr) similar to the range does not consider all observations when looking at the spread of values in a dataset iqr describes 50% of values in your dataset when arranged in ascending order the iqr is the difference between the values in quartile 3 and the values in quartile 1 you can use this measure to identify a value that is an outlier
shape now that you can explain the measures used to explore data by describing its central value its spread from the mean and identified outliers let us describe the distribution of a dataset and assess whether it is normally distributed normally distributed data is useful when making statistical inferences how can we assess the distribution of our data:
skewness measures the degree to which the distribution of data lacks symmetry a dataset with 0 skewness is considered normally distributed data does not always have a skewness of 0; however if you have found skewness to be between -05 and 05 you can ascertain that your data is symmetrical if skewness is between -1 and -05 or 05 and 1 then your data is moderately skewed if skewness is < -1 or > 1 your data is highly skewed
kurtosis looks at the outliers within the distribution this measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution
reading: kurtosis
covariance describes the linear relationship between variables in your sample or population data covariance can be negative meaning your variables have a negative linear relationship zero (0) meaning the variables have no linear relationship or positive meaning a positive linear relationship exists between the variables
correlation coefficient will describe the strength of the linear relationship between the variables x and y it is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables if the correlation coefficient equals:
-1
variables have a perfect negative linear relationship
0
variables are not linearly related
1
variables have a perfect positive linear relationship
whenever you have a rather small or large observation within your dataset compared to other values in the dataset this is called an outlier outliers will affect the performance of your model and prior to getting to that point your exploratory data analysis when you have a large dataset the outliers are not as noticeable as when you have a smaller dataset similar to missing values you must handle outliers when you identify them in your dataset you should refrain from removing them from the dataset until proper investigation is completed you can ""handle"" outliers by following these steps:
construct a boxplot or as it is sometimes called a box and whisker plot this chart is used to graph the five number summary the five number summary is used to identify an outlier in your dataset a five-number summary consists of five values including the the maximum and minimum values in your dataset the lower and upper quartiles and the median these values are then ordered in ascending order and plotted
box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics including the skewness and mean you will typically plot your box plot using a tool of your choice in this course that would be python here is a resource on how to plot a box plot using python
z-score is a measure of the relative position of an observation within a dataset you calculate the z-score by subtracting the value from the mean and dividing that value by the standard deviation if an observation has a z-score that is more than 3 or less than -3 it is an outlier
the saying that a picture tells a thousand words supports the notion that visualizing data is one of the best ways to tell a story to a wide audience with the ability to view the visuals today there are so many tools that are used to visualize data including python r excel tableau microsoft's powerbi among others these tools even allow us to make data visualizations interactive for more effective analysis rene descartes invented the cartesian coordinate system or the x and y axis graph as we know it
data visualizations have evolved greatly including info-graphics plots and statistical graphs communicating insights in the data clearly is a key step in the data science process there is a skill to developing effective visualizations and not all data scientists will have these skills however it is important to know how to use the tools that support visualization and be able to identify the right types of visualizations for the data types and the best ways to tell your story with visualizations visualization experts strongly suggest that visualizations should stimulate the attention of viewers and elicit thoughts and questions during the data science project process visualizations will be used at different times at this stage of exploratory data analysis the visualizations are used to understand the dataset as you proceed through analytic solution building gaining a better understanding of your data will support the performance of your models in the later stages of the project
so far we have talked about visualizing mostly numeric data however we are able to visualize categorical data as well typically this data is categorized and numeric values are derived (counted) to represent the values in the variables a frequency distribution is a useful technique in displaying categorical variables
follow our primer below to learn more about the appropriate visualizations for data science tasks
reading: data exploration and visualization primer
as we study exploratory data analysis we will learn how to describe sample data by making inferences from the population on which the sample data was drawn despite the sampling techniques used there is still a level of uncertainty in drawing conclusions about your population we find that descriptive statistics is not enough to estimate this uncertainty; hence the need for statistical inference on the next page we will explore statistical inference further
additional reading: the dr howard seltman experimental design and analysis book is a text that explores experimental design and analysis and is worth a look through
stephen few3 listed eight types of quantitative data and the types of charts and graphs that can communicate the story in the data
reading: selecting the right graph for your message","Shape. Now that you can explain the measures used to explore data by describing its central value, its spread from the mean, and identified outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:",How can we assess the distribution of our data?,1,1
1400,1403,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory","now that you have studied the elements of a properly framed analytical objective we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science they are provided here as purely illustrative example instances of the general template on which you can base your own formulations
not all framings of analytic objectives will include every individual elements as some of them may not be necessary depending on the situation in industry settings the problem and task may be merged and the added valuable functionality may be evident from a model that performs its function well in academic settings the overarching interest may be that of advancing the state of the art in research and hence the statement may either not include an explicit business objective or state it as a problem solution vision
a constructive analytical objective states that it is in principle possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet one can think of it as a proof-of-concept or prototyping endeavor
in order to increase sales from the company’s online store
 (business objective)
we work towards increasing the click-through rate of its advertising through targeted content
 (problem)
by classifying website visitors into youth middle-age and senior demographics (task)
using supervised learning models on curated internal datasets
 (method)
(business objective omitted due to project being primarily research)
in order to enable more effective search of audio collections
 (problem)
we demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds
 (task)
using neural models on a dataset of short clips of classical music and their descriptions
 (method)
towards developing suitable multi-modal audio-textual encoding
 (valuable functionality)
in scenarios where the feasibility of an analytical task has been established projects may be targeted towards improving over the state of the art in some performance metric by using innovative methods/features/data this is typically the case if one works on leaderboard-type datasets where there are models
the client is a logistics company that wants to speed up its automatic package sorting
 (business objective)
we focus on the problem of handwritten address recognition from shipping label scans 
(problem and task)
we want to combine neural image recognition with language models on company-internal data
 (method and data)
to improve performance beyond the current model based on standard convolutional neural networks without language information
 (valuable functionality)
(business objective omitted due to project being primarily research)
for the task of span-based question answering from text
 (problem and task merged because span-based question answering is a common leaderboard task)
we want to combine graph-based knowledge bases with neural attention models
 (method)
to improve over state of the art performance on realistic news text
 (valuable functionality and data)
exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives
the client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient
 (business objective)
specifically he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified
 (problem and task)
we want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client 
(methods and data)
towards identifying correlating events across the production process that can be used for process optimization

(business objective omitted due to project being primarily research)
the development of ai dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bot’s replies
 (problem)
we want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols
 (methods and data)
in order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset 
(task / valuable insight)",We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds,What is the purpose of a system that retrieves audio pieces from short descriptions of their sounds?,1,0
1468,1471,Analytic Algorithms and Model Building,Data Science Patterns,Active Learning,Query Strategies,"this data science pattern is different from what you have studied in this course it makes assumptions about an algorithm and the data that is used to construct it active learning pattern posits that if an algorithm or learner can choose the data it will learn from it will perform better than an algorithm that does not choose its own data and it will perform better with less training active learning is sometimes referred to as query learning the learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods when you have a large data set that is unlabeled (as is typical) active learning can be a useful technique for labeling
active learning presents scenarios that allow a learner to query the labels of observations in a dataset
membership query synthesis is a scenario that means a learner will generate an observation ie the learner will create a data point that is similar to one or more in the dataset once it is created the new observation will be labeled by the oracle (an information source or teacher)
stream based selective sampling scenario involves unlabeled data points or observations that are examined with the algorithm evaluating its informativeness against query parameters the learner will decide if it should assign a label or query the oracle
pool based sampling as shown in the figure below assumes that you have a pool of unlabeled data and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points) the informativeness measure is applied to all observations in your dataset and then the observations that have the most important measures are selected the selected observations are then labeled
thought: informative data points equal a data point that your algorithm had difficulty classifying informative data points improve your algorithm's abilities (prediction and otherwise)
pool based active learning cycle-source: settles active learning survey1
how does the algorithm decide on the most informative measures let's highlight some of the strategies used to evaluate the informativeness of unlabeled data
uncertainty sampling is an approach that allows the active learner to query the observations about which it is not able to label
query-by-committee involves using group or committee of models that have been trained on a labeled dataset but the catch is that these models have competing hypotheses each model in the committee will vote on the labelings identify the query that all voting models disagree on that becomes the most informative query
expected model change will use an approach that selects the observation that would introduce the most change to a current model if its label was known
expected error change involves labeling the data points that would reduce the model's out of sample error (measure of how accurately your learner can make predictions on new data)
additional reading: survey of active learning this report gives an in depth review of active learning in machine learning and artificial intelligence",Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.,What is an approach that allows the active learner to query the observations about which it is not able to label?,1,1
1231,1234,Analytic Algorithms and Model Building,Data Science Patterns,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","let us assume that you have a medical dataset that contains observations with features including patient age weight height sex and race; you have been presented with the task of identifying the category of diagnosis for these observations in your dataset (the category would be diabetic or not) it would take a long time for you to research each observation and compare their features and symptoms to classical symptoms of diabetes using a data science approach you can assign a diagnosis to each observation based on the historical data for that diagnosis you would be answering a classification problem classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new dataset below you will find the different types of classification problems then later in the course we will explore methods that can be used to solve classification problems
classification tasks that are binary will classify observations in a dataset into two defined categories the observations are grouped based on the presence of characteristics unique to one of the two categories examples can include a decision on a credit card application (ie approve/deny)
also referred to as multinomial classification  classes in a classification task are three or more; observations can be classified into one of the three or more classes each observation can only be classified as one of the multiple classes an observation can not be labeled to multiple classes at once if a fruit image dataset is presented as a classification task a valid assumption would be that the observations will be labeled as one type of fruit from the multiple classes if the labels are orange pineapple peach and mango then each observation can only be classified as one type of fruit
you want to be careful not to confuse multi-label classification and multi-class classification to have the same meaning unlike multi class classification that has an assumption of observations having one class out of the multiple classes; the multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification
a quick thought: can you think about a scenario were observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)
multi-label classification can be applied to classifying textual data if you watch movies you know that some movies can belong to multiple genres eg romantic comedy romantic drama thriller comedy
let us stick with this example and conceptually define how a multi-label classification task would pan out
you are tasked to classify movies based on their plot we can assume that we have defined our analytic objective defined our requirements and we have gathered and prepared our data when you classify the observations in this dataset you might find movie a will belong to romance and comedy let us see the different multi-label classification techniques and how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardize the performance of your model
multi-label classification does not have constraints on the labels that an observation can have and this makes it difficult to learn using the onevsrest technique your classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes
similar to onevsrest the binary relevance technique trains a single label binary classifier for each class ie for each class an observation will either be predicted as belonging to that class or not this technique ignores any correlation between classes
the algorithm for your classification task can also adapt the algorithm to perform multi-label classification a popular example is using a multi-label version of the k-nearest neighbors (a supervised learning technique that makes the assumption that similar data points are always close together)
example: scikit-multilearn for mlknn
you can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier
x
y1
y2
y3
x1
0
1
1
x2
1
1
0
x3
1
1
0
x4
1
0
1
here we see that observations x2 and x3 belong to the same classes this technique will transform our task into a single multi class task and give a unique class to all possible combinations in your training data set
x
y1
x1
1
x2
2
x3
2
x4
3
so far we have seen that onevsrest binary relevance and label powerset techniques do not consider correlations between classes the classifier chains technique will build a chain of binary classifiers to take into account any correlations between classes the classifiers that are constructed equal the number of classes ie if we have classes: comedy drama and romance we will have three classifiers as well c1:c3
we should mention logistic regression in this section because it is an important classification technique you will learn more about it in the next module logistic regression uses a logistic function to model the probability of a class or event will you pass or fail a course will you develop high blood pressure based on certain attributes will 18-35 year old college educated men from georgia vote for the democratic or republican presidential candidate in the 2020 presidential elections the logistic regression model can have independent variables that are of diverse data types but the response is categorical","Similar to OneVsRest, the Binary Relevance technique trains a single label binary classifier for each class i.e. for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.",What technique trains a single label binary classifier for each class?,1,1
207,207,Data Gathering and Wrangling,Data Collection Process,Data Types and Sources,"Data Types,Numeric Data,Categorical Data,Qualitative and Quantitative Data,Structured and Unstructured Data,Internal and External Data,Data Sources","each data science project is unique and will require a data collection technique that suits the problem and objectives it seeks to meet the data collection techniques for a sentiment analysis project analyzing social media tweets will differ from the techniques of an image classification project for patient diagnosis when you defined the project's business and analytic objectives the tasks and methods were proposed as well those tasks and methods drive the type of data that will be used in the project the type of data will influence the source and data collection techniques once you have completed this module you will be able to distinguish between the various data types and sources alongside the traditional data collection techniques
data holds value to businesses and can present itself in various forms you identify data types as it affects the methods you will choose to develop an analytic solution data types are also dependent on its source so you can consider the type of data according to how it is represented in its source reference the link below for a refresher on the common data types:
reading: classes of data types
when discussing data types in data science data is typically categorized as numeric or categorical and classified as one of the four measurement scale types numeric data is represented as continuous or discrete values while categorical data can be nominal or ordinal values
discrete: there are 31 days in the month of may
ordinal: i can rate my customer service experience at the grocery store as good
continuous: you were born on may 13th 1997 so as of may 28th 2020 you are 23 years 2 weeks and 1 day old
nominal: what is your hair color brown
quantitative data is data that is expressed in numerically and derived from questions such as how many courses are you enrolled in this semester; how much do data scientists earn meanwhile qualitative data is descriptive data that is collected from observations instead of measurements qualitative data is sometimes transformed to enable it to be used in certain machine learning modeling techniques that require quantitative data both data types are collected and analyzed in different ways
structured data is organized facts that are presented in fixed formats and are easy to extract this data can be stored in spreadsheets relational databases and other repositories in the row and column format unstructured data is most difficult to extract it is not easily stored in typical relational databases and spreadsheets because it does not fit in the row and column structure it cannot be maintained in formats that are uniform text multimedia files and log files from servers are examples of unstructured data new generation databases also known as nosql databases were built to handle unstructured data unlike structured data unstructured data can be stored without a predefined schema
data can also be classified as internal data which is data collected and/or controlled by an organization an example would be personnel data collected and stored by the human resources department we also have external data data that is collected from sources outside of an organization census data and data gathered from credit reporting agencies are examples of external data
a company has surveyed its customers on their satisfaction with a newly released service the questions asked in the short survey include:
how much do you earn: numeric response
what level of education have you completed: categorical response
what industry are you employed: categorical response
how many people are in your household: numeric response
what part of the country do you reside: categorical response
the different types of data that we explored earlier are collected through different sources primary data sources includes data that is collected and processed by an organization and housed internally secondary data sources include data that is gathered from sources external to an organization keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affect data that is collected from primary and secondary sources as long as they are used by the organization
the largest data sourcethe web is considered the largest data source data from the web can be sourced from social media search engines and machine data in the next module we will focus on this source in depth
primary data can be data that is collected by a data scientist from first hand sources like through questionnaires and focus groups
secondary data is data used in a kaggle competition or a dataset from the popular uci machine learning repository  you did not collect that data and it has been used by others for research or data analysis
internal data is employee/personnel data an internal data source is a company's database that they control an external data source would be twitter and external data could be data that a researcher pulls from twitter users who are discussing a presidential debate
an example of primary data can be data that is collected by a data scientist from first hand sources like through questionnaires and focus groups an example of secondary data is data used in a kaggle competition or a dataset from the popular uci machine learning repository  you did not collect that data and it has been used by others  an example of internal data is employee/personnel data an internal data source is a company's database that they control an external data source would be twitter and external data could be data that a researcher pulls from twitter users who are discussing a presidential debate
example of internal data from secondary source: your company purchases potential client data from data brokers (external source) that data has now become your company's data that will be used for marketing advertising etc (it has now become internal data)
",An example of primary data can be data that is collected by a data scientist from first hand sources like through questionnaires and focus groups. An example of secondary data is data used in a kaggle competition or a dataset from the popular UCI Machine Learning Repository.  You did not collect that data and it has been used by others.  An example of internal data is employee/personnel data. An Internal data source is a company's database that they control. An external data source would be Twitter and external data could be data that a researcher pulls from twitter users who are discussing a presidential debate.,An example of internal data is what?,0,1
58,58,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning","Performance of a regression model can be assessed using the coefficient of determination or R2 which shows the amount of variation in y that is dependent on x. The larger the R2, the better the model can explain variation of the response with various predictors.",How can a regression model be assessed?,1,1
538,539,Analytic Algorithms and Model Building,Data Science Patterns,Ranking,Learning to Rank Algorithms Source-Lucidworks,"a data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is ranking learning to rank is a technique used to train a model for ranking tasks we do not always want to predict the probability of scenarios we might want to rank things ranking is used to solve information retrieval problems including collaborative filtering sentiment analysis and document retrieval the learning to rank technique is applied to supervised learning techniques to rank results according to relevancy when you are building a model using this approach you must decide on the features used but also the adequate relevance criteria
ranking algorithms or learning to rank algorithms are used in recommender systems fault localization computational biology among others
ranking approaches are applied to information retrieval problems assume that you have posted videos on a channel for subscribers to watch you want to know whether subscribers will click on that video and you might want to also
pointwise approach is used under the assumption that each query-document pair in a dataset has an ordinal or numerical score this approach can be used to predict the score of a single query-document pair the ranking task using the pointwise approach will become a classification or regression task
listwise approach reviews the list of documents and produces an optimal ordering
pairwise approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth) ranking using the pairwise approach becomes a classification or regression task
microsoft research developed the three known learning to rank algorithms that all use pairwise ranking:
ranknet uses the gradient descent to update the weights or model parameters for a learning to rank task this algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list
lambdarank uses a cost function to train a ranknet which results in speed and accuracy improvements
lambdamart uses multiple additive regression trees(mart-"" an implementation of the gradient tree boosting methods for regression and classification"") and lambdarank to solve a ranking task
wayfair: application of ltr
additional reading: from ranknet to lambdarank to lambdamart",Additional Reading: From RankNet to LambdaRank to LambdaMART,What is RankNet's name?,1,0
1803,1806,Analytic Requirements Gathering,Requirements Gathering Techniques,Successful Requirements Gathering,Validating Requirements,"the requirements gathering process is not linear it is important to consider evaluation at each step of the process doing this will ensure that errors are identified and fixed early as it can be costly and time consuming for errors to go undetected however it is a best practice to conduct validation and verification exercises for your requirements after they have been defined it is important to confirm that requirements meet the needs of the business and users and the requirements can be traced back to the defined business and analytic objectives
requirements should be validated to ensure that they are complete correct traceable verifiable and testable it seems logical to perform validation when the solution has been developed but before it is delivered to the client this is because you can test the actual solution but what happens when you find errors that require the project team to start from the beginning of the development process time and money have been wasted this grave mistake can be avoided if defined requirements are validated
validating requirements will most likely involve testing without implementation but remember that the project team will also validate and verify the requirements throughout the solution development process  validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost
requirements can be validated by following the steps below:
review requirements the requirements management plan will be peer reviewed to identify that each documented requirement is verifiable and unambiguous the peer review process should be well-defined so that reviewers can discuss their interpretations of requirements this will reveal ambiguity (if any) the reviewers will produce a summary of defects document this document will be used by the business analyst and project team to revise requirements
prototype requirements prototyping can help the project and client team determine if the requirements can be considered complete simulations are a popular prototyping technique but at this stage of solution development creating simulations will be time consuming there are less time consuming prototyping techniques that can help validate requirements proof of concept (poc) prototypes and paper mockups can be used to demonstrate the feasibility and completeness of requirements
test requirements requirements can be validated by users acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria acceptance criteria are set by end-users of the solution a project team might worry about the users inability to define the acceptance criteria the project team can guide users in thinking about how the solution meets their needs acceptance criteria can be defined using the smart goals once acceptance criteria are set users will conduct acceptance tests
a successful requirements gathering exercise is not only  deemed successful when the activities in the requirements management plan have been completed once the results of the requirements gathering process have been documented and validated a formal sign off will confirm that all parties approve and accept the defined requirements this is a signal that the project team can officially begin developing a solution/solutions","Requirements should be validated to ensure that they are complete, correct, traceable, verifiable, and testable. It seems logical to perform validation when the solution has been developed but before it is delivered to the client. This is because you can test the actual solution, but what happens when you find errors that require the project team to start from the beginning of the development process? Time and money have been wasted! This grave mistake can be avoided if defined requirements are validated.",What is logical to perform validation when the solution has been developed but before it is delivered to the client?,1,0
23,23,Model Evaluation,Interpreting Models,Model Interpretation Strategies,,"as a data scientist model interpretation means more than one thing to you and to your clients; in most cases it will mean different things to both parties the data scientist is interested in understanding the results of a task and how it can assist the client and their end users in making decisions a great resource by marco ribeiro explains end user empowerment as the secret weapon to building trust in a model the example given is of a doctor using a model to predict whether a patient has the flu there is a middle ""man"" between the prediction and the explanation of the prediction this explanation is what the decision maker (doctor in this case) will use to make the decision on the right diagnosis and treatment
interpretability is important to data science and machine learning because it directly affects the human decision makers and their understanding of the predictions made by models it is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module) it is often important to know what is predicted and why the prediction was made understanding the why will make the problem clearer and affect problem solving for future challenges
doshi velez & kim (2017) have explained a great detail some of the reasons why interpretability is important the ever growing and unsatisfied curiosity of humans (and by extension our thirst for learning) bias identification is another reason why interpretability is important why does a model grant loans to one person and not to another with similar credit scores and income detecting bias can also lead to better acceptance finally the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable
interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias the studies conducted can still be laden with bias)
looking to the next module is an overview of the assessments or metrics that typically concern you as the data scientist these metrics are a useful tool in deciding whether a model will be considered trustworthy
reading: should you trust that model
the authors of the above article proposed a technique to explain predictions and usefulness of any machine learning model they have tested this technique with a number of classifiers including neural networks for text and image classification
local surrogate models ""explain individual predictions of black box models""
shapley value is concerned with explaining a prediction by assessing the importance of features to the task
additional resource: sara hooker: the myth of the perfect model",Reading: Should you trust that model?,What model does reading have?,0,0
1535,1538,Analytic Requirements Gathering,Requirements Overview,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","suppose a utility company wants to introduce personalized service management to its customers including a secure electronic payment facility and the ability to view usage statistics this business objective will lead to the development of a mobile application for customer service management business requirements for the above scenario will include describing the context scope and background of the business need including reasons for the proposed change
 business requirements are collected and decomposed to define other types of requirements
once business requirements are defined stakeholders and systems that support the business requirement(s) are identified system requirements are a detailed description of the system and its operational and development constraints; this includes the system software that will support the solution processing memory and other application software considerations user requirements describe functions or tasks that a user must perform within the system these tasks will support the business objectives that are defined prior to the requirements gathering process
use cases and user stories represent user requirements and provide a big picture of what the user will be able to do within a system an example of a use case is “make payment” on a mobile application we will describe user and system requirements later in this unit
solution requirements are grounded in software engineering in this course we will be tailoring solution requirements to the data science process the solution requirements for a data science project are classified into functional non-functional requirements as well as requirements that consider parts of an analytic solution that are different from the traditional it systems the typical analytic solution will consider data and models (eg predictive models) a business intelligence solution will include requirements for reports and dashboards","Suppose a utility company wants to introduce personalized service management to its customers including a secure electronic payment facility, and the ability to view usage statistics. This business objective will lead to the development of a mobile application for customer service management. Business requirements for the above scenario will include describing the context, scope, and background of the business need, including reasons for the proposed change.",What is the purpose of a mobile application for customer service management?,1,1
1674,1677,Data Gathering and Wrangling,Data Wrangling Pipeline,Data Wrangling:Transforming Data,"Handling Missing Values,Imputation,Omission,Outliers,Transforming: Categorical Data,Transforming: Numeric Data","the data gathering process looks different for each data related project and is dependent on your business and analytic objectives as well as your data source(s) the data that you acquire during the data gathering process will almost always need to be transformed into a usable format this means it will have variables that contain incorrect or missing values due to data entry errors data might need to be transformed to a different structure to meet the requirements of a data science task
one of the most common data quality issues is missing values in your dataset this can happen due to human error or system issues during data collection as you have inspected your data and identified missing values it is important to determine why the dataset has missing values please note that a dataset that was extracted from an external source might not provide context on the reason behind the missing values when met with such a situation a data scientist or data analyst should still investigate the missing values the reasons behind the missing values will determine the techniques used to handle those values
one of the most notable statisticians classified missing data into three categories those categories explain the likelihood of missing data
missing completely at random (mcar) implies that missing data is not related to the data the probability of data being missing is the same for all observations
missing at random (mar) is the probability that the missing data is the same within certain groups
data that is not missing at random (nmar) means that the probability of data being missing varies for reasons that are unknown
the common strategies that are employed in handling missing values are imputation and omission imputation replaces missing values in the dataset with other values the replacement values are not random you can replace missing values with the mean value eg if you have missing values in the age variable you can replace the missing values with the mean age across all observations this method will work if the group is homogeneous as you know your dataset will not always contain homogeneous groups in this case you will use other imputation techniques that we will discuss in the feature engineering unit those techniques include hot and cold deck imputation regression imputation and interpolation and extrapolation
omission is often the go to technique when there are missing values omission involves excluding the missing values from the dataset remember: you might suffer loss of data if you exclude values instead of finding other missing value handling techniques omission can be done when the amount of missing values is small pairwise deletion is a type of omission it means performing your analysis on just the available values keep in mind that sample sizes will vary with this technique listwise deletion removes all data for an observation that has one or more missing values this would mean your dataset would have observations with values for all variables
you can also omit variables with missing values that variable needs to be one with little to no importance to your dataset and overall objective example i am predicting social media usage habits and my dataset includes a shoe size variable with missing values i can omit that variable
subsetting this process involves extracting portions of a dataset that are relevant to your model or analysis it is a process that is used in data wrangling to prepare data for exploratory data analysis it is a technique that can be used to remove observations with missing values subsetting can also involve excluding variables instead of observations an example is looking at summary measures of three subsets of medical records for diabetes treatments were one subset is for successful treatments another is for unsuccessful treatments and the last is for inconclusive treatments
when you were inspecting your data there was mention of visualizing the data to identify outliers outliers are unusual values in the dataset the values is unusual because it ""lies at an abnormal distance from other values in your dataset"" we discuss using exploratory data analysis techniques to identify outliers in a future unit you should not immediately remove outlier values as they often times can contribute valuable insights to your solution investigating the reason behind the outlier value is your first step in handling it
as you learnt previously there are different types of data and those types of data have specific data transformation techniques that accommodate them
categorical data is divided into groups or nominal category based on a qualitative characteristic gender race and eye color might be variables in a dataset that are useful in predicting a health challenge and due to their low levels of measurement there is a need to transform the data into a numeric format there are techniques that are employed to transform categorical variables
category reduction categorical variables can have many categories or levels a variable with levels that are not useful can negatively affect your analysis and model some categorical variables will have levels that do not occur it will be difficult to capture the interactions within those levels a technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences
creating category scores ordinal data would need to transformed into numeric values for certain statistical techniques ranked values are an example a dataset containing student evaluations would have responses that are ranked by different levels you can transform that data by assuming equal increments between category scores responses to the question:the instructor provided out of class support for the course could be: always most times sometimes hardly never you can assign a score of 1-5 1 being the highest and 5 being the lowest or vice versa the categorical variable can now hold numeric values
creating dummy variables dummy variables are often referred to as binary variables this technique allows for categorical data to be transformed to 0s and 1s a dataset containing customer spending data with a categorical variable-gender with two categories male and female the gender variable can be converted to binary variables please note that there is no order of ranking
creating dummy variables for more than one category what happens when you have a categorical variable containing more than one category consider a dataset with the variable hair color with data represented as brown brunette black gray and blonde the hair color variable can still be transformed into dummy variables using the following steps
determine the number of variables (k-1) where k is the number of categories of the variable
you will create 4 dummy variables (5-1)
you will then create variables for each category in our example we will create black brown  brunette and gray variables
assign 0 or 1 to each category for example the black variable would see values of 0 if an observation does not have black hair and 1 if the observation has black hair
keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset  in this example a dummy variable for blonde was not created this simply means that all other categories will be compared to this category usually you select the category with the largest occurrences as the category that will not transformed into a dummy variable
categorical data is transformed to numeric data so the data can be used for specific statistical techniques  why would we need to transform numeric data if you remember when data is gathered it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task; this will ensure that you do not lose data or lose information during the analysis phase you will encounter numeric data that needs to be transformed to allow you glean insights and use the data for the appropriate statistical techniques
example of a popular numeric transformation scenario is converting date of birth to age
numeric transformations are also explored when performing feature engineering  you will extract features from the numeric data and transform them into formats that can be used by a machine learning model we will explore those techniques in depth right now let us take a look at the techniques for converting numeric data during data wrangling
binning this technique will transform a numeric variable into a categorical variable as mentioned above age can be grouped into intervals in the event that that age group has similar occurrences in the other variables in a dataset if you decide to bin the age variable and create the following groups: 15-19 20-24 25-29 and 30-34 you can reduce redundancy in your dataset and capture outliers binning can also be done using equal intervals
using mathematics you can create new variables using mathematical transformation of existing variables you can use techniques such as standardization min-max scaling and logarithmic transformation we explore these mathematical transformation techniques in a future unit","As you learnt previously, there are different types of data and those types of data have specific data transformation techniques that accommodate them.",What are different types of data?,1,1
1596,1599,Data Gathering and Wrangling,Data Gathering,Data Management,"Data Gathering Overview,Data Management","a data scientist’s role involves utilizing computational and statistical skills to uncover solutions that meet business needs throughout the process of discovery and solution development a data scientist must find it most useful and efficient to focus on developing algorithms and building the models that will support the analytic solution for the business need this unit sheds light on a crucial phase of the data science project lifecycle that will inform the solution development
previously you learned about understanding the business by defining a business problem setting business and analytic objectives and gathering requirements going forward you will learn about the next phase in the data science project lifecycle data understanding the data understanding phase involves data acquisition and data preparation data acquisition involves gathering data from different sources and transforming the data into formats that are suitable for analytic solution development as a data scientist you will spend more than 70% of your time understanding exploring and transforming the data used for model building this is important because the quality of your data has a direct impact on the quality of your analytic solution through your readings and discussions outside of this course you will hear data professionals say ""garbage data in-garbage decisions out"" you want to pay close attention to the data that is used to develop your analytic solution
now that the requirements phase is completed the data science team will embark on data acquisition or data gathering in this unit we will describe the data gathering process to include data management considerations and its implications on the quality of data data collection processes and tools and the data wrangling pipeline although the data science project team might not be able to control the data management operations of their data sources understanding the principles of data management will assist the team in gathering data that is reliable valid and has integrity even the most secure and reliable data needs to be transformed into a format that can be used to develop analytic solutions this transformation process happens in various stages of the data gathering and processing phase a popular term that is referenced in the data transformation process is data wrangling this is the process of gathering selecting and transforming data to ensure that it is usable free of noise and has as little bias as possible to meet defined analytic objectives this process will include checking for missing values identifying outliers and formatting the data
once the data is “wrangled” the data science project team will continue to work with the data until it is ready to be used
you might be wondering if it is not enough to begin working on that analytic solution now that you have defined your objectives now that the project team has defined a data statement set analytic objectives and gathered requirements what more do you need to do to start modeling
prematurely moving past the data understanding phase will result in bad results for your client if your team does not understand data quality and how it affects your solution your solution will be incomplete data management supports the data science process by making sure that the data housed within an organization is accessible and accurate data management is a strategic initiative it is an organization’s way of acquiring storing securing and processing data
the data management structure is managed by the it team in an organization however business users will participate in this initiative as well data management is such an important aspect of a business that over 100 data management practitioners compiled a best practices framework to guide organizations on managing their data and information infrastructure the data management body of knowledge (dama-dmbok) highlights eleven knowledge areas as shown in the figure below these knowledge areas are instrumental to the success of an organization’s data infrastructure
the eleven knowledge areas of data management
referencing the figure above you will note that there are different skill sets and departments involved in the data management process the data management process is not entirely a data science taskit is a data infrastructure task data management informs the data gathering process and can influence the implementation of analytic solutions the data architects in an organization are responsible for designing the organization's data management framework this framework will assist the data science team and other data stakeholders in the organization to use data based on defined policies and regulations to meet business and analytic objectives
data governance – planning oversight and control over management of data and the use
 of data and data-related resources while we understand that governance covers
‘processes’ not ‘things’ the common term for data management governance is data
governance and so we will use this term
data architecture – the overall structure of data and data-related resources as an integral
 part of the enterprise architecture
data modeling & design – analysis design building testing and maintenance (was data
 development in the dama-dmbok 1st edition)
data storage & operations – structured physical data assets storage deployment and management (was data operations in the dama-dmbok 1st edition)
data security – ensuring privacy confidentiality and appropriate access
data integration & interoperability –acquisition extraction transformation movement
 delivery replication federation virtualization and operational support ( a knowledge area
 new in dmbok2)
documents & content – storing protecting indexing and enabling access to data found in
 unstructured sources (electronic files and physical records) and making this data available
 for integration and interoperability with structured (database) data
reference & master data – managing shared data to reduce redundancy and ensure better
 data quality through standardized definition and use of data values
data warehousing & business intelligence – managing analytical data processing and enabling access to decision support data for reporting and analysis

metadata – collecting categorizing maintaining integrating controlling managing and
 delivering metadata

data quality – defining monitoring maintaining data integrity and improving data quality
~source
additional reading: data management for gdpr","Data Modeling & Design – analysis, design, building, testing, and maintenance (was Data","What – analysis, design, building, testing, and maintenance?",0,0
1812,1815,Analytic Requirements Gathering,Requirements Gathering Techniques,Successful Requirements Gathering,Validating Requirements,"the requirements gathering process is not linear it is important to consider evaluation at each step of the process doing this will ensure that errors are identified and fixed early as it can be costly and time consuming for errors to go undetected however it is a best practice to conduct validation and verification exercises for your requirements after they have been defined it is important to confirm that requirements meet the needs of the business and users and the requirements can be traced back to the defined business and analytic objectives
requirements should be validated to ensure that they are complete correct traceable verifiable and testable it seems logical to perform validation when the solution has been developed but before it is delivered to the client this is because you can test the actual solution but what happens when you find errors that require the project team to start from the beginning of the development process time and money have been wasted this grave mistake can be avoided if defined requirements are validated
validating requirements will most likely involve testing without implementation but remember that the project team will also validate and verify the requirements throughout the solution development process  validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost
requirements can be validated by following the steps below:
review requirements the requirements management plan will be peer reviewed to identify that each documented requirement is verifiable and unambiguous the peer review process should be well-defined so that reviewers can discuss their interpretations of requirements this will reveal ambiguity (if any) the reviewers will produce a summary of defects document this document will be used by the business analyst and project team to revise requirements
prototype requirements prototyping can help the project and client team determine if the requirements can be considered complete simulations are a popular prototyping technique but at this stage of solution development creating simulations will be time consuming there are less time consuming prototyping techniques that can help validate requirements proof of concept (poc) prototypes and paper mockups can be used to demonstrate the feasibility and completeness of requirements
test requirements requirements can be validated by users acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria acceptance criteria are set by end-users of the solution a project team might worry about the users inability to define the acceptance criteria the project team can guide users in thinking about how the solution meets their needs acceptance criteria can be defined using the smart goals once acceptance criteria are set users will conduct acceptance tests
a successful requirements gathering exercise is not only  deemed successful when the activities in the requirements management plan have been completed once the results of the requirements gathering process have been documented and validated a formal sign off will confirm that all parties approve and accept the defined requirements this is a signal that the project team can officially begin developing a solution/solutions","Test requirements. Requirements can be validated by users. Acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria. Acceptance criteria are set by end-users of the solution. A project team might worry about the users inability to define the acceptance criteria. The project team can guide users in thinking about how the solution meets their needs. Acceptance criteria can be defined using the S.M.A.R.T. goals. Once acceptance criteria are set, users will conduct acceptance tests.",What is done to assess if a solution will meet predefined criteria called acceptance criteria?,1,1
516,517,Exploratory Data Analysis,Feature Engineering,Summary and Quiz 5,,"in this module we explored a technique used to convert raw data from its numeric or categorical format to a format that can be used when performing modeling
mathematical models mostly ""understand"" data in numeric form hence the need for transforming raw data
during the transformation process features should be normalized as needed to meet the assumptions of the mathematical models
categorical variables can be transformed to numeric format using feature engineering techniques including categorical variable encoding
bias can be introduced to the data during the feature engineering process this bias could be conscious or unconscious there are different kinds of biases and ways to control for these biases
principal component analysis is the first introduction to modeling it is a way of using modeling techniques for dimensionality reduction in the dataset
now that we have completed the data understanding phase we will transition to modeling","Now that we have completed the data understanding phase, we will transition to modeling.",What phase will we transition to?,0,0
264,264,Analytic Algorithms and Model Building,Data Science Patterns,Summary and Quiz 6,,"prediction involves using a model to predict outcomes and inference tasks include understanding how your dependent variable y is affected by changes in independent variables
parametric methods assume that your dataset comes from a population that has a fixed set of parameters non-parametric methods seek an estimate of f that is close to the data points
if you are interested in inference a restrictive linear model that shows the relationship between your dependent variable (y) and independent variables will be the best choice for estimating f when prediction is the task of interest flexibility is key and because interpretability is not a high concern you can use the methods with less interpretability
classification involves an existing dataset that has labeled outcomes and seeks to label the outcomes of a new dataset
types of classification problems include binary classification multi-class classification and multi-label classification
logistic regression uses a logistic function to model the probability of a class or event
regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
simple linear regression model  allows for predicting a response based on one predictor variable single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values)
when we want to study the hidden structure of data and identify different groups within that structure we use the cluster analysis technique
a data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is ranking
active learning pattern involves an algorithm or learner that can choose the data it will learn from it will perform better than an algorithm that does not choose its own data and it will perform better with less training
note: please ask questions about the quiz (if any) by posting a private question to piazza on canvas or by sending an email",A goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values).,What will highlight the lack of balance between observations in the dataset and those that will be introduced to the model?,1,1
676,677,Exploratory Data Analysis,Performing Exploratory Data Analysis,Information Design Overview,"Summarizing Data,**Do you know that if the mean and median of your data set differ greatly, you should check that variable for outliers!**,Source: BPI Consulting LLC,Relationships and Association,Correlation Coefficient,Meaning,Outliers,Box Plot. Source. University of Manchester,Graphical Summaries,One of the first Cartesian Coordinates Graphs,Visualizing Variables","the exploratory data analysis (eda) process is comprised of visualizing data to allow a data scientist or a data analyst explore datasets to gain insights from the data there are some non-graphical techniques that are used to explore data and as well as graphical techniques non-graphical techniques include using summary statistics to describe the data and the graphical techniques are used to describe the frequency distribution of the dataset both techniques can be used to show the skewness and extreme outliers in a dataset
summarizing data is dependent on the types of data present in your dataset when you want to understand the observed data aka your sample you will key descriptors of a dataset when you have a large data set it is difficult to describe it in its raw form you will use specific techniques to summarize and describe your sample some of those techniques include describing central tendency and assessing measures of spread and relationships
you can use the location shape and spread of the data in a dataset to understand the data you might find some of the concepts below as a review of your first statistics course but pay attention to the reason for using these techniques in exploring your data also these concepts are important for when you learn about using statistical inference to draw conclusions on an unknown population parameter
location during the eda process we are looking to describe our data using a central value we will explore the mean which is sometimes called the average this is the sum total of all observations divided by the number of observations you will calculate the mean for your population (population mean = μ)  and for the sample that you have drawn (sample mean = x̄) there are different types of mean including the typical arithmetic mean geometric mean and the harmonic mean
reading: central tendency with different types of means
median is the mid value of a dataset to calculate a median value you will sort your data in ascending order the median value in a data set with odd number of observations is the middle value while you can find the median of a dataset with even observations by calculating the average of the two middle values
mode is the variable of an observation that most frequently occurs in your dataset a uni-modal variable is one that has just one mode and a bimodal variable has two modes if your variable has more than two modes it can be referred to as multi-modal do not think it is useless in the eda process the mode is quite useful when summarizing categorical variables
percentile you will remember this nifty word from your gre scores or height and weight data from your health records it tells you the position of a value in your dataset  if you are 175cm in height and you are in the 10th percentile of height measurement for your gender it means that among all the height data collected for your gender you are taller than 10% of those observations the 50th percentile is considered the median quartiles are values that split the data into quarters
spread how can you describe the spread or variability of your dataset you use the measures of dispersion although used for descriptive statistics you must be careful in relying on this measure to describe variability
range of a set of values can be calculated by subtracting the minimum value in your dataset from the maximum value notice that range only considers two values and ignores all other values of a variable
mean absolute deviation is the ""average distance between each value and the mean of a dataset"" this measure of dispersion can tell you how values are spread out in a dataset and determines whether the mean is a useful indicator of the values within the data the larger the mean absolute deviation the more spread out the data when you work with time series forecasting methods you will use the mean absolute deviation to measure the performance of a forecasting model you will find that the measures used in eda are often used in model selection criteria
reading: mean deviation =  (σ|x − μ|)/n this is the sum of the absolute values minus the mean and divide this by the number of values in the dataset
variance s2 / σ2 is the average of the squared difference between the observations in a dataset and the mean so far we know that you can have measures for both your overall population and your sample drawn from the population keep in mind that you will sometimes have a sample variance s2  or a population variance σ2 the difference between both is that you are looking at the spread of data from the sample mean versus the population mean
standard deviation σ or s is the most common measure of dispersion it tells you the distance of the values from the mean in your dataset a low standard deviation tells you that the values are close to the mean and a high standard deviation means there is a spread the σ is derived by calculating the square root of the variance as you perform exploratory data analysis and even while developing models the importance of the standard deviation can not be overstated despite its mention as a way to summarize data standard deviation is used to ""measure the confidence in statistical conclusions"" and if you remember from the overview of this unit you will be conducting statistical inference to begin to draw some conclusions on your data and hypotheses
interquartile range (iqr) similar to the range does not consider all observations when looking at the spread of values in a dataset iqr describes 50% of values in your dataset when arranged in ascending order the iqr is the difference between the values in quartile 3 and the values in quartile 1 you can use this measure to identify a value that is an outlier
shape now that you can explain the measures used to explore data by describing its central value its spread from the mean and identified outliers let us describe the distribution of a dataset and assess whether it is normally distributed normally distributed data is useful when making statistical inferences how can we assess the distribution of our data:
skewness measures the degree to which the distribution of data lacks symmetry a dataset with 0 skewness is considered normally distributed data does not always have a skewness of 0; however if you have found skewness to be between -05 and 05 you can ascertain that your data is symmetrical if skewness is between -1 and -05 or 05 and 1 then your data is moderately skewed if skewness is < -1 or > 1 your data is highly skewed
kurtosis looks at the outliers within the distribution this measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution
reading: kurtosis
covariance describes the linear relationship between variables in your sample or population data covariance can be negative meaning your variables have a negative linear relationship zero (0) meaning the variables have no linear relationship or positive meaning a positive linear relationship exists between the variables
correlation coefficient will describe the strength of the linear relationship between the variables x and y it is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables if the correlation coefficient equals:
-1
variables have a perfect negative linear relationship
0
variables are not linearly related
1
variables have a perfect positive linear relationship
whenever you have a rather small or large observation within your dataset compared to other values in the dataset this is called an outlier outliers will affect the performance of your model and prior to getting to that point your exploratory data analysis when you have a large dataset the outliers are not as noticeable as when you have a smaller dataset similar to missing values you must handle outliers when you identify them in your dataset you should refrain from removing them from the dataset until proper investigation is completed you can ""handle"" outliers by following these steps:
construct a boxplot or as it is sometimes called a box and whisker plot this chart is used to graph the five number summary the five number summary is used to identify an outlier in your dataset a five-number summary consists of five values including the the maximum and minimum values in your dataset the lower and upper quartiles and the median these values are then ordered in ascending order and plotted
box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics including the skewness and mean you will typically plot your box plot using a tool of your choice in this course that would be python here is a resource on how to plot a box plot using python
z-score is a measure of the relative position of an observation within a dataset you calculate the z-score by subtracting the value from the mean and dividing that value by the standard deviation if an observation has a z-score that is more than 3 or less than -3 it is an outlier
the saying that a picture tells a thousand words supports the notion that visualizing data is one of the best ways to tell a story to a wide audience with the ability to view the visuals today there are so many tools that are used to visualize data including python r excel tableau microsoft's powerbi among others these tools even allow us to make data visualizations interactive for more effective analysis rene descartes invented the cartesian coordinate system or the x and y axis graph as we know it
data visualizations have evolved greatly including info-graphics plots and statistical graphs communicating insights in the data clearly is a key step in the data science process there is a skill to developing effective visualizations and not all data scientists will have these skills however it is important to know how to use the tools that support visualization and be able to identify the right types of visualizations for the data types and the best ways to tell your story with visualizations visualization experts strongly suggest that visualizations should stimulate the attention of viewers and elicit thoughts and questions during the data science project process visualizations will be used at different times at this stage of exploratory data analysis the visualizations are used to understand the dataset as you proceed through analytic solution building gaining a better understanding of your data will support the performance of your models in the later stages of the project
so far we have talked about visualizing mostly numeric data however we are able to visualize categorical data as well typically this data is categorized and numeric values are derived (counted) to represent the values in the variables a frequency distribution is a useful technique in displaying categorical variables
follow our primer below to learn more about the appropriate visualizations for data science tasks
reading: data exploration and visualization primer
as we study exploratory data analysis we will learn how to describe sample data by making inferences from the population on which the sample data was drawn despite the sampling techniques used there is still a level of uncertainty in drawing conclusions about your population we find that descriptive statistics is not enough to estimate this uncertainty; hence the need for statistical inference on the next page we will explore statistical inference further
additional reading: the dr howard seltman experimental design and analysis book is a text that explores experimental design and analysis and is worth a look through
stephen few3 listed eight types of quantitative data and the types of charts and graphs that can communicate the story in the data
reading: selecting the right graph for your message","Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.",How does a skewerness of 0 vary?,1,0
61,61,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning",Ordinary Least Squares Source3,Ordinary Least Squares Source3?,0,0
721,723,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make informed decision regarding the use of these terms.","if you have not already you should notice a pattern at this stage the conceptual progression business need ⇒ business objective ⇒ analytic objective including: problem statement ⇒ task definition ⇒ method & data statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against the statement of methods to be applied will vary in specificity depending on your project but three elements are important:
it must be precise enough so that the data science team understands it as a concise summary of the technical approach

at the same time it should allow for testing different techniques around the main conceptual idea

the methods should in principle be suitable to produce the target functionality/insight for the task given the available data
as you become more proficient in the discipline of data science (eg after having absolved the more advanced units of this course) you will develop refined intuitions about which method to propose in which context the most general categories of methods one can identify in this statement typically include:
supervised learning methods which involves learning to predict a target variable (typically through regression or classification) by training on “true” example data points whose target variable has manually been labeled or is available by other means
unsupervised learning methods deal with finding patterns in unlabeled data without an explicit prediction target
semi-supervised learning methods encompass hybrid methods that combine supervised and unsupervised learning in different ways
this course is very focused on the application of machine learning which has large overlap with various kinds of statistical methods it is possible to phrase your method statement around the use of statistics but it is recommended that one qualifies this rather broad term to something adequate for the project
in many contexts the method will have to be stated much more precisely than that especially if the problem domain is already well-studied in data science or there is some prior work that should be extended it can range from specifying a particular family of models (eg linear vs non-linear) using a new feature set testing the explainability of a particular model’s predictions to optimizing hyperparameters for faster training and inference in neural networks and more we can illustrate the specific vs general statement in the context of an example:
your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the personnel does not reliably find all potential defects (problem) your model should be able to predict product failure within one month after sale (task 1) and identify predictors measured at quality control time (task 2) in a pilot project you propose to apply “traditional supervised learning methods to train a classifier and examine the model for predictive variables” you then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation
variation: your pilot project was a success and your models are being used to better inform quality control personnel however it turns out that it still produces a high false positive rate (problem) and causes shipping delays which the client wants to minimize (business objective) you are asked to improve the model’s performance by reducing its false positive rate (task) you are further given access to quality control diagnostic equipment readings which the engineers believe are useful to discern whether a product is defective or just “needs to be broken in” you hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model
the method and data are the heart of a data science project finding the best tool for the task is of course one of the core skills of being a good data scientist this course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects as you gain deeper knowledge and experience you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with the state of the art at the same time thinking through different approaches and deciding on a set of methods and datasets benefits from teamwork open discussion and active seeking of advice and feedback from your peers mentors and relevant specialists
a good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results
in the above electronics scenario you imagine training a logistic regression model on the features to predict product failure once trained influential features should receive high weight in the regression equation allowing you to identify them easily similar to a correlation analysis if you train a decision tree you can identify features by traversing its branches on the other hand if you were to propose to train a complex random forest model to predict product failure with maximal accuracy one may object that random forests are not as easily interpretable as simpler models it would hence be an unsuitable method for task 2 in the pilot stage a good way would be to go with the simpler models for now and leave more complex models for later phases if needed in the variation a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommend you use a more recently developed encoding you may research both algorithms and make an informed decision or even use both algorithms in a comparative experiment","If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need ⇒ Business Objective ⇒ Analytic Objective including: Problem Statement ⇒ Task Definition ⇒ Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:",What is the purpose of the conceptual progression Business Need  Business Objective  Analytic Objective including: Problem Statement  Task Definition  Method & Data Statement?,0,0
1590,1593,Data Gathering and Wrangling,Data Gathering,Data Management,"Data Gathering Overview,Data Management","a data scientist’s role involves utilizing computational and statistical skills to uncover solutions that meet business needs throughout the process of discovery and solution development a data scientist must find it most useful and efficient to focus on developing algorithms and building the models that will support the analytic solution for the business need this unit sheds light on a crucial phase of the data science project lifecycle that will inform the solution development
previously you learned about understanding the business by defining a business problem setting business and analytic objectives and gathering requirements going forward you will learn about the next phase in the data science project lifecycle data understanding the data understanding phase involves data acquisition and data preparation data acquisition involves gathering data from different sources and transforming the data into formats that are suitable for analytic solution development as a data scientist you will spend more than 70% of your time understanding exploring and transforming the data used for model building this is important because the quality of your data has a direct impact on the quality of your analytic solution through your readings and discussions outside of this course you will hear data professionals say ""garbage data in-garbage decisions out"" you want to pay close attention to the data that is used to develop your analytic solution
now that the requirements phase is completed the data science team will embark on data acquisition or data gathering in this unit we will describe the data gathering process to include data management considerations and its implications on the quality of data data collection processes and tools and the data wrangling pipeline although the data science project team might not be able to control the data management operations of their data sources understanding the principles of data management will assist the team in gathering data that is reliable valid and has integrity even the most secure and reliable data needs to be transformed into a format that can be used to develop analytic solutions this transformation process happens in various stages of the data gathering and processing phase a popular term that is referenced in the data transformation process is data wrangling this is the process of gathering selecting and transforming data to ensure that it is usable free of noise and has as little bias as possible to meet defined analytic objectives this process will include checking for missing values identifying outliers and formatting the data
once the data is “wrangled” the data science project team will continue to work with the data until it is ready to be used
you might be wondering if it is not enough to begin working on that analytic solution now that you have defined your objectives now that the project team has defined a data statement set analytic objectives and gathered requirements what more do you need to do to start modeling
prematurely moving past the data understanding phase will result in bad results for your client if your team does not understand data quality and how it affects your solution your solution will be incomplete data management supports the data science process by making sure that the data housed within an organization is accessible and accurate data management is a strategic initiative it is an organization’s way of acquiring storing securing and processing data
the data management structure is managed by the it team in an organization however business users will participate in this initiative as well data management is such an important aspect of a business that over 100 data management practitioners compiled a best practices framework to guide organizations on managing their data and information infrastructure the data management body of knowledge (dama-dmbok) highlights eleven knowledge areas as shown in the figure below these knowledge areas are instrumental to the success of an organization’s data infrastructure
the eleven knowledge areas of data management
referencing the figure above you will note that there are different skill sets and departments involved in the data management process the data management process is not entirely a data science taskit is a data infrastructure task data management informs the data gathering process and can influence the implementation of analytic solutions the data architects in an organization are responsible for designing the organization's data management framework this framework will assist the data science team and other data stakeholders in the organization to use data based on defined policies and regulations to meet business and analytic objectives
data governance – planning oversight and control over management of data and the use
 of data and data-related resources while we understand that governance covers
‘processes’ not ‘things’ the common term for data management governance is data
governance and so we will use this term
data architecture – the overall structure of data and data-related resources as an integral
 part of the enterprise architecture
data modeling & design – analysis design building testing and maintenance (was data
 development in the dama-dmbok 1st edition)
data storage & operations – structured physical data assets storage deployment and management (was data operations in the dama-dmbok 1st edition)
data security – ensuring privacy confidentiality and appropriate access
data integration & interoperability –acquisition extraction transformation movement
 delivery replication federation virtualization and operational support ( a knowledge area
 new in dmbok2)
documents & content – storing protecting indexing and enabling access to data found in
 unstructured sources (electronic files and physical records) and making this data available
 for integration and interoperability with structured (database) data
reference & master data – managing shared data to reduce redundancy and ensure better
 data quality through standardized definition and use of data values
data warehousing & business intelligence – managing analytical data processing and enabling access to decision support data for reporting and analysis

metadata – collecting categorizing maintaining integrating controlling managing and
 delivering metadata

data quality – defining monitoring maintaining data integrity and improving data quality
~source
additional reading: data management for gdpr", of data and data-related resources. While we understand that governance covers,What is governance covered by?,1,0
103,103,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning",Additional Reading: Introduction to Statistical Learning.,What is the introduction to statistical learning?,0,0
1975,1978,Model Evaluation,Evaluation Metrics,Clustering Evaluation Metrics,,"the previous page focused on the metrics for evaluating supervised learning problems  the presence of labeled data makes it somewhat straightforward to train and test the model's performance now we will focus on metrics that can be used when labeled data is not present  there are two approaches to evaluating clustering the internal and external evaluation approaches the internal approach involves summarizing the clustering task to a single quality score while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable clustering can also be evaluated by an expert
internal evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score the cluster with the best score is seen to be the best internal evaluation although useful can have its drawbacks it gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters  a sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters and many evaluation indexes assume convex clusters on a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity is sound
let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
silhouette coefficient shows how similar a data point is to its cluster compared to other clusters it is calculated using the mean intra cluster distance and the mean nearest cluster distance for each data point a silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster when the silhouette coefficient is close to 0 there is a presence of overlapping clusters
dunn index is also used to evaluate clustering techniques and similar to the silhouette coefficient it is dependent on the data within the clusters a good clustering is one with a higher dunn index when using this evaluation technique you want to be aware of a high computational cost when you have a large number of clusters the dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters the minimum of the pairwise distance is used to determine minimum separation (minseparation) the compactness of a cluster is measured by computing the distance between the data in the same cluster (maxdiameter) finally the dunn index will be:
minseparation/maxdiameter
external evaluation measures the results from a clustering task based on data not used for the clustering task benchmarks are set from a set of pre-classified data external evaluation techniques need ground truth data to evaluate clustering
rand index tells you how similar a cluster or clusters are to a set benchmark this is similar a classification evaluation technique you can calculate the rand index thus:
(tp + tn)/(tp+fp+fn+tn)
purity is considered a no frills technique that assigns each cluster to a class (usually one that occurs often in the cluster) the number of correctly assigned observations is divided by the overall number of observations to determine accuracy purity close to 1 is best and close to 0 is not optimal a large number of clusters can lead to a higher purity there is a tradeoff between quality of clustering and number of clusters when using purity as a metric the normalized mutual information (nmi) can be used to measure and compare the quality of clustering between different clusterings with varying number of clusters
jaccard index is used in cluster analysis evaluation and convolutional neural networks among others it is defined as ""the size of the intersection divided by the size of the union of the sample sets"" the jaccard distance measures dissimilarity between sample sets
f-measure is simply computed as the 2 * ((precision * recall)/(precision + recall)) you might remember it from the classification metrics it is also known as the f1 score
dice index also known as the sorensen-dice indexor dice coefficient can assess the similarity of two samples it ranges from 0 to 1 dice index is a semimetric version of jaccard index and gives less weight to outliers in a dataset it is used to measure the lexical association score of two words
reading: clustering evaluation techniques (20min read)
you should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis this is sometimes done by comparing a dataset against another (one without clusters) the hopkins statistic is used to measure cluster tendency a resulting value of 1 or close enough will show that the data is clustered data that is uniformly distributed will be closer to 0 hopkins statistic is quite good at estimating randomness in a dataset",External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,External evaluation techniques need what to evaluate clustering?,1,1
1061,1064,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistical Inference,"Statistical Inference,Sampling Distribution,Confidence Interval,168.8cm to 181.2cm,Hypothesis Tests","statistical inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods those conclusions are typically drawn using exploratory data analysis or summary statistics the goal of this process is to use probability theory to make inferences about your data  this is the first step of learning about the attributes of your population from the sample that you have drawn
reminder: the characteristics of the sample dataset are called statistics the characteristics of your population are known as parameters
understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes
if you recall from a previous unit you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise you can use statistical inference to draw scientific conclusions and test set hypotheses significance of a sample data set or descriptive statistics is often in question during the eda process using statistical inference techniques can give significance to your conclusions from eda statistical inference techniques are categorized under estimation and hypothesis testing let us explore these methods:
you typically draw a sample dataset from your population as it is quite difficult to perform analysis on an entire population let us consider a quick example assume we are analyzing the income data of all neurologists in the united states of america we can make inferences about the population mean income by calculating the mean of income on a sample of 2000 neurologists this mean is the sample mean  x̄ we also refer to this as the point estimator of the population mean if the mean of our sample is $258900 then we refer to this number as the estimate of the population mean
let's expand this further the american academy of neurology conducted a study that estimated the number of neurologists in the us at 16400 we can draw another sample that will result in a different mean consider you draw multiple samples and record each sample mean you will have what is called a sampling distribution if you continued to draw samples from this population the average value of your point estimator will equal the population mean we can say that a point estimator is unbiased if its expected value equals that of the population
keeping with the example above let us derive the variance of your sample mean if we continued to sample our neurologist population the variance of the sample mean will be the variance of our population divided by 16400 note that the variability between observations is usually larger than variability between sample means this is because your sample contains a range of observations finally you will derive the standard error of the sample mean this is the population standard deviation divided by the square root of the sample size or simply the standard deviation of the sampling distribution
standard error = population standard deviation/
the standard error is important because it measures how accurate the sample distribution represents the population
please note that the sampling distribution is considered normal if the population mean is normally distributed  this is highlighted because you can not use most statistical inference techniques if the sampling distribution of your sample mean is not normally distributed
there is a popular theorem that addresses the situation when your population is not normally distributed this is known as the central limit theorem
statistical inference is not solely applied to means it is also applied to proportions
you will work with proportions in clustering analysis tasks if a telecommunications company is assessing the proportion of customers who sign up for a contract after receiving a promotional advertisement the parameter of interest is the population proportion p as the data scientist tasked with this analysis you will make an inference about the population proportion by drawing a sample in this case the point estimator is the sample proportion p hat or p̂
reading: proportions
when we provide a range of values of estimates for a population parameter we are referring to the confidence interval (ci)  you can think of it as the range of likely values for a population parameter with a specified level of confidence the sampling distributions of your sampling mean or sampling proportion must be normally distributed to derive an accurate ci the sampling distribution of the sampling mean and sampling proportion will be normally distributed if the sample size is large (in most cases that is n is greater than or equal to 30) the sampling distribution of your statistic (mean or proportion) is needed to derive your ci
you will use the margin of error  to account for the standard error of your point estimate and your desired confidence interval  consider this example:
consider that we are measuring the heights of 40 randomly selected male soccer players our sample mean is 175cm we calculated the standard deviation of the athletes heights and it totaled 20cm let us calculate the ci
n = 40 mean = 175 s = 20
you will decide on the ci to use (95%) and then find the z value for the selected ci a 95% ci means that 38 of the 40 confidence intervals will contain the true mean value
the z value for 95% ci is 1960
we calculate the 175 ± 1960 ×  20/√40
175cm ± 620cm
source1
reading: confidence interval
as you conduct research and complete data science projects questions will arise about the likelihood of occurrences as we have seen so far statistical inference helps to ground your insights with statistical significance and does its best to rule out the possibility of chance we have looked at estimation and confidence intervals to help make inferences from your data now we will explore the oldest statistical inference hypothesis testing
a hypothesis is ""an interpretation of a practical situation or condition taken as the ground for action""  similar to the other techniques you are looking to draw a conclusion about a population using a sample data set when you apply statistical hypothesis testing the end results are always favorable because (according to enrico fermi) you make a measurement or a discovery
according to dermatology associates hyper-pigmentation is the number one skin health concern for black females ages 18-45 skincare co is one of the leading manufacturers of skin care products skincare co is looking to develop a 120 day skin care line to target this population and this skin health concern you are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation your preliminary research has found that administering of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage this is different from claims that have been made about this ingredient (previous claims state that there will be no damage) this claim or belief has been formulated and it should be tested with evidence that refutes or proves that it is true you can use hypothesis testing to provide this evidence to construct a hypothesis test:
identify the population parameter of interest
determine whether you will be conducting a one-tailed or two-tailed test
define a null hypothesis often denoted as h0 the null hypothesis is considered the status quo or in the case of our example: the administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage
then define an alternative hypothesis denoted as ha this would be the opposite of the null hypothesis
the example above does not cover the entirety of identifying your null and alternative hypothesis you must know that if proven your alternative hypothesis is a call to action ie if you reject your null hypothesis then the status quo has been changed and the decision makers must take action how do we test our hypothesis statistically
reading: we do this by using the one- and two-tailed tests
let us also keep in mind that these tests are not error-proof you want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted to avoid this we consider the two error types in hypothesis testing
type i error occurs when you reject the null hypothesis when it should be accepted
type ii error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected
considering our skin care manufacturer example above a type i error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so the company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects the consequences of committing a type ii error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so the cost of this error would mean producing a skin damaging treatment product that would lead to loss of customers and possible lawsuits
how do you ensure that both errors do not occur collect more data either increase your sample size or collect more data over a longer period of time
collecting more data does not entirely mean that you will reduce both errors but it ensures that you commit one over the other at a lesser magnitude
in the module we will continue the process by learning how to extract features of variables within the dataset to improve the performance of analytic solution(s)",Determine whether you will be conducting a one-tailed or two-tailed test.,What test does a one-tailed test do?,1,0
600,601,Analytic Algorithms and Model Building,Supervised Techniques,Classification and Regression Trees,"Building Classification Trees,Building Regression Trees","tree based methods are considered to be among the simpler methods for prediction and classification trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an ""if-then"" statement and are easily digestible to an individual with a growing statistics knowledge
we will explore the different tree based methods starting with one of the most popular methods: decision trees using a very simple example let us build a decision tree: decision trees: scikit-learn
a decision tree consists of a root node the leaf nodes and branches in decision sciences it is an effective visualization that is easy to interpret in data mining and machine learning it is used to model predictions the end goal of a decision tree method is to predict the value of a target variable based on several predictors when you have a decision tree model with an outcome response containing a categorical value you have a classification tree when your outcome or target variable is a continuous value you have a regression tree
additional reading: decision trees for decision making
building a classification tree involves recursive partitioning and pruning both concepts are used to ensure the model has a low error rate and overfitting is not an issue
recursive partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset c45 is one of the popular algorithms that employ recursive partitioning it generates models that have more sensitivity and tend to be more accurate partitioning is done by repeatedly splitting and creating subsets until the tree is pure purity means all observations belong to a single class
recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue
another technique is the chi-square automatic interaction detection (chaid) it is used for both classification and prediction and can be used to show the interaction between variables it is most useful when you have a large dataset let us assume that you have received a credit card offer from capital one as a preselected customer chaid will help capital one's marketing firm to predict how your age income credit score will affect your response to the interest rate offered
measures of impurity you can measure impurity using entropy and the gini index the gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen the index varies from 0 to 1 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes it is best practice to select the feature with the lowest gini index as the root node entropy is a measure of uncertainty within a model decision trees will always seek to maximize entropy
reading: gini index and impurity measures
pruning if you have dabbled in horticulture you will be familiar with the term pruning you prune a plant so that it grows without obstacles you can also prune a plant to redirect the growth and shape of the plant you can think about pruning decision trees in a similar light it is one of the solutions to avoid overfitting the training dataset once you have a large decision tree you will prune the weakest branches to reduce complexity of your model and improve accuracy pruning can be done using two techniques
cost complexity pruning will generate a series of trees the tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm the best tree is chosen by generalized accuracy as measured by a training set or cross-validation
reduced error pruning is done by replacing each node with the node's most popular classhowever that replacement is temporary unless the it does not negatively affect the prediction accuracy it is an efficient technique for pruning
application: decision trees and nlp: a case study in pos tagging
when a full tree is built it will result in a fully grown decision tree that represents the maximum number of splits that the cart method will make to identify pure subsets full trees tend to overfit and do not do best at generalizing well to new cases solving for this means pruning the tree the least complex tree with the smallest validation error is called a minimum error tree the least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a best pruned tree
the validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree this way it will generalize new cases well misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree
you will find that decision trees are more explainable than linear regression models a smaller tree is easily interpreted by someone who is not in the field and trees can use qualitative variables without the need to create dummy variables the impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node the predictive accuracy of cart models are not as robust as other methods regression tree performance is evaluated using the root mean square error (rmse)
we will explore some methods that can be used to improve this prediction accuracy and performance; on the next page you will learn more about random forests bagging and boosting",Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm. The best tree is chosen by generalized accuracy as measured by a training set or cross-validation.,The best tree is chosen by what?,1,1
44,44,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning",Let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13.,What is the purpose of exploring the different types of regression techniques in the module 13?,0,1
1688,1691,Data Gathering and Wrangling,Data Wrangling Pipeline,Data Wrangling:Transforming Data,"Handling Missing Values,Imputation,Omission,Outliers,Transforming: Categorical Data,Transforming: Numeric Data","the data gathering process looks different for each data related project and is dependent on your business and analytic objectives as well as your data source(s) the data that you acquire during the data gathering process will almost always need to be transformed into a usable format this means it will have variables that contain incorrect or missing values due to data entry errors data might need to be transformed to a different structure to meet the requirements of a data science task
one of the most common data quality issues is missing values in your dataset this can happen due to human error or system issues during data collection as you have inspected your data and identified missing values it is important to determine why the dataset has missing values please note that a dataset that was extracted from an external source might not provide context on the reason behind the missing values when met with such a situation a data scientist or data analyst should still investigate the missing values the reasons behind the missing values will determine the techniques used to handle those values
one of the most notable statisticians classified missing data into three categories those categories explain the likelihood of missing data
missing completely at random (mcar) implies that missing data is not related to the data the probability of data being missing is the same for all observations
missing at random (mar) is the probability that the missing data is the same within certain groups
data that is not missing at random (nmar) means that the probability of data being missing varies for reasons that are unknown
the common strategies that are employed in handling missing values are imputation and omission imputation replaces missing values in the dataset with other values the replacement values are not random you can replace missing values with the mean value eg if you have missing values in the age variable you can replace the missing values with the mean age across all observations this method will work if the group is homogeneous as you know your dataset will not always contain homogeneous groups in this case you will use other imputation techniques that we will discuss in the feature engineering unit those techniques include hot and cold deck imputation regression imputation and interpolation and extrapolation
omission is often the go to technique when there are missing values omission involves excluding the missing values from the dataset remember: you might suffer loss of data if you exclude values instead of finding other missing value handling techniques omission can be done when the amount of missing values is small pairwise deletion is a type of omission it means performing your analysis on just the available values keep in mind that sample sizes will vary with this technique listwise deletion removes all data for an observation that has one or more missing values this would mean your dataset would have observations with values for all variables
you can also omit variables with missing values that variable needs to be one with little to no importance to your dataset and overall objective example i am predicting social media usage habits and my dataset includes a shoe size variable with missing values i can omit that variable
subsetting this process involves extracting portions of a dataset that are relevant to your model or analysis it is a process that is used in data wrangling to prepare data for exploratory data analysis it is a technique that can be used to remove observations with missing values subsetting can also involve excluding variables instead of observations an example is looking at summary measures of three subsets of medical records for diabetes treatments were one subset is for successful treatments another is for unsuccessful treatments and the last is for inconclusive treatments
when you were inspecting your data there was mention of visualizing the data to identify outliers outliers are unusual values in the dataset the values is unusual because it ""lies at an abnormal distance from other values in your dataset"" we discuss using exploratory data analysis techniques to identify outliers in a future unit you should not immediately remove outlier values as they often times can contribute valuable insights to your solution investigating the reason behind the outlier value is your first step in handling it
as you learnt previously there are different types of data and those types of data have specific data transformation techniques that accommodate them
categorical data is divided into groups or nominal category based on a qualitative characteristic gender race and eye color might be variables in a dataset that are useful in predicting a health challenge and due to their low levels of measurement there is a need to transform the data into a numeric format there are techniques that are employed to transform categorical variables
category reduction categorical variables can have many categories or levels a variable with levels that are not useful can negatively affect your analysis and model some categorical variables will have levels that do not occur it will be difficult to capture the interactions within those levels a technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences
creating category scores ordinal data would need to transformed into numeric values for certain statistical techniques ranked values are an example a dataset containing student evaluations would have responses that are ranked by different levels you can transform that data by assuming equal increments between category scores responses to the question:the instructor provided out of class support for the course could be: always most times sometimes hardly never you can assign a score of 1-5 1 being the highest and 5 being the lowest or vice versa the categorical variable can now hold numeric values
creating dummy variables dummy variables are often referred to as binary variables this technique allows for categorical data to be transformed to 0s and 1s a dataset containing customer spending data with a categorical variable-gender with two categories male and female the gender variable can be converted to binary variables please note that there is no order of ranking
creating dummy variables for more than one category what happens when you have a categorical variable containing more than one category consider a dataset with the variable hair color with data represented as brown brunette black gray and blonde the hair color variable can still be transformed into dummy variables using the following steps
determine the number of variables (k-1) where k is the number of categories of the variable
you will create 4 dummy variables (5-1)
you will then create variables for each category in our example we will create black brown  brunette and gray variables
assign 0 or 1 to each category for example the black variable would see values of 0 if an observation does not have black hair and 1 if the observation has black hair
keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset  in this example a dummy variable for blonde was not created this simply means that all other categories will be compared to this category usually you select the category with the largest occurrences as the category that will not transformed into a dummy variable
categorical data is transformed to numeric data so the data can be used for specific statistical techniques  why would we need to transform numeric data if you remember when data is gathered it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task; this will ensure that you do not lose data or lose information during the analysis phase you will encounter numeric data that needs to be transformed to allow you glean insights and use the data for the appropriate statistical techniques
example of a popular numeric transformation scenario is converting date of birth to age
numeric transformations are also explored when performing feature engineering  you will extract features from the numeric data and transform them into formats that can be used by a machine learning model we will explore those techniques in depth right now let us take a look at the techniques for converting numeric data during data wrangling
binning this technique will transform a numeric variable into a categorical variable as mentioned above age can be grouped into intervals in the event that that age group has similar occurrences in the other variables in a dataset if you decide to bin the age variable and create the following groups: 15-19 20-24 25-29 and 30-34 you can reduce redundancy in your dataset and capture outliers binning can also be done using equal intervals
using mathematics you can create new variables using mathematical transformation of existing variables you can use techniques such as standardization min-max scaling and logarithmic transformation we explore these mathematical transformation techniques in a future unit","Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps.",What is the hair color variable that can be transformed into dummy variables?,1,0
1355,1358,Analytic Requirements Gathering,Requirements Overview,Overview,Requirements Management Plan,"as you have learned in a previous unit gaining an understanding of the business need and formulating project objectives (business and analytic) are best practices for successfully meeting client expectations once the project objectives have been defined the project team will identify the needs and constraints of stakeholders and the current system or environment related to the business need; this is referred to as the requirements gathering process
requirements gathering establishes communication between the solutions development team (referred to as the project team in this unit) and the business stakeholders the objective of the requirements gathering process is to define the system inputs processes outputs and interfaces when properly done requirements gathering will positively influence the outcome of a project requirements gathering can also help the project team to visualize the intended solution
the requirements gathering process involves input from the system users and system owners a system user interacts with the solution and a system owner (who can also be a user) is an official who is responsible for decisions made about systems within their organization in this unit we will refer to these roles as stakeholders
similar to traditional software development projects data science projects are also guided by requirements gathering principles figure 3 lists the steps that are followed during the process requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s)
figure 3: requirements gathering process
the requirements gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data related project
gather information: the first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business need as well as decision makers within the business once stakeholders are identified the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives
 later on in this unit we will discuss the techniques used to gather information
define and prioritize requirements: stakeholders will provide information according to their view of the business needs it is the job of the business analyst to lead the effort in defining and prioritizing requirements it is important to document ""complete"" requirements that capture the needs of the stakeholders as this will guide the project team in developing the right solution(s) stakeholders might provide information that can be used for future projects related to the proposed solution that information should not be discarded it is prioritized as a low priority requirement and considered for future implementation
evaluate requirements: the project team must verify and validate all documented requirements this additional step in the requirements gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders
receive sign-off: this is an indication that the requirements have been approved and agreed upon by the client requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution
a requirements management plan can be used to document the requirements gathering process this document is made available to the client and the project team as it contains information that affects both parties there is no standard template for this document but it is in good practice to include the following sections:
project description is an overview of your project this section describes the purpose of your project
team responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements gathering process data science project team members might take on duties outside of their normal roles eg a data analyst on the data science team might serve in the role of scribe during joint application development sessions
tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture manage and track requirements through the requirements gathering process and throughout the project lifecycle
requirements gathering process should be defined in this plan this section will describe the techniques used in eliciting user and system needs defining the requirements and evaluating the success of the requirements gathering process (these techniques are covered later in this unit)
change control and requirements gathering: modern day collaboration tools will support change control however a change control process should be documented to ensure that changes to requirements are formally managed","The requirements gathering process involves eliciting user and system needs, and defining data and analytic requirements for the successful implementation of a data related project.",What process involves eliciting user and system needs and defining data and analytic requirements for a successful implementation of a data related project?,1,1
1879,1882,Model Evaluation,Evaluation Metrics,Classification and Regression Metrics,"Classification Metrics,MCC Formula,ROC-AUC Source,Regression Metrics","the metrics used to evaluate the results of your task is of great importance the performance of your algorithms need to be measured and compared to ensure that you select the right algorithm
confusion matrix is quite easy to interpret and it is straightforward in its duties this metric simply shows how well a classifier has performed it is a simple visualization of the task's performance you might also find that it is referred to as a contingency table confusion matrix can present the prediction results of a binary or multi class classification problem as shown in the example below the table has two (2) rows and two(2) columns highlighting the number of predictions that were made by the classifier within each category those categories are defined as follows:
true positives (tp) is quite straightforward in that the values in that cell mean that the classifier correctly classified observations or correctly predicted event values
true negatives (tn) indicates that the classifier correctly predicted no-event values an observation in the negative class is correctly classified as being in said class
false positives (fp) is an error in which a classifier improperly indicates classifies as observation in the positive class when in reality it belongs to the negative class this is considered a type i error and is considered a false alarm the conditional probability of a positive test result given an event that was not present is called the false positive rate
false negatives (fn) is an error in which a classifier improperly classifies an observation in the negative class when it belongs to the positive class this is considered a type ii error the conditional probability of a negative test result given an event that was present is called the false negative rate  this error is far less adverse than a false positive but it is not a universal consideration as there are cases were a false negative could be detrimental to society the premise of blackstone's formulation supports the above claim it states that ""it is better that 10 guilty persons escape than that one innocent person should suffer"" it can however be argued in healthcare that if a person reasons a false negative classification for a condition or contagious disease they could possibly die (or spread said disease) because of this type ii error
let us use the example of predicting the diagnosis classes for a dataset with 300 observations the matrix below shows that the classifier was able to accurately classifier all cases according to their respective classes the table below shows a perfect classification exercise it goes without saying that this can not be the case when real data is introduced to a trained classifier the confusion matrix is the first step in telling you the performance of your classifier but there are other metrics that can give additional insights to the performance of your model
diabetic
not-diabetic
diabetic
110
0
not-diabetic
0
90
accuracy is simply a measure of how accurately the classifier performed with classifying data  accuracy can also tell you the error rate and is typically the first metric that is visited when assessing a classifier's performance
recall sometimes referred to as sensitivity is the true positive rate and is calculated as:
# of true positives/ (# of true positives + # of false negatives) it is not holistic because it does not account for the false positive and true negative
recall = tp/(tp+fn)
specificity is the true negative rate and similar to the recall it can result in biased results it is calculated as # of true negatives/(# of true negatives + # of false positives)
specificity = tn/(tn + fp)
precision is the positive predictive value and calculated as:
# of true positives/ (# of true positives/# of false positives)
precision = tp/ (tp + fp)
f1 score: all the metrics above were highlighted to introduce biased results because they do not account for all four rates the f1 score will is calculated as:
2 * (precision * recall) / (precision + recall)
matthews correlation coefficient will solve for the issue of biased results it is often used to assess the quality of a binary classification model it is a correlation coefficient between the observed and predicted binary classification a value of +1 means a perfect prediction 0 indicates that the classifier did the same job as you would if you randomly guessed the events or no-events and finally -1 means the classifier misclassified observations mcc is considers symmetric meaning that no class is more important than another (switch negatives and positives and the result will remain the same)
resource: mcc-scikitlearn
logistic loss (log loss) is a metric that evaluates the predictions of probabilities of an observation's membership to a specific class the prediction input is a probability value between 0 and 1 and the goal is to minimize this probability value log loss will ""take into account the uncertainty of a prediction based on how much it varies from the actual label while accuracy is the count of predictions where the predicted value equals the actual value ""
resource: the math behind log loss
receiver operating characteristic (roc) curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false positive rate at certain thresholds it is the comparison of two operating characteristics the roc curve has been useful for many years according to research it was developed and used by engineers during the world war ii it was used for battlefield detection analysis and has been used in many fields its most prominent use in recent days is for machine learning and model performance assessment
consider the roc curve to be ""sensitivity as a function of false positive rate"" it can be used to select the optimal models
area under the roc curve otherwise known as auc measures the entire area underneath the roc curve and it is the measure of the classifier's ability to distinguish between classes it also provides a measure of performance across different thresholds the auc measures how well predictions are ranked and the quality of the prediction auc might not be useful for certain scenarios such as it does not tell you much about the ""cost of different errors"" it gives similar weight to errors the general interpretation of the chart is that the higher the auc the better the model is at its task of distinguishing between classes eg the model has predicted observations that are apples as apples and observations that are not apples as not apples when the auc is close to 1 it signifies a good measure of separability and closer to 0 means it is not doing a good job of separability
so far we have talked about the auc roc for binary classification it can be used in a multi class model as well note that there will be multiple auc's plotted for each class
auc-roc for multiclass
icml presentation on multiclass roc analysis
regression tasks will predict the state of a target variable based on other correlated input variables as a quick reminder target variables in these tasks are continuous values let us discuss the metrics that are used to evaluate the outcome of regression tasks:
r-squared also known as the coefficient of determination is the proportion of the variance in the outcome variable that can be predicted using the predictor variables it tells you how well ""observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model"" when interpreting r-squared in a simple linear regression model it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2) if r2 is 05 this would mean that 50% of the variation in the dependent variable is explained by the predictor variables a good model has a high r2
when there are multiple regressors then r2 is the square of the coefficient of multiple correlation (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables"")
this metric will provide an indication of how well new data will be predicted by the model
r2 does not come without some issues as a metric it cannot determine whether the coefficient estimates and predictions might be biased the r2 will typically increase when a predictor is added to a model and as you add more predictors your model will likely overfit and result in a high r2  adjusted r-squared will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected it tells you the percentage of variation explained by predictors that will have an effect on the outcome basically adjusted r2 will calculate r2 from the predictors that have a significant impact on the model adjusted r2 is best used to compare models with different number of predictors
mean squared error (mse) is measure of the quality of an estimator or a predictor (depending on context) a value closer to zero is always best mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom
mean absolute error (mae) is the measure of errors between paired observations and is computed as the average of all absolute errors (absolute error is the absolute value of the difference between a predicted value and the actual value) this metric is used to measure accuracy you use the mean absolute percentage error (mape) to compare predictions and interpret whether the size of an error is small or large the mape is a model evaluation technique that clearly interprets the relative error
root mean squared error (rmse) gives weight to large errors since it squares the errors before computing the mean the rmse is computed by first determining the residuals (difference between the actual and predicted y values) residuals are squared and the squares are averaged finally the square root of the averaged squares will result in the rmse an easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y
resource: regression metrics-scikit-learn",ICML Presentation on MultiClass ROC Analysis,What is ICML Presentation on MultiClass ROC Analysis?,0,0
670,671,Exploratory Data Analysis,Performing Exploratory Data Analysis,Information Design Overview,"Summarizing Data,**Do you know that if the mean and median of your data set differ greatly, you should check that variable for outliers!**,Source: BPI Consulting LLC,Relationships and Association,Correlation Coefficient,Meaning,Outliers,Box Plot. Source. University of Manchester,Graphical Summaries,One of the first Cartesian Coordinates Graphs,Visualizing Variables","the exploratory data analysis (eda) process is comprised of visualizing data to allow a data scientist or a data analyst explore datasets to gain insights from the data there are some non-graphical techniques that are used to explore data and as well as graphical techniques non-graphical techniques include using summary statistics to describe the data and the graphical techniques are used to describe the frequency distribution of the dataset both techniques can be used to show the skewness and extreme outliers in a dataset
summarizing data is dependent on the types of data present in your dataset when you want to understand the observed data aka your sample you will key descriptors of a dataset when you have a large data set it is difficult to describe it in its raw form you will use specific techniques to summarize and describe your sample some of those techniques include describing central tendency and assessing measures of spread and relationships
you can use the location shape and spread of the data in a dataset to understand the data you might find some of the concepts below as a review of your first statistics course but pay attention to the reason for using these techniques in exploring your data also these concepts are important for when you learn about using statistical inference to draw conclusions on an unknown population parameter
location during the eda process we are looking to describe our data using a central value we will explore the mean which is sometimes called the average this is the sum total of all observations divided by the number of observations you will calculate the mean for your population (population mean = μ)  and for the sample that you have drawn (sample mean = x̄) there are different types of mean including the typical arithmetic mean geometric mean and the harmonic mean
reading: central tendency with different types of means
median is the mid value of a dataset to calculate a median value you will sort your data in ascending order the median value in a data set with odd number of observations is the middle value while you can find the median of a dataset with even observations by calculating the average of the two middle values
mode is the variable of an observation that most frequently occurs in your dataset a uni-modal variable is one that has just one mode and a bimodal variable has two modes if your variable has more than two modes it can be referred to as multi-modal do not think it is useless in the eda process the mode is quite useful when summarizing categorical variables
percentile you will remember this nifty word from your gre scores or height and weight data from your health records it tells you the position of a value in your dataset  if you are 175cm in height and you are in the 10th percentile of height measurement for your gender it means that among all the height data collected for your gender you are taller than 10% of those observations the 50th percentile is considered the median quartiles are values that split the data into quarters
spread how can you describe the spread or variability of your dataset you use the measures of dispersion although used for descriptive statistics you must be careful in relying on this measure to describe variability
range of a set of values can be calculated by subtracting the minimum value in your dataset from the maximum value notice that range only considers two values and ignores all other values of a variable
mean absolute deviation is the ""average distance between each value and the mean of a dataset"" this measure of dispersion can tell you how values are spread out in a dataset and determines whether the mean is a useful indicator of the values within the data the larger the mean absolute deviation the more spread out the data when you work with time series forecasting methods you will use the mean absolute deviation to measure the performance of a forecasting model you will find that the measures used in eda are often used in model selection criteria
reading: mean deviation =  (σ|x − μ|)/n this is the sum of the absolute values minus the mean and divide this by the number of values in the dataset
variance s2 / σ2 is the average of the squared difference between the observations in a dataset and the mean so far we know that you can have measures for both your overall population and your sample drawn from the population keep in mind that you will sometimes have a sample variance s2  or a population variance σ2 the difference between both is that you are looking at the spread of data from the sample mean versus the population mean
standard deviation σ or s is the most common measure of dispersion it tells you the distance of the values from the mean in your dataset a low standard deviation tells you that the values are close to the mean and a high standard deviation means there is a spread the σ is derived by calculating the square root of the variance as you perform exploratory data analysis and even while developing models the importance of the standard deviation can not be overstated despite its mention as a way to summarize data standard deviation is used to ""measure the confidence in statistical conclusions"" and if you remember from the overview of this unit you will be conducting statistical inference to begin to draw some conclusions on your data and hypotheses
interquartile range (iqr) similar to the range does not consider all observations when looking at the spread of values in a dataset iqr describes 50% of values in your dataset when arranged in ascending order the iqr is the difference between the values in quartile 3 and the values in quartile 1 you can use this measure to identify a value that is an outlier
shape now that you can explain the measures used to explore data by describing its central value its spread from the mean and identified outliers let us describe the distribution of a dataset and assess whether it is normally distributed normally distributed data is useful when making statistical inferences how can we assess the distribution of our data:
skewness measures the degree to which the distribution of data lacks symmetry a dataset with 0 skewness is considered normally distributed data does not always have a skewness of 0; however if you have found skewness to be between -05 and 05 you can ascertain that your data is symmetrical if skewness is between -1 and -05 or 05 and 1 then your data is moderately skewed if skewness is < -1 or > 1 your data is highly skewed
kurtosis looks at the outliers within the distribution this measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution
reading: kurtosis
covariance describes the linear relationship between variables in your sample or population data covariance can be negative meaning your variables have a negative linear relationship zero (0) meaning the variables have no linear relationship or positive meaning a positive linear relationship exists between the variables
correlation coefficient will describe the strength of the linear relationship between the variables x and y it is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables if the correlation coefficient equals:
-1
variables have a perfect negative linear relationship
0
variables are not linearly related
1
variables have a perfect positive linear relationship
whenever you have a rather small or large observation within your dataset compared to other values in the dataset this is called an outlier outliers will affect the performance of your model and prior to getting to that point your exploratory data analysis when you have a large dataset the outliers are not as noticeable as when you have a smaller dataset similar to missing values you must handle outliers when you identify them in your dataset you should refrain from removing them from the dataset until proper investigation is completed you can ""handle"" outliers by following these steps:
construct a boxplot or as it is sometimes called a box and whisker plot this chart is used to graph the five number summary the five number summary is used to identify an outlier in your dataset a five-number summary consists of five values including the the maximum and minimum values in your dataset the lower and upper quartiles and the median these values are then ordered in ascending order and plotted
box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics including the skewness and mean you will typically plot your box plot using a tool of your choice in this course that would be python here is a resource on how to plot a box plot using python
z-score is a measure of the relative position of an observation within a dataset you calculate the z-score by subtracting the value from the mean and dividing that value by the standard deviation if an observation has a z-score that is more than 3 or less than -3 it is an outlier
the saying that a picture tells a thousand words supports the notion that visualizing data is one of the best ways to tell a story to a wide audience with the ability to view the visuals today there are so many tools that are used to visualize data including python r excel tableau microsoft's powerbi among others these tools even allow us to make data visualizations interactive for more effective analysis rene descartes invented the cartesian coordinate system or the x and y axis graph as we know it
data visualizations have evolved greatly including info-graphics plots and statistical graphs communicating insights in the data clearly is a key step in the data science process there is a skill to developing effective visualizations and not all data scientists will have these skills however it is important to know how to use the tools that support visualization and be able to identify the right types of visualizations for the data types and the best ways to tell your story with visualizations visualization experts strongly suggest that visualizations should stimulate the attention of viewers and elicit thoughts and questions during the data science project process visualizations will be used at different times at this stage of exploratory data analysis the visualizations are used to understand the dataset as you proceed through analytic solution building gaining a better understanding of your data will support the performance of your models in the later stages of the project
so far we have talked about visualizing mostly numeric data however we are able to visualize categorical data as well typically this data is categorized and numeric values are derived (counted) to represent the values in the variables a frequency distribution is a useful technique in displaying categorical variables
follow our primer below to learn more about the appropriate visualizations for data science tasks
reading: data exploration and visualization primer
as we study exploratory data analysis we will learn how to describe sample data by making inferences from the population on which the sample data was drawn despite the sampling techniques used there is still a level of uncertainty in drawing conclusions about your population we find that descriptive statistics is not enough to estimate this uncertainty; hence the need for statistical inference on the next page we will explore statistical inference further
additional reading: the dr howard seltman experimental design and analysis book is a text that explores experimental design and analysis and is worth a look through
stephen few3 listed eight types of quantitative data and the types of charts and graphs that can communicate the story in the data
reading: selecting the right graph for your message","Interquartile Range (IQR) similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.",The difference between the values in Quartile 3 and what is the difference between?,1,0
1087,1090,Analytic Algorithms and Model Building,Unsupervised Learning Techniques,k-Means Clustering,"Unsupervised Learning Techniques,Clustering-Source www.pyarmy.com,Types of Clustering,k-Means Clustering","when we want to identify patterns in a dataset from unlabeled data we use unsupervised learning to perform this task unsupervised learning is also referred to as self-organizing; we have touched on the principal component analysis when we discussed feature engineering this is one of the unsupervised techniques we will also look further into the different types of cluster analysis techniques
you can categorize data according to characteristics using a technique called cluster analysis if you think about how we reason and learn as human beings we make sense of events people and things by placing them in groups you have memories that are characterized as happy and sad or people categorized into close friends acquaintances and mentors among others you might even consider clustering data to identify those with similarities as a method of exploring data applications of cluster analysis include market segmentation; this is the segmenting of customer data based on certain criteria including transaction history the different clusters created from the segmentation exercise are useful for targeted advertising or application of customized marketing strategies that will might elicit positive responses increase sales and engagement
hard clustering divides data into a number of groups and can only belong to one cluster all clusters are independent of each other
soft clustering groups data into clusters but a data point can belong to more than one cluster to a degree
overlapping clustering allows data to belong to more than one cluster
hierarchical clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram
reading: an expansion on clustering
this method involves identifying the number of clusters k that the dataset will be grouped into; the data in each cluster should share similarities to the other records within its cluster assume you have k = 5 cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5  the data within clusters adhere to distance measures to ensure that dispersion is minimized k-means clustering technique abides by a number of distance measures but the most popular is the euclidean distance let us look at how clusters are created using this technique:
partition data: the dataset is partitioned into k clusters are pre-specified (chosen by the data scientist)
initialize centroids: within each cluster the distance between the observations is modified so that dispersion is minimized and each observation is closest to nearest centroid centroids are data points that are considered to be the center of a cluster you can also think of this datapoint as the mean of all the observations within the cluster
iteratively initialize centroids from the previous step until the means of the newly formed clusters are negligible
you know that you have a good cluster when there is ""high similarity among within-cluster data and low similarity among inter-cluster data""1
how do we decide k similar to knn there are empirically studied recommendations for the best k to select you can also select k based on previous knowledge (this is hardly the case with this unsupervised task) you can use different values for k and then compare the results gotten from each value of k it is good practice to also run the k-means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k
k can also be chosen by calculating the within cluster sum of squares(wcss) this is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster
assume that we have 1000 observations in a dataset and we have decided that k = 1000 the wcss should be zero (0) this is because all the observations are considered as centroids and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster this is certainly not a computationally sensible way to cluster data think about a dataset with over 100000 observations also think about the information to be gleaned from the cluster analysis; you will lack useful information
when you randomly initialize with a range of k values for the 1000 observations mentioned above ie between 2-10 you can use the elbow method to find out the optimum value for k the elbow method produces a graph that shows this optimum value at the ""elbow"" of the line as shown below you select k as the wcss decreases; the figure below shows that after 5 the decrease in wcss is quite small
elbow method: source1
reading: k-means clustering-sklearn
additional reading: k-means clustering algorithm
k-means clustering and k-nearest neighbors have been known to cause confusion for data scientists who are new to the field afterall we are discussing similarity measures and distances to an observation to classify or cluster into a class the main difference is that one is an unsupervised technique and the other is supervised knn is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points kmeans does not provide a labeled dataset to the model for learning purposes kmeans will partition the data into a number of clusters knn works best with data that is of the same scale but kmeans do not need same scale data to perform well remember when you learned about knn being a lazy learner kmeans is an eager learner it is slow to train but fast to learn and it tends to deal with noise in the training dataset better than a lazy learner","You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad or people categorized into close friends, acquaintances, and mentors, among others. You might even consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation; this is the segmenting of customer data based on certain criteria including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or application of customized marketing strategies that will might elicit positive responses, increase sales, and engagement.","What are memories that are characterized as happy and sad or people categorized into close friends, acquaintances, and mentors?",1,0
141,141,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"motomanager case-in-point
here is an example of an ai consulting firm that used the evp framework to meet the business needs of a popular automotive services provider
mr tire-monro muffler and brake (a subsidiary of monro inc) is a top-50 automotive parts and general repair services provider in the us with over 320 locations nationwide in general the automotive services industry struggles with customer retention companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer) lack of customer “stickiness” leads to a) less potential revenue and b) less data about customers in general which could potentially be used to offer specific products and services to individual customers attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers
monro inc (the parent company of mr tire) approached cognistx (an ai applications company) to develop a data-driven solution to improve customer acquisition and retention the video above provides a brief summary of how cognistx engaged with mr tire to develop motomanager a mobile app that was deployed by mr tire this solution led to measured increases in customer acquisition and retention as well as increased revenue
the cognistx data science team met with the business leaders of monro inc to gain an understanding of the company’s business needs related to customer retention the data science team identified and interviewed all stakeholders from the business and technical teams at monro inc including the data management information technology and service management teams the it managers provided information about the company’s data asset management structure including data governance data architecture and data security management accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet their business needs
data management in the enterprise
finally service managers were interviewed on customer service difficulties that could be addressed by the proposed solution the service managers also identified the hardware and software gaps in the various stores around the country once the interviews were completed the cognistx data science team formulated business objectives that would meet monro inc’s business needs the business objectives included:
create an application that provides customers with a customized service experience for their automotive needs
offer customers $50 coupon to download monro inc mobile app
onboard customers to the application with the creation of customer profiles
provide tailored customer service management to very important (vip) customers
classify customers as vip customers based on defined characteristics
create a loyalty program to increase repeat customer transactions
business objectives should be measurable to ensure that business needs are met the metrics used to assess the success of the project were measured by:
count of people who installed the app and on-boarded upon receiving a $50 e-coupon
the number of times an on-boarded customer visited a store close to them
number of transactions completed with the app
total amount of money generated via the app compared to total amount of money used to maintain the app
a model that can predict a repeat customer from on-boarded customers with an accuracy of 85%
the metrics were both technical (precision and accuracy of model) and business related (calculate return-on-investment)
based on the business objectives cognistx developed an ai-enabled application for monro inc called motomanager motomanager captures a comprehensive profile of customers through an onboarding process monro inc sends a $50 coupon incentive to current and potential customers this coupon can be retrieved when a customer installs the application and completes their user profile the motomanager app uses customer data to provide customized reward incentives for booking services and making purchases from a customer’s local mr tire store as of 2019 the app has a 53000 user strength and monro inc has reported significant increases in customer engagement and retention the company has generated over $14 million us dollars from app-based transactions","Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc called MotoManager. MotoManager captures a comprehensive profile of customers through an onboarding process. Monro Inc. sends a $50 coupon incentive to current and potential customers. This coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customer’s local Mr. Tire store. As of 2019, the app has a 53,000 user strength and Monro Inc has reported significant increases in customer engagement and retention. The company has generated over $14 million US dollars from app-based transactions.",How many user strength does the app have as of 2019?,1,0
606,607,Analytic Algorithms and Model Building,Supervised Techniques,Classification and Regression Trees,"Building Classification Trees,Building Regression Trees","tree based methods are considered to be among the simpler methods for prediction and classification trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an ""if-then"" statement and are easily digestible to an individual with a growing statistics knowledge
we will explore the different tree based methods starting with one of the most popular methods: decision trees using a very simple example let us build a decision tree: decision trees: scikit-learn
a decision tree consists of a root node the leaf nodes and branches in decision sciences it is an effective visualization that is easy to interpret in data mining and machine learning it is used to model predictions the end goal of a decision tree method is to predict the value of a target variable based on several predictors when you have a decision tree model with an outcome response containing a categorical value you have a classification tree when your outcome or target variable is a continuous value you have a regression tree
additional reading: decision trees for decision making
building a classification tree involves recursive partitioning and pruning both concepts are used to ensure the model has a low error rate and overfitting is not an issue
recursive partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset c45 is one of the popular algorithms that employ recursive partitioning it generates models that have more sensitivity and tend to be more accurate partitioning is done by repeatedly splitting and creating subsets until the tree is pure purity means all observations belong to a single class
recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue
another technique is the chi-square automatic interaction detection (chaid) it is used for both classification and prediction and can be used to show the interaction between variables it is most useful when you have a large dataset let us assume that you have received a credit card offer from capital one as a preselected customer chaid will help capital one's marketing firm to predict how your age income credit score will affect your response to the interest rate offered
measures of impurity you can measure impurity using entropy and the gini index the gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen the index varies from 0 to 1 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes it is best practice to select the feature with the lowest gini index as the root node entropy is a measure of uncertainty within a model decision trees will always seek to maximize entropy
reading: gini index and impurity measures
pruning if you have dabbled in horticulture you will be familiar with the term pruning you prune a plant so that it grows without obstacles you can also prune a plant to redirect the growth and shape of the plant you can think about pruning decision trees in a similar light it is one of the solutions to avoid overfitting the training dataset once you have a large decision tree you will prune the weakest branches to reduce complexity of your model and improve accuracy pruning can be done using two techniques
cost complexity pruning will generate a series of trees the tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm the best tree is chosen by generalized accuracy as measured by a training set or cross-validation
reduced error pruning is done by replacing each node with the node's most popular classhowever that replacement is temporary unless the it does not negatively affect the prediction accuracy it is an efficient technique for pruning
application: decision trees and nlp: a case study in pos tagging
when a full tree is built it will result in a fully grown decision tree that represents the maximum number of splits that the cart method will make to identify pure subsets full trees tend to overfit and do not do best at generalizing well to new cases solving for this means pruning the tree the least complex tree with the smallest validation error is called a minimum error tree the least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a best pruned tree
the validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree this way it will generalize new cases well misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree
you will find that decision trees are more explainable than linear regression models a smaller tree is easily interpreted by someone who is not in the field and trees can use qualitative variables without the need to create dummy variables the impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node the predictive accuracy of cart models are not as robust as other methods regression tree performance is evaluated using the root mean square error (rmse)
we will explore some methods that can be used to improve this prediction accuracy and performance; on the next page you will learn more about random forests bagging and boosting","When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving for this means pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a Best Pruned Tree.",What is the least complex tree with the smallest validation error called a Minimum Error Tree?,1,1
1009,1011,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistical Inference,"Statistical Inference,Sampling Distribution,Confidence Interval,168.8cm to 181.2cm,Hypothesis Tests","statistical inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods those conclusions are typically drawn using exploratory data analysis or summary statistics the goal of this process is to use probability theory to make inferences about your data  this is the first step of learning about the attributes of your population from the sample that you have drawn
reminder: the characteristics of the sample dataset are called statistics the characteristics of your population are known as parameters
understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes
if you recall from a previous unit you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise you can use statistical inference to draw scientific conclusions and test set hypotheses significance of a sample data set or descriptive statistics is often in question during the eda process using statistical inference techniques can give significance to your conclusions from eda statistical inference techniques are categorized under estimation and hypothesis testing let us explore these methods:
you typically draw a sample dataset from your population as it is quite difficult to perform analysis on an entire population let us consider a quick example assume we are analyzing the income data of all neurologists in the united states of america we can make inferences about the population mean income by calculating the mean of income on a sample of 2000 neurologists this mean is the sample mean  x̄ we also refer to this as the point estimator of the population mean if the mean of our sample is $258900 then we refer to this number as the estimate of the population mean
let's expand this further the american academy of neurology conducted a study that estimated the number of neurologists in the us at 16400 we can draw another sample that will result in a different mean consider you draw multiple samples and record each sample mean you will have what is called a sampling distribution if you continued to draw samples from this population the average value of your point estimator will equal the population mean we can say that a point estimator is unbiased if its expected value equals that of the population
keeping with the example above let us derive the variance of your sample mean if we continued to sample our neurologist population the variance of the sample mean will be the variance of our population divided by 16400 note that the variability between observations is usually larger than variability between sample means this is because your sample contains a range of observations finally you will derive the standard error of the sample mean this is the population standard deviation divided by the square root of the sample size or simply the standard deviation of the sampling distribution
standard error = population standard deviation/
the standard error is important because it measures how accurate the sample distribution represents the population
please note that the sampling distribution is considered normal if the population mean is normally distributed  this is highlighted because you can not use most statistical inference techniques if the sampling distribution of your sample mean is not normally distributed
there is a popular theorem that addresses the situation when your population is not normally distributed this is known as the central limit theorem
statistical inference is not solely applied to means it is also applied to proportions
you will work with proportions in clustering analysis tasks if a telecommunications company is assessing the proportion of customers who sign up for a contract after receiving a promotional advertisement the parameter of interest is the population proportion p as the data scientist tasked with this analysis you will make an inference about the population proportion by drawing a sample in this case the point estimator is the sample proportion p hat or p̂
reading: proportions
when we provide a range of values of estimates for a population parameter we are referring to the confidence interval (ci)  you can think of it as the range of likely values for a population parameter with a specified level of confidence the sampling distributions of your sampling mean or sampling proportion must be normally distributed to derive an accurate ci the sampling distribution of the sampling mean and sampling proportion will be normally distributed if the sample size is large (in most cases that is n is greater than or equal to 30) the sampling distribution of your statistic (mean or proportion) is needed to derive your ci
you will use the margin of error  to account for the standard error of your point estimate and your desired confidence interval  consider this example:
consider that we are measuring the heights of 40 randomly selected male soccer players our sample mean is 175cm we calculated the standard deviation of the athletes heights and it totaled 20cm let us calculate the ci
n = 40 mean = 175 s = 20
you will decide on the ci to use (95%) and then find the z value for the selected ci a 95% ci means that 38 of the 40 confidence intervals will contain the true mean value
the z value for 95% ci is 1960
we calculate the 175 ± 1960 ×  20/√40
175cm ± 620cm
source1
reading: confidence interval
as you conduct research and complete data science projects questions will arise about the likelihood of occurrences as we have seen so far statistical inference helps to ground your insights with statistical significance and does its best to rule out the possibility of chance we have looked at estimation and confidence intervals to help make inferences from your data now we will explore the oldest statistical inference hypothesis testing
a hypothesis is ""an interpretation of a practical situation or condition taken as the ground for action""  similar to the other techniques you are looking to draw a conclusion about a population using a sample data set when you apply statistical hypothesis testing the end results are always favorable because (according to enrico fermi) you make a measurement or a discovery
according to dermatology associates hyper-pigmentation is the number one skin health concern for black females ages 18-45 skincare co is one of the leading manufacturers of skin care products skincare co is looking to develop a 120 day skin care line to target this population and this skin health concern you are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation your preliminary research has found that administering of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage this is different from claims that have been made about this ingredient (previous claims state that there will be no damage) this claim or belief has been formulated and it should be tested with evidence that refutes or proves that it is true you can use hypothesis testing to provide this evidence to construct a hypothesis test:
identify the population parameter of interest
determine whether you will be conducting a one-tailed or two-tailed test
define a null hypothesis often denoted as h0 the null hypothesis is considered the status quo or in the case of our example: the administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage
then define an alternative hypothesis denoted as ha this would be the opposite of the null hypothesis
the example above does not cover the entirety of identifying your null and alternative hypothesis you must know that if proven your alternative hypothesis is a call to action ie if you reject your null hypothesis then the status quo has been changed and the decision makers must take action how do we test our hypothesis statistically
reading: we do this by using the one- and two-tailed tests
let us also keep in mind that these tests are not error-proof you want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted to avoid this we consider the two error types in hypothesis testing
type i error occurs when you reject the null hypothesis when it should be accepted
type ii error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected
considering our skin care manufacturer example above a type i error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so the company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects the consequences of committing a type ii error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so the cost of this error would mean producing a skin damaging treatment product that would lead to loss of customers and possible lawsuits
how do you ensure that both errors do not occur collect more data either increase your sample size or collect more data over a longer period of time
collecting more data does not entirely mean that you will reduce both errors but it ensures that you commit one over the other at a lesser magnitude
in the module we will continue the process by learning how to extract features of variables within the dataset to improve the performance of analytic solution(s)",Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes.,What ensures that you analyze your data properly and ultimately draw the right conclusions for decision making purposes?,1,1
1748,1751,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"the problem underlying an analytic objective should satisfy the following criteria:
solving the problem must be part of a possible solution vision towards the business objective
domain experts are confident that data exists which when analyzed can facilitate that solution; this data must either be available or sources are available to retrieve the data
the problem must be specific and realistic so that a corresponding data science project can succeed in principle

first with the business objective clearly stated one can propose a solution vision describing how the business can reach the objective the path to realizing this vision can then be decomposed into sub-problems and the data science team will be responsible for leveraging data to help solve these problems and/or produce data-derived insight that allows the client to make good decisions along the way
as data driven methods are still in the phase of being adopted across many fields the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement in such a case it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond “we want to leverage data somehow” and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria even if they include exploratory tasks
an online company asks for a data science project around the use of social media data for understanding their market driven by the board’s desire to keep up with the competition after the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses the client will focus on the need to increase market share; they will identify the business objective of increasing sales of a particular product there seem to be two ways forward: (1) spend money for increased advertising of a unique feature of their product or (2) lower the price they ask for an analysis of social media data to inform their decision this need for gauging consumer preferences now forms the problem component of the analytic goal
second there must be a sound presumption that the problem’s solution must benefit from the use of data a collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution thus may include data that would need to be collected as part of the project in order to be suitable the data should have some informative patterns relating to the problem it takes a combination of substantive expertise and data science skill to assess this criterion
domain-specific problems around the availability of data include:
lack of readiness in the organization (eg the organizations data is not readily processable) unsuitability of the data for the objective (eg data is old/stale out of domain incomplete or suffers from an obvious bias) or difficulty in collecting data because the expert labor involved is too expensive
technical objections too little data for the required methods fragmentation across multiple units with no suitable way of joining or overwhelming imprecision/noise in cases where data is available but the existence of “signal” for the problem is uncertain exploring whether it exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself
typically data science projects/consultations involve a preliminary data survey that informs or even precedes longer substantive discussion it is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client
third both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle in other words the system and/or insight produced by the project must add enough value to be deemed a success if executed properly on the client side this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated for example while it should mostly be avoided it may happen that a solution vision will be idealistic and somewhat resemble a science fiction scenario in such cases the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such on the other hand the technicians must be very careful to not exaggerate analytical capacities or cause unwarranted impressions that certain functionality is within reach for example even once an initial data sample has been surveyed the results should be communicated as being contingent on the assumption that the sample is representative for the larger dataset similarly if a particular neural network model performs a classification task very well on some domain the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level",The problem must be specific and realistic so that a corresponding data science project can succeed in principle,What must be specific and realistic so that a data science project can succeed in principle?,1,1
423,424,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Mapping Raw Data to ML Features. Source-Google Developer Course,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical
 Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling","feature engineering is the process of using domain knowledge to extract features from raw data algorithms need specific features in the model development process feature engineering will ensure your dataset is compatible with your algorithm thereby improving model performance
 
so far we have highlighted the specialized nature of feature engineering and that there is no one size fits all to the process however there are foundational concepts that are essential to your understanding of feature engineering
do you remember this concept from an earlier module it is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering missing values in a dataset can negatively affect the performance of a model missing values can be caused by simple human errors privacy concerns among others how can we fix the problem of missing values a simple but problematic solution is dropping rows or columns a preferable solution is imputation
 you should consider a default value for missing values in a row or column let us visit how you handle this with numeric and categorical data
numerical data imputation if you are not dropping rows and columns with missing data the numerical imputation method will allow you to intuitively replace missing values a column with numbers and some with "" - "" or ""na"" can be replaced with a ""0"" other methods used include using the median or mean values of that variable
categorical data imputation in some cases replacing missing values with a zero will not make sense to the dataset you can replace values in a categorical column with the “maximum occurred value” you can impute “other” in a situation where there is no dominant value in the categorical column
similar to imputation binning can be applied to numerical and categorical data binning makes a model more robust but there is a trade-off between performance and overfitting binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data
 it is also used to capture noisy data when you have values that have variance
in the context of image processing binning is the procedure of combining a cluster of pixels into a single pixel as such in 2x2 binning an array of 4 pixels becomes a single larger pixel reducing the overall number of pixels
source2
you learned about how to visualize your data to detect outliers in an earlier module this method is less error prone you can use some statistical and visualization methods to detect and handle outliers include computing the z-score using percentiles and visualizing the data distribution of your dataset these techniques were discussed in the ""exploratory data analysis"" module
log transform also known as logarithm transform is used to handle skewed data and make the distribution of data less skewed it is widely used because of its ease of use and it decreases the effect of outliers in a dataset log transform is not usually applied to values that are less than or equal to zero
one hot coding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions this technique replaces categorical variables with different boolean variables that indicate whether or not a category of the variable was part of the observation those boolean variables are called dummy variables
one hot encoding is easy to implement it will retain all information of the categorical variable this method does not add valuable information that can make a variable more predictive
assume that a categorical variable education with labels less than high school and high school we can generate the boolean variable “high school” which becomes 1 if the person has high school or 0 if the person has less than high school
when you have a variable with multiple categories one-hot encoding might increase the dimensionality of your data the binary encoding method can be used to create a smaller number of variables without losing information
when you have ordinal data that is useful to your analytic solution you can transform those features using ordinal encoding here we convert string labels to integer values if you have an ordinal variable with string values that satisfied dissatisfied highly satisfied highly dissatisfied not applicable and somewhat satisfied ordinal feature encoding will map the the values to a corresponding integer as you can see in the table below all values are not integers
highly satisfied
1
satisfied
2
somewhat satisfied
3
not applicable
4
dissatisfied
5
highly dissatisfied
6
when you split features the features become easier to bin and this improves model performance there are many ways of splitting features and it depends on the variable if your dataset contains the variable address you might split the column by extracting street address city state and postal code you run the risk of increasing dimensions; in this case we employ techniques that assess the value of the extracted dimensions
some machine learning algorithms need to have scaled continuous features as model inputs scaling is not necessary for most algorithms but it can make continuous features identical in respect to range
there are instances that require the use of scaled data including algorithms that use gradient descent (""an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient in machine learning we use gradient descent to update the parameters of our model"") neural networks and linear regression are some of those examples the data is scaled before been fed to the model algorithms like k-nearest neighbors clustering analysis like k-means clustering and other distance based algorithms would need data that is scaled
quick thought: how about tree based algorithms like decision trees and random forest they are not affected as they are not distance based
normalization this technique involves values ranging between 0 and 1 prior to normalization all outliers in the dataset should be handled
standardization also known as z-score normalization it is useful for feature engineering in logistic regression artificial neural network and support vector machine tasks
how-to: scaling in python","One hot encoding is easy to implement, it will retain all information of the categorical variable. This method does not add valuable information that can make a variable more predictive.",What is a hot encoding easy to implement?,1,0
607,608,Analytic Algorithms and Model Building,Supervised Techniques,Classification and Regression Trees,"Building Classification Trees,Building Regression Trees","tree based methods are considered to be among the simpler methods for prediction and classification trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an ""if-then"" statement and are easily digestible to an individual with a growing statistics knowledge
we will explore the different tree based methods starting with one of the most popular methods: decision trees using a very simple example let us build a decision tree: decision trees: scikit-learn
a decision tree consists of a root node the leaf nodes and branches in decision sciences it is an effective visualization that is easy to interpret in data mining and machine learning it is used to model predictions the end goal of a decision tree method is to predict the value of a target variable based on several predictors when you have a decision tree model with an outcome response containing a categorical value you have a classification tree when your outcome or target variable is a continuous value you have a regression tree
additional reading: decision trees for decision making
building a classification tree involves recursive partitioning and pruning both concepts are used to ensure the model has a low error rate and overfitting is not an issue
recursive partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset c45 is one of the popular algorithms that employ recursive partitioning it generates models that have more sensitivity and tend to be more accurate partitioning is done by repeatedly splitting and creating subsets until the tree is pure purity means all observations belong to a single class
recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue
another technique is the chi-square automatic interaction detection (chaid) it is used for both classification and prediction and can be used to show the interaction between variables it is most useful when you have a large dataset let us assume that you have received a credit card offer from capital one as a preselected customer chaid will help capital one's marketing firm to predict how your age income credit score will affect your response to the interest rate offered
measures of impurity you can measure impurity using entropy and the gini index the gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen the index varies from 0 to 1 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes it is best practice to select the feature with the lowest gini index as the root node entropy is a measure of uncertainty within a model decision trees will always seek to maximize entropy
reading: gini index and impurity measures
pruning if you have dabbled in horticulture you will be familiar with the term pruning you prune a plant so that it grows without obstacles you can also prune a plant to redirect the growth and shape of the plant you can think about pruning decision trees in a similar light it is one of the solutions to avoid overfitting the training dataset once you have a large decision tree you will prune the weakest branches to reduce complexity of your model and improve accuracy pruning can be done using two techniques
cost complexity pruning will generate a series of trees the tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm the best tree is chosen by generalized accuracy as measured by a training set or cross-validation
reduced error pruning is done by replacing each node with the node's most popular classhowever that replacement is temporary unless the it does not negatively affect the prediction accuracy it is an efficient technique for pruning
application: decision trees and nlp: a case study in pos tagging
when a full tree is built it will result in a fully grown decision tree that represents the maximum number of splits that the cart method will make to identify pure subsets full trees tend to overfit and do not do best at generalizing well to new cases solving for this means pruning the tree the least complex tree with the smallest validation error is called a minimum error tree the least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a best pruned tree
the validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree this way it will generalize new cases well misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree
you will find that decision trees are more explainable than linear regression models a smaller tree is easily interpreted by someone who is not in the field and trees can use qualitative variables without the need to create dummy variables the impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node the predictive accuracy of cart models are not as robust as other methods regression tree performance is evaluated using the root mean square error (rmse)
we will explore some methods that can be used to improve this prediction accuracy and performance; on the next page you will learn more about random forests bagging and boosting","The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree, this way it will generalize new cases well. Misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree.",What dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree?,1,1
73,73,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning","Backward elimination which tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the ""most statistically insignificant deterioration of the model fit"".  This technique should not be used if predictors are more than the observations in the dataset.",What technique tests the effect that each variable has on a model by deleting it?,1,1
751,753,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make informed decision regarding the use of these terms.","if you have not already you should notice a pattern at this stage the conceptual progression business need ⇒ business objective ⇒ analytic objective including: problem statement ⇒ task definition ⇒ method & data statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against the statement of methods to be applied will vary in specificity depending on your project but three elements are important:
it must be precise enough so that the data science team understands it as a concise summary of the technical approach

at the same time it should allow for testing different techniques around the main conceptual idea

the methods should in principle be suitable to produce the target functionality/insight for the task given the available data
as you become more proficient in the discipline of data science (eg after having absolved the more advanced units of this course) you will develop refined intuitions about which method to propose in which context the most general categories of methods one can identify in this statement typically include:
supervised learning methods which involves learning to predict a target variable (typically through regression or classification) by training on “true” example data points whose target variable has manually been labeled or is available by other means
unsupervised learning methods deal with finding patterns in unlabeled data without an explicit prediction target
semi-supervised learning methods encompass hybrid methods that combine supervised and unsupervised learning in different ways
this course is very focused on the application of machine learning which has large overlap with various kinds of statistical methods it is possible to phrase your method statement around the use of statistics but it is recommended that one qualifies this rather broad term to something adequate for the project
in many contexts the method will have to be stated much more precisely than that especially if the problem domain is already well-studied in data science or there is some prior work that should be extended it can range from specifying a particular family of models (eg linear vs non-linear) using a new feature set testing the explainability of a particular model’s predictions to optimizing hyperparameters for faster training and inference in neural networks and more we can illustrate the specific vs general statement in the context of an example:
your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the personnel does not reliably find all potential defects (problem) your model should be able to predict product failure within one month after sale (task 1) and identify predictors measured at quality control time (task 2) in a pilot project you propose to apply “traditional supervised learning methods to train a classifier and examine the model for predictive variables” you then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation
variation: your pilot project was a success and your models are being used to better inform quality control personnel however it turns out that it still produces a high false positive rate (problem) and causes shipping delays which the client wants to minimize (business objective) you are asked to improve the model’s performance by reducing its false positive rate (task) you are further given access to quality control diagnostic equipment readings which the engineers believe are useful to discern whether a product is defective or just “needs to be broken in” you hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model
the method and data are the heart of a data science project finding the best tool for the task is of course one of the core skills of being a good data scientist this course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects as you gain deeper knowledge and experience you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with the state of the art at the same time thinking through different approaches and deciding on a set of methods and datasets benefits from teamwork open discussion and active seeking of advice and feedback from your peers mentors and relevant specialists
a good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results
in the above electronics scenario you imagine training a logistic regression model on the features to predict product failure once trained influential features should receive high weight in the regression equation allowing you to identify them easily similar to a correlation analysis if you train a decision tree you can identify features by traversing its branches on the other hand if you were to propose to train a complex random forest model to predict product failure with maximal accuracy one may object that random forests are not as easily interpretable as simpler models it would hence be an unsuitable method for task 2 in the pilot stage a good way would be to go with the simpler models for now and leave more complex models for later phases if needed in the variation a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommend you use a more recently developed encoding you may research both algorithms and make an informed decision or even use both algorithms in a comparative experiment","In the above electronics scenario, you imagine training a logistic regression model on the features to predict product failure. Once trained, influential features should receive high weight in the regression equation, allowing you to identify them easily, similar to a correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, one may object that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommend you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.",What type of model can you train to predict product failure?,1,1
858,860,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"finally the analytic objective should state (1) what specific functionality insight or resource is gained from leveraging that data and methods you propose (2) relative to the current situation and (3) assuming the project is successful as proposed
valuable functionality: in many practical scenarios this may simply be that for example a predictive model successfully can solve the task and contribute to the problem solution as stated in more complex situations the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task for example the task may be to link passages in news text that state false information about certain historical events to a reference database of those events initially one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address this detector if implemented successfully would add valuable functionality towards the linking task
it should be noted that analytic objectives can of course be reframed to focus on specific tasks in which case an incremental step would become the main goal in the example just given one can change the analytic objective so that historic text detection becomes the main task in the first phase of the project in fact if the circumstances and the client allow for such a rescoping then this may even be preferable one of the main points of this unit is that explicitly formulating discussing and committing to analytic objectives supports a productive data science project lifecycle and ensures a proper alignment of the work with the business objective
valuable insight: in some project contexts the client may not need a piece of software that automates and analytic functionality but rather requires insight from data in order to make decisions this area of data science blends into what is commonly referred to as “business intelligence”
valuable resource: data science projects can also produce resources to be used by the organization or further projects for example the project may be intended to collect and curate a competition dataset and publish it along with some baseline results to facilitate research on a certain topic
improvement over the current situation: naturally the project should improve over the currently available functionality information and resources in industry settings this is usually obvious in academic and other research settings however you will be characterizing your project as improving over the state of the art this can be done via a survey of related work and possibly some exploration of existing methods and datasets while this will often be left implicit in the statement of the analytic objective project proposals (especially academic ones) may require an explicit section on related work
assumption of project success: while it is natural that one would propose a project with confidence to succeed it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees the dataset may contain too much noise on top of the interesting patterns or the computational effort may be too large be mindful of what can be done to distill some value added even if the main objective fails due to factors beyond your control","Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state of the art. This can be done via a survey of related work, and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.",What is usually obvious in industry settings?,1,1
609,610,Analytic Algorithms and Model Building,Supervised Techniques,Classification and Regression Trees,"Building Classification Trees,Building Regression Trees","tree based methods are considered to be among the simpler methods for prediction and classification trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an ""if-then"" statement and are easily digestible to an individual with a growing statistics knowledge
we will explore the different tree based methods starting with one of the most popular methods: decision trees using a very simple example let us build a decision tree: decision trees: scikit-learn
a decision tree consists of a root node the leaf nodes and branches in decision sciences it is an effective visualization that is easy to interpret in data mining and machine learning it is used to model predictions the end goal of a decision tree method is to predict the value of a target variable based on several predictors when you have a decision tree model with an outcome response containing a categorical value you have a classification tree when your outcome or target variable is a continuous value you have a regression tree
additional reading: decision trees for decision making
building a classification tree involves recursive partitioning and pruning both concepts are used to ensure the model has a low error rate and overfitting is not an issue
recursive partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset c45 is one of the popular algorithms that employ recursive partitioning it generates models that have more sensitivity and tend to be more accurate partitioning is done by repeatedly splitting and creating subsets until the tree is pure purity means all observations belong to a single class
recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue
another technique is the chi-square automatic interaction detection (chaid) it is used for both classification and prediction and can be used to show the interaction between variables it is most useful when you have a large dataset let us assume that you have received a credit card offer from capital one as a preselected customer chaid will help capital one's marketing firm to predict how your age income credit score will affect your response to the interest rate offered
measures of impurity you can measure impurity using entropy and the gini index the gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen the index varies from 0 to 1 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes it is best practice to select the feature with the lowest gini index as the root node entropy is a measure of uncertainty within a model decision trees will always seek to maximize entropy
reading: gini index and impurity measures
pruning if you have dabbled in horticulture you will be familiar with the term pruning you prune a plant so that it grows without obstacles you can also prune a plant to redirect the growth and shape of the plant you can think about pruning decision trees in a similar light it is one of the solutions to avoid overfitting the training dataset once you have a large decision tree you will prune the weakest branches to reduce complexity of your model and improve accuracy pruning can be done using two techniques
cost complexity pruning will generate a series of trees the tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm the best tree is chosen by generalized accuracy as measured by a training set or cross-validation
reduced error pruning is done by replacing each node with the node's most popular classhowever that replacement is temporary unless the it does not negatively affect the prediction accuracy it is an efficient technique for pruning
application: decision trees and nlp: a case study in pos tagging
when a full tree is built it will result in a fully grown decision tree that represents the maximum number of splits that the cart method will make to identify pure subsets full trees tend to overfit and do not do best at generalizing well to new cases solving for this means pruning the tree the least complex tree with the smallest validation error is called a minimum error tree the least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a best pruned tree
the validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree this way it will generalize new cases well misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree
you will find that decision trees are more explainable than linear regression models a smaller tree is easily interpreted by someone who is not in the field and trees can use qualitative variables without the need to create dummy variables the impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node the predictive accuracy of cart models are not as robust as other methods regression tree performance is evaluated using the root mean square error (rmse)
we will explore some methods that can be used to improve this prediction accuracy and performance; on the next page you will learn more about random forests bagging and boosting","You will find that decision trees are more explainable than linear regression models. A smaller tree is easily interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node. The predictive accuracy of CART models are not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",What is more explainedable than linear regression models?,1,1
736,738,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make informed decision regarding the use of these terms.","if you have not already you should notice a pattern at this stage the conceptual progression business need ⇒ business objective ⇒ analytic objective including: problem statement ⇒ task definition ⇒ method & data statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against the statement of methods to be applied will vary in specificity depending on your project but three elements are important:
it must be precise enough so that the data science team understands it as a concise summary of the technical approach

at the same time it should allow for testing different techniques around the main conceptual idea

the methods should in principle be suitable to produce the target functionality/insight for the task given the available data
as you become more proficient in the discipline of data science (eg after having absolved the more advanced units of this course) you will develop refined intuitions about which method to propose in which context the most general categories of methods one can identify in this statement typically include:
supervised learning methods which involves learning to predict a target variable (typically through regression or classification) by training on “true” example data points whose target variable has manually been labeled or is available by other means
unsupervised learning methods deal with finding patterns in unlabeled data without an explicit prediction target
semi-supervised learning methods encompass hybrid methods that combine supervised and unsupervised learning in different ways
this course is very focused on the application of machine learning which has large overlap with various kinds of statistical methods it is possible to phrase your method statement around the use of statistics but it is recommended that one qualifies this rather broad term to something adequate for the project
in many contexts the method will have to be stated much more precisely than that especially if the problem domain is already well-studied in data science or there is some prior work that should be extended it can range from specifying a particular family of models (eg linear vs non-linear) using a new feature set testing the explainability of a particular model’s predictions to optimizing hyperparameters for faster training and inference in neural networks and more we can illustrate the specific vs general statement in the context of an example:
your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the personnel does not reliably find all potential defects (problem) your model should be able to predict product failure within one month after sale (task 1) and identify predictors measured at quality control time (task 2) in a pilot project you propose to apply “traditional supervised learning methods to train a classifier and examine the model for predictive variables” you then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation
variation: your pilot project was a success and your models are being used to better inform quality control personnel however it turns out that it still produces a high false positive rate (problem) and causes shipping delays which the client wants to minimize (business objective) you are asked to improve the model’s performance by reducing its false positive rate (task) you are further given access to quality control diagnostic equipment readings which the engineers believe are useful to discern whether a product is defective or just “needs to be broken in” you hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model
the method and data are the heart of a data science project finding the best tool for the task is of course one of the core skills of being a good data scientist this course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects as you gain deeper knowledge and experience you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with the state of the art at the same time thinking through different approaches and deciding on a set of methods and datasets benefits from teamwork open discussion and active seeking of advice and feedback from your peers mentors and relevant specialists
a good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results
in the above electronics scenario you imagine training a logistic regression model on the features to predict product failure once trained influential features should receive high weight in the regression equation allowing you to identify them easily similar to a correlation analysis if you train a decision tree you can identify features by traversing its branches on the other hand if you were to propose to train a complex random forest model to predict product failure with maximal accuracy one may object that random forests are not as easily interpretable as simpler models it would hence be an unsuitable method for task 2 in the pilot stage a good way would be to go with the simpler models for now and leave more complex models for later phases if needed in the variation a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommend you use a more recently developed encoding you may research both algorithms and make an informed decision or even use both algorithms in a comparative experiment","This course is very focused on the application of machine learning, which has large overlap with various kinds of statistical methods. It is possible to phrase your method statement around the use of statistics, but it is recommended that one qualifies this rather broad term to something adequate for the project.",How is it possible to phrase your method statement around the use of statistics?,1,1
1925,1928,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","as you develop analytic models or perform exploratory data analysis you will encounter datasets with a large number of variables a small dataset can also become quite large post data cleansing think about when you transform variables by creating new variables eg dummy variables considerations for a dataset with a large number of variables include issues with over-fitting and computing costs we think about the dimensionality of a model when we consider the number of variables used by the model famous mathematician r bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space this just means that when there are more features in a dataset you are prone to more errors a dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective  how can you address the curse of dimensionality without losing useful information we use dimensionality reduction: this technique is sometimes referred to as feature extraction or factor selection this technique is approached using mathematical modeling
so far we have talked about techniques that focus on features of an observation as you know by now feature engineering informs the models that you will build and its techniques involve looking at features of the data now we will explore a technique that is considered a model-based feature engineering technique
principal component analysis (pca) is used to reduce the dimensionality of a dataset you might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models well when you have a dataset that has a large number of variables you have to assess the relationship between those variables identify variables that might violate the assumptions of your chosen ml model and generally select the variables that are useful to your task when implementing pca you will be reducing the dimension of your feature space
you use the principal component analysis technique when you want to ensure variables in the dataset are independent of each other it is a useful technique to use when there are variables that need to be dropped but dropping these variables with pca is better justified than using the omission technique since you are using mathematical modeling
there are other techniques for dimension reduction including linear discriminant analysis (lda) and those techniques are mentioned in a future unit as well as in your upcoming machine learning courses
principal component analysis involves performing the eigendecomposition on the covariance matrix  a principal component analysis is a linear transformation technique as it finds a low dimensional representation of your high dimensional data pca will seek out a ""small"" number of dimensions in the dataset that are useful to the analytic task pca is considered an unsupervised technique and will be mentioned in that unit as well
the following steps are used when performing pca
standardize the data
compute the covariance matrix of dimensions in the data the covariance matrix
compute the eigenvectors and eigenvalues from the covariance matrix eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it meanwhile an eigenvalue is known as a characteristic value1 or a set of scalars
sort eigenvalues in descending order and choose the top k eigenvectors that correspond to the k largest eigenvalues
construct the projection matrix w from the selected k eigenvectors
transform the original data set x via w to obtain the new k-dimensional feature subspace y
pca in python example: principal component analysis in three (3) steps
reading: a brief article-principal component analysis (lever krzywinski and altman 2017)",Construct the projection matrix W from the selected k Eigenvectors.,What is a projection matrix?,1,1
1442,1445,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory","now that you have studied the elements of a properly framed analytical objective we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science they are provided here as purely illustrative example instances of the general template on which you can base your own formulations
not all framings of analytic objectives will include every individual elements as some of them may not be necessary depending on the situation in industry settings the problem and task may be merged and the added valuable functionality may be evident from a model that performs its function well in academic settings the overarching interest may be that of advancing the state of the art in research and hence the statement may either not include an explicit business objective or state it as a problem solution vision
a constructive analytical objective states that it is in principle possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet one can think of it as a proof-of-concept or prototyping endeavor
in order to increase sales from the company’s online store
 (business objective)
we work towards increasing the click-through rate of its advertising through targeted content
 (problem)
by classifying website visitors into youth middle-age and senior demographics (task)
using supervised learning models on curated internal datasets
 (method)
(business objective omitted due to project being primarily research)
in order to enable more effective search of audio collections
 (problem)
we demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds
 (task)
using neural models on a dataset of short clips of classical music and their descriptions
 (method)
towards developing suitable multi-modal audio-textual encoding
 (valuable functionality)
in scenarios where the feasibility of an analytical task has been established projects may be targeted towards improving over the state of the art in some performance metric by using innovative methods/features/data this is typically the case if one works on leaderboard-type datasets where there are models
the client is a logistics company that wants to speed up its automatic package sorting
 (business objective)
we focus on the problem of handwritten address recognition from shipping label scans 
(problem and task)
we want to combine neural image recognition with language models on company-internal data
 (method and data)
to improve performance beyond the current model based on standard convolutional neural networks without language information
 (valuable functionality)
(business objective omitted due to project being primarily research)
for the task of span-based question answering from text
 (problem and task merged because span-based question answering is a common leaderboard task)
we want to combine graph-based knowledge bases with neural attention models
 (method)
to improve over state of the art performance on realistic news text
 (valuable functionality and data)
exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives
the client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient
 (business objective)
specifically he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified
 (problem and task)
we want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client 
(methods and data)
towards identifying correlating events across the production process that can be used for process optimization

(business objective omitted due to project being primarily research)
the development of ai dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bot’s replies
 (problem)
we want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols
 (methods and data)
in order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset 
(task / valuable insight)", (Problem),What is the name of the problem?,0,0
1158,1161,Analytic Algorithms and Model Building,Data Science Patterns,Estimating f,"Parametric Methods,Non-Parametric Methods","prediction involves using a model to predict outcomes if you want to predict the number of hospital beds that will be needed in the event of a surge in ebola cases in the drc using historic data from past outbreaks we will assume that there is a linear relationship between the number of hospital beds needed and other variables related to the outbreak you will fit a linear model on the training data and select the model that minimizes test error
inference instances may arise when you need to understand how your dependent variable y is affected by changes in independent variables your goal is not to make predictions in this case instead you will estimate f by making inferences or gaining understanding on the relationship between your independent and dependent variables you might also be interested in assessing if the relationship between your dependent variable and each independent variable can be explained using a linear equation
estimating f
when creating analytic solutions you encounter the model understanding phase which might include using both linear and non-linear approaches to estimate f when estimating f  you would have gathered a dataset keeping in mind that the entire dataset will not be used to estimate f instead we will use a portion of the dataset to train our model to estimate f this dataset is called the training data set a statistical method will be applied to the training data to then estimate f typically you will use one of the following methods for this process:
parametric methods assume that your dataset comes from a population that has a fixed set of parameters non-parametric methods seek an estimate of f that is close to the data points a thin plate spline can be used to estimate f it seeks to estimate f that is close to the observed data thin plate spline is a two-dimensional analog of a spline constructed of piecewise third-order polynomials which pass through a set of control points a spline is a piecewise polynomial function that has a simple form locally but can be flexible globally a polynomial is a mathematical expression that involves a sum of powers in variables multiplied by coefficients non parametric methods include spearman correlation test and u-test for two independent means
the table below summarizes the differences between the two methods
models are built with a fixed number of parameters
models are built with any number of parameters
it has strong assumptions about data
it has fewer assumptions about data
needs less data than non-parametric methods eg sign test
needs significant amount of data
possess statistical power
has less statistical power
source: islr1 flexibility and interpretability of statistical methods
you will study different methods to estimate f in future modules and you will find that some of those models are either very flexible with low interpretability or very interpretable with low flexibility take a look at the figure above you will see that if you are interested in inference a restrictive linear model that shows the relationship between your dependent variable (y) and independent variables will be the best choice for estimating f a generalized additive model (gam) will extend a linear model to encompass non-linear relationships making gams more flexible methods that are more flexible tend to allow for non-linear relationships and they have low interpretability than your less flexible methods when prediction is the task of interest flexibility is key and because interpretability is not a high concern you can use the methods with less interpretability
using a method with low interpretability is not the only route to take when pursuing a prediction task this process of selecting the right method for a data science modeling task can be quite daunting but we will be exploring the different types of selection techniques throughout unit 5 going forward we will discuss the different data science patterns and the methods that are used to estimate f  you will also learn about the criteria that is used to select the best model","Inference. Instances may arise when you need to understand how your dependent variable Y is affected by changes in independent variables. Your goal is not to make predictions in this case, instead you will estimate f by making inferences or gaining understanding on the relationship between your independent and dependent variables. You might also be interested in assessing if the relationship between your dependent variable and each independent variable can be explained using a linear equation.",What is your goal instead of making predictions in this case?,0,0
97,97,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning","Adjusted R2 addresses the issue highlighted with the R2 , an independent variable that has a strong correlation to the dependent variable increases the adjusted R-squared and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted R2 is a suitable criteria to use.","When a variable without a correlation to a dependent variable is added, what is adjusted R2 a suitable criteria to use?",1,1
322,323,Data Gathering and Wrangling,Data Wrangling Pipeline,Overview,Data Wrangling Tools,"the quality of your data has a direct effect on the decisions made long after the models are developed when data is gathered it can present quality issues ranging from missing values to inconsistent formats data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that is usable data that is collected from different sources is considered raw data raw data should be studied before it is used in an enterprise data is used for immediate analysis model development with the goal of producing automated results or strategic decision making data will move through different stages to ensure continuous use
at this stage of the data science lifecycle we are considering data in its raw form you should also view all data (whether cleaned from its source) as raw data we want to know how to enrich data to further understand the data once you have completed this module you will be able to discuss the techniques used to enrich data through a process called data wrangling
data wrangling is the process of cleaning formatting and enriching raw data to make it usable for analysis as mentioned earlier data wrangling is also a best practice for an organization with a good data management framework the data architects engineers and/or administrators will store data that has been processed to allow for enterprise wide access and usage
data wrangling is a time consuming process as a data science team considers all data that has been extracted as raw data the data wrangling process can assign value to a dataset after the data has been cleaned and transformed data wrangling is a part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business; defining the business and analytic objectives and requirements for the analytic solution
despite its importance data wrangling presents some challenges that is common in data science projects
data wrangling opportunities and challenges
so far you might have interacted with datasets from sources such as kaggle kdnuggets or other avenues with ""cleaned"" datasets you might also be collecting data from social media using inbuilt data gathering tools to generate csv files you must consider these datasets as raw data it is best practice to study the data to determine its quality
consider an organization that collects or purchases customer data from a marketing firm the data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes the file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture
a quick search of the data wrangling process will produce multiple definitions and perspectives you might find that data wrangling is sometimes referred to as feature engineering in this course we separate both processes while data wrangling you are concerned with cleaning your data feature engineering will involve domain knowledge of the data this process involves selecting the right features from the data to further improve the performance of your models
the data wrangling process can be made more efficient with the use of tools below you will find open source tools used by data professionals with programming and spreadsheet skills as well as proprietary tools:
python the scikit learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data pandas numpy matplotlib and theano are libraries in python that support data cleaning and transformation
r uses specific packages to optimize the data wrangling process tidyr and dplyr are popular packages for data transformation
proprietary tools each year gartner releases a research based quadrant report for various tools including data pre processing tools the list typically features proprietary tools that are adopted by businesses of all sizes  microsoft power bi tableau alteryx ibm sap and qlik provide data wrangling capabilities to data professionals
excel anyone familiar with spreadsheets to study the data perform structuring and transformation of data data professionals without programming skills can use functions to identify missing values and for data validation among others","Python. The Scikit Learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are libraries in Python that support data cleaning and transformation.","Pandas, Numpy, Matplotlib, and Theano are libraries that support what?",0,0
1744,1747,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"the problem underlying an analytic objective should satisfy the following criteria:
solving the problem must be part of a possible solution vision towards the business objective
domain experts are confident that data exists which when analyzed can facilitate that solution; this data must either be available or sources are available to retrieve the data
the problem must be specific and realistic so that a corresponding data science project can succeed in principle

first with the business objective clearly stated one can propose a solution vision describing how the business can reach the objective the path to realizing this vision can then be decomposed into sub-problems and the data science team will be responsible for leveraging data to help solve these problems and/or produce data-derived insight that allows the client to make good decisions along the way
as data driven methods are still in the phase of being adopted across many fields the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement in such a case it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond “we want to leverage data somehow” and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria even if they include exploratory tasks
an online company asks for a data science project around the use of social media data for understanding their market driven by the board’s desire to keep up with the competition after the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses the client will focus on the need to increase market share; they will identify the business objective of increasing sales of a particular product there seem to be two ways forward: (1) spend money for increased advertising of a unique feature of their product or (2) lower the price they ask for an analysis of social media data to inform their decision this need for gauging consumer preferences now forms the problem component of the analytic goal
second there must be a sound presumption that the problem’s solution must benefit from the use of data a collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution thus may include data that would need to be collected as part of the project in order to be suitable the data should have some informative patterns relating to the problem it takes a combination of substantive expertise and data science skill to assess this criterion
domain-specific problems around the availability of data include:
lack of readiness in the organization (eg the organizations data is not readily processable) unsuitability of the data for the objective (eg data is old/stale out of domain incomplete or suffers from an obvious bias) or difficulty in collecting data because the expert labor involved is too expensive
technical objections too little data for the required methods fragmentation across multiple units with no suitable way of joining or overwhelming imprecision/noise in cases where data is available but the existence of “signal” for the problem is uncertain exploring whether it exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself
typically data science projects/consultations involve a preliminary data survey that informs or even precedes longer substantive discussion it is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client
third both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle in other words the system and/or insight produced by the project must add enough value to be deemed a success if executed properly on the client side this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated for example while it should mostly be avoided it may happen that a solution vision will be idealistic and somewhat resemble a science fiction scenario in such cases the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such on the other hand the technicians must be very careful to not exaggerate analytical capacities or cause unwarranted impressions that certain functionality is within reach for example even once an initial data sample has been surveyed the results should be communicated as being contingent on the assumption that the sample is representative for the larger dataset similarly if a particular neural network model performs a classification task very well on some domain the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level",The problem underlying an analytic objective should satisfy the following criteria:,What is the problem at the base of an analytic objective?,1,1
390,391,Model Evaluation,Evaluation Metrics,Summary,,"classification metrics are used to evaluate the class or probability output of classifiers a confusion matrix is a matrix style metric that uses the accuracy precision recall and specificity to assess whether data was classified accordingly
f-measure or f1 score is a harmonic mean of precision and recall values for classification tasks it is also an external evaluation for clustering output (only when class labeled data is available)
the auc-roc is a common metric used for classification problems the roc is a visualization of the sensitivity and specificity it can be used for both class and probability outputs  the auc roc does not account for the order of probabilities or the model's ability to predict higher probability for data
regression problems are evaluated using the rmse (root mean squared error) among others and it is considered the most popular evaluation metric used for regression problems when the rmse decreases it means the model's performance will improve
unsupervised learning algorithms are not as straightforward as the supervised learning algorithms to evaluate the dunn index silhouette coefficient and jaccard's index are used to evaluate clustering techniques
proceed to module 15 and complete quiz 10","Regression problems are evaluated using the RMSE (Root Mean Squared Error) among others and it is considered the most popular evaluation metric used for regression problems. When the RMSE decreases, it means the model's performance will improve.",What is the most popular evaluation metric used for regression problems?,1,1
835,837,Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"as we have seen the data science process involves multiple steps that require the expertise of team members in defined roles the data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem certain organizations can provide the structure and support for the different responsibilities within the process in smaller organizations personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and development of solutions below you will find the roles in a typical data science team this will vary depending on the domain and size of the organization
data scientist this role involves solving business tasks using machine learning model development and statistical techniques this individual identifies trends and patterns within the data and makes predictions based on trends the data scientist will write code to support the data analysis and model building process
data engineer the data engineer specializes in data structures and algorithms as well as in working with data through the operation of databases and other large repositories
solutions architect this is a customer facing role that ensures end-to-end customer deployment for company-related data services the solutions architect interacts with clients to design coordinate and execute solution prototypes
machine learning (ml) engineer the ml engineer performs modeling and software engineering tasks and different from the data scientist in that she is further away from the domain-side of the project this individual spends a considerable amount of time programming and creating ml solutions but must also have strong statistical skills
data/business analyst a data analyst has data gathering analysis and visualization skills like the data scientist she provides insights from data to inform decision making  analyst she develops key performance indicators and utilizes business intelligence and analytics tools compared to data scientists however they are typically firmly rooted in the business domain and less technically proficient in systems programming and advanced machine learning
software engineer the software engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise
domain experts also known as subject matter experts they are the actors who know the most about the problem on the business side their role is to define the framework for the data science project and hence they are a key participant in the process the domain expert will translate business needs and characteristics to the data scientists and eventually assess the solution as successful or not from the perspective of whether it achieved the business objective","Machine Learning (ML) Engineer. The ML Engineer performs modeling and software engineering tasks, and different from the data scientist in that she is further away from the domain-side of the project. This individual spends a considerable amount of time programming and creating ML solutions but must also have strong statistical skills.",What is a Machine Learning Engineer?,1,1
759,761,Data Gathering and Wrangling,Data Collection Process,Summary and Quiz 2,,"each data science project is unique and will require a data collection technique that suits the problem and objectives it seeks to meet the data collection techniques for a sentiment analysis project analyzing social media tweets will differ from the techniques of an image classification project for patient diagnosis when you defined the project's business and analytic objectives the tasks and methods were proposed as well those tasks and methods drive the type of data that will be used in the project the type of data will influence the source and data collection techniques
this module described the traditional techniques for data collection those techniques include conducting interviews focus groups document analysis and observations there are various types of data and this affects the method employed to solve your analytic objective
a data scientist should also know the different sources of data and how it affects the solution development process there is data that is sourced by your client but controlled by an external source and data that is collected and owned by your client",A data scientist should also know the different sources of data and how it affects the solution development process. There is data that is sourced by your client but controlled by an external source and data that is collected and owned by your client.,What should a data scientist know about?,0,0
3,3,Data Gathering and Wrangling,Data Wrangling Pipeline,Data Wrangling:Integrating Data,Data Wrangling to Data Exploration,"data integration involves ingesting transforming and integrating the transformed data for access the data is integrated to allow for analytic solution development ie modeling and analysis a popular example is integrating data into a data warehouse so that olap (online analytical processing) servers dss (decision support) systems and other enterprise wise analytic tools can access the data the data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data when there is a data warehouse data integration can be done with the assistance of an etl (extract transform and load) mechanism
once you have enriched and integrated your data you are ready to visually explore your data and perform feature engineering you might find that feature engineering is an extension of the transformation process done during data wrangling
in the next unit we will be taking an indepth look into data exploration techniques this is typically referred to as exploratory data analysis (eda) the results of an eda exercise can give insights to the project this is why it is important to begin the data understanding process with wrangling at this point in the data science lifecycle you have preprocessed your data for use during the eda process and beyond remember: data wrangling is not just for analytic solutions
the extensiveness of the data understanding phase shows that data quality can truly make or break an analytic solution the data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data if new data is sourced then the data wrangling process is repeated this is to say the data understanding phase is iterative","In the next unit, we will be taking an indepth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). The results of an EDA exercise can give insights to the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, you have preprocessed your data for use during the EDA process and beyond. Remember: Data wrangling is not just for analytic solutions.",What is the term for data exploration techniques?,1,1
425,426,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Mapping Raw Data to ML Features. Source-Google Developer Course,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical
 Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling","feature engineering is the process of using domain knowledge to extract features from raw data algorithms need specific features in the model development process feature engineering will ensure your dataset is compatible with your algorithm thereby improving model performance
 
so far we have highlighted the specialized nature of feature engineering and that there is no one size fits all to the process however there are foundational concepts that are essential to your understanding of feature engineering
do you remember this concept from an earlier module it is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering missing values in a dataset can negatively affect the performance of a model missing values can be caused by simple human errors privacy concerns among others how can we fix the problem of missing values a simple but problematic solution is dropping rows or columns a preferable solution is imputation
 you should consider a default value for missing values in a row or column let us visit how you handle this with numeric and categorical data
numerical data imputation if you are not dropping rows and columns with missing data the numerical imputation method will allow you to intuitively replace missing values a column with numbers and some with "" - "" or ""na"" can be replaced with a ""0"" other methods used include using the median or mean values of that variable
categorical data imputation in some cases replacing missing values with a zero will not make sense to the dataset you can replace values in a categorical column with the “maximum occurred value” you can impute “other” in a situation where there is no dominant value in the categorical column
similar to imputation binning can be applied to numerical and categorical data binning makes a model more robust but there is a trade-off between performance and overfitting binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data
 it is also used to capture noisy data when you have values that have variance
in the context of image processing binning is the procedure of combining a cluster of pixels into a single pixel as such in 2x2 binning an array of 4 pixels becomes a single larger pixel reducing the overall number of pixels
source2
you learned about how to visualize your data to detect outliers in an earlier module this method is less error prone you can use some statistical and visualization methods to detect and handle outliers include computing the z-score using percentiles and visualizing the data distribution of your dataset these techniques were discussed in the ""exploratory data analysis"" module
log transform also known as logarithm transform is used to handle skewed data and make the distribution of data less skewed it is widely used because of its ease of use and it decreases the effect of outliers in a dataset log transform is not usually applied to values that are less than or equal to zero
one hot coding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions this technique replaces categorical variables with different boolean variables that indicate whether or not a category of the variable was part of the observation those boolean variables are called dummy variables
one hot encoding is easy to implement it will retain all information of the categorical variable this method does not add valuable information that can make a variable more predictive
assume that a categorical variable education with labels less than high school and high school we can generate the boolean variable “high school” which becomes 1 if the person has high school or 0 if the person has less than high school
when you have a variable with multiple categories one-hot encoding might increase the dimensionality of your data the binary encoding method can be used to create a smaller number of variables without losing information
when you have ordinal data that is useful to your analytic solution you can transform those features using ordinal encoding here we convert string labels to integer values if you have an ordinal variable with string values that satisfied dissatisfied highly satisfied highly dissatisfied not applicable and somewhat satisfied ordinal feature encoding will map the the values to a corresponding integer as you can see in the table below all values are not integers
highly satisfied
1
satisfied
2
somewhat satisfied
3
not applicable
4
dissatisfied
5
highly dissatisfied
6
when you split features the features become easier to bin and this improves model performance there are many ways of splitting features and it depends on the variable if your dataset contains the variable address you might split the column by extracting street address city state and postal code you run the risk of increasing dimensions; in this case we employ techniques that assess the value of the extracted dimensions
some machine learning algorithms need to have scaled continuous features as model inputs scaling is not necessary for most algorithms but it can make continuous features identical in respect to range
there are instances that require the use of scaled data including algorithms that use gradient descent (""an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient in machine learning we use gradient descent to update the parameters of our model"") neural networks and linear regression are some of those examples the data is scaled before been fed to the model algorithms like k-nearest neighbors clustering analysis like k-means clustering and other distance based algorithms would need data that is scaled
quick thought: how about tree based algorithms like decision trees and random forest they are not affected as they are not distance based
normalization this technique involves values ranging between 0 and 1 prior to normalization all outliers in the dataset should be handled
standardization also known as z-score normalization it is useful for feature engineering in logistic regression artificial neural network and support vector machine tasks
how-to: scaling in python","Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable “high school”, which becomes 1 if the person has high school or 0, if the person has less than high school.",What does the Boolean variable “high school” become?,1,0
50,50,Analytic Algorithms and Model Building,Data Science Patterns,Regression,"Linear Regression,Assumptions of Linear Regression,Polynomial Regression,Stepwise Regression,Model Accuracy,Selecting the Right Regression Method","when your output variable is a continuous value you are able to make predictions using the widely known regression analysis the input variables for a regression task can be categorical discrete or continuous data so far we have read about getting qualitative responses or output using classification techniques regression techniques return a quantitative response to a task it is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s)
thought: the delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses some techniques that we will explore in the next modules can return both types of responses those techniques include knn among others
regression is one of the easier techniques to implement we perform regression analysis because it can highlight the impact of independent variables on a dependent variable for example you can tell the effect of changes to temperature and terrain on the outcome of a football game regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model regression is used for forecasting tasks as well when the goal is to infer relationships between the x and y variables you can use regression techniques when you identify independent variables that are highly correlated you can say that the variables are multicollinear if the correlation between two independent variables is ""1 or -1"" then you have perfect multicollinearity you can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed
a regression model will have certain components including the independent variables often denoted as x and the dependent variable y a regression model also accounts for random error ε the random error is not found in the dataset instead it is the difference between an expected outcome and the actual observation it is usually an unpredictable occurrence that you can not account for in your dataset then you have unknown parameters β your goal with a regression model is to estimate the function f(x β) with the best fit to the data f should be specified when performing regression analysis  this will ensure that you are deciding on the right regression methods to use
when performing regression analysis you might encounter data that has outliers if not handled during the data understanding phase it can affect the results of your regression analysis
let us explore the different types of regression techniques in this module with the goal of exploring each technique further in module 13
this regression technique is used to model the relationship between independent variable x and dependent variable y when you have two or more independent variables you will represent them as the vector x=(x1txkt) where t denotes a row of data k is the number of inputs the model is said to be linear because the output is a linear combination of independent variables
there is the simple linear regression model that allows for predicting a response based on one predictor variable most times you will be predicting a response with multiple predictor variables single linear regression does not allow for multiple predictor variables so instead of training multiple simple linear regression models for each predictor you use the multiple linear regression method to account for multiple predictors
this model can also be used for classification if you replace the gaussian output with a bernoulli distribution1  let us represent a regression model as:
𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k + 𝜀
regression function for multiple linear regression:
f(x1xk) = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽k𝑥k
y is a straight line function of each independent variable x the slopes of the individual straight line relationships of x1xk with y are the constants b1bk also known as the coefficients of the variables translate this to mean bi is the change in the predicted value of your dependent variable y per unit of change in xi with other things being equal consider b0  as the intercept (prediction that your model will make if all the independent variables were zero) you must also account for the random error e in the equation
you estimate b1bk and b0 using least squares this method will minimize the sum of squared residuals (a residual is the difference between an observed value and the fitted value given by a model) least squares can be linear or ordinary or nonlinear  ordinary least squares chooses the parameters of a linear function of a set of independent variables by the principle of least squares non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters it will approximate the model by a linear model and refine its parameters by iterations
performance of a regression model can be assessed using the coefficient of determination or r2 which shows the amount of variation in y that is dependent on x the larger the r2 the better the model can explain variation of the response with various predictors
ordinary least squares source3
reading: four principal assumptions these assumptions justify the use of linear regression models for prediction modeling these assumptions should be met to avoid producing misleading analytic solutions and insights
when the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x this is called a polynomial regression it seeks to model the expected value of y in relation to the value of x pay attention to the figure below you will note that using a linear regression line to fit the data would result in a high value of error
trying to fit a simple linear regression line source4
now refer to the image below to see the outcome when you fit a polynomial line through the data points the polynomial regression provides a better view of the relationship between the y and x variables a polynomial regression can fit a broader range of function however it is sensitive to outliers and those outliers can affect the result of a polynomial regression analysis
polynomial regression with lower error source4
when you have a regression analysis task you might have multiple independent variables (in reality you will) and you will need a method that fits the regression model with the most significant predictors stepwise regression will increase the prediction power of a model with a minimum number of predictors the process of fitting the model with the predictors is done automatically without human intervention there are two techniques for stepwise regression including:
backward elimination which tests the effect that each variable has on a model by deleting it the deleted variables are those that have the ""most statistically insignificant deterioration of the model fit""  this technique should not be used if predictors are more than the observations in the dataset
forward selection is the reverse of the backward elimination variables are added to assess model fit and included if the variable shows a significant improvement to the fit
we also have the mixed selection technique which can be considered a hybrid selection method with the backward elimination and forward selection techniques
stepwise regression is prone to overfitting issues and one way to guard against this is to check how significant the least significant variable will be based on chance model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or hold out sample you can check the extent to which a model fits the data with the residual standard error(rse is standard deviation of error e) ie ""the average amount that the response will deviate from the true regression line"" a large rse means the model was not a good fit to the data and the r2 is independent of your response variable unlike the rse
r-squares is calculated using the total sum of squares which is the total variance in y and rss is the ""discrepancy between the data and an estimation model""
a goodness of fit of a model will show how the model fits the data that it is trained with; it will highlight a lack of balance between observations in the dataset and those that will be introduced to the model (new values) when you want to select the right method you can use the different metrics below including:
aic known as the akaike information criterion is used to select models and you choose the model with the smallest aic as the best model the aic puts more emphasis on the model performance on a training set and will tend to select more complex models5
bic known as bayesian information criterion and a model with the lowest bic is considered the best model it is related to the aic and is appropriate for models fit under the maximum likelihood estimation the bic penalizes complex models unlike the aic
r2 can be defined as 1-residual sum of squares/total sum of squares the r2 will increase as more dimensions are added to the dataset (this is considered a weakness of this metric) a value of 0 means that a model does not explain any variability and 1 means the model explains full variability
adjusted r2 addresses the issue highlighted with the r2  an independent variable that has a strong correlation to the dependent variable increases the adjusted r-squared and decreases it when a variable without a correlation to the dependent variable is added when you have a model with more than one variable the adjusted r2 is a suitable criteria to use
mallow's cp is used to assess the fit of a regression model that has been estimated using ordinary least squares the goal is to find the best model involving a subset of these predictors note that you want a small cp
we will continue to learn more about regression analysis in an upcoming module and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge in regression analysis
additional reading: introduction to statistical learning",This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution1.  Let us represent a regression model as:,What model can be used for classification if you replace the gaussian output with a Bernoulli distribution?,1,1
109,109,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"motomanager case-in-point
here is an example of an ai consulting firm that used the evp framework to meet the business needs of a popular automotive services provider
mr tire-monro muffler and brake (a subsidiary of monro inc) is a top-50 automotive parts and general repair services provider in the us with over 320 locations nationwide in general the automotive services industry struggles with customer retention companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer) lack of customer “stickiness” leads to a) less potential revenue and b) less data about customers in general which could potentially be used to offer specific products and services to individual customers attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers
monro inc (the parent company of mr tire) approached cognistx (an ai applications company) to develop a data-driven solution to improve customer acquisition and retention the video above provides a brief summary of how cognistx engaged with mr tire to develop motomanager a mobile app that was deployed by mr tire this solution led to measured increases in customer acquisition and retention as well as increased revenue
the cognistx data science team met with the business leaders of monro inc to gain an understanding of the company’s business needs related to customer retention the data science team identified and interviewed all stakeholders from the business and technical teams at monro inc including the data management information technology and service management teams the it managers provided information about the company’s data asset management structure including data governance data architecture and data security management accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet their business needs
data management in the enterprise
finally service managers were interviewed on customer service difficulties that could be addressed by the proposed solution the service managers also identified the hardware and software gaps in the various stores around the country once the interviews were completed the cognistx data science team formulated business objectives that would meet monro inc’s business needs the business objectives included:
create an application that provides customers with a customized service experience for their automotive needs
offer customers $50 coupon to download monro inc mobile app
onboard customers to the application with the creation of customer profiles
provide tailored customer service management to very important (vip) customers
classify customers as vip customers based on defined characteristics
create a loyalty program to increase repeat customer transactions
business objectives should be measurable to ensure that business needs are met the metrics used to assess the success of the project were measured by:
count of people who installed the app and on-boarded upon receiving a $50 e-coupon
the number of times an on-boarded customer visited a store close to them
number of transactions completed with the app
total amount of money generated via the app compared to total amount of money used to maintain the app
a model that can predict a repeat customer from on-boarded customers with an accuracy of 85%
the metrics were both technical (precision and accuracy of model) and business related (calculate return-on-investment)
based on the business objectives cognistx developed an ai-enabled application for monro inc called motomanager motomanager captures a comprehensive profile of customers through an onboarding process monro inc sends a $50 coupon incentive to current and potential customers this coupon can be retrieved when a customer installs the application and completes their user profile the motomanager app uses customer data to provide customized reward incentives for booking services and making purchases from a customer’s local mr tire store as of 2019 the app has a 53000 user strength and monro inc has reported significant increases in customer engagement and retention the company has generated over $14 million us dollars from app-based transactions","Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive parts and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon), but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer “stickiness” leads to a) less potential revenue, and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing, and television and online advertisements did little to increase the proportion of repeat customers.",Why do companies record one-time-only transactions?,1,0
779,781,Model Evaluation,Interpreting Models,Accuracy versus Interpretability,,"throughout this course you have learned about understanding your client's needs develop and implement the right analytic solution to meet those objectives at this stage we want to think through the interpretability of models this will be helpful for fixing issues with the model and explaining why a model produced its results interpretability is a very important research area in data science and machine learning we want to explain why a model produces certain results and what happens when there are changes within a model; also known as explainability interpretability ensures that a data scientist can measure the effects of any trade-offs within a model
let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions as you know by now errors can be the difference between a useful solution and a solution that will lead to loss of money and (with how data science solutions are integrated into everyday life) lives accuracy can be defined as the measurement used to determine the best model for a task if the model can properly generalize new data it will produce better results (such as predictions)
there are certain sectors that are restricted by laws and standards in their use of certain techniques an example is the banking and education industry some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision making process as you read on the last page the more we learn about interpretability and employ interpretability strategies these issues might become a thing of the past accuracy is very important in these sectors and as we have learned accuracy will typically lead to less interpretability the big question is ""how can we retain interpretability while improving accuracy""
hall (2016) has recommended the following steps:
train black box models and use them as benchmarks
use different regression techniques
use black box models in the deployment process
train small interpretable ensemble models
create nonlinear predictors using black box techniques
explain black box models better using variable importance measures
source: rane-20181
in the next module we will discuss the different metrics to evaluate model accuracy",Source: Rane-20181,What is Rane-20181?,1,0
812,814,Analytic Algorithms and Model Building,Unsupervised Learning Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","let us explore the second type of clustering technique called the hierarchical clustering technique here you will begin clustering to form hierarchies of clusters and those hierarchies are presented using a dendrogram (reading a dendrogram) there are two techniques used for hierarchical clustering
this technique involves starting the clustering process with one observation forming its own cluster clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left essentially at each step of agglomerating clusters the clusters with the smallest distance from each other will be combined as shown in the figure below the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left
the technique can take advantage of any distance measure but you will find that most studies will use the euclidean distance as a distance metric
raw data-source1
the first round of merges finds clusters with observations/clusters b and c merged to form one cluster and d and e are also merged into one now we have cluster a bc de and f
next de and f are combined to form cluster a bc and def
clusters are further combined to form a and bcdef
finally abcdef is formed
hierarchical clustering technique-source1
single linkage method is based on grouping clusters using the agglomerative method with two clusters merged at each step those clusters contain the closest observations that are not yet part of the same cluster the distance between the nearest pair of observations in the two clusters are used to determine the best clusters to combine this method will produce clusters that have small distances while ignoring observations in clusters that are further from it as clusters are merged the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters
single linkage suffers from chaining in order to merge two
 groups only need one pair of points to be close irrespective
 of all others therefore clusters can be too spread out and
 not compact enough
complete linkage method uses the maximum distance between data points within each cluster also known as the farthest neighbor method clusters are combined into larger clusters until all data points are in the same cluster the distance between clusters is the distance between two data points (ie one per each cluster) that are farthest from each other complete linkage avoids chaining but suffers from crowding;  because its score is based on the worst-case dissimilarity
 between pairs a point can be closer to points in other clusters
 than to points in its own cluster
average linkage method is the average distance between data points within each cluster you can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity over all points in opposite groups
centroid linkage method is based on maximum distance and uses the centroid distance between clusters the mean for data points in a cluster is the centroid in complete linkage dissimilarity
 between clusters is the largest dissimilarity between two points in
 opposite groups
divisive clusteringthis is the opposite of the agglomerative method data is clustered using a top-down approach all data will belong to one cluster and then the largest clustered is split until each observation is in its own cluster this method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder this method is great at identifying large clusters and the agglomerative method is great at identifying small clusters
reading: hierarchical clustering of words and application to nlp tasks",Average Linkage Method is the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity over all points in opposite groups.,What method is used to think about dissimilarity between clusters using the average linkage method?,1,1
945,947,Data Gathering and Wrangling,Data Collection Process,Data Collection Tools,Data Collection Methods,"the data collection process in the data science lifecycle can be compared to the data collection process in scholarly research data collection is the process of gathering and analyzing data that can meet defined business and analytic objectives data collection should be conducted systematically to ensure that the data is valid and reliable in this section we will explore the traditional methods used to collect data and describe how data can be collected via methods that reduce bias
data collection before and during the data science project lifecycle includes data collected to define business and analytic objectives data collected to define business requirements and then the data needed for developing an analytic solution let us explore the traditional methods to collecting data:
questionnaires and surveys questionnaires are used to collect data from a group of individuals questionnaires can be administered on paper or online it might be easier to distribute questionnaires online as there are efficient tools that can analyze the data questionnaires can have open-ended closed-ended rating likert scale or multiple-choice questions data cleansing is still a consideration with questionnaire data as errors can occur for example responses to open-ended questions can contain misspellings among other errors
interviews interviews are used to collect data from small groups and unlike questionnaires the questions are typically open-ended
observations data collected during an observation can support data collected during interviews and from questionnaires observations allow for the examination of subjects as they perform their duties
focus groups focus groups can be used to explore topics that are difficult to observe and sensitive topics
the collection methods above are can be considered as obtrusive data collection methods meaning the participants of the data collection exercise are aware that data is being collected from them for a purpose there are unobtrusive data collection methods those methods involve collecting data from web sources (data from the web is used for data science related tasks including sentiment analysis text classification among others) document analysis and use of available datasets from a data repository unobtrusive data collection methods can be done without the knowledge of the subject of the study as is done in research studies and emphasized with data governance policies data gathering must be done in an ethical mannerin the next module we will explore collecting data from non traditional sources using non transforming and enriching the data to make it usable for analysis","Data collection before and during the data science project lifecycle includes data collected to define business and analytic objectives, data collected to define business requirements, and then the data needed for developing an analytic solution. Let us explore the traditional methods to collecting data:",What type of data is collected before and during the data science project lifecycle?,1,1
1467,1470,Analytic Algorithms and Model Building,Data Science Patterns,Active Learning,Query Strategies,"this data science pattern is different from what you have studied in this course it makes assumptions about an algorithm and the data that is used to construct it active learning pattern posits that if an algorithm or learner can choose the data it will learn from it will perform better than an algorithm that does not choose its own data and it will perform better with less training active learning is sometimes referred to as query learning the learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods when you have a large data set that is unlabeled (as is typical) active learning can be a useful technique for labeling
active learning presents scenarios that allow a learner to query the labels of observations in a dataset
membership query synthesis is a scenario that means a learner will generate an observation ie the learner will create a data point that is similar to one or more in the dataset once it is created the new observation will be labeled by the oracle (an information source or teacher)
stream based selective sampling scenario involves unlabeled data points or observations that are examined with the algorithm evaluating its informativeness against query parameters the learner will decide if it should assign a label or query the oracle
pool based sampling as shown in the figure below assumes that you have a pool of unlabeled data and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points) the informativeness measure is applied to all observations in your dataset and then the observations that have the most important measures are selected the selected observations are then labeled
thought: informative data points equal a data point that your algorithm had difficulty classifying informative data points improve your algorithm's abilities (prediction and otherwise)
pool based active learning cycle-source: settles active learning survey1
how does the algorithm decide on the most informative measures let's highlight some of the strategies used to evaluate the informativeness of unlabeled data
uncertainty sampling is an approach that allows the active learner to query the observations about which it is not able to label
query-by-committee involves using group or committee of models that have been trained on a labeled dataset but the catch is that these models have competing hypotheses each model in the committee will vote on the labelings identify the query that all voting models disagree on that becomes the most informative query
expected model change will use an approach that selects the observation that would introduce the most change to a current model if its label was known
expected error change involves labeling the data points that would reduce the model's out of sample error (measure of how accurately your learner can make predictions on new data)
additional reading: survey of active learning this report gives an in depth review of active learning in machine learning and artificial intelligence",How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.,What are some strategies used to evaluate the informativeness of unlabeled data?,1,1
785,787,Analytic Algorithms and Model Building,Unsupervised Learning Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","let us explore the second type of clustering technique called the hierarchical clustering technique here you will begin clustering to form hierarchies of clusters and those hierarchies are presented using a dendrogram (reading a dendrogram) there are two techniques used for hierarchical clustering
this technique involves starting the clustering process with one observation forming its own cluster clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left essentially at each step of agglomerating clusters the clusters with the smallest distance from each other will be combined as shown in the figure below the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left
the technique can take advantage of any distance measure but you will find that most studies will use the euclidean distance as a distance metric
raw data-source1
the first round of merges finds clusters with observations/clusters b and c merged to form one cluster and d and e are also merged into one now we have cluster a bc de and f
next de and f are combined to form cluster a bc and def
clusters are further combined to form a and bcdef
finally abcdef is formed
hierarchical clustering technique-source1
single linkage method is based on grouping clusters using the agglomerative method with two clusters merged at each step those clusters contain the closest observations that are not yet part of the same cluster the distance between the nearest pair of observations in the two clusters are used to determine the best clusters to combine this method will produce clusters that have small distances while ignoring observations in clusters that are further from it as clusters are merged the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters
single linkage suffers from chaining in order to merge two
 groups only need one pair of points to be close irrespective
 of all others therefore clusters can be too spread out and
 not compact enough
complete linkage method uses the maximum distance between data points within each cluster also known as the farthest neighbor method clusters are combined into larger clusters until all data points are in the same cluster the distance between clusters is the distance between two data points (ie one per each cluster) that are farthest from each other complete linkage avoids chaining but suffers from crowding;  because its score is based on the worst-case dissimilarity
 between pairs a point can be closer to points in other clusters
 than to points in its own cluster
average linkage method is the average distance between data points within each cluster you can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity over all points in opposite groups
centroid linkage method is based on maximum distance and uses the centroid distance between clusters the mean for data points in a cluster is the centroid in complete linkage dissimilarity
 between clusters is the largest dissimilarity between two points in
 opposite groups
divisive clusteringthis is the opposite of the agglomerative method data is clustered using a top-down approach all data will belong to one cluster and then the largest clustered is split until each observation is in its own cluster this method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder this method is great at identifying large clusters and the agglomerative method is great at identifying small clusters
reading: hierarchical clustering of words and application to nlp tasks","This technique involves starting the clustering process with one observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.",Clusters are then formed by combining or agglomerating the nearest clusters until there is what?,1,1
1973,1976,Model Evaluation,Evaluation Metrics,Clustering Evaluation Metrics,,"the previous page focused on the metrics for evaluating supervised learning problems  the presence of labeled data makes it somewhat straightforward to train and test the model's performance now we will focus on metrics that can be used when labeled data is not present  there are two approaches to evaluating clustering the internal and external evaluation approaches the internal approach involves summarizing the clustering task to a single quality score while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable clustering can also be evaluated by an expert
internal evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score the cluster with the best score is seen to be the best internal evaluation although useful can have its drawbacks it gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters  a sound example from wikipedia that illustrates this: k-means clustering can only find convex clusters and many evaluation indexes assume convex clusters on a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity is sound
let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
silhouette coefficient shows how similar a data point is to its cluster compared to other clusters it is calculated using the mean intra cluster distance and the mean nearest cluster distance for each data point a silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster when the silhouette coefficient is close to 0 there is a presence of overlapping clusters
dunn index is also used to evaluate clustering techniques and similar to the silhouette coefficient it is dependent on the data within the clusters a good clustering is one with a higher dunn index when using this evaluation technique you want to be aware of a high computational cost when you have a large number of clusters the dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters the minimum of the pairwise distance is used to determine minimum separation (minseparation) the compactness of a cluster is measured by computing the distance between the data in the same cluster (maxdiameter) finally the dunn index will be:
minseparation/maxdiameter
external evaluation measures the results from a clustering task based on data not used for the clustering task benchmarks are set from a set of pre-classified data external evaluation techniques need ground truth data to evaluate clustering
rand index tells you how similar a cluster or clusters are to a set benchmark this is similar a classification evaluation technique you can calculate the rand index thus:
(tp + tn)/(tp+fp+fn+tn)
purity is considered a no frills technique that assigns each cluster to a class (usually one that occurs often in the cluster) the number of correctly assigned observations is divided by the overall number of observations to determine accuracy purity close to 1 is best and close to 0 is not optimal a large number of clusters can lead to a higher purity there is a tradeoff between quality of clustering and number of clusters when using purity as a metric the normalized mutual information (nmi) can be used to measure and compare the quality of clustering between different clusterings with varying number of clusters
jaccard index is used in cluster analysis evaluation and convolutional neural networks among others it is defined as ""the size of the intersection divided by the size of the union of the sample sets"" the jaccard distance measures dissimilarity between sample sets
f-measure is simply computed as the 2 * ((precision * recall)/(precision + recall)) you might remember it from the classification metrics it is also known as the f1 score
dice index also known as the sorensen-dice indexor dice coefficient can assess the similarity of two samples it ranges from 0 to 1 dice index is a semimetric version of jaccard index and gives less weight to outliers in a dataset it is used to measure the lexical association score of two words
reading: clustering evaluation techniques (20min read)
you should also be interested in measuring to what degree clusters exist in the data to be clustered before beginning the cluster analysis this is sometimes done by comparing a dataset against another (one without clusters) the hopkins statistic is used to measure cluster tendency a resulting value of 1 or close enough will show that the data is clustered data that is uniformly distributed will be closer to 0 hopkins statistic is quite good at estimating randomness in a dataset",External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,External Evaluation measures the results from a clustering task based on what?,1,1
1049,1052,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistical Inference,"Statistical Inference,Sampling Distribution,Confidence Interval,168.8cm to 181.2cm,Hypothesis Tests","statistical inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods those conclusions are typically drawn using exploratory data analysis or summary statistics the goal of this process is to use probability theory to make inferences about your data  this is the first step of learning about the attributes of your population from the sample that you have drawn
reminder: the characteristics of the sample dataset are called statistics the characteristics of your population are known as parameters
understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes
if you recall from a previous unit you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise you can use statistical inference to draw scientific conclusions and test set hypotheses significance of a sample data set or descriptive statistics is often in question during the eda process using statistical inference techniques can give significance to your conclusions from eda statistical inference techniques are categorized under estimation and hypothesis testing let us explore these methods:
you typically draw a sample dataset from your population as it is quite difficult to perform analysis on an entire population let us consider a quick example assume we are analyzing the income data of all neurologists in the united states of america we can make inferences about the population mean income by calculating the mean of income on a sample of 2000 neurologists this mean is the sample mean  x̄ we also refer to this as the point estimator of the population mean if the mean of our sample is $258900 then we refer to this number as the estimate of the population mean
let's expand this further the american academy of neurology conducted a study that estimated the number of neurologists in the us at 16400 we can draw another sample that will result in a different mean consider you draw multiple samples and record each sample mean you will have what is called a sampling distribution if you continued to draw samples from this population the average value of your point estimator will equal the population mean we can say that a point estimator is unbiased if its expected value equals that of the population
keeping with the example above let us derive the variance of your sample mean if we continued to sample our neurologist population the variance of the sample mean will be the variance of our population divided by 16400 note that the variability between observations is usually larger than variability between sample means this is because your sample contains a range of observations finally you will derive the standard error of the sample mean this is the population standard deviation divided by the square root of the sample size or simply the standard deviation of the sampling distribution
standard error = population standard deviation/
the standard error is important because it measures how accurate the sample distribution represents the population
please note that the sampling distribution is considered normal if the population mean is normally distributed  this is highlighted because you can not use most statistical inference techniques if the sampling distribution of your sample mean is not normally distributed
there is a popular theorem that addresses the situation when your population is not normally distributed this is known as the central limit theorem
statistical inference is not solely applied to means it is also applied to proportions
you will work with proportions in clustering analysis tasks if a telecommunications company is assessing the proportion of customers who sign up for a contract after receiving a promotional advertisement the parameter of interest is the population proportion p as the data scientist tasked with this analysis you will make an inference about the population proportion by drawing a sample in this case the point estimator is the sample proportion p hat or p̂
reading: proportions
when we provide a range of values of estimates for a population parameter we are referring to the confidence interval (ci)  you can think of it as the range of likely values for a population parameter with a specified level of confidence the sampling distributions of your sampling mean or sampling proportion must be normally distributed to derive an accurate ci the sampling distribution of the sampling mean and sampling proportion will be normally distributed if the sample size is large (in most cases that is n is greater than or equal to 30) the sampling distribution of your statistic (mean or proportion) is needed to derive your ci
you will use the margin of error  to account for the standard error of your point estimate and your desired confidence interval  consider this example:
consider that we are measuring the heights of 40 randomly selected male soccer players our sample mean is 175cm we calculated the standard deviation of the athletes heights and it totaled 20cm let us calculate the ci
n = 40 mean = 175 s = 20
you will decide on the ci to use (95%) and then find the z value for the selected ci a 95% ci means that 38 of the 40 confidence intervals will contain the true mean value
the z value for 95% ci is 1960
we calculate the 175 ± 1960 ×  20/√40
175cm ± 620cm
source1
reading: confidence interval
as you conduct research and complete data science projects questions will arise about the likelihood of occurrences as we have seen so far statistical inference helps to ground your insights with statistical significance and does its best to rule out the possibility of chance we have looked at estimation and confidence intervals to help make inferences from your data now we will explore the oldest statistical inference hypothesis testing
a hypothesis is ""an interpretation of a practical situation or condition taken as the ground for action""  similar to the other techniques you are looking to draw a conclusion about a population using a sample data set when you apply statistical hypothesis testing the end results are always favorable because (according to enrico fermi) you make a measurement or a discovery
according to dermatology associates hyper-pigmentation is the number one skin health concern for black females ages 18-45 skincare co is one of the leading manufacturers of skin care products skincare co is looking to develop a 120 day skin care line to target this population and this skin health concern you are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation your preliminary research has found that administering of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage this is different from claims that have been made about this ingredient (previous claims state that there will be no damage) this claim or belief has been formulated and it should be tested with evidence that refutes or proves that it is true you can use hypothesis testing to provide this evidence to construct a hypothesis test:
identify the population parameter of interest
determine whether you will be conducting a one-tailed or two-tailed test
define a null hypothesis often denoted as h0 the null hypothesis is considered the status quo or in the case of our example: the administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage
then define an alternative hypothesis denoted as ha this would be the opposite of the null hypothesis
the example above does not cover the entirety of identifying your null and alternative hypothesis you must know that if proven your alternative hypothesis is a call to action ie if you reject your null hypothesis then the status quo has been changed and the decision makers must take action how do we test our hypothesis statistically
reading: we do this by using the one- and two-tailed tests
let us also keep in mind that these tests are not error-proof you want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted to avoid this we consider the two error types in hypothesis testing
type i error occurs when you reject the null hypothesis when it should be accepted
type ii error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected
considering our skin care manufacturer example above a type i error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so the company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects the consequences of committing a type ii error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so the cost of this error would mean producing a skin damaging treatment product that would lead to loss of customers and possible lawsuits
how do you ensure that both errors do not occur collect more data either increase your sample size or collect more data over a longer period of time
collecting more data does not entirely mean that you will reduce both errors but it ensures that you commit one over the other at a lesser magnitude
in the module we will continue the process by learning how to extract features of variables within the dataset to improve the performance of analytic solution(s)",Source1,What is the name of the source1?,0,0
1085,1088,Analytic Algorithms and Model Building,Unsupervised Learning Techniques,k-Means Clustering,"Unsupervised Learning Techniques,Clustering-Source www.pyarmy.com,Types of Clustering,k-Means Clustering","when we want to identify patterns in a dataset from unlabeled data we use unsupervised learning to perform this task unsupervised learning is also referred to as self-organizing; we have touched on the principal component analysis when we discussed feature engineering this is one of the unsupervised techniques we will also look further into the different types of cluster analysis techniques
you can categorize data according to characteristics using a technique called cluster analysis if you think about how we reason and learn as human beings we make sense of events people and things by placing them in groups you have memories that are characterized as happy and sad or people categorized into close friends acquaintances and mentors among others you might even consider clustering data to identify those with similarities as a method of exploring data applications of cluster analysis include market segmentation; this is the segmenting of customer data based on certain criteria including transaction history the different clusters created from the segmentation exercise are useful for targeted advertising or application of customized marketing strategies that will might elicit positive responses increase sales and engagement
hard clustering divides data into a number of groups and can only belong to one cluster all clusters are independent of each other
soft clustering groups data into clusters but a data point can belong to more than one cluster to a degree
overlapping clustering allows data to belong to more than one cluster
hierarchical clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram
reading: an expansion on clustering
this method involves identifying the number of clusters k that the dataset will be grouped into; the data in each cluster should share similarities to the other records within its cluster assume you have k = 5 cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5  the data within clusters adhere to distance measures to ensure that dispersion is minimized k-means clustering technique abides by a number of distance measures but the most popular is the euclidean distance let us look at how clusters are created using this technique:
partition data: the dataset is partitioned into k clusters are pre-specified (chosen by the data scientist)
initialize centroids: within each cluster the distance between the observations is modified so that dispersion is minimized and each observation is closest to nearest centroid centroids are data points that are considered to be the center of a cluster you can also think of this datapoint as the mean of all the observations within the cluster
iteratively initialize centroids from the previous step until the means of the newly formed clusters are negligible
you know that you have a good cluster when there is ""high similarity among within-cluster data and low similarity among inter-cluster data""1
how do we decide k similar to knn there are empirically studied recommendations for the best k to select you can also select k based on previous knowledge (this is hardly the case with this unsupervised task) you can use different values for k and then compare the results gotten from each value of k it is good practice to also run the k-means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k
k can also be chosen by calculating the within cluster sum of squares(wcss) this is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster
assume that we have 1000 observations in a dataset and we have decided that k = 1000 the wcss should be zero (0) this is because all the observations are considered as centroids and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster this is certainly not a computationally sensible way to cluster data think about a dataset with over 100000 observations also think about the information to be gleaned from the cluster analysis; you will lack useful information
when you randomly initialize with a range of k values for the 1000 observations mentioned above ie between 2-10 you can use the elbow method to find out the optimum value for k the elbow method produces a graph that shows this optimum value at the ""elbow"" of the line as shown below you select k as the wcss decreases; the figure below shows that after 5 the decrease in wcss is quite small
elbow method: source1
reading: k-means clustering-sklearn
additional reading: k-means clustering algorithm
k-means clustering and k-nearest neighbors have been known to cause confusion for data scientists who are new to the field afterall we are discussing similarity measures and distances to an observation to classify or cluster into a class the main difference is that one is an unsupervised technique and the other is supervised knn is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points kmeans does not provide a labeled dataset to the model for learning purposes kmeans will partition the data into a number of clusters knn works best with data that is of the same scale but kmeans do not need same scale data to perform well remember when you learned about knn being a lazy learner kmeans is an eager learner it is slow to train but fast to learn and it tends to deal with noise in the training dataset better than a lazy learner","When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing; We have touched on the Principal Component Analysis when we discussed feature engineering. This is one of the unsupervised techniques. We will also look further into the different types of cluster analysis techniques.",What is self-organizing also called when we discussed feature engineering?,1,1
653,654,Exploratory Data Analysis,Performing Exploratory Data Analysis,Information Design Overview,"Summarizing Data,**Do you know that if the mean and median of your data set differ greatly, you should check that variable for outliers!**,Source: BPI Consulting LLC,Relationships and Association,Correlation Coefficient,Meaning,Outliers,Box Plot. Source. University of Manchester,Graphical Summaries,One of the first Cartesian Coordinates Graphs,Visualizing Variables","the exploratory data analysis (eda) process is comprised of visualizing data to allow a data scientist or a data analyst explore datasets to gain insights from the data there are some non-graphical techniques that are used to explore data and as well as graphical techniques non-graphical techniques include using summary statistics to describe the data and the graphical techniques are used to describe the frequency distribution of the dataset both techniques can be used to show the skewness and extreme outliers in a dataset
summarizing data is dependent on the types of data present in your dataset when you want to understand the observed data aka your sample you will key descriptors of a dataset when you have a large data set it is difficult to describe it in its raw form you will use specific techniques to summarize and describe your sample some of those techniques include describing central tendency and assessing measures of spread and relationships
you can use the location shape and spread of the data in a dataset to understand the data you might find some of the concepts below as a review of your first statistics course but pay attention to the reason for using these techniques in exploring your data also these concepts are important for when you learn about using statistical inference to draw conclusions on an unknown population parameter
location during the eda process we are looking to describe our data using a central value we will explore the mean which is sometimes called the average this is the sum total of all observations divided by the number of observations you will calculate the mean for your population (population mean = μ)  and for the sample that you have drawn (sample mean = x̄) there are different types of mean including the typical arithmetic mean geometric mean and the harmonic mean
reading: central tendency with different types of means
median is the mid value of a dataset to calculate a median value you will sort your data in ascending order the median value in a data set with odd number of observations is the middle value while you can find the median of a dataset with even observations by calculating the average of the two middle values
mode is the variable of an observation that most frequently occurs in your dataset a uni-modal variable is one that has just one mode and a bimodal variable has two modes if your variable has more than two modes it can be referred to as multi-modal do not think it is useless in the eda process the mode is quite useful when summarizing categorical variables
percentile you will remember this nifty word from your gre scores or height and weight data from your health records it tells you the position of a value in your dataset  if you are 175cm in height and you are in the 10th percentile of height measurement for your gender it means that among all the height data collected for your gender you are taller than 10% of those observations the 50th percentile is considered the median quartiles are values that split the data into quarters
spread how can you describe the spread or variability of your dataset you use the measures of dispersion although used for descriptive statistics you must be careful in relying on this measure to describe variability
range of a set of values can be calculated by subtracting the minimum value in your dataset from the maximum value notice that range only considers two values and ignores all other values of a variable
mean absolute deviation is the ""average distance between each value and the mean of a dataset"" this measure of dispersion can tell you how values are spread out in a dataset and determines whether the mean is a useful indicator of the values within the data the larger the mean absolute deviation the more spread out the data when you work with time series forecasting methods you will use the mean absolute deviation to measure the performance of a forecasting model you will find that the measures used in eda are often used in model selection criteria
reading: mean deviation =  (σ|x − μ|)/n this is the sum of the absolute values minus the mean and divide this by the number of values in the dataset
variance s2 / σ2 is the average of the squared difference between the observations in a dataset and the mean so far we know that you can have measures for both your overall population and your sample drawn from the population keep in mind that you will sometimes have a sample variance s2  or a population variance σ2 the difference between both is that you are looking at the spread of data from the sample mean versus the population mean
standard deviation σ or s is the most common measure of dispersion it tells you the distance of the values from the mean in your dataset a low standard deviation tells you that the values are close to the mean and a high standard deviation means there is a spread the σ is derived by calculating the square root of the variance as you perform exploratory data analysis and even while developing models the importance of the standard deviation can not be overstated despite its mention as a way to summarize data standard deviation is used to ""measure the confidence in statistical conclusions"" and if you remember from the overview of this unit you will be conducting statistical inference to begin to draw some conclusions on your data and hypotheses
interquartile range (iqr) similar to the range does not consider all observations when looking at the spread of values in a dataset iqr describes 50% of values in your dataset when arranged in ascending order the iqr is the difference between the values in quartile 3 and the values in quartile 1 you can use this measure to identify a value that is an outlier
shape now that you can explain the measures used to explore data by describing its central value its spread from the mean and identified outliers let us describe the distribution of a dataset and assess whether it is normally distributed normally distributed data is useful when making statistical inferences how can we assess the distribution of our data:
skewness measures the degree to which the distribution of data lacks symmetry a dataset with 0 skewness is considered normally distributed data does not always have a skewness of 0; however if you have found skewness to be between -05 and 05 you can ascertain that your data is symmetrical if skewness is between -1 and -05 or 05 and 1 then your data is moderately skewed if skewness is < -1 or > 1 your data is highly skewed
kurtosis looks at the outliers within the distribution this measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution
reading: kurtosis
covariance describes the linear relationship between variables in your sample or population data covariance can be negative meaning your variables have a negative linear relationship zero (0) meaning the variables have no linear relationship or positive meaning a positive linear relationship exists between the variables
correlation coefficient will describe the strength of the linear relationship between the variables x and y it is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables if the correlation coefficient equals:
-1
variables have a perfect negative linear relationship
0
variables are not linearly related
1
variables have a perfect positive linear relationship
whenever you have a rather small or large observation within your dataset compared to other values in the dataset this is called an outlier outliers will affect the performance of your model and prior to getting to that point your exploratory data analysis when you have a large dataset the outliers are not as noticeable as when you have a smaller dataset similar to missing values you must handle outliers when you identify them in your dataset you should refrain from removing them from the dataset until proper investigation is completed you can ""handle"" outliers by following these steps:
construct a boxplot or as it is sometimes called a box and whisker plot this chart is used to graph the five number summary the five number summary is used to identify an outlier in your dataset a five-number summary consists of five values including the the maximum and minimum values in your dataset the lower and upper quartiles and the median these values are then ordered in ascending order and plotted
box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics including the skewness and mean you will typically plot your box plot using a tool of your choice in this course that would be python here is a resource on how to plot a box plot using python
z-score is a measure of the relative position of an observation within a dataset you calculate the z-score by subtracting the value from the mean and dividing that value by the standard deviation if an observation has a z-score that is more than 3 or less than -3 it is an outlier
the saying that a picture tells a thousand words supports the notion that visualizing data is one of the best ways to tell a story to a wide audience with the ability to view the visuals today there are so many tools that are used to visualize data including python r excel tableau microsoft's powerbi among others these tools even allow us to make data visualizations interactive for more effective analysis rene descartes invented the cartesian coordinate system or the x and y axis graph as we know it
data visualizations have evolved greatly including info-graphics plots and statistical graphs communicating insights in the data clearly is a key step in the data science process there is a skill to developing effective visualizations and not all data scientists will have these skills however it is important to know how to use the tools that support visualization and be able to identify the right types of visualizations for the data types and the best ways to tell your story with visualizations visualization experts strongly suggest that visualizations should stimulate the attention of viewers and elicit thoughts and questions during the data science project process visualizations will be used at different times at this stage of exploratory data analysis the visualizations are used to understand the dataset as you proceed through analytic solution building gaining a better understanding of your data will support the performance of your models in the later stages of the project
so far we have talked about visualizing mostly numeric data however we are able to visualize categorical data as well typically this data is categorized and numeric values are derived (counted) to represent the values in the variables a frequency distribution is a useful technique in displaying categorical variables
follow our primer below to learn more about the appropriate visualizations for data science tasks
reading: data exploration and visualization primer
as we study exploratory data analysis we will learn how to describe sample data by making inferences from the population on which the sample data was drawn despite the sampling techniques used there is still a level of uncertainty in drawing conclusions about your population we find that descriptive statistics is not enough to estimate this uncertainty; hence the need for statistical inference on the next page we will explore statistical inference further
additional reading: the dr howard seltman experimental design and analysis book is a text that explores experimental design and analysis and is worth a look through
stephen few3 listed eight types of quantitative data and the types of charts and graphs that can communicate the story in the data
reading: selecting the right graph for your message","Spread. How can you describe the spread or variability of your dataset? You use the measures of dispersion. Although used for descriptive statistics, you must be careful in relying on this measure to describe variability.",How can you describe the spread or variability of your dataset?,1,1
381,382,Analytic Algorithms and Model Building,Supervised Techniques,Overview,,"when a model is trained on data that is labelled it is learning from past experiences that allow it to perform a task when introduced to new unlabeled data  the labeled data is considered previous knowledge that has both input and output parameters this method of training a model is called supervised learning deepai has a concise definition of supervised learning it is defined as ""a class of systems and algorithms that determine a predictive model using data points with known outcomes""
supervised learning tasks can be used for classification and  regression tasks both tasks use algorithms to train the models we will explore in the next pages and onwards
we have covered techniques should as linear and logistic regression we will explore some popular techniques including k-nearest neighbors tree methods and naive bayes as you continue your journey in the mcds program especially in the machine learning course you will explore supervised learning tasks outside of the well known algorithms introduced in this course
reading: a comprehensive list of supervised learning algorithms
reading: supervised learning and natural language processing",Reading: A comprehensive list of Supervised Learning algorithms.,What is a comprehensive list of Supervised Learning algorithms?,1,0
133,133,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"motomanager case-in-point
here is an example of an ai consulting firm that used the evp framework to meet the business needs of a popular automotive services provider
mr tire-monro muffler and brake (a subsidiary of monro inc) is a top-50 automotive parts and general repair services provider in the us with over 320 locations nationwide in general the automotive services industry struggles with customer retention companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer) lack of customer “stickiness” leads to a) less potential revenue and b) less data about customers in general which could potentially be used to offer specific products and services to individual customers attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers
monro inc (the parent company of mr tire) approached cognistx (an ai applications company) to develop a data-driven solution to improve customer acquisition and retention the video above provides a brief summary of how cognistx engaged with mr tire to develop motomanager a mobile app that was deployed by mr tire this solution led to measured increases in customer acquisition and retention as well as increased revenue
the cognistx data science team met with the business leaders of monro inc to gain an understanding of the company’s business needs related to customer retention the data science team identified and interviewed all stakeholders from the business and technical teams at monro inc including the data management information technology and service management teams the it managers provided information about the company’s data asset management structure including data governance data architecture and data security management accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet their business needs
data management in the enterprise
finally service managers were interviewed on customer service difficulties that could be addressed by the proposed solution the service managers also identified the hardware and software gaps in the various stores around the country once the interviews were completed the cognistx data science team formulated business objectives that would meet monro inc’s business needs the business objectives included:
create an application that provides customers with a customized service experience for their automotive needs
offer customers $50 coupon to download monro inc mobile app
onboard customers to the application with the creation of customer profiles
provide tailored customer service management to very important (vip) customers
classify customers as vip customers based on defined characteristics
create a loyalty program to increase repeat customer transactions
business objectives should be measurable to ensure that business needs are met the metrics used to assess the success of the project were measured by:
count of people who installed the app and on-boarded upon receiving a $50 e-coupon
the number of times an on-boarded customer visited a store close to them
number of transactions completed with the app
total amount of money generated via the app compared to total amount of money used to maintain the app
a model that can predict a repeat customer from on-boarded customers with an accuracy of 85%
the metrics were both technical (precision and accuracy of model) and business related (calculate return-on-investment)
based on the business objectives cognistx developed an ai-enabled application for monro inc called motomanager motomanager captures a comprehensive profile of customers through an onboarding process monro inc sends a $50 coupon incentive to current and potential customers this coupon can be retrieved when a customer installs the application and completes their user profile the motomanager app uses customer data to provide customized reward incentives for booking services and making purchases from a customer’s local mr tire store as of 2019 the app has a 53000 user strength and monro inc has reported significant increases in customer engagement and retention the company has generated over $14 million us dollars from app-based transactions",The number of times an on-boarded customer visited a store close to them.,What is the number of times a customer visited a store close to them?,1,0
824,826,Model Evaluation,Interpreting Models,Summary and Quiz 10,Ensure you complete Module 16 before starting Quiz 10!,"there is a tradeoff between model accuracy and model interpretation models that are easily interpretable tend to be less accurate and models that are highly accurate tend to be less interpretable we want to be able to explain a model to a human being to ensure there is better decision making for your client and their end users
interpretability is important in the model building process as we want to know why models produce the results they have produced understanding the why behind results will lead to early bias detection (and prevention for future challenges) better acceptance model debugging and to satisfy human curiosity","Interpretability is important in the model building process as we want to know why models produce the results they have produced. Understanding the why behind results will lead to early bias detection (and prevention for future challenges), better acceptance, model debugging, and to satisfy human curiosity.",What is important in the model building process as we want to know why models produce the results they have produced?,1,1
399,400,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Mapping Raw Data to ML Features. Source-Google Developer Course,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical
 Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling","feature engineering is the process of using domain knowledge to extract features from raw data algorithms need specific features in the model development process feature engineering will ensure your dataset is compatible with your algorithm thereby improving model performance
 
so far we have highlighted the specialized nature of feature engineering and that there is no one size fits all to the process however there are foundational concepts that are essential to your understanding of feature engineering
do you remember this concept from an earlier module it is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering missing values in a dataset can negatively affect the performance of a model missing values can be caused by simple human errors privacy concerns among others how can we fix the problem of missing values a simple but problematic solution is dropping rows or columns a preferable solution is imputation
 you should consider a default value for missing values in a row or column let us visit how you handle this with numeric and categorical data
numerical data imputation if you are not dropping rows and columns with missing data the numerical imputation method will allow you to intuitively replace missing values a column with numbers and some with "" - "" or ""na"" can be replaced with a ""0"" other methods used include using the median or mean values of that variable
categorical data imputation in some cases replacing missing values with a zero will not make sense to the dataset you can replace values in a categorical column with the “maximum occurred value” you can impute “other” in a situation where there is no dominant value in the categorical column
similar to imputation binning can be applied to numerical and categorical data binning makes a model more robust but there is a trade-off between performance and overfitting binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data
 it is also used to capture noisy data when you have values that have variance
in the context of image processing binning is the procedure of combining a cluster of pixels into a single pixel as such in 2x2 binning an array of 4 pixels becomes a single larger pixel reducing the overall number of pixels
source2
you learned about how to visualize your data to detect outliers in an earlier module this method is less error prone you can use some statistical and visualization methods to detect and handle outliers include computing the z-score using percentiles and visualizing the data distribution of your dataset these techniques were discussed in the ""exploratory data analysis"" module
log transform also known as logarithm transform is used to handle skewed data and make the distribution of data less skewed it is widely used because of its ease of use and it decreases the effect of outliers in a dataset log transform is not usually applied to values that are less than or equal to zero
one hot coding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions this technique replaces categorical variables with different boolean variables that indicate whether or not a category of the variable was part of the observation those boolean variables are called dummy variables
one hot encoding is easy to implement it will retain all information of the categorical variable this method does not add valuable information that can make a variable more predictive
assume that a categorical variable education with labels less than high school and high school we can generate the boolean variable “high school” which becomes 1 if the person has high school or 0 if the person has less than high school
when you have a variable with multiple categories one-hot encoding might increase the dimensionality of your data the binary encoding method can be used to create a smaller number of variables without losing information
when you have ordinal data that is useful to your analytic solution you can transform those features using ordinal encoding here we convert string labels to integer values if you have an ordinal variable with string values that satisfied dissatisfied highly satisfied highly dissatisfied not applicable and somewhat satisfied ordinal feature encoding will map the the values to a corresponding integer as you can see in the table below all values are not integers
highly satisfied
1
satisfied
2
somewhat satisfied
3
not applicable
4
dissatisfied
5
highly dissatisfied
6
when you split features the features become easier to bin and this improves model performance there are many ways of splitting features and it depends on the variable if your dataset contains the variable address you might split the column by extracting street address city state and postal code you run the risk of increasing dimensions; in this case we employ techniques that assess the value of the extracted dimensions
some machine learning algorithms need to have scaled continuous features as model inputs scaling is not necessary for most algorithms but it can make continuous features identical in respect to range
there are instances that require the use of scaled data including algorithms that use gradient descent (""an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient in machine learning we use gradient descent to update the parameters of our model"") neural networks and linear regression are some of those examples the data is scaled before been fed to the model algorithms like k-nearest neighbors clustering analysis like k-means clustering and other distance based algorithms would need data that is scaled
quick thought: how about tree based algorithms like decision trees and random forest they are not affected as they are not distance based
normalization this technique involves values ranging between 0 and 1 prior to normalization all outliers in the dataset should be handled
standardization also known as z-score normalization it is useful for feature engineering in logistic regression artificial neural network and support vector machine tasks
how-to: scaling in python", ,What kind of questions do you have?,0,0
991,993,Analytic Algorithms and Model Building,Supervised Techniques,Summary and Quiz 8,,"supervised learning involves training models with labeled input and output data
knn is a similarity function and a weak learner this is a method in which training data is generalized and most useful for large datasets that will be updated continuously in this case a model typically depends on (or queries) a small number of attributes in the dataset there are three steps involved in making predictions with k-nn: calculate the euclidean distance identify nearest neighbor(s) and then perform task knn had been described for the purposes of predicting a categorical response but it is also effective for predicting continuous value responses just like you would with a linear regression model
bayes theorem describes the probability of an event based on prior knowledge of conditions related to that event bayesian inference is applied when bayes theorem seeks to update the probability for a hypothesis as more information becomes available used in sports medicine and law among other fields naive bayes is a simple classifier that can be applied to categorical predictors when classifying observations using nb the classifier locates all observations that have similar predictor values to the observation that is to be classified and then assigns it to the class that it belongs to; when the problem calls for predicting the probability that an observation belongs to a class we can use this method
tree based methods are considered to be among the simpler methods for prediction and classification in this module we discuss classification and regression trees (cart)
cart methods do not have good predictive performance; there are methods that can be used to compensate for this deficiency called ensemble models ensemble models combine other models to produce an optimal predictive model ensemble models also solve the problem of overfitting as faced by single tree models bagging reduces variance in a decision tree method random forest method is an extension of bagging and makes needed changes to bagged trees when there is overfitting with decision trees random forests will remedy this issue similar to bagging boosting can be used to improve the predictive accuracy of certain methods including decision trees it differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it)
now you are able to take the quiz please be sure that you post your quiz questions as private on piazza or send an email and you will receive a response","KNN is a similarity function and a weak learner, This is a method in which training data is generalized and most useful for large datasets that will be updated continuously. In this case, a model typically depends on (or queries) a small number of attributes in the dataset. There are three steps involved in making predictions with k-NN: Calculate the euclidean distance, identify nearest neighbor(s) and then perform task. kNN had been described for the purposes of predicting a categorical response but it is also effective for predicting continuous value responses just like you would with a linear regression model.",What is a similarity function?,1,1
275,275,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","so far we have discussed data as a entity in the data science process and how it is transformed during the cleansing/wrangling process used for exploratory data analysis and used to draw conclusions with inferential statistics now we will focus on the parts of data that can be useful in the model building process parts of data that will assist in performing the tasks that you have defined in earlier stages of the data  science process; those tasks that are done to meet our analytic objective developing your analytic solution will involve the use of statistical modeling we must understand that those models consist of formulae that only relates numerical quantities to each other1 how then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service how can a mathematical model understand variables that are not numeric
a feature is a numeric representation of a part of the raw data the wikipedia definition of a feature best describes it as ""an individual measurable property or characteristic of an observation"" 
features are the parts of an observation that are represented in a way that a machine learning model can use consider an image classification task to properly represent the features of your image they are processed into a numerical format that allows the mathematical model to use it
when raw data is transformed into features a data scientist must consider the right features that are useful for the data science task a good feature is one that is appropriate to the statistical modeling technique and data science task features should also provide information ie if you are performing a predictive task your features should have predictive values
transforming or processing features from data is an important task in the data science project life cycle but often glossed over the price for badly selected features is a costly one that rears its head when you are training your model as shown in the figure below features will directly affect the models that you develop and the insights gleaned from your models the snowball effect of badly selected features will end up leading decision makers down a wrong path as efficiency and accuracy are key in the data science process it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling note that feature engineering requires both domain and technical expertise
feature engineering and analytic solution building source: zheng & casari (2018)1
feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model
 feature engineering leads to higher quality models and better insights for decision makers when you think about the diverse machine learning techniques data science tasks and contexts in which we apply machine learning you will see that feature engineering can not be generalized it is not a one size fits all process it is dependent on your analytic objective and your data feature engineering requires domain knowledge and intuition during the feature engineering process the data scientist will remove features from the data that do not provide task specific information (eg the feature has no predictive value) and also features that introduce redundancy this is called feature selection
numeric data type: even though we defined a feature as a numeric representation of data raw data that is in numeric form should also undergo feature engineering this is because the data must meet the assumptions of the chosen model
scalar: single numeric feature eg mass
vector: ordered list of scalars
; also defined as an object that has both a magnitude and direction
spaces: vectors exist within a vector space
 and are also a collection of vectors that can be added or multiplied by scalars
in machine learning the input to a model is represented as a numeric vector","Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task, to properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use it.",What are the parts of an observation that are represented in a way that a machine learning model can use?,1,1
171,171,Data Gathering and Wrangling,Data Collection Process,Data Types and Sources,"Data Types,Numeric Data,Categorical Data,Qualitative and Quantitative Data,Structured and Unstructured Data,Internal and External Data,Data Sources","each data science project is unique and will require a data collection technique that suits the problem and objectives it seeks to meet the data collection techniques for a sentiment analysis project analyzing social media tweets will differ from the techniques of an image classification project for patient diagnosis when you defined the project's business and analytic objectives the tasks and methods were proposed as well those tasks and methods drive the type of data that will be used in the project the type of data will influence the source and data collection techniques once you have completed this module you will be able to distinguish between the various data types and sources alongside the traditional data collection techniques
data holds value to businesses and can present itself in various forms you identify data types as it affects the methods you will choose to develop an analytic solution data types are also dependent on its source so you can consider the type of data according to how it is represented in its source reference the link below for a refresher on the common data types:
reading: classes of data types
when discussing data types in data science data is typically categorized as numeric or categorical and classified as one of the four measurement scale types numeric data is represented as continuous or discrete values while categorical data can be nominal or ordinal values
discrete: there are 31 days in the month of may
ordinal: i can rate my customer service experience at the grocery store as good
continuous: you were born on may 13th 1997 so as of may 28th 2020 you are 23 years 2 weeks and 1 day old
nominal: what is your hair color brown
quantitative data is data that is expressed in numerically and derived from questions such as how many courses are you enrolled in this semester; how much do data scientists earn meanwhile qualitative data is descriptive data that is collected from observations instead of measurements qualitative data is sometimes transformed to enable it to be used in certain machine learning modeling techniques that require quantitative data both data types are collected and analyzed in different ways
structured data is organized facts that are presented in fixed formats and are easy to extract this data can be stored in spreadsheets relational databases and other repositories in the row and column format unstructured data is most difficult to extract it is not easily stored in typical relational databases and spreadsheets because it does not fit in the row and column structure it cannot be maintained in formats that are uniform text multimedia files and log files from servers are examples of unstructured data new generation databases also known as nosql databases were built to handle unstructured data unlike structured data unstructured data can be stored without a predefined schema
data can also be classified as internal data which is data collected and/or controlled by an organization an example would be personnel data collected and stored by the human resources department we also have external data data that is collected from sources outside of an organization census data and data gathered from credit reporting agencies are examples of external data
a company has surveyed its customers on their satisfaction with a newly released service the questions asked in the short survey include:
how much do you earn: numeric response
what level of education have you completed: categorical response
what industry are you employed: categorical response
how many people are in your household: numeric response
what part of the country do you reside: categorical response
the different types of data that we explored earlier are collected through different sources primary data sources includes data that is collected and processed by an organization and housed internally secondary data sources include data that is gathered from sources external to an organization keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affect data that is collected from primary and secondary sources as long as they are used by the organization
the largest data sourcethe web is considered the largest data source data from the web can be sourced from social media search engines and machine data in the next module we will focus on this source in depth
primary data can be data that is collected by a data scientist from first hand sources like through questionnaires and focus groups
secondary data is data used in a kaggle competition or a dataset from the popular uci machine learning repository  you did not collect that data and it has been used by others for research or data analysis
internal data is employee/personnel data an internal data source is a company's database that they control an external data source would be twitter and external data could be data that a researcher pulls from twitter users who are discussing a presidential debate
an example of primary data can be data that is collected by a data scientist from first hand sources like through questionnaires and focus groups an example of secondary data is data used in a kaggle competition or a dataset from the popular uci machine learning repository  you did not collect that data and it has been used by others  an example of internal data is employee/personnel data an internal data source is a company's database that they control an external data source would be twitter and external data could be data that a researcher pulls from twitter users who are discussing a presidential debate
example of internal data from secondary source: your company purchases potential client data from data brokers (external source) that data has now become your company's data that will be used for marketing advertising etc (it has now become internal data)
",Discrete: There are 31 days in the month of May,How many days are there in May?,1,0
1345,1348,Analytic Requirements Gathering,Requirements Overview,Overview,Requirements Management Plan,"as you have learned in a previous unit gaining an understanding of the business need and formulating project objectives (business and analytic) are best practices for successfully meeting client expectations once the project objectives have been defined the project team will identify the needs and constraints of stakeholders and the current system or environment related to the business need; this is referred to as the requirements gathering process
requirements gathering establishes communication between the solutions development team (referred to as the project team in this unit) and the business stakeholders the objective of the requirements gathering process is to define the system inputs processes outputs and interfaces when properly done requirements gathering will positively influence the outcome of a project requirements gathering can also help the project team to visualize the intended solution
the requirements gathering process involves input from the system users and system owners a system user interacts with the solution and a system owner (who can also be a user) is an official who is responsible for decisions made about systems within their organization in this unit we will refer to these roles as stakeholders
similar to traditional software development projects data science projects are also guided by requirements gathering principles figure 3 lists the steps that are followed during the process requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s)
figure 3: requirements gathering process
the requirements gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data related project
gather information: the first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business need as well as decision makers within the business once stakeholders are identified the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives
 later on in this unit we will discuss the techniques used to gather information
define and prioritize requirements: stakeholders will provide information according to their view of the business needs it is the job of the business analyst to lead the effort in defining and prioritizing requirements it is important to document ""complete"" requirements that capture the needs of the stakeholders as this will guide the project team in developing the right solution(s) stakeholders might provide information that can be used for future projects related to the proposed solution that information should not be discarded it is prioritized as a low priority requirement and considered for future implementation
evaluate requirements: the project team must verify and validate all documented requirements this additional step in the requirements gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders
receive sign-off: this is an indication that the requirements have been approved and agreed upon by the client requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution
a requirements management plan can be used to document the requirements gathering process this document is made available to the client and the project team as it contains information that affects both parties there is no standard template for this document but it is in good practice to include the following sections:
project description is an overview of your project this section describes the purpose of your project
team responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements gathering process data science project team members might take on duties outside of their normal roles eg a data analyst on the data science team might serve in the role of scribe during joint application development sessions
tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture manage and track requirements through the requirements gathering process and throughout the project lifecycle
requirements gathering process should be defined in this plan this section will describe the techniques used in eliciting user and system needs defining the requirements and evaluating the success of the requirements gathering process (these techniques are covered later in this unit)
change control and requirements gathering: modern day collaboration tools will support change control however a change control process should be documented to ensure that changes to requirements are formally managed","Requirements gathering establishes communication between the solutions development team (referred to as the project team in this unit) and the business stakeholders. The objective of the requirements gathering process is to define the system inputs, processes, outputs, and interfaces. When properly done, requirements gathering will positively influence the outcome of a project. Requirements gathering can also help the project team to visualize the intended solution.",What does requirements gathering establish?,1,1
1141,1144,Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"data science projects can be complex in nature and require the input and efforts of many stakeholders a data scientist will lead the process and it is important that a well-defined workflow is followed the workflow will ensure that all stakeholders are on the same page requirements are defined and met
a data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need in this module we will explore the data science lifecycle keep in mind that just like the system development life cycle (sdlc) the data science life cycle is not linear real world problems will introduce hurdles that require the process to be iterative in nature the lifecycle will give structure to the process and ensure that the data scientist stays on task
figure 2 data science life cycle courtesy: microsoft
you can draw similarities between the crisp-dm lifecycle and the data science lifecycle perusing the internet you will also find that different data science vendors have adapted the lifecycle to fit their products and the solutions supported by their tools
 the data science lifecycle is briefly explained below:
business understanding “we fail more often because we solve the wrong problem than because we get the wrong solution to the right problem” – professor russell l ackoff
data scientists are tasked with providing solutions to difficult business problems and those solutions should be supported by factual dataprior to solving a problem it is important to gain context of the business and the problem this must include defining business and analytical objectives as well as and identifying data sources
data acquisition this process involves obtaining data from various sources and may also require  setting up a data collection task and infrastructure
subsequently you will perform data preparation to ensure the data is ready for analysis
data preparation this is the process of cleaning and transforming raw data prior to processing and analysis this needs to be done carefully as assumptions made here may influence or even limit the use of the data during analysis
data exploration and cleaning the quality of your dataset will determine your success in meeting your business objectives data exploration includes identifying variables conducting uni-variate and multi-variate analysis identifying outliers anomalies and missing values as well as feature creation and selection we will cover these topics in future units
modeling
 later in this course you will explore modeling and learn about choosing the appropriate model based on the problem you will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance we will learn about the balance between generalizability and performance you want your model to learn and perform well but also to be robust  when introduced to unseen data
feature engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features
model training is made efficient when you have adequately prepared your data and engineered new features model training involves maximizing performance and finding a balance between performance and generalization even in cases when a data scientist has collected millions of records data should be considered and treated as a scarce resource since it may be expensive to obtain
models are trained on dedicated training data and evaluated on dedicated test data models should not be tested on the data they have been trained on the ability to match the training performance on unseen test data is referred to as the models ability to generalize to operationalize this during training a validation data set is often sampled from the test data to allow an estimation of test performance during training in the lifecycle it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design in the beginning of the project without violating the training/test split
model evaluation is an essential step in the lifecycle typically analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced depending on the nature of the task (as stated in the analytic objective) model evaluation will follow corresponding metrics and techniques that will be explored in this course
deployment once you have evaluated your models to ensure accuracy and performance you will deploy the model to an environment for application consumption"," Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models, and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance, you want your model to learn and perform well, but also to be robust  when introduced to unseen data.",What type of model will you study to implement analytical models?,0,0
1498,1501,Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"once a set of business goals has been identified one can proceed to formulate analytic objectives that state how the application of analytical methods to data can facilitate reaching the business goal an analytic objective can typically be phrased along the following template:
as an incremental step towards business objective o
we work towards solving problem p
by focusing on specific task t
and applying analytic methods m in conjunction with data d
to create valuable functionality f and/or produce insight i
this formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to a treatment versus a control condition) in fact in academic settings the effectiveness of data science methods will regularly be scrutinized using statistical tests and it is good practice to principally strive for similar methodological rigor in industry applications while data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts it is the connecting element between the problem the technique to be applied the associated requirements and evaluation as well as the business objective also keep in mind that projects often comprise multiple analytic objectives that form a larger plan
in order for you to become proficient in working with this definition we will now move to examining its components",As an incremental step towards business objective O,What is an incremental step towards business objective O?,1,0
881,883,Analytic Requirements Gathering,Requirements Overview,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","reminder: the data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client the requirements gathering process will involve identifying the stakeholders eliciting needs and defining requirements the difference between the requirements of a traditional it project and those of a data science project is the focus on the requirements for the analytic solution
functional requirements define the functions of a system and how users will interact with the system functional requirements are derived from the user and system requirements  that are needed to satisfy the business requirements; in essence defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system as mentioned earlier user requirements are captured in use cases and those use cases can help the project team define the functional requirements a use case will describe the interaction between the system and its users also known as actors the interactions between the system and the user are known as goals
not all functional requirements are implemented in the first iteration of solution development this is why functional requirements are organized by priority high priority functional requirements must be implemented to meet the business objectives medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives these requirements may also be implemented in later iterations or updates to the system
traditional functional requirements considered in the software and application development process include the business rules process flows audit tracking  transaction handling  reporting requirements administration functions authorization requirements and data management
reading: ieee ansi 830 documentation on functional requirements

requirements can be captured in different formats including user stories use case specifications voice of the customer and business rules this unit will focus on defining functional requirements from use case specifications
initial user requirements are often written too broad to unambiguously define what the proposed system should do at each step in a solution the danger is that the software provider will produce a system that does not meet the business objectives because they misunderstood what the customer would consider an acceptable solution different forms of use case analysis are typically used to capture discuss and verify the details of a solution with the customer for each expected capability or interaction (functional requirement) we work with the customer to write detailed use specifications and gather them into a single document
the use case specification provides a textual description of a use case as mentioned earlier it will decompose a user requirement into functional requirements the use case specification details the steps involved in a goal or action figure 4 below shows the sections of a use case specification:
figure 4: use case specification
a business analyst (ba) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers the ba analyzed the information from the questionnaire and defined some user requirements one of the user requirements is  customer’s ability to update their billing address in the new system this requirement describes what customers can do with the solution but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement we will illustrate how we can simply decompose that user requirement into functional requirements
use case/user requirement: update billing address
(1) the user shall be able to view billing address in the system 
(2) the user shall be able to update a billing address in the system
(2) the system shall display updated customer service and billing addresses
functional requirements considered in the software and application development process include the business rules user and system authorization levels authentication and regulatory requirements
non-functional requirements (nfr) describe the performance and behavior of a system they are also referred to as operational requirements the nfrs for a traditional it project will describe the attributes of a system including the system's scalability usability maintainability performance reliability availability capacity interoperability and security
performance and scalability: considerations for the throughput of the system when exposed to high traffic
usability: the ease of use of the system as well as the ability for users to find help with navigating the system
portability and compatibility: hardware systems and application software and the solution’s compatibility to other applications and processes within the existing environment
availability: this is the degree to which a system is in an operable state
security: the integrity of your system and data are closely related to the security rules implemented during the development process
modules and architecture: these are requirements that have technical considerations for the system including operating system compatibility and the programming logic employed by the developers
module requirement specifications
non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules framing a non-functional requirement for a data science project will include the above mentioned attributes and additional requirements that are related to machine learning models and ai analytic solution(s)",Figure 4: Use Case Specification,Figure 4: What is the use case specification?,1,0
1908,1911,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","as you develop analytic models or perform exploratory data analysis you will encounter datasets with a large number of variables a small dataset can also become quite large post data cleansing think about when you transform variables by creating new variables eg dummy variables considerations for a dataset with a large number of variables include issues with over-fitting and computing costs we think about the dimensionality of a model when we consider the number of variables used by the model famous mathematician r bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space this just means that when there are more features in a dataset you are prone to more errors a dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective  how can you address the curse of dimensionality without losing useful information we use dimensionality reduction: this technique is sometimes referred to as feature extraction or factor selection this technique is approached using mathematical modeling
so far we have talked about techniques that focus on features of an observation as you know by now feature engineering informs the models that you will build and its techniques involve looking at features of the data now we will explore a technique that is considered a model-based feature engineering technique
principal component analysis (pca) is used to reduce the dimensionality of a dataset you might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models well when you have a dataset that has a large number of variables you have to assess the relationship between those variables identify variables that might violate the assumptions of your chosen ml model and generally select the variables that are useful to your task when implementing pca you will be reducing the dimension of your feature space
you use the principal component analysis technique when you want to ensure variables in the dataset are independent of each other it is a useful technique to use when there are variables that need to be dropped but dropping these variables with pca is better justified than using the omission technique since you are using mathematical modeling
there are other techniques for dimension reduction including linear discriminant analysis (lda) and those techniques are mentioned in a future unit as well as in your upcoming machine learning courses
principal component analysis involves performing the eigendecomposition on the covariance matrix  a principal component analysis is a linear transformation technique as it finds a low dimensional representation of your high dimensional data pca will seek out a ""small"" number of dimensions in the dataset that are useful to the analytic task pca is considered an unsupervised technique and will be mentioned in that unit as well
the following steps are used when performing pca
standardize the data
compute the covariance matrix of dimensions in the data the covariance matrix
compute the eigenvectors and eigenvalues from the covariance matrix eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it meanwhile an eigenvalue is known as a characteristic value1 or a set of scalars
sort eigenvalues in descending order and choose the top k eigenvectors that correspond to the k largest eigenvalues
construct the projection matrix w from the selected k eigenvectors
transform the original data set x via w to obtain the new k-dimensional feature subspace y
pca in python example: principal component analysis in three (3) steps
reading: a brief article-principal component analysis (lever krzywinski and altman 2017)","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. Well, when you have a dataset that has a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. When implementing PCA, you will be reducing the dimension of your feature space.",What does PCA mean?,1,1
549,550,Analytic Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,Requirements Gathering Techniques,"the process of discovering the requirements of stakeholders and systems is called requirements gathering  the requirements gathering process involves eliciting user and system needs defining the formal requirements from those needs and evaluating the success of the requirements gathering process the data science project team will understand needs of the system and stakeholders set by the business team and end-users (elicit needs/expectations) and then analyze and align the expectations to the business and analytic objectives to define the requirements for the project
requirements gathering should be performed systematically to ensure that information is extracted from diverse sources this will ensure that little to no bias is introduced during the process and all user and system needs are represented and met it is important to note that different scenarios and circumstances call for a specific type of requirements gathering technique
interviews interviews allow the project team to thoroughly investigate the needs of the stakeholder interview questions are usually open-ended providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles bear in mind that this can be a time consuming process; to ensure that an interview yields the right information an interviewer should develop questions that answer the right questions and in certain cases probe for answers to get useful information related to the business system and user interviews can be conducted in a one-on-one setting or in a group setting
brainstorming brainstorming sessions involve gathering ideas from multiple stakeholders at once in a brainstorming session a facilitator monitors the session to ensure all possible solutions are identified and all parties contribute to the process in a reasonable amount of time
this technique can yield new ideas and themes that would otherwise be difficult for the analyst to produce  a brainstorming session should have five to eight representatives from each shareholder group including management users and support staff
questionnaires the project team will distribute predefined questions to stakeholders in the company the feedback from the questionnaires will be analyzed and used to define the project requirements a well designed questionnaire can be completed by non-technical system users and stakeholders with little or no guidance from the project team this technique can be used when interviews are not possible due to budget time and scheduling constraints
document analysis this is an information gathering technique that involves reviewing existing documentation to elicit needs document analysis can be the first step in the requirements gathering process and it can be used to create questions for questionnaires or interview sessions document analysis is considered a supporting technique and can serve as a completeness check for requirements
facilitated workshops the workshop is facilitated by a neutral party usually an outside consultant whose task is to collect information from stakeholders this workshop is also known as a joint application design session these workshops should be structured yet interactive; facilitated workshops can lead to the discovery of underlying issues within the business  a successfully executed workshop can result in the development of requirements participants of a workshop include a scribe who records the discussions taking place during the session; an executive sponsor who has the authority to make decisions about the project the sponsor will set the vision of the project and resolve conflicts the appropriate client stakeholders subject matter experts and silent observers","Brainstorming. Brainstorming sessions involve gathering ideas from multiple stakeholders at once. In a brainstorming session, a facilitator monitors the session to ensure all possible solutions are identified and all parties contribute to the process in a reasonable amount of time.",What does a facilitator monitor in a brainstorming session to ensure all possible solutions are identified?,1,1
1034,1036,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistical Inference,"Statistical Inference,Sampling Distribution,Confidence Interval,168.8cm to 181.2cm,Hypothesis Tests","statistical inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods those conclusions are typically drawn using exploratory data analysis or summary statistics the goal of this process is to use probability theory to make inferences about your data  this is the first step of learning about the attributes of your population from the sample that you have drawn
reminder: the characteristics of the sample dataset are called statistics the characteristics of your population are known as parameters
understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes
if you recall from a previous unit you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise you can use statistical inference to draw scientific conclusions and test set hypotheses significance of a sample data set or descriptive statistics is often in question during the eda process using statistical inference techniques can give significance to your conclusions from eda statistical inference techniques are categorized under estimation and hypothesis testing let us explore these methods:
you typically draw a sample dataset from your population as it is quite difficult to perform analysis on an entire population let us consider a quick example assume we are analyzing the income data of all neurologists in the united states of america we can make inferences about the population mean income by calculating the mean of income on a sample of 2000 neurologists this mean is the sample mean  x̄ we also refer to this as the point estimator of the population mean if the mean of our sample is $258900 then we refer to this number as the estimate of the population mean
let's expand this further the american academy of neurology conducted a study that estimated the number of neurologists in the us at 16400 we can draw another sample that will result in a different mean consider you draw multiple samples and record each sample mean you will have what is called a sampling distribution if you continued to draw samples from this population the average value of your point estimator will equal the population mean we can say that a point estimator is unbiased if its expected value equals that of the population
keeping with the example above let us derive the variance of your sample mean if we continued to sample our neurologist population the variance of the sample mean will be the variance of our population divided by 16400 note that the variability between observations is usually larger than variability between sample means this is because your sample contains a range of observations finally you will derive the standard error of the sample mean this is the population standard deviation divided by the square root of the sample size or simply the standard deviation of the sampling distribution
standard error = population standard deviation/
the standard error is important because it measures how accurate the sample distribution represents the population
please note that the sampling distribution is considered normal if the population mean is normally distributed  this is highlighted because you can not use most statistical inference techniques if the sampling distribution of your sample mean is not normally distributed
there is a popular theorem that addresses the situation when your population is not normally distributed this is known as the central limit theorem
statistical inference is not solely applied to means it is also applied to proportions
you will work with proportions in clustering analysis tasks if a telecommunications company is assessing the proportion of customers who sign up for a contract after receiving a promotional advertisement the parameter of interest is the population proportion p as the data scientist tasked with this analysis you will make an inference about the population proportion by drawing a sample in this case the point estimator is the sample proportion p hat or p̂
reading: proportions
when we provide a range of values of estimates for a population parameter we are referring to the confidence interval (ci)  you can think of it as the range of likely values for a population parameter with a specified level of confidence the sampling distributions of your sampling mean or sampling proportion must be normally distributed to derive an accurate ci the sampling distribution of the sampling mean and sampling proportion will be normally distributed if the sample size is large (in most cases that is n is greater than or equal to 30) the sampling distribution of your statistic (mean or proportion) is needed to derive your ci
you will use the margin of error  to account for the standard error of your point estimate and your desired confidence interval  consider this example:
consider that we are measuring the heights of 40 randomly selected male soccer players our sample mean is 175cm we calculated the standard deviation of the athletes heights and it totaled 20cm let us calculate the ci
n = 40 mean = 175 s = 20
you will decide on the ci to use (95%) and then find the z value for the selected ci a 95% ci means that 38 of the 40 confidence intervals will contain the true mean value
the z value for 95% ci is 1960
we calculate the 175 ± 1960 ×  20/√40
175cm ± 620cm
source1
reading: confidence interval
as you conduct research and complete data science projects questions will arise about the likelihood of occurrences as we have seen so far statistical inference helps to ground your insights with statistical significance and does its best to rule out the possibility of chance we have looked at estimation and confidence intervals to help make inferences from your data now we will explore the oldest statistical inference hypothesis testing
a hypothesis is ""an interpretation of a practical situation or condition taken as the ground for action""  similar to the other techniques you are looking to draw a conclusion about a population using a sample data set when you apply statistical hypothesis testing the end results are always favorable because (according to enrico fermi) you make a measurement or a discovery
according to dermatology associates hyper-pigmentation is the number one skin health concern for black females ages 18-45 skincare co is one of the leading manufacturers of skin care products skincare co is looking to develop a 120 day skin care line to target this population and this skin health concern you are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation your preliminary research has found that administering of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage this is different from claims that have been made about this ingredient (previous claims state that there will be no damage) this claim or belief has been formulated and it should be tested with evidence that refutes or proves that it is true you can use hypothesis testing to provide this evidence to construct a hypothesis test:
identify the population parameter of interest
determine whether you will be conducting a one-tailed or two-tailed test
define a null hypothesis often denoted as h0 the null hypothesis is considered the status quo or in the case of our example: the administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage
then define an alternative hypothesis denoted as ha this would be the opposite of the null hypothesis
the example above does not cover the entirety of identifying your null and alternative hypothesis you must know that if proven your alternative hypothesis is a call to action ie if you reject your null hypothesis then the status quo has been changed and the decision makers must take action how do we test our hypothesis statistically
reading: we do this by using the one- and two-tailed tests
let us also keep in mind that these tests are not error-proof you want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted to avoid this we consider the two error types in hypothesis testing
type i error occurs when you reject the null hypothesis when it should be accepted
type ii error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected
considering our skin care manufacturer example above a type i error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so the company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects the consequences of committing a type ii error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so the cost of this error would mean producing a skin damaging treatment product that would lead to loss of customers and possible lawsuits
how do you ensure that both errors do not occur collect more data either increase your sample size or collect more data over a longer period of time
collecting more data does not entirely mean that you will reduce both errors but it ensures that you commit one over the other at a lesser magnitude
in the module we will continue the process by learning how to extract features of variables within the dataset to improve the performance of analytic solution(s)",Reading: Proportions.,What does Reading: Proportions?,0,0
760,762,Data Gathering and Wrangling,Data Collection Process,Summary and Quiz 2,,"each data science project is unique and will require a data collection technique that suits the problem and objectives it seeks to meet the data collection techniques for a sentiment analysis project analyzing social media tweets will differ from the techniques of an image classification project for patient diagnosis when you defined the project's business and analytic objectives the tasks and methods were proposed as well those tasks and methods drive the type of data that will be used in the project the type of data will influence the source and data collection techniques
this module described the traditional techniques for data collection those techniques include conducting interviews focus groups document analysis and observations there are various types of data and this affects the method employed to solve your analytic objective
a data scientist should also know the different sources of data and how it affects the solution development process there is data that is sourced by your client but controlled by an external source and data that is collected and owned by your client",A data scientist should also know the different sources of data and how it affects the solution development process. There is data that is sourced by your client but controlled by an external source and data that is collected and owned by your client.,What is data that is sourced by your client but controlled by an external source?,1,1
578,579,Analytic Algorithms and Model Building,Supervised Techniques,Classification and Regression Trees,"Building Classification Trees,Building Regression Trees","tree based methods are considered to be among the simpler methods for prediction and classification trees can be built using both numerical and categorical variables and the tree method is rated highly as an interpretable method certain data science practitioners and thought leaders favor the simplicity of tree based models because it can be seen to mirror an ""if-then"" statement and are easily digestible to an individual with a growing statistics knowledge
we will explore the different tree based methods starting with one of the most popular methods: decision trees using a very simple example let us build a decision tree: decision trees: scikit-learn
a decision tree consists of a root node the leaf nodes and branches in decision sciences it is an effective visualization that is easy to interpret in data mining and machine learning it is used to model predictions the end goal of a decision tree method is to predict the value of a target variable based on several predictors when you have a decision tree model with an outcome response containing a categorical value you have a classification tree when your outcome or target variable is a continuous value you have a regression tree
additional reading: decision trees for decision making
building a classification tree involves recursive partitioning and pruning both concepts are used to ensure the model has a low error rate and overfitting is not an issue
recursive partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset c45 is one of the popular algorithms that employ recursive partitioning it generates models that have more sensitivity and tend to be more accurate partitioning is done by repeatedly splitting and creating subsets until the tree is pure purity means all observations belong to a single class
recursive partitioning will split each node on your tree to create decision rules that are easily interpretable but overfitting can be an issue
another technique is the chi-square automatic interaction detection (chaid) it is used for both classification and prediction and can be used to show the interaction between variables it is most useful when you have a large dataset let us assume that you have received a credit card offer from capital one as a preselected customer chaid will help capital one's marketing firm to predict how your age income credit score will affect your response to the interest rate offered
measures of impurity you can measure impurity using entropy and the gini index the gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen the index varies from 0 to 1 0 denotes that all elements are members of a class and 1 denotes that elements are distributed (randomly) across various classes it is best practice to select the feature with the lowest gini index as the root node entropy is a measure of uncertainty within a model decision trees will always seek to maximize entropy
reading: gini index and impurity measures
pruning if you have dabbled in horticulture you will be familiar with the term pruning you prune a plant so that it grows without obstacles you can also prune a plant to redirect the growth and shape of the plant you can think about pruning decision trees in a similar light it is one of the solutions to avoid overfitting the training dataset once you have a large decision tree you will prune the weakest branches to reduce complexity of your model and improve accuracy pruning can be done using two techniques
cost complexity pruning will generate a series of trees the tree is created by removing a subtree and replacing it with a leaf node with value chosen as in the tree building algorithm the best tree is chosen by generalized accuracy as measured by a training set or cross-validation
reduced error pruning is done by replacing each node with the node's most popular classhowever that replacement is temporary unless the it does not negatively affect the prediction accuracy it is an efficient technique for pruning
application: decision trees and nlp: a case study in pos tagging
when a full tree is built it will result in a fully grown decision tree that represents the maximum number of splits that the cart method will make to identify pure subsets full trees tend to overfit and do not do best at generalizing well to new cases solving for this means pruning the tree the least complex tree with the smallest validation error is called a minimum error tree the least complex tree with a validation error that is ""within one standard error of the minimum error tree"" is called a best pruned tree
the validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree this way it will generalize new cases well misclassification rate is a performance measure for classification trees and used to identify the tree that has the lowest error or the minimum error tree
you will find that decision trees are more explainable than linear regression models a smaller tree is easily interpreted by someone who is not in the field and trees can use qualitative variables without the need to create dummy variables the impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal node the predictive accuracy of cart models are not as robust as other methods regression tree performance is evaluated using the root mean square error (rmse)
we will explore some methods that can be used to improve this prediction accuracy and performance; on the next page you will learn more about random forests bagging and boosting","We will explore the different tree based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.",What is the name of the decision tree that we will use to build a simple example of?,1,0
2003,2006,Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"ai philosophy: a process not a product
ai philosophy: a process not a product
the provision of analytical solutions to an organization requires understanding the organization’s needs and their readiness to incorporate and support any analytical solution a good solution will fail if the organization and its stakeholders are not equipped to support the solution when engaging with potential clients seeking analytical solutions it is important to assess the organization’s readiness
data science ready the organization has identified all the current sources of data to be considered and those data have been normalized and integrated into a form that supports data science (statistical analysis correlation prediction etc) at this point the organization is ready to begin an engagement with data scientists and ai developers the case in point scenario showed that monro inc was data science ready during interviews with the information technology team the data manager provided the adequate information about the company’s data architecture appropriate data sources that would support the motomanager app as well as data that would be useful to build the desired prediction model
data science enabled preliminary analysis indicates that the data support the desired analytic objectives (useful correlations are identified predictive models prove to be accurate enough etc) at this point the organization can claim that it is ready to use data science to influence the capabilities of its workflows products and services
ai ready the organization has determined how to leverage the insights from data science (eg predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow product or service at this point the organization can claim that it understands how to incorporate ai into its workflows products and services to provide enhanced capabilities for end users
ai enabled the organization has deployed the new software in a relevant context and is able to directly measure the impact (eg increased sales) at this point the organization can claim that it has implemented ai and is gathering feedback to show that it really works with real end users an organization can introduce the data science decision into a real world setting and measure if this implementation works","Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided the adequate information about the company’s data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.",What has been normalized and integrated into a form that supports data science?,1,0
371,372,Data Gathering and Wrangling,Data Gathering,Overview-Data Governance,,"data governance defines how data is accessed and managed within an organization it is important because it facilitates effective data management and has positive implications for the quality security and integrity of data used for analysis an organization that handles data efficiently understands that data governance impacts data quality and the decisions that can be made from the data available to the organization this unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and decision making any organization that stores and utilizes data should have a data governance strategy for internal data or data stored within the organization and external data
data governance is beneficial because it provides a reliable and consistent view of enterprise wide data it ensures that there is a plan for improved quality of data maps the location of data in the enterprise reducing the scourge of data silos and improves data management overall
reading: data governance in the cloud 
data governance is the responsibility of an entire organization although it is administered by the data management team; all users of data in an organization are considered stakeholders of the organization's data the data governance institute defines a stakeholder as an individual or group that make or is affected by data driven decisions within an organization in addition to data governance policies data stakeholders will have an influence on the state and use of data as a quick reminder the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle as well as determining the requirements for the analytic solution so why is data governance important to a member of a data science project team
data governance is not just policy making for data it is more than that; it influences business strategy because data is now (more than ever) considered as an asset to an organization an organization that has embedded data governance principles into its data infrastructure or data management framework will abide by data standards (industry set or company defined)
data governance best practices for organizations are met when data has integrity and data related decisions and controls are transparent and can be audited another best practice that is gaining ground in industry is the push for organizations to collect and store data that is unbiased unbiased data looks different to each organization and industry but the general idea is that the data represents all members of a population that could be served by an organization this best practice will positively influence the development of ethical models and algorithms for analytic solutions
reading: data governance and its implications for ethical models","Data governance is not just policy making for data, it is more than that; it influences business strategy because data is now (more than ever) considered as an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework will abide by data standards (industry set, or company defined).",Who has embedded data governance principles into its data infrastructure?,1,0
