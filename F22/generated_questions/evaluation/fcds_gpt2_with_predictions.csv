,Unit,Module,Title,Subheaders,Paragraph,Question,predicted_label
0,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.",The decoder and the encoder are used together to create what?, 1
1,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having cbi-directionald attention and are often called auto-encoding models.",These models are often characterized as having what?, 0
2,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.","In contrast to the pretraining of sentences, what is the task of the model?", 1
3,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The pretraining of decoder models usually revolves around predicting the next word in the sentence.,What does decoder models predict?, 1
4,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,These models are best suited for tasks involving text generation.,Text generation is a part of what type of work?, 0
5,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Representatives of this family of models include CTRL, GPT, GPT-2.",GPT is a family of models that are modeled after what?, 1
6,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.",BERT is a model that is based on what?, 0
7,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.",The total amount of information in the network is how many layers?, 1
8,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.",The total number of attention heads is how many?, 1
9,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"For any NLP task, BERT is generally trained in two steps:","For any NLP task, BERT is generally trained in two steps: first, what step is BERT trained in? Second, what step is BERT trained in?", 0
10,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.",The model is trained in a semi-supervised manner on what?, 0
11,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.","The model is trained to predict if the second sentence is the subsequent sentence in the original document, and if so, what else is the second sentence selected as?", 1
12,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.,The final layer of the classification layer is called what?, 1
13,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.",The model is trained by using what?, 0
14,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.",The score is then used to determine what?, 0
15,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.,The model reduced the training time and inference time by how much?, 0
16,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.",SOP is similar to what?, 0
17,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,RoBERTa (Robustly Optimized BERT pre-training Approach),RoBERTa (Robustly Optimized BERT pre-training Approach) is a well known pre-training program for what?, 0
18,Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.","In RTD, what is replaced by a model?", 1
19,Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,Kaushik Shakkari,Kaushik Shakkari is a famous poet and poet of what nationality?, 1
20,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.",The LeNet architecture was made by using what kind of layers?, 1
21,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.",The first layer of the AlexNet convolutional filter was used to create a new layer called what?, 1
22,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 11. Image filters learned by the first layer of AlexNet.,Image filters learned by the sixth layer of what?, 1
23,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.",ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to what?, 1
24,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 14. ResNet-18 architecture.,ResNet-18 architecture is a type of architecture that is based on what type of architecture?, 1
25,Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Information about the intended users of the system (e.g., working professionals or consumers) may affect the type of information your system has to expose. In your design, this will translate into specific outlets of information from your model.","In addition to the intended users, what else might you have to do to the system?", 0
26,Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If performance is an important NFR, then a good assumption to have is the response time for your system (e.g., the requirement may indicate that an API call has to respond back within no more than 2 ms).","If performance is an important NFR, then what is the response time for your system?", 1
27,Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,A matrix is a data structure that encodes the relationship between rows and columns. The disadvantage of this format is that matrices can be very sparse in certain domains. Sparsity refers to the fact that the majority of entries are unknown or missing. Sparsity leads to a waste of space and computational resources.,Sparsity leads to a waste of what?, 1
28,Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them.,"Instead of storing the non-zero values, what is the purpose of the sparse matrix?", 1
29,Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,"The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","In addition to model parameters, what else is used to store model parameters?", 1
30,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.","Exemplar theory states that people store the attributes of observed examples, called what?", 1
31,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).",The authors believe that the two extremes of bias-variance and variance are what?, 0
32,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.",The final step was to see how many students in the group learn the classification?, 0
33,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.",The local model is called what?, 1
34,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?","This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?", 1
35,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?","This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?", 1
36,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.",The model has a better fit than the prototype model because it is fitted on what?, 0
37,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.",The prototype models performance is similar to what?, 0
38,Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.","Humans do not assume that each concept is a single prototype, but rather what?", 1
39,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.",Data wrangling is also part of the data understanding phase of what?, 1
40,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Despite its importance, data wrangling presents some challenges that are common in data science projects.","For example, what is a common problem with data science projects?", 0
41,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.,The data from the marketing firm can be sent to the organization by what?, 1
42,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.",Pandas is a library that is used by data scientists and analysts for what?, 0
43,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.",The lines extending from the box are called what?, 1
44,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.",The data plots provide a visual summary of what?, 0
45,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.","In practice, what is the best way to determine if an outlier is a potential outlier?", 1
46,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.,The number of standard deviations that an observation falls from the mean is called what?, 1
47,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.,The probability of missing data is the same for all observations except for what?, 1
48,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.,"For example, what is missing in the data?", 0
49,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.","For example, if the missing values are small, what can you do to mitigate the loss of data?", 1
50,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.",What are the data transformations that are used to transform data?, 1
51,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",An exampleof a popular quantitative transformation is converting the date of birth to age.,"In addition to age, what else is a popular transformation?", 1
52,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.",The data is integrated to allow for what?, 0
53,Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.",What is the term for the process of processing data for use in the EDA process?, 1
54,Data Science Project Planning,Design and Plan Overview,Overview,,The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.,A system architecture diagram can help explain what?, 0
55,Data Science Project Planning,Design and Plan Overview,Overview,,"Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)","Overview of ACAI Architecture (MCDS Capstone Project, 2020) is a project that is being led by a group of people from what city?", 1
56,Data Science Project Planning,Design and Plan Overview,Overview,,"While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.",Activity diagrams show the user interactions with what?, 1
57,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Besides memory, the other main aspect of any hardware system you will need to assess in order to understand if its important for computing are the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.",CPU's are the two most expensive to use in data science because they are what?, 0
58,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.",The Central Processing Unit is used on most systems for what?, 1
59,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.",Graphics processing units are used in what type of applications?, 1
60,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.",Domain-Specific Architectures are built to solve what type of problem?, 1
61,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.",What is another example of a technique that can be used to analyze a dependent variable?, 1
62,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.",The model is said to be linear because the output is expected to be what?, 0
63,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.","The larger the the the the model, the better the model can explain the variation of the response with what?", 1
64,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Polynomial regression with lower error. Source4,"The term ""R2"" is used to describe the results of a regression term that is used to describe what?", 1
65,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. Stepwise Regression will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:",There are two techniques for stepwise regression: the first is called what?, 0
66,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the residual standard error (RSE is the standard deviation of error \\(\\epsilon\\)), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the \\(R^2\\) is independent of your response variable, unlike the RSE.",RSE is independent of what?, 0
67,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"\\(R^2\\) is calculated using the total sum of squares which is the total variance in Y, and RSS is the discrepancy between the data and an estimation model.",The difference between the variance in Y and the variance in RSS is called what?, 1
68,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.,The AIC also puts more emphasis on what type of model?, 0
69,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.",BIC penalizes complex models with what type of model?, 1
70,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,MotoManager Case-in-Point,MotoManager Case-in-Point: What is the name of the case in which a user is responsible for managing a user's account?, 1
71,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.",The video below provides a brief summary of what was achieved by the app?, 0
72,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Data Management in the Enterprise,Data Management in the Enterprise environment is a term that refers to what type of environment?, 1
73,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Classifying customers as VIP customers based on defined characteristics.,VIP customers are defined by what?, 1
74,Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The number of times an on-boarded customer visited a store close to them.,"In addition to the number of times an on-boarded customer visited a store close to them, how many times did an on-boarded customer visit a store close to them?", 1
75,Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Requirement,Requirement: What is required to obtain the required information?, 0
76,Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Consider this requirement for a report-generating solution:,Consider this requirement for a report-generating solution: What is the name of the solution that is required for a report-generating solution?, 1
77,Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","This requirement leaves room for interpretation, and this can lead to not meeting client expectations. A better representation of the requirement is to include a time frame, a responsible party, and a deliverable.",A better representation of the requirement is to include what?, 0
78,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.","In an observational study, the threat to validity can arise by the inability to account for what?", 1
79,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.","In addition to violating statistical assumptions, what else is violated?", 0
80,Collecting and Understanding Data,Data Collection,Validity and Bias,,Construct validity refers to the validity of inferences about the constructs (or variables) in the study.,"In the study, what is the study called that involves inferences about the constructs?", 0
81,Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.","In addition to the covariate, what else is a potential problem with using data to determine the relationship between the covariate and outcome?", 1
82,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.","In educational experiments, what is impossible to have?", 1
83,Collecting and Understanding Data,Data Collection,Validity and Bias,,Maturation,Maturation of the brain is a process called what?, 1
84,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.","In addition to the pre-test, what is the test that is used to determine the age of a person?", 1
85,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)","Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment) and are able to be used in a treatment setting?", 1
86,Collecting and Understanding Data,Data Collection,Validity and Bias,,Mortality (also called study attrition),Mortality (also called study attrition) is a term that refers to the process of what?, 1
87,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.","For example, what may be difficult to draw conclusions from?", 0
88,Collecting and Understanding Data,Data Collection,Validity and Bias,,Diffusion of treatments (also called cross-contamination of groups),Diffusion of treatments (also called cross-contamination of groups) is a term used to describe what happens when a treatment is combined with a treatment?, 1
89,Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory equalization,Compensatory equalization is a term that refers to the process of equalization of a group of people by means of what?, 1
90,Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory rivalry,Compensatory rivalry is a type of rivalry that involves a rivalry between two people who are competing for what?, 1
91,Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.,"Instead of trying to avoid the problem, what should a scientist try to avoid?", 1
92,Collecting and Understanding Data,Data Collection,Validity and Bias,,Resentful demoralization,Resentful demoralization of the people of the country is a symptom of what?, 1
93,Collecting and Understanding Data,Data Collection,Validity and Bias,,Participants become familiar with the outcome measure and remember responses for later testing,The outcome measure is similar to what measure?, 0
94,Collecting and Understanding Data,Data Collection,Validity and Bias,,"To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.","In addition to the post-test, what else can the data scientist use to measure the outcome?", 1
95,Collecting and Understanding Data,Data Collection,Validity and Bias,,Instrumentation,"Instrumentation and instrumentation are two things that are often used in the production of music, but are often used interchangeably in the production of what?", 1
96,Collecting and Understanding Data,Data Collection,Validity and Bias,,"The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.",The pre-test is a test of what?, 0
97,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.","Bias can be intentionally or unintentionally, but what is the most important aspect of a statistical model?", 1
98,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.","In addition to the data scientist, what else is a concern of studies using convenience samples?", 1
99,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",Self-selection bias is associated with certain characteristics of what?, 1
100,Collecting and Understanding Data,Data Collection,Validity and Bias,,"Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.","When we do this, we tend to use what?", 0
101,Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.",What is the internal approach?, 0
102,Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,External evaluation techniques need ground truth data to evaluate what?, 1
103,Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.",NMI is used to measure and compare the quality of what?, 1
104,Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",The Sorensen-Dice index is also known as what?, 1
105,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules And Their Relationships - How code is structured,Modules And Their Relationships - How code is structured in the code?, 1
106,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Runtime Components and Connections - How data flow during runtime,Runtime Components and Connections - How data flow during runtime?, 1
107,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Deployment Diagrams - What is Infrastructure for the architecture,Deployment Diagrams - What is Infrastructure for the architecture of the internet?, 1
108,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"These diagrams are useful if you would like to show modules and their relationships with each other. Modules represent a static way of structuring the system. In these diagrams, we do not care much about how the system behaves at runtime.","Instead, what does the diagrams show?", 0
109,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What are the different business functions in your system?,What are the different business functions in your system?, 1
110,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What other modules does each business function depend on?,What other modules does each business function depend on?, 1
111,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Are there any external dependencies for each module?,Are there any external dependencies for each module?, 1
112,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Does any module inherit behavior from another module?,Does any module inherit behavior from another module?, 1
113,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 1. Execution Graph of a Completed Run of the Continuous Training Pipeline (ACAI, MCDS Capstone Project, 2020)",The execution of a continuous training pipeline is called what?, 1
114,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Peers,Peers were considered to be a part of what?, 1
115,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Data pipelines,Data pipelines are used to create a pipeline that is able to be used by a user to create a new file or a new file without having to do anything else?, 1
116,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What are the major components of your system at runtime?,What are the major components of your system at runtime?, 1
117,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?","Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?", 1
118,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,How does data progress through the system?,How does data progress through the system?, 1
119,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Does anything run in parallel?,Does anything run in parallel?, 1
120,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 2. A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020)","A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020) is called what?", 1
121,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Representing deployment models can be confusing because they look very similar to runtime diagrams.,What are the different kinds of deployment models?, 1
122,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"The important distinction between the two is that deployment diagrams are meant to display the interaction of the solution with non-software structures like CPUs, file systems, networks, development teams, etc.",The deployment diagrams are meant to show what?, 0
123,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"In a deployment model, you will need to make infrastructural considerations for your solution. Your deployment model will help answer questions like:",Your deployment model will help answer questions like: What is the name of the model that you deploy to?, 1
124,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What cloud instance type does your solution execute on?,What cloud instance type does your solution execute on?, 1
125,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What type of cloud data store are you expected to use?,What type of cloud data store are you expected to use?, 1
126,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Who deploys the solution?,Who deploys the solution?, 0
127,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What type of queuing system are you expected to use?,What type of queuing system are you expected to use?, 1
128,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"In the following example, we know that we have docker containers that interact with a Job Monitor, a Log Server, and a Launcher.",The Job Monitor is a container that is used to monitor what?, 0
129,Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 3. Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020)","Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020) is a project that involves the creation of what?", 1
130,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Optical character recognition (OCR),Optical character recognition (OCR) is a term that refers to the recognition of a character that is recognized by a computer as a physical object that is recognized by a computer as what?, 1
131,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.",OCR is a technology to convert images of text into what?, 1
132,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Face Recognition,Face Recognition is a feature of the device that allows users to recognize what?, 0
133,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 6. Apples Face ID.,Apples face ID is a feature of what?, 1
134,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.",Facial recognition systems use what kind of feature?, 1
135,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Image augmentation applications on various social media services, such as Snapchat, use an algorithm to detect faces and perform augmentation to swap faces in an image for entertainment (Figure 7).",The algorithm is used to do what?, 0
136,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 7. Snapchats Face Swap.,Snapchats face swap is a feature of what?, 0
137,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Vision-based Biometrics,Vision-based Biometrics is a type of biometrics that involves the use of what kind of biometrics?, 1
138,Collecting and Understanding Data,Data Collection,Where do data come from?,,"As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:","In general, there are two key types of data: what is the first type of data that we want to analyze?", 1
139,Collecting and Understanding Data,Data Collection,Where do data come from?,,Organic or process data,Organic or process data is stored in a secure location where it can be accessed by anyone who wants to use it for what?, 1
140,Collecting and Understanding Data,Data Collection,Where do data come from?,,Data collected from a designed study,Data collected from a designed study to determine the relationship between the two groups of people in a study?, 0
141,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.",What is the process that generates data?, 1
142,Collecting and Understanding Data,Data Collection,Where do data come from?,,Financial or stock market exchange transactions,Certain exchanges and exchanges subject to certain restrictions and restrictions that may apply to certain types of exchanges and exchanges are subject to certain restrictions and what?, 1
143,Collecting and Understanding Data,Data Collection,Where do data come from?,,Web browser history,Web browser history is a history of what?, 1
144,Collecting and Understanding Data,Data Collection,Where do data come from?,,Web or mobile application activity history,Web or mobile application activity history is stored in a file called what??, 0
145,Collecting and Understanding Data,Data Collection,Where do data come from?,,Netflix viewing history,Netflix viewing history?, 0
146,Collecting and Understanding Data,Data Collection,Where do data come from?,,Surveillance camera video recordings,Surveillance camera video recordings of what kind of video were taken?, 1
147,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.",Organic data is collected from what type of study?, 1
148,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.",What is a possible cause of errors in the questionnaire?, 1
149,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.",The interview protocol is designed to provide instructions for what?, 0
150,Collecting and Understanding Data,Data Collection,Where do data come from?,,"Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.","In addition to the data collected during observations, what else can be used to support the data collected?", 1
151,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?",How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?, 1
152,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.",A good feature is one that is appropriate to what?, 0
153,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.",What is the process of extracting features from raw data?, 1
154,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.",The feature selection process is called what?, 1
155,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.",The data must meet the assumptions of what?, 1
156,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.,A vector space is a collection of what?, 1
157,Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","In machine learning, the input to a model is represented as a numeric vector.",The input to a model is represented as what?, 1
158,Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The next section in the requirements document will be the system functionality section. This section describes the goals of the project using multiple simplified diagrams, such as context diagrams and component diagrams, which can be used to illustrate these goals.",The system functionality section describes the goals of the project using what?, 0
159,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.",A population parameter is a statistical procedure that you use to test some assumption about what?, 1
160,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Performing a hypothesis test involves three major steps:,"Performing a hypothesis test involves three major steps: first, what is the first step? What is the first step? What is the first step?", 0
161,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.","If we are higher than what threshold, we can reject the null hypothesis?", 1
162,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.",Welchs t-test is a test of what?, 1
163,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.",The first option is to use what?, 0
164,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this test, our test metric is actually much simpler:","For this test, our test metric is actually much simpler: what is the test metric?", 1
165,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.","In addition to Welchs t-test, what other test is used to test for more accuracy?", 1
166,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","However, both of these tests do suffer a single, incredibly critical flaw.",The flaw is that the test results are not what?, 0
167,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Namely, they suffer from cp-hacking.d","Namely, they suffer from cp-hacking.dmg and what else?", 0
168,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.","In addition to the p-value, what else is the p-value?", 1
169,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.","In a study, what did the results of a study show?", 1
170,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?","Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?", 0
171,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.",The Friedman test is based on the assumption that the rank of the algorithm is what?, 1
172,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",and the associated statistic is:,and the associated statistic is: What is the name of the statistic that is used to show that the number of people in the population is increasing?, 1
173,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:","As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests: one type of test is called what?", 0
174,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:",To construct a hypothesis test: What is the name of the product that is used to treat hyperpigmentation?, 1
175,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?",How do we test our hypothesis statistically?, 1
176,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","So, we have our p-value <  \\(\\alpha_z\\)","So, we have our p-value <  \\(\\alpha_z\\)> and what else?", 1
177,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.","In addition to the null hypothesis, what else is considered to be significant?", 1
178,Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists have virtually unlimited access to data and analytical techniques with which to analyze that data. As data scientists, we should be thinking about whether we should do something just because it is technically possible.",What should we be thinking about when analyzing data?, 1
179,Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",Consent is based on what principles?, 1
180,Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most ""normal"" about a given dataset.",What is the most common bias in data science?, 1
181,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.","Text processing tasks are used to help streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like what?", 1
182,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Instead of representing the entire corpus as a one-dimensional list of numbers indicating word counts, term frequency takes into account the word frequencies for each member document in the corpus.",Term frequency is taken into account for each document in the corpus by using what?, 1
183,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Jack,"Jack, the leader of the Free World, was a leader of what?", 1
184,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,ate the world?, 0
185,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,"an, the city of Bostons, is located in what state?", 1
186,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",on,on the world is a place where people are able to do what?, 0
187,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,"the-time, in which case the time is called what?", 0
188,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",likes,"likes, and what else?", 0
189,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Jack,"Jack, the leader of the Free World, was a leader of what?", 1
190,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,ate the world?, 0
191,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,"an, the city of Bostons, is located in what state?", 1
192,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",on,on the world is a place where people are able to do what?, 0
193,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,"the-time, in which case the time is called what?", 0
194,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",likes,"likes, and what else?", 0
195,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Each word has an inverse document frequency associated with it. Hence,","Hence, the inverse document frequency is associated with what?", 1
196,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Irreducible Error is the noise term in the true relationship that cannot fundamentally be reduced by any model. When x can not determine y because there are other predictors that might improve the prediction error, you can incorporate those variables. The irreducible error is the error that we can not remove with our model or with any model. The error is caused by elements outside our control, such as statistical noise in the observations.",Irreducible error is caused by what?, 1
197,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Have you come across the term ""lazy learning""? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.",A recommendation system relies on what?, 1
198,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.",kNN is also known as what?, 0
199,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are three steps involved in making predictions with kNN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.",The Euclidean distance of the new observation is known as what?, 1
200,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",The RMS is used to determine the best k for a classification task in which the classification task is performed by the person who performed the classification task?, 1
201,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.","In 2x2 binning, an array of 4 pixels becomes a single larger what?", 1
202,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.","Instead, it is used to handle what?", 0
203,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.,"Instead, it adds information that is what?", 0
204,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Highly Dissatisfied,"Highly Dissatisfied with the way the government handled the economy, the government decided to take a look at what?", 0
205,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.","For example, a model that has a continuous feature that is scaled to a certain range is called what?", 1
206,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","There are instances that require the use of scaled data, including algorithms that use gradient descent","There are instances that require the use of scaled data, including algorithms that use gradient descent, which involves using what kind of data?", 1
207,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.","Algorithms like k-Means clustering, k-Means clustering, and k-Means clustering are examples of what?", 0
208,Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Normalization. This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled.","The values in the first row should be handled by the first row of the dataset, and the values in the second row should be handled by what?", 1
209,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.",Sequences are also called what?, 1
210,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Intersection and union of sets,","Intersection and union of sets, and what else?", 0
211,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Subtraction of one set from another set,","Subtraction of one set from another set, in a way that is similar to what is done in a car?", 1
212,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Inserting elements into a set or removing an element from a set,","Inserting elements into a set or removing an element from a set, removing an element from a set, or removing an element from a set is called what?", 1
213,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Checking if a given element of the domain is a member of a set,","Checking if a given element of the domain is a member of a set, what is the name of the set?", 1
214,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Computing the size of a set.,"In the case of a large data set, what is the size of the data set?", 1
215,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Find the \\(i^{th}\\) smallest element of a set,","Find the \\(i^{th}\\) smallest element of a set, and what is the smallest element of a set?", 1
216,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Tables: Tables are an extension of sets. Each element in a table \\((k, v)\\) consists of a key \\(k\\) and an associated value \\(v\\). Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value \\(v\\) associated with a key. Similarly, tables can be ordered based on a key if there is a need.",A table with a key that is unique is called what?, 1
217,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Graphs: Graphs are the most versatile abstract data types. They are typically used to represent a set of items (called nodes) along with asymmetric or symmetric relations between those items (called edges). For example, graphs can be used to represent","For example, graphs can be used to represent a set of items called what?", 0
218,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Social networks with nodes representing people and edges representing cfriendshipd or cfollowsd relations between them.,The cfriendshipd and cfollowsd relations are represented by what?, 1
219,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Neural networks with nodes representing neurons and edges representing weighted connections.,The network with nodes representing neurons and edges representing weights is called what?, 1
220,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,What is the shortest distance between any two intersections?,What is the shortest distance between any two intersections?, 1
221,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Who are the people on a social network with more that 100 connections?,Who are the people on a social network with more that 100 connections?, 1
222,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Trees: Trees are special cases of graphs and are used to represent hierarchical relations such as parentchild relations. This restriction usually allows for more memory-efficient representations or time-efficient operations for specific classes of operations.,Graphs are used to represent hierarchical relations such as what?, 1
223,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Priority Queues: Priority queues are essentially sets where each element consists of a value, and a priority, and the only operations one can do are",The priority is determined by what?, 0
224,Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Inserting a value with its priority,Inserting a value with its priority is called what?, 1
225,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Enable/enrich human-human communication,Enable/enrich human-human communication with the human being?, 0
226,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Dictating email messages,Dictating email messages are often used to send and receive information about what?, 1
227,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Listening to email messages,"Listening to email messages is a common practice in the business world, where it is a common practice to use what kind of email format?", 1
228,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Using human language to give commands to the operating system,Using human language to give commands to the operating system?, 1
229,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Human-human communication,What is the topic of debate among scientists and philosophers?, 1
230,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Real-time text and speech translation,Real-time text and speech translation is a part of what?, 1
231,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Summarizing meeting conversations,Summarizing meeting conversations with the leaders of the Muslim world?, 0
232,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Improving the efficiency of tasks requiring human language faculty,What is the main goal of the research that involves human language faculty?, 0
233,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Correcting typing/grammatical errors,Correcting typing/grammatical errors in a document is called what?, 1
234,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Answering e-mails automatically,Answering e-mails automatically creates a new record called a record called what?, 1
235,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Searching large document databases,Searching large document databases is a common practice in the field of search engines and what else?, 0
236,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.",Siri is a multi-stage turn-taking goal driven conversation driven by what?, 0
237,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).,The visual impairments would be similar to what?, 0
238,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Opinion and sentiment analysis:  Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.",What is the term for the type of analysis that involves analyzing the content of news articles?, 1
239,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,The following figure gives a birds eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.,NLP applications are evaluated by evaluating what?, 0
240,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.",The output is a representation of what?, 0
241,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).",The output is text or what?, 0
242,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",Machine learning is a type of what?, 0
243,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,The Transformer model was introduced in the famous paper Attention is All You Need in 2017.,The model was introduced in what year?, 0
244,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?",Wouldnt using the input embeddings directly make more sense?, 1
245,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.,Sequences2Sequence  models replace RNNs with what?, 1
246,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.","Instead of using RNNs, they use what?", 0
247,Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.","Instead of a single step, what is the output step?", 1
248,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.",High variance models are usually more complex and are more likely to be what?, 0
249,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Referring to \\(f\\left(x_0\\right)\\) and \\(g\\left(x_0\\right)\\) as f and g, respectively and skipping the conditional on X:","Referring to \\(f\\left(x_0\\right)\\) and \\(g\\left(x_0\\right)\\) as f and g, respectively and skipping the conditional on X: what is the conditional on?", 1
250,Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Agile board: This is a tracking system that helps your team track the progress of your project.,What is the name of the system that helps you track the progress of your project?, 1
251,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"This module focused on formulating the analytic objective, which is also considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances.",The objective of the module is to make a hypothesis about what?, 0
252,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Understanding the business objective,","Understanding the business objective, what is the business objective?", 1
253,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Identifying the problem,","Identifying the problem, the problem is called what?", 0
254,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Focusing on a well-defined task,","Focusing on a well-defined task, what is the focus of the focus?", 1
255,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,Checking the proposed method against the target insight,Checking the proposed method against the target insight is called what?, 1
256,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Proposing data collection and curation methods, and","Proposing data collection and curation methods, and what else?", 0
257,Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Analytic objectives can be reframed to focus on specific tasks, in which case an incremental step would become the primary goal.",The goal of the incremental step is to focus on what?, 0
258,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data. Data is typically categorized as numeric or categorical. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.",What is the term for data that is not continuous or discrete?, 1
259,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.",Unstructured Data is easy to use and easy to understand what?, 0
260,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Unlike structured data, unstructured data can be stored without a predefined schema. New-generation database frameworks, also known as NoSQL databases, have been developed specifically to handle this type of data.",New-generation databases are also known as what?, 1
261,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. Data wrangling is sometimes referred to as feature engineering. Feature engineering involves selecting the right features from the data to further improve the performance of your models.","What is the process of cleaning, formatting, and enriching data to make it usable for analysis?", 1
262,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.,EDA uses what kind of techniques?, 0
263,Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables.",The relationship between x and y is described by what?, 1
264,Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Similar to traditional software development projects, data science projects are also guided by requirements gathering principles. Figure 1 lists the steps that are followed during the process. Requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s).",The requirements gathering for a data science project will involve gathering the needs of the stakeholders and defining what?, 0
265,Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project.,The process involves gathering user and system needs and defining what?, 0
266,Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Receive Sign-Off: This is an indication that the requirements have been approved and agreed upon by the client. Requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution.,Sign-offs are done in the event that the client has to do what?, 0
267,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets.",Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on what?, 1
268,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Elastic Net, a convex combination of Ridge and Lasso.",The convex combination is similar to what other type of combination?, 0
269,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.,The algorithm is based on what?, 0
270,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.,LambdaRank is an implementation of what?, 0
271,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Additional Reading: From RankNet to LambdaRank to LambdaMART,"Additional Reading: From RankNet to LambdaRank to LambdaMART, what is the name of the system that ranks the top performers in the world?", 1
272,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:","However, these techniques had the following two significant limitations: 1) What is the most simple representation technique?", 1
273,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.",The size of the vocabulary was represented by the size of what?, 0
274,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.",What is the name of the model that outputs one embedding for each word?, 1
275,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"This problem was solved by the ELMo (""Embeddings from Language Model"") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.",ELMo is trained through the task of predicting what?, 0
276,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.,What is the term for the general word embeddings?, 1
277,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.",The language model is trained on a corpus of what?, 1
278,Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.,The training model is trained on a target task and trained on a target task using what?, 1
279,Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Interviews. Interviews allow the project team to thoroughly investigate the needs of the stakeholder. Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles. Bear in mind that this can be a time-consuming process. In order to ensure that an interview elicits the right information, an interviewer should develop questions that address the right issues and, in certain cases, probe for answers to get useful information related to the business, system, and user. Interviews can be conducted in a one-on-one setting or in a group setting.","In addition to the group setting, what is an example of a group setting?", 0
280,Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"This technique can yield new ideas and themes that would otherwise be difficult for the analyst to produce. A brainstorming session should have five to eight representatives from each shareholder group, including management, users, and support staff.",A brainstorming session should have how many representatives from each shareholder group?, 1
281,Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",The goal of a workshop is to help the participants discover what?, 0
282,Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is the responsibility of an entire organization; although it is often administered by the data management team, all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that makes or is affected by data-driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as to determine the requirements for the analytic solution. So why is data governance important to a member of a data science project team?",So why is data governance important to a member of a data science project team?, 1
283,Collecting and Understanding Data,Ethics of Data Science,Governance,,Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).,An organization that has embedded data governance principles into its data infrastructure is an organization able to abide by what?, 1
284,Collecting and Understanding Data,Ethics of Data Science,Governance,,Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.d,Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.dcts and dcts are examples of what?, 1
285,Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.",Data quality is judged on how many dimensions?, 1
286,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.",Tree-based models are also known as what?, 1
287,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.",What is the goal of a decision tree method?, 1
288,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.",What is one of the popular algorithms that employ recursive partitioning?, 1
289,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.",The first is to use what kind of tree?, 0
290,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.",The best tree is chosen by the trained set or by what?, 1
291,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",What is the name of the measure that measures the accuracy of regression trees?, 1
292,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need  Business Objective  Analytic Objective including Problem Statement  Task Definition  Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:","The statement of methods to be applied will vary in specificity depending on your project, but three elements are important: What is the goal of the step by step analysis?", 0
293,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","At the same time, it should allow for testing different techniques around the main conceptual idea.",What should be used to test different techniques?, 0
294,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Supervised learning methods which involve learning to predict a target variable (typically through regression or classification) by training on ctrued example data points whose target variable has manually been labeled or is available by other means.,The training method is called what?, 0
295,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Unsupervised learning methods which deal with finding patterns in unlabeled data without an explicit prediction target.,These methods are called what?, 0
296,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"For our purposes, before we get into the differences between CPUs and GPUs, it is important to take some time to first understand memory and its role in computation.",What is the main difference between CPUs and GPUs?, 1
297,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a multi-layer perceptron.",An input layer is connected to a hidden layer with a weight of what?, 1
298,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.",CNN has been used to help with recognition and recognition tasks within what?, 1
299,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).",RNNs can be unfolded in time and trained with what?, 1
300,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.",The LSTM network is similar to what type of network?, 1
301,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.",The training algorithm optimizes the weights based on what?, 1
302,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.",The GRU model is similar to what?, 0
303,Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?",So is data science backward-looking?, 1
304,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as cactivatedd or cinhibited,d thus affecting the transformations in the subsequent layers.",The activation function of a neural network is called what?, 1
305,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.",The sigmoid function is used in intermediate layers of what?, 1
306,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.",The slope coefficient is determined before training and thus is considered what?, 1
307,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.,The softmax ensures that the sum of outputs for each class is equal to what?, 1
308,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Above, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.","In addition to the time an operation takes, what else is considered?", 1
309,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.",The tool is based on what?, 0
310,Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).","Instead, we model how the growth rate of two functions compares to large input?", 0
311,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.",The first is called what?, 0
312,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.","Cluster clusters are not compact enough to be used in a single linkage, because they are not what?", 1
313,Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.",Data collection is also called what?, 1
314,Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.",What is the most important aspect of a sound study design?, 1
315,Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Validity is the development of sound evidence to demonstrate the intended test interpretation.  In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.", In a study of what is the threat to validity of the test interpretation?, 1
316,Collecting and Understanding Data,Data Collection,Module 7 Summary,,Threats to internal validity are problems in drawing correct inferences about whether the covariation between the presumed treatment variable and the outcome reflects a causal relationship.,"In addition to the covariate, what else is a problem with drawing inferences about the covariate?", 1
317,Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.","In addition to the problems with the sample data, what else is a problem with the sample data?", 1
318,Collecting and Understanding Data,Data Collection,Module 7 Summary,,Selection Bias: a threat to internal validity occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about.,The mismatch between data and the subject matter that the data scientist wants to make inferences about is called what?, 1
319,Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Confirmation Bias: This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.","In addition to evidence that confirms our personal beliefs, values, and hypotheses, evidence that confirms what else?", 0
320,Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.,The ML engineer is also known as what?, 1
321,Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.",Data analysts are not necessarily as proficient in what?, 0
322,Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",A domain expert is also known as what?, 1
323,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: candidate models M1, M2, , Ml","Input: candidate models M1, M2,, Ml, and Ml are all named after what?", 1
324,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,Procedure: What is the name of the procedure that involves the use of a metal detector?, 1
325,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.","The train set can be split into a train subset and validation subset, and what else?", 1
326,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: candidate models M1, M2, , Ml and hyperparameter space S.",The model S is a model of what?, 1
327,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,Procedure: What is the name of the procedure that involves the use of a metal detector?, 1
328,Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,Procedure: What is the name of the procedure that involves the use of a metal detector?, 1
329,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.",The encoder-decoder architecture is similar to what?, 1
330,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.",The encoder takes a word embedding and produces an output called what?, 1
331,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,Figure 1: Traditional Sequence2Sequence model architecture.,Traditional Sequence2Sequence model architecture is based on what?, 1
332,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.,"In addition to remembering the first part of a long sequence, what else can be done to the input?", 1
333,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this cbottleneck problemd.",The cbottleneck problem was resolved by the use of what?, 0
334,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.",The focus of a model is on what?, 0
335,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.",What is the color term for the food that we see?, 1
336,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",The attention vector is used to estimate the importance of what element?, 1
337,Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",The attention vector is used to estimate the importance of what element?, 1
338,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):","In this way, the matrix A is represented as a tuple of three lists of what?", 1
339,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its DOK representation is,its DOK representation is done by using what?, 0
340,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col]]>: a list of the column indices of the non-zero values,<![CDATA[col]]>: a list of the column indices of the non-zero values in a CDATA is called?, 1
341,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its CSR representation is,its CSR representation is based on what?, 1
342,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).",The index 3 at index 3 is at what index?, 1
343,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:","For example, a matrix-vector product Mx involves computing the dot product between every row of M and x: Mx is a matrix-vector product of what?", 1
344,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?","In this case, what is the time-complexity of conversion to COO?", 1
345,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:",The columns are compressed in a similar manner to what?, 0
346,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix,<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix?, 1
347,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its CSC representation is,its CSC representation is based on what?, 1
348,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?","As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?", 1
349,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.","The size of the vector is the size of the corpus itself, which can easily reach what?", 1
350,Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \\times n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \\times k}, V \\in R^{k \\times n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.",The matrix data format is used as input to what?, 1
351,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Insight: In some project contexts, the client may not need a piece of software that automates analytic functionality but rather requires insight from data in order to make decisions. This area of data science blends into what is commonly referred to as cbusiness intelligence.d",This area of data science blends into what is commonly referred to as cbusiness intelligence.d?, 0
352,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.",A use case will describe the interaction between the system and what other user?, 1
353,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.",The unit will focus on defining what from use case specifications?, 0
354,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:",Figure 1 below shows the sections of a use case specification: The use case specification details the steps involved in what?, 1
355,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(1) The user shall be able to view the billing addresses in the system.,The user shall be able to view the billing addresses in the system by using what?, 1
356,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(2) The system shall display updated customer service and billing addresses.,The system shall display updated customer service and billing addresses in a format that is consistent with what other service provider?, 0
357,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.",User and system authorization levels are considered what?, 1
358,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.","NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, and what else?", 1
359,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Interoperability,The Apple Watch is a device that allows users to do what?, 0
360,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Interoperability refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context.,"In a particular context, an interface that is able to exchange meaningful information via interfaces is called what?", 1
361,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modifiability,Modifiability of the aircraft is determined by what?, 1
362,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modifiability refers to the ease of modifying the system with minimal changes to the architecture.,The ease of modifying the system with minimal changes to the architecture is called what?, 1
363,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Performance,"Performance in the game, what is the name of the game that is based on the game?", 1
364,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.","When events occur, what must the system respond to?", 1
365,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Security,"Security, and what else?", 0
366,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Usability,Usability of the system is determined by what?, 1
367,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Scalability,Scalability and performance of the components of a computer is determined by what?, 1
368,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Scalability refers to the ease of adding new resources to a system to cope with increasing demands on its use.,"In addition to the ease of adding new resources, what else can be added to the system?", 0
369,Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modules and Architecture,Modules and Architecture are examples of what type of architecture?, 1
370,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,True Positives (TP) is the number of positive data points that are correctly predicted as positive.,Positive data points are the number of positive data points that are correctly predicted as negative data points?, 1
371,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).",False positive means the label is not true but what is the label is not true?, 1
372,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,FN,FN: What is the name of the person who is the subject of the book?, 0
373,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).",What is the cost of cancer detection models?, 1
374,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,F1 score is the harmonic mean of precision and recall:,F1 score is the harmonic mean of precision and recall: what is the harmonic mean of precision and recall?, 1
375,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"where \\(y\\) is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.","In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as what?", 1
376,Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the ""cost of different errors,"" instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.",The AUC is also known as what?, 0
377,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.",What is the study of how to equip computers with?, 1
378,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.",The Deep Neural Network concept is a good example of what?, 0
379,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of image understanding in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer's view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).",The input to the computer models is what?, 1
380,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 2. What a person sees.,What is the name of the person who sees?, 1
381,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 3. What a computer sees.,What a computer sees?, 1
382,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?,Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?, 1
383,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,What distribution do the individual phenomena in the data follow?,What distribution do the individual phenomena in the data follow?, 1
384,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?,Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?, 1
385,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the data complete or are some parts of it missing?,Is the data complete or are some parts of it missing?, 1
386,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset clean or noisy? Does measurement error play a role?,Is the dataset clean or noisy? Does measurement error play a role?, 1
387,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?","Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?", 1
388,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.",The goal of a well-studied dataset is to be able to be used for what?, 1
389,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"While statistical analysis and machine learning are, of course, core pillars of data science, effective collection, curation, and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects. As such, the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis.","In addition to training a model, what else is the data component of an analytic objective important to?", 1
390,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"When proposing data collection as part of an analytical objective, the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource. Data collection, especially involving human annotators, is its own research field. Relevant considerations include:",Relevant considerations include: What is the purpose of data collection?, 1
391,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?,How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?, 1
392,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Does the collection require human annotators? Can it be done using crowdworkers?,Does the collection require human annotators? Can it be done using crowdworkers?, 1
393,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,What qualifications do the human annotators need to possess? How can they be effectively trained for the task?,What qualifications do the human annotators need to possess? How can they be effectively trained for the task?, 1
394,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Do the human annotators need to be examined/tested before and/or after collection?,Do the human annotators need to be examined/tested before and/or after collection?, 1
395,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?, 1
396,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?, 1
397,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Once the data has been gathered, what cleaning and curation needs to happen?","Once the data has been gathered, what cleaning and curation needs to happen?", 1
398,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there any approvals/permissions that need to be obtained before the collection can proceed (e.g., human subject research)?","Are there any approvals/permissions that need to be obtained before the collection can proceed (e.g., human subject research)?", 1
399,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Once the team has identified the precise problem for which data should be leveraged, it is advisable to explicitly characterize the task to be done at the analytical level. To do this, one casts the cheartd of the problem into one or more categories of typical data science tasks:","To do this, one casts the cheartd of the problem into one or more categories of typical data science tasks: what is the goal of the cheartd?", 0
400,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Classification: Individual instances in the dataset have a categorical (i.e., non-numerical) label associated with them or will be labeled as such. The goal of the project is to develop a system capable of categorizing data points using these labels. A common variant of classification is sequence labeling, where individual data points are not independent but form a series. Typical examples of classification are sentiment analysis of text and labeling images as depicting certain objects (animals, cars, etc.).",What is the goal of the project?, 0
401,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Regression: Individual instances in the dataset have a numerical label associated with them whose magnitude carries a specific meaning. The goal of the project is to develop a model capable of predicting this target score for individual data instances. Like classification, regression is often applied to dependent series of data points. Typical examples of regression include predicting measurements in medical or demographic data over time.",What is the goal of the project?, 0
402,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Retrieval & Ranking: The dataset can be thought of as one or more collections of data points and queries. In response to the query, one or more cideald data points should be retrieved and presented in ccorrectd order. A typical example is a  search engine returning a ranked list of results in response to a query.",What is the query that is returned in response to a query?, 1
403,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,Anomaly Detection: The dataset is assumed to have some latent structure that should be discovered in order to identify instances that do not adhere to the pattern. A typical example is the detection of fake customer reviews in online retail data.,Fake customer reviews are usually found in what type of data?, 1
404,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score. Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an cintersection over uniond threshold.","In addition to machine translation, what other type of task is often characterized as a text generation task?", 1
405,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design: Planning how to gather data for research studies,","Design: Planning how to gather data for research studies, what is the name of the study that is used to gather data for research studies?", 1
406,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description: Summarizing the data, and","Description: Summarizing the data, and using the data to make a statement about the data, what is the purpose of the statement?", 1
407,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.","For a survey, for example, what would specify how to select the people to interview?", 1
408,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.",What is the term used to describe the variance among subjects?, 1
409,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.",What is the term for quantitative data?, 1
410,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Discrete: I have one sibling.,What is the name of the sibling that is the only sibling that is not a sibling?, 1
411,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.","For Americans, the province of residence is what?", 1
412,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.",What is the average for qualitative data?, 0
413,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.",Unstructured data is easy to extract because it is easy to store in what?, 1
414,Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.",What is an example of external data?, 1
415,Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"The difference between a traditional IT project and a Data Science project is the focus on the analytical data, model, and operations requirements.","The focus on the analytical data, model and operations requirements is what?", 0
416,Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Requirements are capabilities needed to achieve an objective. Requirements are gathered from stakeholders of a project, including the client, end-users, and the data science team. Good requirements are correct, complete, unambiguous, verifiable, measurable, and traceable.",Good requirements are what?, 0
417,Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are different types of requirements including business and solution requirements. Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.","Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe what?", 1
418,Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Similar to a traditional IT project, it is a best practice to collect requirements for analytic projects. The requirements for an analytic project include determining how data, models, and results or outputs of the models will support meeting the analytic and business objectives.","The requirements for an analytic project include determining how data, models, and results support what?", 1
419,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.",Cluster data is only used for the purpose of determining what?, 1
420,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.",k is a set of what?, 0
421,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Each data point is then assigned to the cluster with the nearest mean.,The cluster with the nearest mean is assigned to the cluster with what?, 1
422,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.",kNN is a supervised classification method that involves what?, 0
423,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.","It is slow to train, but it tends to deal with what?", 0
424,Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"Metrics are employed to objectively evaluate the performance of machine learning models. When selecting a metric, you should always have the end goal of the machine learning application in mind.",Machine learning models are used to evaluate what?, 0
425,Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.",The context of a problem is important to understand what?, 1
426,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:",Model selection is a systematic process of identifying this best mapping and builds upon the following concepts: what is the process of identifying best mapping?, 1
427,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search",The model space is defined by what?, 1
428,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The model parameters are the numeric values or structures that are derived from the learning process.,The model parameters are derived from what process?, 1
429,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.,The model hyperparameters are used to determine what?, 0
430,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The learning algorithm specifies the way in which model parameters are updated or derived from the input data.,The learning algorithm is used to determine what?, 0
431,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Model 1,Model 1 of the Model 1 was built on what type of building material?, 1
432,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Gradient descent over the logistic loss function with L2 regularization,The L2 regularization and L3 regularization are used to determine what?, 1
433,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Gradient descent over the logistic loss function with L2 regularization,The L2 regularization and L3 regularization are used to determine what?, 1
434,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Pre-processing,Pre-processing the data in a data file is called?, 1
435,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,None of the above are examples of what?, 0
436,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Post-processing,Post-processing is a term that refers to what?, 1
437,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,None of the above are examples of what?, 0
438,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,None of the above are examples of what?, 0
439,Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.,Model interpretability is important because it allows a politician to know which factors have the largest influence on what?, 1
440,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.,The encoder unit is made up of how many individual encoders and decoders?, 1
441,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Figure 6: Transformer architecture.,Transformer architecture is a type of architecture that is based on what type of architecture?, 1
442,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.",The output of the transformer is then added to what?, 1
443,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.",The encoder then passes the encoder's input into what layer?, 1
444,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.","The output sequence is generated by the output layer, and the output layer is masked by what?", 1
445,Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.",The encoder-decoder attention layer is what?, 1
446,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","As the name suggests, sentiment analysis is used to identify the sentiments in a fragment of a natural language text. Expressions like sarcasm, threat, exclamation, etc., are often very hard to be recognized by the computer. A use case of sentiment analysis is companies identify the opinion and sentiments of their customers based on online reviews and feedback.",Companies use sentiment analysis to identify what?, 0
447,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real time.",Natural language generation systems have evolved over time from what?, 1
448,Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.,The EVP is a suitable framework for what?, 1
449,Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",What is the most important aspect of an analytical solution?, 1
450,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).",The classification task is based on what?, 0
451,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.","In addition to fruit, what else is an observation classified as belonging to?", 1
452,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.",Multi-label classification allows for observations to be classified under what?, 1
453,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?,A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?, 1
454,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-label classification does not have constraints on the labels that observation can have, and this makes it difficult to learn. Using the OneVsRest Technique, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes.","Instead, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for what?", 1
455,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.",This technique is similar to what?, 0
456,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.",What is the name of the technique that uses k-Nearest Neighbors?, 1
457,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y3,"Y3, and what else?", 0
458,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X1,X1 and X2 processors are examples of what?, 0
459,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X3,X3D is a 3D game that uses 3D technology to create what?, 1
460,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X4,X4 and X4 Pro are examples of what kind of device?, 1
461,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X1,X1 and X2 processors are examples of what?, 0
462,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X3,X3D is a 3D game that uses 3D technology to create what?, 1
463,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X4,X4 and X4 Pro are examples of what kind of device?, 1
464,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.",What is the name of the technique that uses classifiers to build a chain of binary classifiers?, 1
465,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?",How then can you test and validate your model to ensure that real-world data can be introduced to it?, 1
466,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.","With k-fold CV, when k is greater than n, what is the output of your models not as correlated with?", 1
467,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Classification Problems: When Y is qualitative, we use the number of misclassified observations as a measure of the model's test error.",What is the number of misclassified observations?, 1
468,Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Reading: Cross-Validation: Python,Reading: Cross-Validation: Python is a cross-validation system that uses cross validation to determine what?, 1
469,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.",Deep neural networks consist of what?, 1
470,Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?","Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?", 1
471,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:","For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets: what is the usual way to represent data?", 1
472,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,inserting an element into a set,inserting an element into a set of elements is called what?, 1
473,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,deleting an element from a set,deleting an element from a set of elements to a set of elements?, 1
474,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"intersecting, unionizing two sets","intersecting, unionizing two sets of people together to form a union?", 0
475,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,subtracting a set from another set,subtracting a set from another set of values to a new set of values is called what?, 1
476,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,testing whether a set is equal to another one or is a subset of another one,testing whether a set is equal to another one or is a subset of another one?, 1
477,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,testing if a set contains a certain element,testing if a set contains a certain element that is not present in the set?, 1
478,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the mathematical descriptions of each of the operations one can do?,What are the mathematical descriptions of each of the operations one can do?, 1
479,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the types of data items that are input to each operation?,What are the types of data items that are input to each operation?, 1
480,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the types of data items that are output from each operation?,What are the types of data items that are output from each operation?, 1
481,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"So ideally, the programmer decides on the abstract data types that will be used in the algorithmic solution to a problem by concentrating on the operations that will be needed. At this point, she does not need to worry about how those data structures are represented in detail and how the operations are implemented.","Instead, she uses what?", 0
482,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is communicated through typically an API that names the operations and the input-output data to each call in the API along with some description of what function that call provides.",The most important aspect of an abstract data type is what?, 1
483,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"A data structure specifies how data of an abstract data type is represented and how the operations are implemented. For example, a set can be represented in many different ways:","For example, a set can be represented in many different ways: how many different ways can a data structure be represented?", 1
484,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,There are many other concrete data structures that one can use to implement the abstract data type for sets.,"For example, one can use data structures that are similar to what type of data structure?", 0
485,Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Complex numbers, where the pair either encodes the real and imaginary parts of a cartesian representation OR the magnitude and the argument of a polar representation of a complex number. All operations on complex numbers (e.g., exponentiation of a complex number to a complex number) would then be implemented in the said representations.",Complex numbers are represented by what?, 1
486,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",AI projects are data intensive. Data can be,Data can be accessed by using what?, 0
487,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",public (from government websites and open data),public (from government websites and open data) to open source software and open source software?, 0
488,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","paid (from the marketplace, brokers, and other services)","paid (from the marketplace, brokers, and other services) that are not regulated by the state or federal government?", 0
489,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",gathered (from customers using the product platform),gathered (from customers using the product platform) to customers using the product platform to get information about what?, 0
490,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",owned (from employees like annotators who manually create data),owned (from employees like annotators who manually create data) and who are responsible for creating data for the company?, 1
491,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Managing the Data Quality,"Managing the Data Quality of the Data Science and Data Science departments at the University of Southampton, Southampton, and Southampton Universities is a part of what type of research?", 0
492,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Automating testing and deployment,Automating testing and deployment of software is a part of what?, 1
493,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Easy integration with MLflow, which helps in tracking experiments",MLflow is easy to use and easy to use because it allows you to easily integrate with what other ML programs?, 0
494,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",1. Install Anaconda,Install Anaconda on a Mac or PC?, 0
495,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",6. Install dvc[s3],dvcs3 is installed on a system that has what?, 0
496,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","7. Initiate DVC which creates .dvc/.gitignore, .dvc/config and .dvcignore files","Initiate DVC which creates.dvc/.gitignore,.dvc/config and.dvcignore files in the same place as the.dvc files?", 0
497,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",10. DVC tracks the data files by adding a data folder to the DVC cache. It prevents adding to GIT by implicitly adding the data folder to .gitignore.,GIT is a file system that uses a file system that uses what?, 1
498,Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",13. DVC Remote Adda-acreates a remote section in DVC's config file,DVC Remote Adda-acreates a remote section in DVC's config file for the remote section of the app called what?, 1
499,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbersit is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.",What is the law of large numbers?, 1
500,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.",Kahneman's book is a great example of what type of bias?, 0
501,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.d","So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.dietary restriction, and what else might cause the cancer to spread?", 0
502,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.",What are the two main reasons that people want to pursue data roles?, 1
503,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.",The AI professional might use python scripts to extract data from what?, 1
504,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.","For example, what can AI professionals do to classify structured data?", 1
505,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.,EDA can help identify what type of data?, 1
506,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Data Modeling: The goal of a model is to try to replicate domain experts decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.,The goal of a model is to try to replicate what?, 0
507,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.",A component is organized by what?, 1
508,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.",The new variables are called what?, 0
509,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",A log level that is critical is called what?, 1
510,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.","DVC, Delta Lake, and LakeFS are examples of what kind of data versioning?", 1
511,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Often based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.",The goal of a server is to deliver what?, 1
512,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.",The production environment is a phase where the models are scaled and served in real-time by what?, 1
513,Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.",The model health can be measured by how many different metrics?, 1
514,Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources:","In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources: what is the primary issue with placeholder values?", 1
515,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.",What technique is used to identify groups within a data set?, 1
516,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.","The y-axis of a dendrogram marks the distance where clusters merge, and what else?", 1
517,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.,The representation choices for a language processing system depend on what?, 1
518,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.","Sequences of phonemes are then transcribed into orthographic symbols making up words taken into context, usually through what?", 1
519,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Morphology is the study of word structures, especially how morphemes, which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morphemes).",Bound morphemes are what type of morphemes?, 1
520,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).",A morphemes is a representation of what?, 1
521,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.",What is the lexeme representation usually added to?, 1
522,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:","Consider, for instance, the following sentences: ""I am a scientist, what is the name of the scientist that I am a scientist?", 1
523,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I want a flight to Tokyo,I want a flight to Tokyo to be a part of it?, 0
524,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I want to fly to Tokyo,"I want to fly to Tokyo, where I want to fly to?", 0
525,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found a flight to Tokyo,I found a flight to Tokyo from Japan in which I was flying with a friend who was from Japan?, 0
526,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found to fly to Tokyo,"I found to fly to Tokyo in the summer of 2008, where I flew to see what was happening?", 0
527,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.", Grammars are similar to what?, 0
528,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.","Semantically, what is semantically meaningless?", 1
529,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.,The syntax tree is based on what?, 1
530,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For example, the following tree representation captures the structure of the sentence, cA boy with a flower sees a girl with a telecope.d The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.",The structure of a sentence is represented by what?, 1
531,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of cbooksd above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.", What is the most difficult problem in NLP?, 1
532,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.",The label is used to indicate that the relation is what?, 1
533,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.",cbirdsd is related to cbirdsd as a prepositional object which itself is related to what?, 1
534,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event.  For example, all these sentences:"," For example, all these sentences: ""I am a robot"" and ""I am a robot"" are similar to each other in what way?", 1
535,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","are describing the same csellingd event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent.  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.", What is the semantic representation for these sentences?, 0
536,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.","In deep learning contexts, embeddings are computed by what?", 1
537,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.","In addition to the set of all propositions that all discourse participants agree on for the purpose of going on with the discourse, what else is required?", 1
538,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.", The model is known as what?, 0
539,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:",Consider the following two sequences of sentences: The first sequence is a sequence of sentences that are what?, 1
540,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have cmeaningful relationships.d",Coherence refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of what?, 1
541,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to increase sales from the companys online store\r (Business objective)\r,"In order to increase sales from the companys online store\r (Business objective)\r, they need to do what?", 0
542,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","...by classifying website visitors into youth, middle-age, and senior demographics (Task)\r","...by classifying website visitors into youth, middle-age, and senior demographics (Task)\r

Task is a term that describes how a website visitor is classified according to what?", 1
543,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,"(Business objective omitted due to project being primarily research)\r

In addition to project, what else is included in the project?", 0
544,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to enable more effective search of audio collections\r (Problem),"In order to enable more effective search of audio collections\r (Problem) in audio collections, what must be done?", 0
545,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds\r (Task)\r,We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds\r (Task)\r and then analyzes them to determine what?, 0
546,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client is a logistics company that wants to speed up its automatic package sorting\r (Business objective)\r,The client is a logistics company that wants to speed up its automatic package sorting\r (Business objective)\r (business objective) and what else?, 0
547,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We focus on the problem of handwritten address recognition from shipping label scans \r(Problem and Task),We focus on the problem of handwritten address recognition from shipping label scans \r(Problem and Task) and what else?, 0
548,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to combine neural image recognition with language models on company-internal data\r (Method and Data)\r,What is the name of the method and data that we want to combine?, 1
549,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",To improve performance beyond the current model based on standard convolutional neural networks without language information\r (Valuable functionality),"To improve performance beyond the current model based on standard convolutional neural networks without language information\r (Valuable functionality) and without the need for language information, what is the goal of the current model?", 1
550,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,"(Business objective omitted due to project being primarily research)\r

In addition to project, what else is included in the project?", 0
551,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to combine graph-based knowledge bases with neural attention models\r (Method),We want to combine graph-based knowledge bases with neural attention models\r (Method) and what other type of model?, 0
552,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient\r (Business objective)\r,The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient\r (Business objective)\r (Business objective) or less efficient (business objective) or what?, 1
553,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified\r (Problem and Task)\r","Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified\r (Problem and Task)\r and then identified by a method that is similar to what?", 0
554,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client \r(Methods and Data)\r,The client's data is used to conduct a qualitative survey on what?, 0
555,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",Towards identifying correlating events across the production process that can be used for process optimization\r.,What is the process that is used to optimise the output of a process?, 1
556,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,"(Business objective omitted due to project being primarily research)\r

In addition to project, what else is included in the project?", 0
557,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bots replies\r (Problem)\r,The problem is that the user doesn't know what?, 0
558,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset. \r(Task/Valuable Insight)\r,\r(Task/Valuable Insight)\r(Task/Valuable Insight) is a term that describes what type of quality score?, 1
559,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.,Scenarios that allow a learner to query the labels of observations in a dataset are called what?, 1
560,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).",The oracle can then label the new observation by using what?, 1
561,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.,The approach is called what?, 0
562,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.,The model would use what type of model change?, 0
563,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).,The model is called what?, 0
564,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.",A sampling distribution is a type of what?, 1
565,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Suppose a student decides to record her commuting times on various days. She selects these days at random from the school year, and her daily commuting time has the cumulative distribution function in Figure 1.",The curve is in the form of a curve with a line that points to where the student is going to go to where?, 0
566,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.","In addition to the values of the commuting time on each of the different days, what else is distributed randomly?", 1
567,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The situation described is an example of the simplest sampling scheme used in statistics, called simple random sampling, in which n objects are selected at random from a population (the population of commuting days) and each member of the population (each day) is equally likely to be included in the sample.",The population of commuting days is equal to the population of what?, 1
568,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The n observations in the sample are denoted \\(Y_{1}\\), , \\(Y_{n}\\), where \\(Y_{1}\\) is the first observation, \\(Y_{2}\\) is the second observation, and so forth. In the commuting example, \\(Y_{1}\\) is the commuting time on the first of her n randomly selected days, and \\(Y_{i}\\) is the commuting time on the \\(i^{th}\\) of her randomly selected days.","In the commuting example, what is the first observation?", 1
569,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.",The act of random sampling means that what is the result?, 1
570,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because \\(Y_{1}\\), , \\(Y_{n}\\) are randomly drawn from the same population, the marginal distribution of \\(Y_{i}\\) is the same for each i = 1,.., n; this marginal distribution is the distribution of Y in the population being sampled. When \\(Y_{i}\\) has the same marginal distribution for i = 1,..., n, then \\(Y_{1}\\), , \\(Y_{n}\\), are said to be identically distributed.","When the marginal distribution of Y is equal to the distribution of Y in the population being sampled, then what is the marginal distribution of Y in the population being sampled?", 1
571,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Under simple random sampling, knowing the value of \\(Y_{1}\\) provides no information about \\(Y_{2}\\), so the conditional distribution of \\(Y_{2}\\) given \\(Y_{1}\\), is the same as the marginal distribution of \\(Y_{2}\\). In other words, under simple random sampling, \\(Y_{1}\\) is distributed independently of \\(Y_{2}\\), , \\(Y_{n}\\).","In simple random sampling, what is the distribution of the conditional distribution of?", 1
572,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For a random sample of size n, the standard error of \\(\\bar{y}\\) depends on n and the population standard deviation \\(\\sigma\\) by \\(\\sigma _{\\bar{y}}=\\frac{\\sigma }{\\sqrt{n}}\\).",The standard error of the standard error depends on what?, 1
573,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Consider this example:,Consider this example: What is the name of the person who is accused of being a part of the conspiracy to commit genocide?, 1
574,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.,A 95% CI means that what percentage of the 40 confidence intervals contain the true mean value?, 1
575,Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.","If R2 is 0.5, what would be the R2 of the model?", 1
576,Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"When there are multiple regressors, then R2 is the square of the coefficient of multiple correlations (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables""). This metric will provide an indication of how well new data will be predicted by the model.",R2 is also known as what?, 0
577,Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.,The MSE is also known as what?, 1
578,Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y.",What is the square root of the averaged squares?, 1
579,Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",A model with a larger MDL value is considered better for inference because it has more information about what?, 0
580,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.",The middle man is the person who makes the decision on what?, 1
581,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.,What is the next module that you might want to use to determine if a model is trustworthy?, 0
582,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Reading: Should you trust that model?,Reading: Should you trust that model?, 0
583,Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.,What is the purpose of Sapley Value?, 1
584,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.",EDA uses what kind of techniques?, 0
585,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.",Describing Central Tendency and Assessing Measures of Spread and Relationships is a good way to describe what?, 0
586,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The are several measures to describe the spread, variability, or dispersion of a dataset","The are several measures to describe the spread, variability, or dispersion of a dataset in a way that is consistent with the data?", 0
587,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.",The square root of the variance is the most commonly used measure of what?, 1
588,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \\right |\\leq 1\\)",The correlation value lies between -1 and 1: \\(\\left | r_{XY} \\right |\\leq 1\\) and what is the relationship between the two variables?, 1
589,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.",The slope of the line is called what?, 1
590,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.",The correlation coefficient is a measure of what?, 0
591,Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.,What does not mean there is a causal relationship between them?, 0
592,Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:","Second, to give a tour of what hardware resources one should look at when deciding what hardware to get for a specific task?", 1
593,Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,As an incremental step towards business objective O,"As an incremental step towards business objective OTP, what is the name of the process that involves incremental steps towards business objectives?", 1
594,Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,We work towards solving problem P,We work towards solving problem P2P with the help of what?, 0
595,Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,by focusing on specific tasks T,by focusing on specific tasks THe task of focusing on specific tasks is called what?, 1
596,Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,to create valuable functionality F and/or produce insight I,to create valuable functionality F and/or produce insight I want to share with others?, 0
597,Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,"Feature engineering can be performed before the model building process, i.e., during the data wrangling and exploratory data analysis phase, or it can be performed during model building. Later in this course, we will discuss cross-validation, but we must note here that feature engineering can be done during the cross-validation process. Cross-Validation is a model validation technique used to assess how a model will generalize to a new data set. At this time, feature engineering is done during the cross-validation loop.",What is a model validation technique used to assess how a model will generalize to a new data set?, 1
598,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.",Agile Scrum is popular in what industry?, 1
599,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Lets first understand the roles in the Agile Scrum framework:,Lets first understand the roles in the Agile Scrum framework: what is the Agile Scrum framework?, 1
600,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.",The Scrum Master is responsible for removing or resolving any issues that may arise from what?, 0
601,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.","In a sprint, what is the goal of the sprint?", 1
602,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"User stories: A user story is simply a high-level definition of a work request. It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request. This short, simple description is written from the users perspective and focuses on outlining what the stakeholder wants (their goals) and why.",The short description is written from the team's perspective and focuses on what?, 0
603,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to navigate my way using the directions so that I can reach my destination.",I want to be able to do what?, 0
604,Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to change the destination so that I can reach a new destination.",I want to be able to do what?, 0
605,Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Once business requirements are defined, stakeholders and systems that support the business requirement(s) are identified. System requirements are a detailed description of the system and its operational and development constraints. They include the system software that will support the solution, processing and memory requirements, and other application software considerations. User requirements describe functions or tasks that a user must perform within the system. These tasks will support the business objectives that are defined prior to the requirements gathering process.",User requirements are detailed descriptions of what?, 0
606,Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Solution requirements are grounded in software engineering. In this course, we will be tailoring solution requirements to the data science process. The solution requirements for a data science project are classified into functional and non-functional requirements, as well as requirements that consider parts of an analytic solution that are different from the traditional IT systems. The typical analytic solution will consider data and models (e.g., predictive models), and a business intelligence solution will include requirements for reports and dashboards.",The solution requirements for a data science project are classified into what?, 1
607,Data Science Project Planning,Developing a Vision,The Vision Document,,"Proposed Solution. Next, come up with an overarching framework for your solution. An example solution formulation could be cA framework that supports a declarative description of a configuration space and automatically evaluates all the options, finding the best given some measure and labeled dataset.d Compare your solution with the existing solutions identified previously. What makes your solution better?",What makes your solution better?, 0
608,Data Science Project Planning,Developing a Vision,The Vision Document,,cFormatived: The proposed solution is cfast /good enoughd,cFormatived: The proposed solution is cfast /good enoughd format for the format to be used in the format that is used by the format?, 0
609,Data Science Project Planning,Developing a Vision,The Vision Document,,Features should be distinct in the sense that no two features should overlap.,"In addition to the feature that is distinct, what else should be distinct?", 0
610,Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.",Bayesian Methods are also used in what other discipline?, 0
611,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.",The model is modeled after what?, 0
612,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.",The gradient at the current position is called what?, 1
613,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.",Backpropagation is used heavily in what?, 1
614,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Easy to compute,Easy to compute?, 0
615,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Easy to implement,Easy to implement?, 0
616,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting stuck in a local minima,Susceptible to getting stuck in a local minima culpa?, 0
617,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computation for entire dataset requires a large memory,"Computation for entire dataset requires a large memory footprint, which is what?", 1
618,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.",The model parameters are updated after the computation of what?, 0
619,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Converges in lesser time because of frequent updates,Converges in lesser time because of frequent updates and what else?, 0
620,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Lesser memory requirements for calculating updates,Lesser memory requirements for calculating updates to a document are called what?, 1
621,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Moderate amount of memory requirements,Moderate amount of memory requirements for a game to run on a high-end system is called what?, 1
622,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting trapped in a local minima,Susceptible to getting trapped in a local minima culpa?, 0
623,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Reduces the high variance in model updates by SGD,"Reduces the high variance in model updates by SGD and other factors such as the presence of a high variance in the model update, which is known as what?", 0
624,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Faster convergence than Gradient Descent,Faster convergence than Gradient Descent?, 1
625,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Reduces the need to manually modify learning rate,Reduces the need to manually modify learning rate settings in a student's computer to make sure they're doing what?, 0
626,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Tends to have faster convergence than Gradient Descent and SGD,Gradient Descent is a feature of what?, 0
627,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate","The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate that is not what?", 0
628,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Requires lesser tuning than other optimization algorithms,Requires lesser tuning than other optimization algorithms?, 0
629,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Faster convergence,"Faster convergence is a term that refers to the convergence of two or more things at once, in which case the convergence is called what?", 1
630,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.",The state variables are stored in a separate file called what?, 0
631,Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Rapid convergence,"Rapid convergence is a term that refers to the convergence of two or more things at once, in which case the convergence is called what?", 1
632,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Statistical Inference is the process of drawing inferences from your data using probability theory. Statistical inference is used to draw scientific conclusions and test hypotheses.,"In addition to inference, what else is used to draw scientific conclusions?", 1
633,Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,"As the sample size n increases, the standard error approaches the true standard deviation for large n. This is because the standard error is an estimate of the true value of the standard deviation.",The standard error is an estimate of what?, 1
634,Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Lack of technical data science skills,","Lack of technical data science skills, technical data science skills, and technical data science skills are all examples of what?", 0
635,Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Inaccurate interpretation of results, and","Inaccurate interpretation of results, and the use of inaccurate information to make conclusions about what?", 0
636,Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"The remainder of this unit focuses on the ability to identify problems and envision a solution, which is among the most important skills a data scientist must possess.",The unit is also known as what?, 0
637,Collecting and Understanding Data,Data Collection,Study Design,,"To connect the studys objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.",What is one of the main categories of study designs?, 1
638,Collecting and Understanding Data,Data Collection,Study Design,,Exploratory,The computer in the computer's memory is called what?, 1
639,Collecting and Understanding Data,Data Collection,Study Design,,Bottom-up (without a pre-specified question),Bottom-up (without a pre-specified question) is a term that is used to describe what happens to a person when they are in a position of high risk?, 1
640,Collecting and Understanding Data,Data Collection,Study Design,,Can lead to knowledge discovery or new theory,Can lead to knowledge discovery or new theory?, 0
641,Collecting and Understanding Data,Data Collection,Study Design,,Uses inductive logic and the logic of discovery,Uses inductive logic and the logic of discovery are examples of what?, 0
642,Collecting and Understanding Data,Data Collection,Study Design,,Top-down (with a specified falsifiable hypothesis),Top-down (with a specified falsifiable hypothesis) is a term that is used to describe a set of hypotheses that are based on what?, 1
643,Collecting and Understanding Data,Data Collection,Study Design,,Tests an existing theory,"Tests an existing theory of the theory of relativity, which is based on what theory?", 0
644,Collecting and Understanding Data,Data Collection,Study Design,,"Use deductive logic, the logic of justification, and reconstructed logic","Use deductive logic, the logic of justification, and reconstructed logic are all examples of what?", 0
645,Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.",The data scientist has the responsibility of being able to assign subjects to what?, 1
646,Collecting and Understanding Data,Data Collection,Study Design,,"To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.","In the case of experiments, what is the name of the plan of manipulation?", 1
647,Collecting and Understanding Data,Data Collection,Study Design,,Good experimental designs use randomization to determine which treatment a subject receives.,Randomization is used to determine which treatment a subject receives?, 1
648,Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of observational studies is to draw inferences about the effect of an cexposured or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.","In addition to the data scientist, what else is involved in the study?", 0
649,Collecting and Understanding Data,Data Collection,Study Design,,"Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.","In addition to individual preferences, practice patterns, and policy decisions, what else is determined by individual preferences, practice patterns, and policy decisions?", 1
650,Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in todays data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive to do so. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do from an IT level.",What is the main requirement of data science?, 1
651,Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,,"We use a framework to formulate questions that carefully define business needs and extract business objectives that can be met using data science techniques. This framework can help a data science team meet the expectations of the client and deliver a solution that meets the business needs. The Evidence Value Proposition (EVP) framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology. This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.\r","This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.\r

The Evidence Value Proposition framework is used to determine what?", 0
652,Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,,Figure 1. Evidence Value Proposition Framework,Evidence Value Proposition Framework is a framework that provides evidence value for what?, 1
653,Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.",Modeling is usually done by what type of person?, 1
654,Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle can seem daunting at first, but a data scientist will not complete all the tasks alone. A productive team will consist of individuals with complementary skills filling various roles that can ensure the project is successfully executed.",A team that has a diverse team of people working together will be able to do what?, 0
655,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.,ML researchers and practitioners are targeting what?, 0
656,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.",The paper reports an unintuitive result that what is better than complex or black-box models?, 1
657,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects:","The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects: what is the main goal of the experimental procedure?", 1
658,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ1: How well can people simulate a models prediction?,RQ1: How well can people simulate a models prediction?, 1
659,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ2: To what extent do people follow a models prediction when its beneficial for them to do so?,RQ2: To what extent do people follow a models prediction when its beneficial for them to do so?, 1
660,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ3: How well can people detect when a model has made a mistake and correct it?,RQ3: How well can people detect when a model has made a mistake and correct it?, 1
661,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",A clear model with a smaller number of features was easiest for participants to simulate.,The smaller size of the model was also a benefit for participants because it allowed them to simulate more of what?, 0
662,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There were no significant differences in the participants trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.,The model with 8 features was used to test the model with what other model?, 0
663,Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.",What is the key to understanding the interdisciplinary approach?, 0
664,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).","In addition to the data science team, what other team members may be involved in the workshop?", 0
665,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",Data scientists may decide to specialize in a field because of the depth of their knowledge and what else?, 1
666,Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.",The data must be available to the experts in order for them to be able to do what?, 0
667,Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,Review requirements. The requirements management plan will be peer-reviewed to identify that each documented requirement is verifiable and unambiguous. The peer-review process should be well-defined so that reviewers can discuss their interpretations of requirements. This will reveal any ambiguity. The reviewers will produce a summary of the defects document. This document will be used by the business analyst and project team to revise requirements.,The summary of the defects document will be used by the business analyst and project team to revise what?, 0
668,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.",What is the name of the technique that focuses on features of an observation?, 1
669,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.",What is the purpose of PCA?, 1
670,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.",Eigenvectors and Eigenvalues from the covariance matrix are known as what?, 1
671,Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Construct the projection matrix W from the selected k Eigenvectors.,The projection matrix W from the selected k Eigenvectors is called what?, 1
672,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","In this scenario, requirements elicitation involves interviewing stakeholders and document analysis. Once user and system requirements are defined, the project team can now define the requirements for the analytic solution.",The project team can now define what?, 0
673,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Defining data requirements involves understanding the data needed to reach the business and analytic objectives. Defining the data requirements will also involve considerations for data sources, data management, and data extraction. The following questions must be answered to determine the data requirements:",The following questions must be answered to determine the data requirements: What is the data required to reach the business and analytic objectives?, 1
674,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",How can data be used to meet the business and analytic objectives?,How can data be used to meet the business and analytic objectives?, 1
675,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",What data is needed for developing the analytic solution? Can the data be collected from credible sources?,What data is needed for developing the analytic solution? Can the data be collected from credible sources?, 1
676,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",How can the data be prepared to visualize and answer relevant questions?,How can the data be prepared to visualize and answer relevant questions?, 1
677,Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Although the talent for developing models resides within the project team, the model requirements are defined collaboratively. Model explainability is important to ensure that a client understands the insights that can be gleaned from the results/outputs from the model(s).",Model explainability is important to ensure that a client understands what?, 0
678,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?","To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?", 1
679,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.",What is the responsibility of a data scientist?, 1
680,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.",Elish's article was published in what year?, 0
681,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",Transparency can privilege seeing over understanding,Transparency can privilege seeing over understanding what?, 1
682,Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",Auditing has been used in social science research to discover if a system is doing what?, 1
683,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.","Smithd, what is the downstream task in the pipeline?", 1
684,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.","German, Hungarian, Finnish, Turkish, and Arabic are examples of languages with rich morphology that are examples of what?", 1
685,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d","Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d, and what else?", 0
686,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.",What would have to be taken into account in order to analyze words?, 1
687,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.",The morphological interpretation of a word is called what?, 1
688,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:"," For example, the word cwordd has 6 possible part-of-speech categories: what is the third person present tense verb?", 1
689,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Adjective (JJ) c..down payment..d,Adjective (JJ) c..down payment..dependence on what?, 0
690,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Verb (VBP) cwe down five glasses of beer every nightd,"Verb (VBP) cwe down five glasses of beer every nightd in the mornings and evenings, and how many glasses of beer does it take to drink every night?", 1
691,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Noun (NN) cthey fill the comforter with downd,"Noun (NN) cthey fill the comforter with downds and downds, and what else?", 0
692,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,","In the rather artificial sentence, what tags are used to tag multiple words?", 1
693,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).,What is the task of determining the contextually correct tag for a word in a sentence?, 1
694,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.",The sequence of the words going in and the sequence of the POS tags coming out are trained with what?, 1
695,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.",What does the term NER stand for?, 1
696,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.",What does O label?, 1
697,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.","In the last colored blue named entity, cThed would get what label?", 1
698,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d",NER uses speech and language as a classifier for what?, 1
699,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.",The precision metric is used to determine what?, 1
700,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.",The stack is used to determine what kind of relationship is formed between lexical items?, 1
701,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.",The classifier can be trained by using what?, 1
702,Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product","AI Philosophy: A Process, not a Product, is a term that describes what type of process?", 1
703,Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product","AI Philosophy: A Process, not a Product, is a term that describes what type of process?", 1
704,Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",What is a good solution to an organization?, 1
705,Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.",The data science team will work with the client to understand what needs of the business?, 0
706,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).",The conditional probability that the n-grams last word follows the particular n-1 gram is called what?, 1
707,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,These days people build (classical) language models using well-established toolkits:,These days people build (classical) language models using well-established toolkits: what are they called?, 1
708,Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.",The toolkit provides options for what?, 0
709,Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Functionality is the most vital characteristic of an abstract data type that every user must understand. It is often communicated via an API that specifies the operations and input-output data for each API call, as well as a description of the function that the call delivers.",The description of the function that the call delivers is usually what?, 1
710,Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.",The model is called what?, 0
711,Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"It is important to understand how the time and memory cost grow with the size of the input, i.e., the time and space complexity for an algorithm. There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime).",The tradeoff between the two is called what?, 0
712,Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"An algorithm represents data and relations between data items using a variety of abstract data types. Some of the most commonly used abstract data types include Sequences, Sets, Tables, Graphs, Trees, and Priority Queues.","Sequences, Sets, Tables, Graphs, and Priority Queues are examples of what?", 0
713,Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.,The new transformer architecture is called what?, 1
714,Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):","Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence): what is the training procedure for the model?", 1
715,Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",Step 3 is similar to Step 1 but carried out on the output sequence tokens.,The output sequence tokens are used to store the output sequence tokens in a file called what??, 1
716,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.",Tuskegee University was located in what state?, 1
717,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.","The treatment was intended to help patients with minor medical issues, but not with what?", 0
718,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",In what year did the HEW terminate the experiment?, 1
719,Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.",Consent is based on what principles?, 1
