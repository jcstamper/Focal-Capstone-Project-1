Unit,Module,Title,Subheaders,Paragraph,Generated Question,Answer_generated
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.",What are the two stacks of the encoder and decoder?,Form a Transformer model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.",What can each of the two parts be used independently?,Transformer model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having cbi-directionald attention and are often called auto-encoding models.",What model uses only the encoder of a Transformer model?,Encoder model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having cbi-directionald attention and are often called auto-encoding models.",What are the attention layers able to access at each stage of the first sentence?,All the words
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.",What does the pretraining of these models usually revolve around?,corrupting a given sentence
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.",What does masking random words in a given sentence do?,corrupt the sentence
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.",What type of model is best suited for tasks that require an understanding of the full sentence?,encoder
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.",What is the name of a given entity?,Word Classification
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Representatives of this family of models include BERT, ALBERT, RoBERTa.","What type of model is BERT, ALBERT, RoBERTa?",Family
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.",What type of model uses only the decoder of a Transformer model?,Decoder model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.",What can only access the words positioned before a given word in a sentence?,Attention layers
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The pretraining of decoder models usually revolves around predicting the next word in the sentence.,What is the pretraining of decoder models usually revolves around?,predicting the next word in the sentence
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The pretraining of decoder models usually revolves around predicting the next word in the sentence.,What type of model is used to predict the next word in the sentence?,Decoder model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,These models are best suited for tasks involving text generation.,What types of models are best suited for tasks involving text generation?,Types of models
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,These models are best suited for tasks involving text generation.,What is the best way to use text generation models?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Representatives of this family of models include CTRL, GPT, GPT-2.","What type of model does CTRL, GPT, and GPT-2 represent?",Family
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.",What is the name of the encoder model that we are ready to study?,BERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.",What is BERT?,Encoder model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.",What is the name of the event that marks the beginning of a new era in NLP?,BERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.",What is BERT called?,Bidirectional Encoder Representations from Transformers
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms  an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERTs goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.",What does BERT use in its vanilla form?,The Transformer architecture.
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms  an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERTs goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.",What is BERT's goal to generate a language model?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms  an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERTs goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.",BERT is basically what?,Transformer encoder stack
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that its non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.",What is a mechanism that can be described as mapping a query and a set of key-value pairs to an output?,Attention
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that its non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.",What is the output computed as a weighted sum of the values?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The original BERT paper presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.,How many variants of BERT were presented in the original BERT paper?,Two
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The original BERT paper presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.,What is the name of the number of encoder units used in the architecture?,Transformer Blocks
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.",How many Encoder layers is BERT BASE composed of?,12
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.",How many attention heads are there?,12
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.",What is the total number of attention heads?,12
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.",How many Encoder layers is BERT LARGE composed of?,24
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.",How many attention heads are there?,Sixteen
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"For any NLP task, BERT is generally trained in two steps:",What is BERT generally trained in?,Two steps
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"For any NLP task, BERT is generally trained in two steps:",How many steps does BERT have?,Two
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.",How is the model trained on textual data?,Semi-supervised
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.",How does the model develop a general sense of the language?,By utilizing textual data.
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.",What is required to build good language understanding?,Text
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.",What is the name of the pre-trained model that is fine-tuned for a specific task in a supervised manner?,Model - A labeled dataset
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.",What can be added on top of the core model if needed?,Additional layers
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.",What is the first step that is common across all tasks of a particular language?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.",Why are models pre-trained on large amounts of text often distributed publicly for fine-tuning directly for the task at hand?,The first step is common across all tasks of a particular language
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.",What is done using Masked Language Modeling and Next Sentence Prediction?,Pre-training process
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERTs prediction of the masked token.",What percentage of words in each sequence are replaced with a special [MASK] token?,15%
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERTs prediction of the masked token.",What does the model attempt to predict?,The original value of the masked words
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERTs prediction of the masked token.",How is output embedding from BERT corresponding to the input token passed through a final classification layer?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Figure 1. Illustration of masking and input flow across the model in BERT.,What is the illustration of masking and input flow across the model in BERT?,Figure 1.
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.",When does the model receive pairs of sentences as input?,While training
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.",What percentage of the inputs in the model are a pair in which the second sentence is the subsequent sentence in the original document?,50%
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.,What are special tokens used to represent the start of the first and second sentences in the input?,CLS and SEP
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.,What is the final classification layer that predicts the likelihood of sentence B belonging with sentence A?,feed-forward+softmax
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training.,Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.",What is the goal of the BERT model?,minimizing the combined loss function of the two strategies
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.",What are the two strategies that are used together?,Masked LM and Next Sentence Prediction
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Figure 3: Overview of fine-tuning BERT and usage of BERT in various downstream tasks.,Figure 3: Overview of fine-tuning BERT and use of what in various downstream tasks?,BERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,Pre-trained BERT models can be used for a wide variety of language tasks by fine-tuning. Some examples are:,What can be used for a wide variety of language tasks?,Pre-trained BERT models
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Classification tasks such as sentiment analysis are done like Next Sentence Prediction classification, by adding a classification layer on top of the Transformer output for the [CLS] token.",What type of classification tasks are done by adding a classification layer on top of the Transformer output for the [CLS] token?,Next Sentence Prediction
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.",What is a task called when the system receives a question regarding a text sequence and is required to mark the answer in the sequence?,Question Answering
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.",How can a Q&A model be trained by learning two extra vectors?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.",What is the name of a system that receives a text sequence?,Named Entity Recognition (NER)
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.",What is a NER model that can be trained by feeding the output vector into a classification layer that predicts the NER label?,Named Entity Recognition (BERT)
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.",What is one of the most plausible sentence continuation tasks?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.",How are all the possible concatenations passed through BERT?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.",What is a task-specific parameter vector introduced?,A dot product
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.,What architecture has been modified to improve performance in terms of speed or accuracy?,BERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.,What are some of the famous variants discussed below?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ALBERT (A Lite BERT),What is a Lite BERT?,ALBERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.,How many parameters does the ALBERT model have?,12 million
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.,How many hidden layers does the BERT-Base model have compared to?,110 million
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.,What model reduced training time and inference time?,Lighter model
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.",What is used to achieve a lesser number of parameters?,Cross-layer parameter sharing
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.",What is the value of a cross-layer parameter sharing?,0
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.",How is the embedding layer reduced?,By factorization to 128 layers
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.",What concept does ALBERT work on?,SOP
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.",What is a cclassification model?,SOP
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.",How many sentences are swapped?,2
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,DistilBERT (Distilled BERT),DistilBERT (Distilled BERT)?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERTs performances. It reduced the number of layers in BERT by a factor of two.",How much less parameters does DistilBERT have than BERT-Base?,40%
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERTs performances. It reduced the number of layers in BERT by a factor of two.",What percentage of BERTs performs faster?,95%
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERTs performances. It reduced the number of layers in BERT by a factor of two.",How much more layers are in BERT?,two
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.",What is distillation?,A technique
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.",What is the name of the technique used by DistilBERT?,distillation
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.",How can a large neural network be approximated by a smaller network?,student
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,RoBERTa (Robustly Optimized BERT pre-training Approach),What is RoBERTa?,A robustly Optimized BERT pre-training Approach
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,RoBERTa (Robustly Optimized BERT pre-training Approach),What is a BERT pre training approach?,RoBERTa
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.",What is the name of the retraining of BERT?,RoBERTa
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.",How much more data and compute power is used in BERT's training?,1000%
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.",What does RoBERTa have?,retraining of BERT
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERTs pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.",What task did RoBERTa remove from the BERTs pre-training?,Next Sentence Prediction (NSP)
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERTs pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.",What did the masked token change during the training epochs?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERTs pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.",How many batch-training sizes were found to be useful in the training procedure?,Larger
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately),ELECTRA (Efficient Learning an Encoder that Classifies Token Replacements Accurately)?,
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.",What does ELECTRA use instead of MLM for pre-training?,a task called cReplaced Token Detection (RTD)
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.",What is the name of the task that ELEcTRA uses instead of masking the token?,cReplaced Token Detection (RTD)
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.","In RTD, what is the model expected to classify?",A wrong token
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.",What method of pre-training the model as a discriminator rather than a generator is more sample-efficient?,This method of pre-training the model as a discriminator rather than a generator is
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),,"This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.",What do the learned contextual representations outperform?,BERT
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,This module is a special collaboration with a Subject Matter Expert. It was written by Kaushik Shakkari.,What is the name of the module that was written by Kaushik Shakkari?,The subject matter expert
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,This module is a special collaboration with a Subject Matter Expert. It was written by Kaushik Shakkari.,Who wrote the module?,Kaushik Shakkari
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,"Kaushik is a senior data scientist at Cognistx. He proposed and leads SQUARE, an end-end question answering product at Cognistx. He is also a fellow and alumni mentor at the Insights Data Science program. His research and areas of interest include user behavioral analysis, semantic search, and deep learning.",Who is a senior data scientist at Cognistx?,Kaushik
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,"Kaushik is a senior data scientist at Cognistx. He proposed and leads SQUARE, an end-end question answering product at Cognistx. He is also a fellow and alumni mentor at the Insights Data Science program. His research and areas of interest include user behavioral analysis, semantic search, and deep learning.",What is SQUARE?,An end-end question answering product
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,"Kaushik is a senior data scientist at Cognistx. He proposed and leads SQUARE, an end-end question answering product at Cognistx. He is also a fellow and alumni mentor at the Insights Data Science program. His research and areas of interest include user behavioral analysis, semantic search, and deep learning.",Who is an alumni mentor at Insights Data Science?,Kaushik
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,You can reach Kaushik on LinkedIn and read his articles on Medium.,How can you reach Kaushik on LinkedIn?,
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,,Kaushik Shakkari,What is the name of Kaushik Shakkari?,"The name of Kaushik Shakkari is ""All the Kaushik Shakkari"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",The CNN architecture incorporates a number of layer types as follows:,What type of architecture does the CNN architecture incorporate?,Layer Type
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",The input layer accepts a 3D matrix of size W1,The input layer accepts a 3D matrix of what size?,W1
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The convolutional layer accepts a 3D matrix of size W1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K \\]",How many hyperparameters does the convolutional layer have?,Four
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The convolutional layer accepts a 3D matrix of size W1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K \\]","What is the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P?","The number of filters K, the spatial extent F, the stride S, and the amount"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The pooling layer accepts a 3D matrix of size W1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F}{S}+1, \\quad H_{2}=\\frac{H_{1}-F}{S}+1, \\quad D_{2}=D_{1} \\]",How many hyperparameters does the pooling layer have?,Two
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The pooling layer accepts a 3D matrix of size W1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F}{S}+1, \\quad H_{2}=\\frac{H_{1}-F}{S}+1, \\quad D_{2}=D_{1} \\]",What is the spatial extent F and the stride S?,Two hyperparameters.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The pooling layer accepts a 3D matrix of size W1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F}{S}+1, \\quad H_{2}=\\frac{H_{1}-F}{S}+1, \\quad D_{2}=D_{1} \\]",How does it output a 3D matrix of size W2?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The fully connected layer is identical to a fully connected network layer. It accepts a K-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).",What does the fully connected layer accept?,K-dimensional vector
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","The fully connected layer is identical to a fully connected network layer. It accepts a K-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).",What is the number of nodes in the full connected layer?,l
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.",Who made LeNet one of the first successful applications of CNNs?,Yann LeCun et al.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.",Who proposed LeNet for the task of document recognition?,Yann LeCun et al.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.",How many convolutional layers are in LeNet?,Two
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.",What was the stride of LeNet's pooling layers?,2x2
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.",What is the output of a handwritten digit in LeNet?,Probability over 10 possible outcomes
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.",What is a probability over 10 possible outcomes?,Output
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 10. Data Flow of AlexNet,What is the Data Flow of AlexNet?,Figure 10
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.",What is AlexNet considered a breakthrough in applying deep learning to computer vision?,AlexNet is considered a breakthrough in applying deep learning to computer vision.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.",What was AlexNet's error rate on ImageNet in 2012?,15.3 percent
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.","How many layers did AlexNet contain before making 1,000 class prediction problems?",Eight
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 11. Image filters learned by the first layer of AlexNet.,What is the first layer of AlexNet?,Figure 11. Image filters learned by the first layer of AlexNet.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.",How many layers did VGGNet use?,16
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.",What was the error rate of VGGnet on ImageNet?,7.3 percent
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.",How many model parameters did VggNet have in total?,138 million
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 12. Comparison of AlexNet and VGGNet.,What is the comparison of AlexNet and VGGNet?,Figure 12.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).",What does VGGNet replace with large kernel-sized filters?,11 and 5
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).",What is the first and second convolutional layer?,11 and 5
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).",How many kernel-size filters are replaced with multiple 3X3?,One after another
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.","What did He et al. introduce as ""shortcut connections""?",Residual Network or ResNet
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.",What did ResNet researchers show that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network?,Higher testing error
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.",The key idea behind a ResNet is to fit the residual value of what?,Signal
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 13. Comparison of Normal CNN layer and Residual layer.,Figure 13. Comparison of Normal CNN layer and Residual layer?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.",How does a ResNet and a regular CNN operationalize a residual layer?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.",What does the left figure show in a CNN where it tries to fit the actual desired mapping H of x?,A standard layer
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.",How is the residual layer acquired?,With a residual or a short-circuit connection.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 14. ResNet-18 architecture.,What is the architecture of ResNet-18?,Figure 14
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Now that we have seen several milestone architectures for CNNs, let's now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a 2017 paper by Canziani et al., shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.",What is the name of the paper that shows the accuracies of different models on the ImageNet dataset?,Canziani et al.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research","Now that we have seen several milestone architectures for CNNs, let's now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a 2017 paper by Canziani et al., shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.",What is a remarkably rapid area of research with successive innovations?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017),What is a comparison of deep neural network models?,Complexity
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research",Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017),Canziani et al. (2017)?,
Advanced Natural Language Processing,BERT,Module 25 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Before we talk deeper about diagrams, it is important to outline what factors will affect your design, i.e., what you have to keep in mind when designing your solution.",What is important before we talk deeper about diagrams?,To outline what factors will affect your design.
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Before we talk deeper about diagrams, it is important to outline what factors will affect your design, i.e., what you have to keep in mind when designing your solution.",What factors will affect your design?,
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Before we talk deeper about diagrams, it is important to outline what factors will affect your design, i.e., what you have to keep in mind when designing your solution.","When designing a solution, what should you keep in mind?",What factors will affect your design?
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"For this section, it will be useful to go back to your requirements document. Most constraints will be apparent from the Non-Functional Requirements (NFRs) and Scope sections, but other sections like Intended Users may also bring up some interesting constraints.",What section will be useful to go back to your requirements document?,This section
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"For this section, it will be useful to go back to your requirements document. Most constraints will be apparent from the Non-Functional Requirements (NFRs) and Scope sections, but other sections like Intended Users may also bring up some interesting constraints.",What sections will most constraints be apparent from?,Non-Functional Requirements (NFRs) and Scope sections
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"For this section, it will be useful to go back to your requirements document. Most constraints will be apparent from the Non-Functional Requirements (NFRs) and Scope sections, but other sections like Intended Users may also bring up some interesting constraints.",Intended Users may also bring up what?,interesting constraints
Data Science Project Planning,Design and Plan Overview,Design Considerations,,Some examples are:,What are some examples of examples?,
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If your system is a prototype or a proof of concept, you may want to include that assumption in this section. Accordingly, in your design, the deployment model may not be really important as it is a research project. You can have a cbest-cased deployment model, i.e., a model that indicates the best way to deploy a full-scale version of the idea.",What does a cbest-cased deployment model indicate?,the best way to deploy a full-scale version of the idea
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If your system is a prototype or a proof of concept, you may want to include that assumption in this section. Accordingly, in your design, the deployment model may not be really important as it is a research project. You can have a cbest-cased deployment model, i.e., a model that indicates the best way to deploy a full-scale version of the idea.",What is the best way to deploy a full-scale version of the idea?,A deployment model
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Information about the intended users of the system (e.g., working professionals or consumers) may affect the type of information your system has to expose. In your design, this will translate into specific outlets of information from your model.",What may affect the type of information your system has to expose?,"Information about the intended users of the system (e.g., working professionals or consumers)"
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Information about the intended users of the system (e.g., working professionals or consumers) may affect the type of information your system has to expose. In your design, this will translate into specific outlets of information from your model.",What is the purpose of your system's design?,To translate into specific outlets of information from your model.
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Information about the intended users of the system (e.g., working professionals or consumers) may affect the type of information your system has to expose. In your design, this will translate into specific outlets of information from your model.",How does information about the intended users affect your system?,In your design.
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Related to the above point, if your system deals with sensitive consumer information, your design will have to be careful of which outlets are exposed to the external world and what information is part of that outlet.",What does your system deal with sensitive consumer information?,
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"Related to the above point, if your system deals with sensitive consumer information, your design will have to be careful of which outlets are exposed to the external world and what information is part of that outlet.",What will your design have to be careful of if it deals with sensitive consumers?,Which outlets are exposed to the external world and what information is part of that outlet.
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If performance is an important NFR, then a good assumption to have is the response time for your system (e.g., the requirement may indicate that an API call has to respond back within no more than 2 ms).",What is an important NFR?,Performance
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If performance is an important NFR, then a good assumption to have is the response time for your system (e.g., the requirement may indicate that an API call has to respond back within no more than 2 ms).",What is a good assumption to have?,Response time
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If performance is an important NFR, then a good assumption to have is the response time for your system (e.g., the requirement may indicate that an API call has to respond back within no more than 2 ms).",How long does an API call have to respond?,2 ms
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If the client has specifically asked for the system to be deployed on AWS/Azure/GCP or wants a specific platform to be used for the solution, one should make sure to include this requirement here. This will affect your design, as these platforms have their own constraints, which you may want to consider in your design.",What type of platform does the client want to use for the system?,AWS/Azure/GCP
Data Science Project Planning,Design and Plan Overview,Design Considerations,,"If the client has specifically asked for the system to be deployed on AWS/Azure/GCP or wants a specific platform to be used for the solution, one should make sure to include this requirement here. This will affect your design, as these platforms have their own constraints, which you may want to consider in your design.",What is the name of the platform that will affect your design?,AWS/Azure/GCP
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,A matrix is a data structure that encodes the relationship between rows and columns. The disadvantage of this format is that matrices can be very sparse in certain domains. Sparsity refers to the fact that the majority of entries are unknown or missing. Sparsity leads to a waste of space and computational resources.,What is a data structure that encodes the relationship between rows and columns?,A matrix
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,A matrix is a data structure that encodes the relationship between rows and columns. The disadvantage of this format is that matrices can be very sparse in certain domains. Sparsity refers to the fact that the majority of entries are unknown or missing. Sparsity leads to a waste of space and computational resources.,What is the disadvantage of a matrix?,It can be very sparse in certain domains.
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them.,What do Equivalent representations of A as a sparse matrix aim instead to store only the non-empty values and operate on them?,Non-zero (non-empty) values
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,"The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.",What format is suitable for storing not only input data but also model parameters in certain domains?,Sparse format
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,"The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","In computational biology, we typically need to build what over a large number of features?",Predictive models
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,,"The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.",What format can we opt to store model weights in?,Sparse vector/matrix format
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Module 22 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.",What is the purpose of the paper reading modules?,To familiarize you with the styles of data science literature and the contexts in which advances in
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.",What do you need to know about the contexts in which advances in data science are introduced?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.",How can you learn technical papers?,skimming through the original article and then reading our synthesis
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.",What does the synthesis structure of each paper contain?,Key questions
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.",What is the outline of a paper that you may find useful in a future seminar course?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","[Required Reading] Paper: Briscoe, E., & Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff. Cognition, 118(1), 2-16. (Requires CMU credentials to access)","What paper did Briscoe, E., and Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff?",Cognition
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.",What is the name of the first author who is currently a Senior Research Scientist?,The name of the first author is Senior Research Scientist.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.","Who is the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory?",The first author is currently a Senior Research Scientist.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.",When did she receive her Ph.D. in cognitive psychology from Rutgers University?,2008
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.",Who received his Ph.D. in 1992?,The second author
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.",What are the main research interests of Rutgers?,"Visual perception, especially perceptual organization and shape"
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.",How is the mind organizes groups of objects into coherent collections and hierarchies?,By categorization and concept learning
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.,What is the purpose of the paper?,Machine learning researchers and practitioners
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.,What do machine learning researchers build that interact with the end user?,Machine learning systems
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.",What are two popular psychological theories on how humans perform categorization?,Exemplar theory and categorizing a new object with a new category
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.",What do people store the attributes of observed examples?,exemplars
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.",How would people categorize an object as a bird if its similar to any type of bird they have come across?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).",What are the two extremes on the same continuum of bias-variance?,Humanity and categorization
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).",The way humans actually perform categorization lies somewhere in the middle of what continuum?,Bias-variance
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).",When was statistical ML not as popular as it is today?,In 2010
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The paper presents a number of conceptual and empirical contributions:,The paper presents a number of conceptual and empirical contributions?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The characterization of exemplar theory as the low bias, high variance extreme, and prototype theory as the high bias, low variance extreme on the bias-variance continuum.","What is the characterization of exemplar theory as the low bias, high variance extreme, and prototype theory?","High bias, high variance extreme, and prototype theory"
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",A class of experiments to evaluate human learners position on this continuum when the complexity of the training data varies has not been systematically attempted before.,What is a class of experiments to evaluate human learners position on this continuum when the complexity of the training data varies?,Not been systematically attempted before.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.,What is the proposed model that had the best fit for human performance?,Locally regularized model
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.,What is a reasonable explanation for how humans perform?,A locally regularized model
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The experiment reported in the paper has the following phases:,What is the name of the experiment reported in the paper?,The experiment described in the paper is called The experiment described in the paper is called The experiment described
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.",How many bivariate Gaussian mixtures were constructed?,Five
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.",What is pK?,A composite of five bivariate Gaussian mixtures.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.",How many components are used in pk?,Five
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.",When did the authors create a ship flag?,When they created a ship flag.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.",How many undergraduate students were recruited for the study?,Thirteen
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.",How many Gaussian distributions were sampled?,five
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.",What was the first step for the students to learn the categorization?,Classify each flag individually
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.",What is the exemplar model called?,GCM
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.",What does the sensitivity parameter c correspond to?,More cexemplar-liked
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.",How is the value set independently?,At each partition of the feature space.
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?",What are the fitted parameters optimized to fit the ensemble of each subjects responses?,Fit the models to subject data
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?",What does the complexity K vary?,Which models best reflect the human performance in this flag classification task?
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?",Which models reflect the human performance in flag classification?,Which models reflect the human performance in flag classification task?
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?",What are the fitted parameters optimized to maximize the likelihood of the training examples observed so far at each point in the experiment?,Fit the models to concept data
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?",What is used in this process instead of the human classifications?,ground truth labels of the flags
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?",The important findings from the experiment are as follows:,What are the important findings from the experiment?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.",What are human subjects proficient at?,Catgorizing simple concepts (K = 1)
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.",What is K = 1)?,Simple concepts
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.",How does K = 4 differ from K = 5?,Random guessing
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.",What type of model has a better fit than the prototype model?,exemplar model
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.",What is the difference between the two models at larger complexity levels?,Convergence
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.","When fitting models to concept data, the prototype models performance decreases much faster than what?",Human performance
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.",What does exemplar models performance not decrease fast enough to match human performance at higher complexity levels?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.",The locally regularized model represents a middle point in what continuum?,Bias-variance continuum
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.",What model consistently fitted subject data better than the exemplar?,The locally regularized model
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.",What does the paper finding suggest that humans adopt a middle ground?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.",What do humans treat concepts as mixtures of?,Sub-concepts
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.",What should be done at different levels of conceptual complexity?,Evaluation of human learning
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.",What model may have a similar fit with human learning on simple data?,the exemplar model and prototype model
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.",Complexity should be systematically varied over a range of levels to reflect what?,a comprehensive picture of general human learning
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.",What is the main question about machine learning?,Whether it should follow the process of human learning
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.",What does deep neural networks typically require?,a very large amount of training data
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.",How is machine learning more human-like?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?","From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.",Why is the bias/variance trade-off?,It originates from statistical learning.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.",What has a direct effect on the decisions made long after the models are developed?,The quality of your data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.","When data is gathered, what can present quality issues?",Missing values to inconsistent formats
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.",What should be studied before it is used in an enterprise?,Raw data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.",When is data considered in its raw form?,At this stage of the data science lifecycle.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.",What type of data should one view all data as?,Raw data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.",How can you discuss the techniques used to enrich data?,through a process called Data Wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.","What is the process of cleaning, formatting, and enriching raw data?",Data Wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.",What is a best practice for an organization with a good data management framework?,Data wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.",Who will store data that has been processed to allow for enterprise-wide access and usage?,"The data architects, engineers, and/or administrators."
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.",What process can assign value to a dataset after the data has been cleaned and transformed?,Data wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.",What phase of the data science lifecycle requires a clear understanding of the business and defining what?,The business and analytic objectives and requirements for the analytic solution.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Despite its importance, data wrangling presents some challenges that are common in data science projects.",What does data wrangling present?,Challenges
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Despite its importance, data wrangling presents some challenges that are common in data science projects.",What is a common challenge in data science?,Data wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.",What kind of data can you collect from social media?,CSV files
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.",What is the best practice to study the data to determine its quality?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.,What is an organization that collects or purchases customer data from a marketing firm?,Consider
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.,What can the data from the marketing firm be sent to the organization by a simple file transfer or through more automated sharing processes?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.",What is sometimes referred to as a feature engineering?,data wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.",What does Feature Engineering involve?,Domain knowledge of the data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.","When you perform data wrangling, you are concerned with what?",cleaning your data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.",What is the scikit-learn preprocessing package widely used by?,Data scientists and analysts
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.","Pandas, Numpy, Matplotlib, and Theano are libraries that support what?",data cleaning and transformation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.",How can you inspect data once it has been gathered?,Basic sorting techniques as well as creating visuals.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.",What can you use to identify outliers in your dataset?,Box plots
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.",How do sorting techniques expose missing values and show the range of values?,Using visuals such as box plots
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:",What is a small or large observation within your dataset called?,An outlier
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:",What affects the performance of your model?,Outliers
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:","When you have a large dataset, the outliers are not as noticeable as when you have what?",smaller dataset
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.",What is a box plot called?,A box and whisker plot
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.",How many values are in a five-number summary?,five
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.",What is the maximum and minimum values in your dataset?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.",What percentage of the distribution is in the box plot?,50%
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.",What is the median marked by?,A line drawn within the box
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.",Which lines extend from the box are called?,Whiskers
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.",What is a visual summary of the data?,Box plots
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.",What does skewness and mean mean?,Summary statistics
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.",What do the whiskers extend to in box plots?,smallest and largest observations
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.",What are the most extreme observations within 1.5 IQR?,The smallest and largest observations
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.",Where are the outliers marked?,Separately
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.",What is one reason to show outliers separately in a box plot?,They do not provide much information about the shape of the distribution.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.",What can be informative to investigate?,Outliers
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.",In what way was the observation incorrectly recorded?,Was it fundamentally different from the others in some way?
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.",What is the 1.5 IQR criterion for an outlier in practice?,arbitrary
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.",What is better to regard an observation satisfying this critterion as?,A potential outlier
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.","When a distribution has a long right tail, some observations may fall more than what?",1.5 IQR
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.,What is another way to measure position?,By the number of standard deviations that a point falls from the mean.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.,What is the number of standard deviations that a point falls from the mean called?,z-score
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",\\(z = (x-\\mu )/\\sigma\\),(z = (x-mu / sigma)?,(z = (x-mu/sigma)
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!",What is a measure of the relative position of an observation within a dataset?,z-score
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!",What does the Empirical Rule mean for a bell-shaped distribution?,It means that an observation must fall more than three standard deviations from the mean.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!",How many standard deviations does an observation fall from the mean?,Three
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task,What does the data gathering process depend on?,Business and analytic objectives and your data source(s).
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task,What type of data will need to be converted into a usable format to meet the requirements of a data science task?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.",What is one of the most common data quality issues?,Missing values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.",What can happen due to human error or system issues during data collection?,Missing values in your dataset.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.",How does a dataset that was extracted from an external source might not provide context on the reason behind missing values?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.",How many categories are missing data classified in statistics?,three
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.",What is the probability of missing data?,3 categories
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.,What implies that missing data is not related to the data?,Missing completely at random
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.,What is the probability of data being missing for all observations?,0
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Missing at random (MAR) is the probability that the missing data is the same within certain groups.,What is the probability that the missing data is the same within certain groups?,Missing at random (MAR)
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.,What does NMAR mean?,Not missing at random
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.,What is the probability of data being missing at random?,NMAR
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.",What are the common strategies that are employed in handling missing values?,imputation and omission
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.",What does imputation replace missing values in the dataset with?,other values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.",How can a missing value be replaced with the mean age?,Imputation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.",What is often the go-to technique when there are missing values?,Omission
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.",What can be done when the number of missing values is small?,Omission
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.",What type of deletion is a type of omission?,Pairwise
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.",What is the size of a sample?,500 MB
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.,What removes all data for an observation that has one or more missing values?,Listwise deletion
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.,What would mean that your dataset would have observations with values for all variables?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.",What type of variables can you omit with missing values?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.",What is a shoe size variable with a missing value?,Social media usage habits
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.",What process involves extracting portions of a dataset that are relevant to your model or analysis?,Subsetting
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.",What can be used to remove observations with missing values?,Subsetting
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.",How many subsets of medical records are for successful treatments?,One
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.",When did we discuss inspecting the data?,When we discussed inspecting the data.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.",What are outliers unusual values in the dataset?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.",Why is the outlier value unusual?,It clies at an abnormal distance from other values in your dataset.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.",How can outlier values contribute valuable insights to your solution?,Of course.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.",What are different types of data?,Different types of data.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.",What type of techniques are used for data transformation?,Specific techniques
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.",Which type of data can be customized to suit the needs of the data transformation process?,Types of data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.",What type of data is divided into groups or nominal categories based on a qualitative characteristic?,categorical
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.",What might be variables in a dataset that is useful in predicting a health challenge?,"Gender, race, and eye color"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.","For processing purposes, such data may need to be transformed into what format?",Quantitative
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.",What can have many categories or levels?,Categorical variables
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.",What can negatively affect your analysis and model?,A variable with levels that are not useful
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.",How can a variable with levels that do not occur?,Negatively affect your analysis and model
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.",What can be converted into quantitative values for certain statistical techniques?,Ordinal data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.",What is an example of a dataset containing student evaluations that would have responses that are ranked by different levels?,Ranked values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.",Who provided out-of-class support for the coursed?,The instructor
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.",What are Dummy variables often referred to as?,Binary variables
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.",What can be converted into 0s and 1s?,categorical data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.",How many categories can a dataset containing customer spending data have?,two
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:",What is the name of the variable that can be transformed into dummy variables?,Hair color
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:",What type of variable can be used to create Dummy Variables for more than one category?,Categoric variable
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).",How many dummy variables will one create for a variable with (k>2) categories?,One will create 4 dummy variables (5-1).
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).",What is the number of categories of the variable?,4
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.",How many categories can one assign to each category?,0 or 1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.",What would the black variable get if the observation does not have black hair?,0
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.",What type of variable is not included in the creation of dummy variables?,blonde
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.",What does this mean?,All other categories will be compared to this category.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.",What is the purpose of transforming quantitative data into quantitative data?,So the data can be used for specific statistical techniques.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.",What is often noisy with missing values when data is gathered?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration",An exampleof a popular quantitative transformation is converting the date of birth to age.,What is an example of a popular quantitative transformation?,converting the date of birth to age
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.",What are quantitative transformations also useful when performing feature engineering?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.",What will be extracted from the quantitative data and transformed into formats that can be used by a machine learning model?,Features
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.",What does Binning transform a quantitative variable into?,A categorical variable
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.",What can be grouped into intervals?,Values for age
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.",How can Binning reduce redundancy?,By using unequal intervals
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.",How can you create new variables using mathematical transformations on existing variables?,Using Mathematics.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.",What techniques can you use?,"Standards, min-max scaling, and logarithmic transformation"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.",How can we explore these techniques in a future unit?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.","What involves ingesting, transforming, and integrating the transformed data for access?",Data integration
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.",What is a popular example of integrating data into a data warehouse so that OLAP servers can access the data?,A data warehouse example
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.",How can data integration be done?,"With the assistance of an ETL (extract, transform, and load) mechanism."
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.",What is an extension of the transformation process done during data wrangling?,feature engineering
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.",What is the term for EDA?,Exploratory Data Analysis
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.",What type of analysis is EDA used to describe?,Exploratory data analysis
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.",When is data wrangling preprocessed?,During the EDA process and beyond
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.",The extent of the data understanding phase shows that data quality can truly make or break an analytic solution?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.",The data wrangling process has informed the data science team on what?,The state of their dataset
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration","The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.","If new data is sourced, what is repeated in an iterative fashion?",The data wrangling process
Data Science Project Planning,Design and Plan Overview,Overview,,"Following the requirements document, it is vital to develop a design for the project. This is the most important documentation of the project as it provides not only a low-level design of the system but also dives deep into the implementation details of the system. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, the system environment, and the design methodology.",What is the most important documentation of the project?,The requirements document
Data Science Project Planning,Design and Plan Overview,Overview,,"Following the requirements document, it is vital to develop a design for the project. This is the most important documentation of the project as it provides not only a low-level design of the system but also dives deep into the implementation details of the system. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, the system environment, and the design methodology.",What are some key design considerations that developers should address?,
Data Science Project Planning,Design and Plan Overview,Overview,,The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.,What should the data science project's design document explain?,The entire system architecture
Data Science Project Planning,Design and Plan Overview,Overview,,The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.,What can simplify the explanation of the solution's architecture?,A system architecture diagram
Data Science Project Planning,Design and Plan Overview,Overview,,"Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)",What is the name of the ACAI Architecture?,MCDS Capstone Project
Data Science Project Planning,Design and Plan Overview,Overview,,"Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)",What year was the MCDS Capstone Project?,2020
Data Science Project Planning,Design and Plan Overview,Overview,,"While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.",What can be used to explain the flow of information from one module to another?,Dataflow diagrams
Data Science Project Planning,Design and Plan Overview,Overview,,"While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.",What can the dependencies in a project be depicted via entity-relationship diagrams?,
Data Science Project Planning,Design and Plan Overview,Overview,,Figure 2. Use Case Diagram (Source: https://venngage.com/blog/use-case-diagram/),What is the source of the use case diagram?,https://venngage.com/blog/use-case-diagram/
Data Science Project Planning,Design and Plan Overview,Overview,,"Apart from employing diagrams, it is also a good practice to make a list of tools and dependencies along with the suitable versions that the project may require.",What is a good practice to make a list of tools and dependencies along with the appropriate versions that the project may require?,
Data Science Project Planning,Design and Plan Overview,Overview,,"This documentation also helps the developers think about various risks and challenges involved in the data science project. These risks could be domain, technical or business-related risks that are a part of the solution proposed for this project.",What does this documentation help developers think about?,Different risks and challenges involved in the data science project.
Data Science Project Planning,Design and Plan Overview,Overview,,"This documentation also helps the developers think about various risks and challenges involved in the data science project. These risks could be domain, technical or business-related risks that are a part of the solution proposed for this project.",What could be a part of the solution proposed for the data science project?,"Domain, technical or business-related risks"
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Besides memory, the other main aspect of any hardware system you will need to assess in order to understand if its important for computing are the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.",What is the other main aspect of a hardware system you will need to assess in order to understand if it is important for computing?,The processors inside the system.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Besides memory, the other main aspect of any hardware system you will need to assess in order to understand if its important for computing are the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.","In data science, what are the three main categories of processors being used?","CPUs, GPUs, and DSAs"
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.",What is the primary processor on a system?,Central Processing Unit
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.",What is used on most systems for complex calculations unless the application developer specifically invokes another processor?,The primary processor
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.",How can the central processing unit handle more parallelism than GPUs?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.",What is the Graphics Processing Unit also known as?,Graphics Processing Unit
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.",What is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism?,Graphics Processing Unit
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.","In data science, we tend to focus on systems that have what?",a separate GPU
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.",What are Domain-Specific Architectures also known as?,Domain-Specific Architectures
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.",What is the name of the domain-specific architectures?,Intels Crest
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.",How are Domains specific architectures built?,To solve specific problems
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"While it is tempting to use the most-efficient DSAs and try to squeeze as much performance as possible out of the newest systems, remember that, in the data science process, you will need to budget cost as well as time. It might be useful to use a DSA or a GPU, but you should remember to do the following when deciding whether to use either chip for a project or not:",What is a good idea to use a DSA or a GPU?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"While it is tempting to use the most-efficient DSAs and try to squeeze as much performance as possible out of the newest systems, remember that, in the data science process, you will need to budget cost as well as time. It might be useful to use a DSA or a GPU, but you should remember to do the following when deciding whether to use either chip for a project or not:",What is the most efficient DSA?,A DSA for a CPU
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If you have access to trial usage of the DSA/GPU for your project, check that the tools you are using utilize the accelerators at all. As these chips require separate programming APIs to utilize, the tools which you use them might not be compatible out of the box or at all.",What is the DSA/GPU?,A testing tool
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If you have access to trial usage of the DSA/GPU for your project, check that the tools you are using utilize the accelerators at all. As these chips require separate programming APIs to utilize, the tools which you use them might not be compatible out of the box or at all.",What do accelerators require to be used for?,Programming APIs
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Additionally, before setting and forgetting your system, check the usage patterns of the code you are able to run.","Before setting and forgetting your system, check the usage patterns of what?",The code you are able to run.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If the code is relatively I/O, Network, or Memory intensive, it might not make sense to use an accelerator chip, i.e., a GPU or a DSA, in your system. Instead, it might pay to try to use multiple processors or multiple computers together to solve the work in question.",What type of chip might not make sense to use in your system?,Accelerator chip
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If the code is relatively I/O, Network, or Memory intensive, it might not make sense to use an accelerator chip, i.e., a GPU or a DSA, in your system. Instead, it might pay to try to use multiple processors or multiple computers together to solve the work in question.",What kind of chip would you use to solve the problem?,Accelerator chip
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.",What is your GPU/DSA's memory limit?,100
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.",What can ensure that your code runs smoothly?,Keeping to these limits
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.","If the code is computationally expensive, check to see if the memory usage aligns with what?",the memory limits of your GPU/DSA
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.",What tool can be difficult to use at first?,Profiling tools
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.",What is the best way to see where the potential slowdowns are?,Profiler tools
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,,"Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.",How do you count the time taken to run hot sections?,A simple timer
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When your output variable is a continuous value, you are able to make predictions using the widely known regression analysis. The input variables for a regression task can be categorical, discrete, or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s).",What are the input variables for a regression task?,"categorical, discrete, or continuous"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When your output variable is a continuous value, you are able to make predictions using the widely known regression analysis. The input variables for a regression task can be categorical, discrete, or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s).",What is used to investigate the relationship between your input (independent) variables and your output (dependent) variable?,Regression techniques
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Thought: The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses; those techniques include kNN, among others.",What does the delineation above do not mean that all supervised techniques will either return qualitative or quantitative responses?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Thought: The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses; those techniques include kNN, among others.",What can some techniques that we will explore in the next modules return?,qualitative and quantitative responses
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.",What is one of the easier techniques to implement?,Regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.",What can one tell the effect of changes in temperature and terrain on the outcome of a football game?,Regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.",How can a data scientist evaluate the best variables that can be used to construct a predictive model?,Regression analysis
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"If the independent variables are highly correlated, we can say that the variables are multicollinear. If the correlation between two independent variables is 1 or -1, then you have perfect multicollinearity. One can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed.",What is the correlation between two independent variables?,1 or 1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"If the independent variables are highly correlated, we can say that the variables are multicollinear. If the correlation between two independent variables is 1 or -1, then you have perfect multicollinearity. One can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed.",What does one detect when there are large changes in the estimated regression coefficient when an independent variable is added?,Multicollinearity
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"A regression model will have certain components, including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error \\(\\epsilon\\); the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters \xce. Your goal with a regression model is to estimate the function f(X, \xce) with the best fit to the data. The function f should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.",What does a regression model account for?,Random error
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"A regression model will have certain components, including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error \\(\\epsilon\\); the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters \xce. Your goal with a regression model is to estimate the function f(X, \xce) with the best fit to the data. The function f should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.",What is the difference between an expected outcome and an actual observation?,The random error is not found in the dataset.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"A regression model will have certain components, including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error \\(\\epsilon\\); the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters \xce. Your goal with a regression model is to estimate the function f(X, \xce) with the best fit to the data. The function f should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.",How do regression models estimate the function f?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When performing regression analysis, you might encounter data that has outliers. This is not handled during the data understanding phase. It can affect the results of your regression analysis.",What is not handled during the data understanding phase?,data that has outliers
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When performing regression analysis, you might encounter data that has outliers. This is not handled during the data understanding phase. It can affect the results of your regression analysis.",What can affect the results of your regression analysis?,Outliers
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Let us explore the different types of regression techniques in this section with the goal of exploring each technique further in the subsequent sections.,What are the different types of regression techniques in this section?,Let's explore
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Let us explore the different types of regression techniques in this section with the goal of exploring each technique further in the subsequent sections.,What is the goal of exploring each technique in the subsequent sections?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.",What is used to model the relationship between independent variable x and dependent variable y?,This regression technique is used to model the relationship between independent variable x and dependent variable y
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.","When you have two or more independent variables, you will represent them as what?",The vector (x=(X__ldots X
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.",What is the number of inputs?,k
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"There is the simple linear regression model that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the multiple linear regression method to account for multiple predictors.",What is the simple linear regression model that allows for predicting a response based on one predictor variable?,One predictor variable.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"There is the simple linear regression model that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the multiple linear regression method to account for multiple predictors.",What does single linear regression not allow for multiple predictor variables?,Predictor variables
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution.,What model can be used for classification if you replace the gaussian output with a Bernoulli distribution?,This model can be used for classification if you replace the gaussian output with 
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Let us represent a regression model as:,What model did we represent as a model?,A regression model
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,\\[y=\+\ x_1+\\cdots+\\beta_{\\mathrm{k}} x_{\\mathrm{k}}+\\varepsilon\\],[y=+ x_1+cdots+beta_mathrmk?,x__++varepsilon
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,The regression function for multiple linear regression is:,What is the regression function for multiple linear regression?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,\\[f\\left(x_1 \\ldots x_k\\right)=\+\ x_1+\\cdots+\\beta_k x_k\\],[fleft(x_1 ldots x_kright=+ x_______1+cdotes+beta_k]?,x_1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of\\(X_{1} \\ldots X_{k}\\) with Y are the constants \\(\, \\ldots \\beta_k\\), also known as the coefficients of the variables. One can interpret this to mean \\(\\beta_i\\) is the change in the predicted value of your dependent variable Y per unit of change in \\(X_{i}\\), with other things being equal. Consider \\(\\\)  as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error $\\epsilon$ in the equation.",What is a straight line function of each independent variable?,Y
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of\\(X_{1} \\ldots X_{k}\\) with Y are the constants \\(\, \\ldots \\beta_k\\), also known as the coefficients of the variables. One can interpret this to mean \\(\\beta_i\\) is the change in the predicted value of your dependent variable Y per unit of change in \\(X_{i}\\), with other things being equal. Consider \\(\\\)  as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error $\\epsilon$ in the equation.","What are the slopes of the individual straight-line relationships of, ldots X_k) with Y?","The constants,ldots beta_k"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of\\(X_{1} \\ldots X_{k}\\) with Y are the constants \\(\, \\ldots \\beta_k\\), also known as the coefficients of the variables. One can interpret this to mean \\(\\beta_i\\) is the change in the predicted value of your dependent variable Y per unit of change in \\(X_{i}\\), with other things being equal. Consider \\(\\\)  as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error $\\epsilon$ in the equation.",How is the change in the predicted value of your dependent variable Y per unit of change in (X_i)?,(X_i)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"You estimate \\(\, \\ldots \\beta_k\\), and \\(\\\) using the Least Squares method. This method will minimize the sum of squared residuals (a residual is a difference between an observed value and the fitted value given by a model). The least squares method can be linear or ordinary, or nonlinear. Ordinary Least Squares choose the parameters of a linear function of a set of independent variables by the principle of least squares. Non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters; that is, it will approximate the model by a linear model and refine its parameters by iterations.","What method is used to estimate (, ldots beta_k)?",Least squares method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"You estimate \\(\, \\ldots \\beta_k\\), and \\(\\\) using the Least Squares method. This method will minimize the sum of squared residuals (a residual is a difference between an observed value and the fitted value given by a model). The least squares method can be linear or ordinary, or nonlinear. Ordinary Least Squares choose the parameters of a linear function of a set of independent variables by the principle of least squares. Non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters; that is, it will approximate the model by a linear model and refine its parameters by iterations.",What method minimizes the sum of squared residuals?,Least Squares method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.",What is the coefficient of determination?,(R2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.",What measure the proportion of the variation in the dependent variable that is predictable from the independent variable?,(R2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.","The larger the (R2), the better the model can explain the variation of the response with what?",Variable predictors
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Ordinary Least Squares Source3,Ordinary Least Squares Source3?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Reading: Four Principal Assumptions. These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights.,How many Principal Assumptions do these assumptions justify?,Four
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Reading: Four Principal Assumptions. These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights.,What should be met to avoid producing misleading analytic solutions and insights?,These assumptions
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called a polynomial regression. Pay attention to the figure below. You will note that using a linear regression line to fit the data would result in a high value for the error.","When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called what?",A polynomial regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called a polynomial regression. Pay attention to the figure below. You will note that using a linear regression line to fit the data would result in a high value for the error.",What is a linear regression line to fit the data?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Trying to fit a simple linear regression line. Source4,What is the purpose of a simple linear regression line?,To fit
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the y and x variables. So a polynomial regression can fit a broader range of functions. However, it is sensitive to outliers, and those outliers can affect the result of a polynomial regression analysis.",What is a better view of the relationship between y and x variables?,A polynomial regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the y and x variables. So a polynomial regression can fit a broader range of functions. However, it is sensitive to outliers, and those outliers can affect the result of a polynomial regression analysis.",What can a polynomial regression fit a broader range of functions?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Polynomial regression with lower error. Source4,What is the result of a polynomial regression with lower error?,The result of a polynomial regression with lower error.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. Stepwise Regression will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:",What will increase the prediction power of a model with a minimum number of predictors?,Stepwise Regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. Stepwise Regression will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:",What is the process of fitting the model with the predictedors done automatically without human intervention?,Stepwise Regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Backward elimination tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the ""most statistically insignificant deterioration of the model fit."" This technique should not be used if the number of predictors is more than the observations in the dataset.",What tests the effect that each variable has on a model by deleting it?,Backward elimination
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Backward elimination tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the ""most statistically insignificant deterioration of the model fit."" This technique should not be used if the number of predictors is more than the observations in the dataset.",What are the deleted variables that have the most statistically insignificant deterioration of the model fit?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Forward selection is the reverse of backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit.,What is the reverse of backward elimination?,Forward selection
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Forward selection is the reverse of backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit.,Variables are added to assess what?,model fit
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"We also have the mixed selection technique, which can be considered a hybrid selection method with the backward elimination and forward selection techniques.",What technique can be considered a hybrid selection method?,Mixed selection technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"We also have the mixed selection technique, which can be considered a hybrid selection method with the backward elimination and forward selection techniques.",What is the backward elimination technique?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the residual standard error (RSE is the standard deviation of error \\(\\epsilon\\)), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the \\(R^2\\) is independent of your response variable, unlike the RSE.",What is a stepwise regression prone to?,overfitting issues
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the residual standard error (RSE is the standard deviation of error \\(\\epsilon\\)), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the \\(R^2\\) is independent of your response variable, unlike the RSE.","What is the standard deviation of error (epsilon), what is the average amount that the response will deviate from?",True regression line
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"\\(R^2\\) is calculated using the total sum of squares which is the total variance in Y, and RSS is the discrepancy between the data and an estimation model.",What is calculated using the total sum of squares?,(R2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"\\(R^2\\) is calculated using the total sum of squares which is the total variance in Y, and RSS is the discrepancy between the data and an estimation model.",What is the total variance in Y?,Total sum of squares
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The goodness of fit of a model will show how the model fits the data that it is trained with, and it will highlight any lack of balance between observations in the dataset and those that will be introduced to the model (new values). To select the right method, one can use the different metrics below:",What will show how the model fits the data that it is trained with?,goodness of fit
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"The goodness of fit of a model will show how the model fits the data that it is trained with, and it will highlight any lack of balance between observations in the dataset and those that will be introduced to the model (new values). To select the right method, one can use the different metrics below:",What will highlight a lack of balance between observations in the dataset and those that will be introduced to the model?,Good fit of a model
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.,What is the name of the AIC?,Kaike Information Criterion
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.,What type of model is the best?,with the smallest AIC
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.,How does AIC focus on model performance?,On a training set
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.",What is the BIC?,Bayesian Information Critterion
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.",What is BIC related to?,AIC
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.",How does BIC penalize complex models?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"\\(R^2\\) will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability, and 1 means the model explains full variability.",What will increase as more dimensions are added to the dataset?,(R2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"\\(R^2\\) will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability, and 1 means the model explains full variability.",What does a value of 0 mean that a model does not explain?,Any variability
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Adjusted \\(R^2\\) addresses the issue highlighted with n independent variable that has a strong correlation with the dependent variable increases the adjusted \\(R^2\\) and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted \\(R^2\\) is a suitable criterion to use.",What does Adjusted (R2) address the issue highlighted with?,n independent variable
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"Adjusted \\(R^2\\) addresses the issue highlighted with n independent variable that has a strong correlation with the dependent variable increases the adjusted \\(R^2\\) and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted \\(R^2\\) is a suitable criterion to use.",What is a criterion to use when you have a model with more than one variable?,Adjusted (R2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Mallow's Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.,Mallow's Cp is used to assess the fit of what?,A regression model
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Mallow's Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.,What is the goal of Mallow to find the best model?,involving a subset of these predictors
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"We will continue to learn more about regression analysis in an upcoming module, and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge of Regression analysis.",In what module will we continue to learn more about regression analysis?,Module
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,"We will continue to learn more about regression analysis in an upcoming module, and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge of Regression analysis.",In what section are you encouraged to find the materials in the additional reading section to strengthen your knowledge of Regression Analysis?,Additional reading section
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,Model Accuracy,Additional Reading: Elements of Statistical Learning,What are the Elements of Statistical Learning?,
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"As we practice data science, it is important that we do so with an ethical framework in mind. There are many ways we can cause unintended negative consequences through our work as data scientists if we blindly focus on the techniques we learn without considering their consequences. In this module, we will talk about how we could potentially make mistakes and how we can avoid making these mistakes.",What is important to do as we practice data science?,So do so with an ethical framework in mind.
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"As we practice data science, it is important that we do so with an ethical framework in mind. There are many ways we can cause unintended negative consequences through our work as data scientists if we blindly focus on the techniques we learn without considering their consequences. In this module, we will talk about how we could potentially make mistakes and how we can avoid making these mistakes.",What can cause unintended negative consequences through our work as data scientists if we blindly focus on the techniques we learn without considering their consequences?,There are many ways we can cause unintended negative consequences through our work as data scientists.
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Lets start with a simple definition of ethics. Ethics are rules that we, as fellow citizens, voluntarily follow because we believe they make the world a better place. Ethics guide us to distinguish right from wrong and are at the bedrock of human civilization.",What is a simple definition of ethics?,
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Lets start with a simple definition of ethics. Ethics are rules that we, as fellow citizens, voluntarily follow because we believe they make the world a better place. Ethics guide us to distinguish right from wrong and are at the bedrock of human civilization.",What are rules that we voluntarily follow because we believe they make the world a better place?,Ethics
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Consider this simple example: Ethics stop you from shoplifting. There may be other things that could stop you from shoplifting, such as the possibility of being caught by the store owner,  but ethics would stop you even if there is no one in the store to catch you. What stops you is the fundamental principle of what you think is right. Ethics are the shared values of the society that compel us to think about how we want to be treated, which then translates into how we treat others.",What does ethics stop you from shoplifting?,
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Consider this simple example: Ethics stop you from shoplifting. There may be other things that could stop you from shoplifting, such as the possibility of being caught by the store owner,  but ethics would stop you even if there is no one in the store to catch you. What stops you is the fundamental principle of what you think is right. Ethics are the shared values of the society that compel us to think about how we want to be treated, which then translates into how we treat others.",What is the fundamental principle of what you think is right?,Ethics
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Consider this simple example: Ethics stop you from shoplifting. There may be other things that could stop you from shoplifting, such as the possibility of being caught by the store owner,  but ethics would stop you even if there is no one in the store to catch you. What stops you is the fundamental principle of what you think is right. Ethics are the shared values of the society that compel us to think about how we want to be treated, which then translates into how we treat others.",How do we treat others?,How we want to be treated
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are distinct from the law. Consider an example where you tell a friend a secret, and the friend promises not to tell anyone else. The friend then breaks that promise and spreads your story. The friend may not have broken any law and is unlikely to face any legal consequences. Nonetheless, we can agree that the friends behavior is unethical because it is a common principle of human relationships and interactions to keep ones promises, especially to ones own friends.",What is an example of an example where you tell a friend a secret?,
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are distinct from the law. Consider an example where you tell a friend a secret, and the friend promises not to tell anyone else. The friend then breaks that promise and spreads your story. The friend may not have broken any law and is unlikely to face any legal consequences. Nonetheless, we can agree that the friends behavior is unethical because it is a common principle of human relationships and interactions to keep ones promises, especially to ones own friends.",What does the friend promise not to tell someone else?,A secret
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are distinct from the law. Consider an example where you tell a friend a secret, and the friend promises not to tell anyone else. The friend then breaks that promise and spreads your story. The friend may not have broken any law and is unlikely to face any legal consequences. Nonetheless, we can agree that the friends behavior is unethical because it is a common principle of human relationships and interactions to keep ones promises, especially to ones own friends.",Who breaks the friend's promise?,The friend breaks the promise and spreads your story.
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are not laws, but laws often follow the shared social values of ethics. Laws are created and enforced to ensure these common social values. However, despite having generally agreed upon shared values, not everyone will act in accordance with those values, either because they dont share them or because theyve justified to themselves in some way that theyre not truly violating them. For example, just because we have a shared principle saying it is wrong to shoplift doesnt mean there are no shoplifters in society. It merely means that we all have agreed that stealing is wrong as a society and can therefore pass legislation that punishes thieves. These laws are then enforced by our criminal justice system, which also determines the punishments for those who violate them.",What are laws that follow the shared social values of ethics?,Laws
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are not laws, but laws often follow the shared social values of ethics. Laws are created and enforced to ensure these common social values. However, despite having generally agreed upon shared values, not everyone will act in accordance with those values, either because they dont share them or because theyve justified to themselves in some way that theyre not truly violating them. For example, just because we have a shared principle saying it is wrong to shoplift doesnt mean there are no shoplifters in society. It merely means that we all have agreed that stealing is wrong as a society and can therefore pass legislation that punishes thieves. These laws are then enforced by our criminal justice system, which also determines the punishments for those who violate them.",What do laws follow to ensure common social values?,Common social values of ethics
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Ethics are not laws, but laws often follow the shared social values of ethics. Laws are created and enforced to ensure these common social values. However, despite having generally agreed upon shared values, not everyone will act in accordance with those values, either because they dont share them or because theyve justified to themselves in some way that theyre not truly violating them. For example, just because we have a shared principle saying it is wrong to shoplift doesnt mean there are no shoplifters in society. It merely means that we all have agreed that stealing is wrong as a society and can therefore pass legislation that punishes thieves. These laws are then enforced by our criminal justice system, which also determines the punishments for those who violate them.",Why do not everyone act in accordance with these values? What does stealing mean?,They don't share them or they justify themselves in some way that they're not violat
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"One practical way to think about the benefit of ethics is through the lens of economics. In general, society as a whole does best when individuals maximize their own private returns. This is the fundamental principle of a modern free-market economy. However, there are situations where we see that the results of collective actions from individuals under the name of maximizing private benefits are a detriment to society.",What is one practical way to think about the benefit of ethics?,Economics
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"One practical way to think about the benefit of ethics is through the lens of economics. In general, society as a whole does best when individuals maximize their own private returns. This is the fundamental principle of a modern free-market economy. However, there are situations where we see that the results of collective actions from individuals under the name of maximizing private benefits are a detriment to society.",What is the fundamental principle of a modern free-market economy?,maximizing private returns
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that cthe population problem has no technical solution; it requires a fundamental extension in morality,d in which cpopulationd refers to members of society and cextension in moralityd refers to ethical principles. It explains individuals tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole of which they are a part themselves.",When was the tragedy of the commons published?,1968
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that cthe population problem has no technical solution; it requires a fundamental extension in morality,d in which cpopulationd refers to members of society and cextension in moralityd refers to ethical principles. It explains individuals tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole of which they are a part themselves.",What is the name of the term used for the first time by Garret Hardin?,tragedy of the commons
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that cthe population problem has no technical solution; it requires a fundamental extension in morality,d in which cpopulationd refers to members of society and cextension in moralityd refers to ethical principles. It explains individuals tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole of which they are a part themselves.",When did Hardin first use this term?,1968
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that cthe population problem has no technical solution; it requires a fundamental extension in morality,d in which cpopulationd refers to members of society and cextension in moralityd refers to ethical principles. It explains individuals tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole of which they are a part themselves.",How does cpopulationd refer to members of society?,By an extension in morality.
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"In 2009, Elinor Ostrom became the first woman to be awarded the Nobel Memorial Prize in Economic Sciences for revisiting the tragedy of the commons and her work on governing the commons (requires CMU credentials to access). She showed cases where such ctragedyd was not inevitable and that if humans cooperated with one another and monitored and enforced rules to manage the commons, then the ctragedyd could be avoided.",Who was the first woman to be awarded the Nobel Prize in Economic Sciences in 2009?,Elinor Ostrom
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"In 2009, Elinor Ostrom became the first woman to be awarded the Nobel Memorial Prize in Economic Sciences for revisiting the tragedy of the commons and her work on governing the commons (requires CMU credentials to access). She showed cases where such ctragedyd was not inevitable and that if humans cooperated with one another and monitored and enforced rules to manage the commons, then the ctragedyd could be avoided.",What was Elinor Ostrom's work on governing the commons called?,Ctragedyd
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"In 2009, Elinor Ostrom became the first woman to be awarded the Nobel Memorial Prize in Economic Sciences for revisiting the tragedy of the commons and her work on governing the commons (requires CMU credentials to access). She showed cases where such ctragedyd was not inevitable and that if humans cooperated with one another and monitored and enforced rules to manage the commons, then the ctragedyd could be avoided.",When did Ostrom receive the Nobel Memorial Prize?,2009
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Now that weve learned about what ethics are and how together we can avoid unintended negative consequences from selfish and unethical practices, in the subsequent modules, we will link this with data science and demonstrate the core values of ethical data science practices to motivate the discussion of how data scientists can apply ethical principles.",What are ethics and how can we avoid negative consequences from selfish and unethical practices?,
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,,"Now that weve learned about what ethics are and how together we can avoid unintended negative consequences from selfish and unethical practices, in the subsequent modules, we will link this with data science and demonstrate the core values of ethical data science practices to motivate the discussion of how data scientists can apply ethical principles.",What are the core values of ethical data science practices to motivate the discussion of?,How data scientists can apply ethical principles
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,MotoManager Case-in-Point,What is the name of the MotoManager Case-in-Point?,"The name of the MotoManager Case-in-Point is ""Adam and Eve"""
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Here is an example of an AI consulting firm that used the EVP framework to meet the business needs of a popular automotive services provider.,What framework did an AI consulting firm use to meet the needs of a popular automotive service provider?,EVP
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer cstickinessd leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.",Tire-Monro Muffler and Brake is a subsidiary of what company?,Monro Inc.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer cstickinessd leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.",How many locations are there in the U.S.?,Thirty-three
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer cstickinessd leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.",What does the automotive service industry struggle with?,customer retention
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.",What company approached Monro Inc. to develop a data-driven solution to improve customer acquisition and retention?,Cognistx
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.",What company was the parent company of Mr. Tire?,Monro Inc.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.",Who developed MotoManager?,Mr. Tire
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the companys business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the companys data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.",What team met with the business leaders of Monro Inc. to gain insight into the companys business needs related to customer retention?,The data science team
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the companys business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the companys data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.",What did the Cognistx data science team identify and interview all stakeholders from the business and technical teams?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the companys business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the companys data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.","Who provided information about the company's data asset management structure, including data governance, data architecture and data security management?",The IT managers
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Data Management in the Enterprise,What is Data Management in the Enterprise?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Data Management in the Enterprise,What is the purpose of Data Management?,To help businesses grow their business
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Finally, service managers were interviewed on customer service difficulties that could be addressed by the proposed solution. The service managers also identified the hardware and software gaps at various stores around the country. Once the interviews were completed, the Cognistx data science team formulated business objectives that would meet Monro Inc.s business needs. The business objectives included:",What did the Cognistx data science team work on?,Business objectives
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Finally, service managers were interviewed on customer service difficulties that could be addressed by the proposed solution. The service managers also identified the hardware and software gaps at various stores around the country. Once the interviews were completed, the Cognistx data science team formulated business objectives that would meet Monro Inc.s business needs. The business objectives included:",What was the purpose of the company's mission?,To help customers understand and use Monro Inc.'s business needs.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Creating an application that provides customers with a customized service experience for their automotive needs.,What is the purpose of creating an application that provides customers with a customized service experience?,For their automotive needs.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Offering customers $50 coupon to download Monro Inc mobile app.,What does Monro Inc offer to customers?,A $50 coupon to download the Monro Inc mobile app.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Offering customers $50 coupon to download Monro Inc mobile app.,What is the price of a monro app?,$50
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Onboarding customers to the application with the creation of customer profiles.,What is the purpose of onboarding customers to the application?,Creation of customer profiles
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Onboarding customers to the application with the creation of customer profiles.,What does the creation of customer profiles mean?,Onboarding customers to the application
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Providing tailored customer service management to very important (VIP) customers.,What is the purpose of providing tailored customer service management to very important (VIP) customers?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Classifying customers as VIP customers based on defined characteristics.,What type of customers are classified as VIP customers?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Classifying customers as VIP customers based on defined characteristics.,What is the classification of VIP customers based on?,Definated characteristics
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Creating a loyalty program to increase repeat customer transactions.,What is the purpose of creating a loyalty program?,Increase repeat customer transactions
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Business objectives should be measurable to ensure that business needs are met. The metrics used to assess the success of the project were:,What should business objectives be measurable to ensure that business needs are met?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,Business objectives should be measurable to ensure that business needs are met. The metrics used to assess the success of the project were:,What metrics were used to assess the success of the project?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The number of people who installed the app and on-boarded upon receiving a $50 e-coupon.,How many people installed the app after receiving a $50 e-coupon?,The number of people who installed the app was 50.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The number of times an on-boarded customer visited a store close to them.,What is the number of times a customer visited a store close to them?,The number of times an on-boarded customer visited a store close to them.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The number of transactions completed with the app.,What is the number of transactions completed with the app?,The number of transactions completed with the app.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The total amount of revenue generated via the app compared to total cost of  maintaining the app.,What is the total amount of revenue generated via the app?,Compared to the total cost of maintaining the app.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The total amount of revenue generated via the app compared to total cost of  maintaining the app.,How much does the total cost of maintaining the app compare to?,The total cost of maintaining the app is 1.5 times higher.
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,A model that can predict a repeat customer from among on-boarded customers with an accuracy of 85%.,What is a model that can predict a repeat customer from among onboarded customers with an accuracy of 85%?,
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,The metrics were both technical (precision and accuracy of the model) and business-related (calculate return-on-investment).,What metrics were both technical and business related?,Calculate return-on-investment
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a $50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customers local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over $14 million US dollars from app-based transactions.",What is the name of the AI-enabled application for Monro Inc.?,MotoManager
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a $50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customers local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over $14 million US dollars from app-based transactions.",What does the MotoManager app send to current and potential customers?,A $50 coupon incentive
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,,"Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a $50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customers local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over $14 million US dollars from app-based transactions.",How many users have the app in 2019?,"53,000"
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",The IEEE defines a requirement as a documented condition or capability needed by a user or system to meet a business need or achieve a business objective. Requirements become useful to the solution development process when they have been converted into specifications. Requirements must meet certain criteria to be useful for achieving business objectives. Let us define the general characteristics of a good requirement.,What defines a requirement as a documented condition or capability needed by a user or system to meet a business need?,The IEEE
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",The IEEE defines a requirement as a documented condition or capability needed by a user or system to meet a business need or achieve a business objective. Requirements become useful to the solution development process when they have been converted into specifications. Requirements must meet certain criteria to be useful for achieving business objectives. Let us define the general characteristics of a good requirement.,What are requirements that become useful to the solution development process when they have been converted into specifications?,Requirements must meet certain criteria to be useful for achieving a business objective.
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Reading: IEEE Guidelines on Software Requirements Gathering.,What is reading: IEEE Guidelines on Software Requirements Gathering?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A good requirement should be complete and correct. You must identify the relevant stakeholders and define a set of needs, goals, and objectives for your project. As there is no perfect scenario, you must define the constraints that are applicable to a project. Those constraints might include cost, scope, existing systems and processes, time, and technology. Defined scenarios can also result in identifying all the stakeholders and their needs within the business context. A requirement should also be traceable. This refers to tracking the life cycle of a requirement from its development to its specification and deployment in various versions of the solution. Traceability can be supported in a straightforward way via a matrix which associates a unique identifier, a description, and a design specification element to each requirement, as shown below.",What should a good requirement be complete and correct?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A good requirement should be complete and correct. You must identify the relevant stakeholders and define a set of needs, goals, and objectives for your project. As there is no perfect scenario, you must define the constraints that are applicable to a project. Those constraints might include cost, scope, existing systems and processes, time, and technology. Defined scenarios can also result in identifying all the stakeholders and their needs within the business context. A requirement should also be traceable. This refers to tracking the life cycle of a requirement from its development to its specification and deployment in various versions of the solution. Traceability can be supported in a straightforward way via a matrix which associates a unique identifier, a description, and a design specification element to each requirement, as shown below.",What are the constraints that are applicable to a project?,"Cost, scope, existing systems and processes, time, and technology."
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A good requirement should be complete and correct. You must identify the relevant stakeholders and define a set of needs, goals, and objectives for your project. As there is no perfect scenario, you must define the constraints that are applicable to a project. Those constraints might include cost, scope, existing systems and processes, time, and technology. Defined scenarios can also result in identifying all the stakeholders and their needs within the business context. A requirement should also be traceable. This refers to tracking the life cycle of a requirement from its development to its specification and deployment in various versions of the solution. Traceability can be supported in a straightforward way via a matrix which associates a unique identifier, a description, and a design specification element to each requirement, as shown below.",How can a requirement be traceable?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Requirement,What is a requirement for a requirement?,Minimum of two hours
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Design Specification,What is the Design Specification?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",1.0.0 Consumption reports should be integrated with the Dashboard API.,1.0.0 Consumption reports should be integrated with what API?,Dashboard API
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Data from the hourly consumption report will use data_integrate_trail in the Update event procedure.,Data from the hourly consumption report will use what in the Update event procedure?,data_integrate_trail
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A requirement should be unambiguous. Given that the requirements gathering process involves contributions from multiple stakeholders, the requirement should be explicit and clear to all. It should have the same meaning to everyone involved and not be open to interpretation. Unambiguous requirements must have defined acceptance criteria, metrics for success, expected outcomes, and acceptable values. The use of an active voice in its description will make a requirement clearer. Finally, a requirement should be verifiable. Testers should be able to verify that the requirement is implemented correctly. A requirement is considered complete when it is verifiable, unambiguous, and traceable.",What should an unambiguous requirement have?,"defined acceptance criteria, metrics for success, expected outcomes, and acceptable values"
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A requirement should be unambiguous. Given that the requirements gathering process involves contributions from multiple stakeholders, the requirement should be explicit and clear to all. It should have the same meaning to everyone involved and not be open to interpretation. Unambiguous requirements must have defined acceptance criteria, metrics for success, expected outcomes, and acceptable values. The use of an active voice in its description will make a requirement clearer. Finally, a requirement should be verifiable. Testers should be able to verify that the requirement is implemented correctly. A requirement is considered complete when it is verifiable, unambiguous, and traceable.",What should a requirement have in order to be clearer?,An active voice
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","A requirement should be unambiguous. Given that the requirements gathering process involves contributions from multiple stakeholders, the requirement should be explicit and clear to all. It should have the same meaning to everyone involved and not be open to interpretation. Unambiguous requirements must have defined acceptance criteria, metrics for success, expected outcomes, and acceptable values. The use of an active voice in its description will make a requirement clearer. Finally, a requirement should be verifiable. Testers should be able to verify that the requirement is implemented correctly. A requirement is considered complete when it is verifiable, unambiguous, and traceable.",How is a mandatory requirement considered complete?,"It is verifiable, unambiguous, and traceable."
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Consider this requirement for a report-generating solution:,What is the requirement for a report-generating solution?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","This requirement leaves room for interpretation, and this can lead to not meeting client expectations. A better representation of the requirement is to include a time frame, a responsible party, and a deliverable.",What does this requirement leave room for interpretation?,
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.","This requirement leaves room for interpretation, and this can lead to not meeting client expectations. A better representation of the requirement is to include a time frame, a responsible party, and a deliverable.",What is a better representation of the requirement?,"Include a time frame, a responsible party, and a deliverable"
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard.",Reading: Traceability in Requirements and Other Artifacts.,Reading: Traceability in Requirements and Other Artifacts?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.",What is critical in any research to have a clear and unambiguous definition of the population of interest?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.","What does it pay to be explicit, rather than vague about the nature of a population we are interested in studying?",
Collecting and Understanding Data,Data Collection,Validity and Bias,,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.","Regardless of the study design, any analysis could suffer from what?",Potential incorrect actions taken
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.",What measures how much the intended test interpretation matches the proposed purpose of the test?,Validity
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.",What is the evidence leading to the assessment of validity?,"Test content, response processes, internal structure, and the consequences of testing."
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.",What do threats to validity refer to in an observational study?,specific reasons for why we can be wrong
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.",What does the threat to validity arise from?,Inability to account for whether the observed changes can be attributed to the exposure or intervention.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.",How can we generalize that exposure causes outcomes?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,There are four types of validity:,How many types of validity are there?,Four
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.",What refers to the appropriate use of statistical conclusion validity?,Statistical conclusion validity
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.",What does Statistical conclusion validity refer to?,The appropriate use of statistics
Collecting and Understanding Data,Data Collection,Validity and Bias,,Construct validity refers to the validity of inferences about the constructs (or variables) in the study.,What refers to the validity of inferences about constructs?,Construct validity
Collecting and Understanding Data,Data Collection,Validity and Bias,,Internal validity relates to the validity of inferences drawn about the cause-and-effect relationship between the independent and dependent variables.,How does internal validity relate to the validity of inferences drawn about the cause and effect relationship between the independent and dependent variables?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"External validity refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures.","What refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures?",External validity
Collecting and Understanding Data,Data Collection,Validity and Bias,,"In this module, we will discuss external and internal validity in more detail.",In what module will we discuss external and internal validity in more detail?,Module
Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.",What do data scientists do once we have defined the population of interest for the study?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.",What do we need to do to determine the impact of class size on high school student achievement?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.","How do we determine whether we want to study students in public schools, private schools, charter schools, etc?",
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.",What is the term for bias?,Sampling bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.","What does a study measure the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender undermine?",The external validity of the study
Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.",What do we want to conduct as data scientists?,"Sound research that produces meaningful, impactful, or novel results for stakeholders."
Collecting and Understanding Data,Data Collection,Validity and Bias,,"As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.",What is a problem in drawing correct inferences about the difference between the presumed treatment variable and the outcome?,A causal relationship
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.",What is the table 1 adapted from?,Creswell (2012)
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.",What does the Table 1 show?,"Threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a"
Collecting and Understanding Data,Data Collection,Validity and Bias,,Type of Threat to Internal Validity,What type of Threat to Internal Validity relates to internal validity?,Type of Threat to Internal Validity relates to internal validity.
Collecting and Understanding Data,Data Collection,Validity and Bias,,Description,What is the name of the story?,The story is called The Case of Being A Joke.
Collecting and Understanding Data,Data Collection,Validity and Bias,,Description,What does the description of a story show?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,Suggested response by Data Scientist,Who suggested a response from Data Scientist?,Laura
Collecting and Understanding Data,Data Collection,Validity and Bias,,Suggested response by Data Scientist,What type of response did Data Scientist suggest?,He suggested that you provide a more specific response.
Collecting and Understanding Data,Data Collection,Validity and Bias,,History,What is the history of history?,What is the history of history?
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.",What happens between the beginning and end of the experiment?,Time passes
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.",What is impossible in educational experiments to have a tightly controlled environment?,It is impossible to have a tightly controlled environment and monitor all events.
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can have the control and experimental groups experience the same activities (except for the treatment) during the experiment.,Who can have control and experimental groups experience the same activities during the experiment?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,Maturation,What is the name of the Maturation?,"The name of the Maturation is ""Phoenix""."
Collecting and Understanding Data,Data Collection,Validity and Bias,,Maturation,What is a maturation?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.",What happens during the experiment?,Individuals develop or change
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.",What is the difference between the pre-test and post-test?,Scores
Collecting and Understanding Data,Data Collection,Validity and Bias,,A careful selection of participants who mature or develop in a similar way for both the control and experimental groups helps guard against this problem.,What does a careful selection of participants help guard against?,This problem
Collecting and Understanding Data,Data Collection,Validity and Bias,,Regression to the mean,Regression to what is the mean?,The mean
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.",What are participants with extreme scores selected for the experiment?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.",What are the results of the experiment that will likely change during the test?,Scores
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can select participants who do not have extreme scores as entering characteristics for the experiment.,Who can select participants who do not have extreme scores as entering characteristics for the experiment?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,Selection,What is the name of the selection process?,The Selection Process
Collecting and Understanding Data,Data Collection,Validity and Bias,,Selection,What type of selection does the Selection process generate?,General
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)",What characteristics do participants have that predispose them to have?,Certain characteristics
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)","What does a person's cognitive ability, receptiveness to treatment or familiarity with a treatment do?",Predispose them to have certain outcomes.
Collecting and Understanding Data,Data Collection,Validity and Bias,,Random selection may partly address this threat.,What may be a part of the threat of random selection?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,Random selection may partly address this threat.,What may a random selection address?,Threat
Collecting and Understanding Data,Data Collection,Validity and Bias,,Mortality (also called study attrition),What is a term for the term study attrition?,Mortality
Collecting and Understanding Data,Data Collection,Validity and Bias,,Mortality (also called study attrition),What is the term for Mortality?,Study attrition
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.",What kind of reason did participants drop out during the experiment?,Any number of reasons
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.",What may be difficult to draw conclusions from?,Scores
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.,Who can recruit a large sample to account for potential dropouts?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.,Who can compare the outcome of those who drop out with?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,Diffusion of treatments (also called cross-contamination of groups),What is the term for diffusion of treatments?,Cross-contamination of groups
Collecting and Understanding Data,Data Collection,Validity and Bias,,Diffusion of treatments (also called cross-contamination of groups),What is a term for cross contamination of groups?,Diffusion of treatments
Collecting and Understanding Data,Data Collection,Validity and Bias,,Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.,What group of participants communicates with each other?,Control and experimental
Collecting and Understanding Data,Data Collection,Validity and Bias,,Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.,What can influence how both groups score on outcomes?,Communication
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist must keep the two groups as separate as possible during the experiment.,What must the data scientist keep the two groups as separate as possible during the experiment?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory equalization,Compensatory equalization equalization is what type of equalization?,CFP
Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory equalization,What kind of equalisation is equalized?,Compensatory
Collecting and Understanding Data,Data Collection,Validity and Bias,,"When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.","When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study?",
Collecting and Understanding Data,Data Collection,Validity and Bias,,"When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.",The benefits of the experimental treatment need to be equally distributed among what group?,the study
Collecting and Understanding Data,Data Collection,Validity and Bias,,"The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.",Who can provide benefits to both groups?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,"The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.",What can the data scientist give to the control group after the experiment ends?,treatment
Collecting and Understanding Data,Data Collection,Validity and Bias,,"The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.",Who can give a different type of treatment?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory rivalry,What kind of rivalry does Compensatory rivalry rivalry create?,It is a type of rivalry created by Compensatory rivalry.
Collecting and Understanding Data,Data Collection,Validity and Bias,,Compensatory rivalry,What is a rivalry between rivalry?,It is a rivalry between rivalry in different fields.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Participants in the control group feel that they are being devalued, as compared to the experimental group, because they do not experience the treatment.",What group feel they are being devalued as compared to the experimental group?,control group
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.,What can the data scientist try to avoid?,This threat
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.,What can a data scientist attempt to reduce?,Awareness and expectations of the presumed benefits of the experimental treatment.
Collecting and Understanding Data,Data Collection,Validity and Bias,,Resentful demoralization,What kind of demoralization is a result of?,Resentful
Collecting and Understanding Data,Data Collection,Validity and Bias,,Resentful demoralization,What is the result of resentful demoralisation?,A result of resentful demoralization is a greater sense of relief.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"When a control group is used, individuals in this group may become resentful and demoralized because they perceive that they receive a less desirable treatment than other groups.","When a control group is used, people in the control group may become resentful and demoralized because they perceive that they receive what?",a less desirable treatment than other groups
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.,Who can provide treatment to this group after the experiment has concluded?,The data scientist
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.,What is the purpose of the data scientist?,Provide treatment to this group
Collecting and Understanding Data,Data Collection,Validity and Bias,,Testing,What is the name of the test?,The Test of Independence
Collecting and Understanding Data,Data Collection,Validity and Bias,,Testing,What does testing do?,It does what it says
Collecting and Understanding Data,Data Collection,Validity and Bias,,Participants become familiar with the outcome measure and remember responses for later testing,What do participants learn about the outcome measure?,Responses
Collecting and Understanding Data,Data Collection,Validity and Bias,,Participants become familiar with the outcome measure and remember responses for later testing,What are the participants able to remember?,Responses
Collecting and Understanding Data,Data Collection,Validity and Bias,,"To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.",What can the data scientist use to measure the outcome of a test?,Different items on the post-test.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.",What can a data scientist do to overcome the threat?,Metricate the outcome less frequently and use different items on the post-test than those used
Collecting and Understanding Data,Data Collection,Validity and Bias,,Instrumentation,What is the name of instrumentation?,The name of instrumentation is The following is a word of instrumentation.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.",What changes the instrument between a pre-test and post-test?,
Collecting and Understanding Data,Data Collection,Validity and Bias,,"The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.",What affects the results of the outcome?,The instrument changes between a pre-test and post-test.
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.,What does the data scientist need to standardize?,Procedures
Collecting and Understanding Data,Data Collection,Validity and Bias,,The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.,What must be used throughout the experiment?,Observational scales or instrument
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.",What is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters?,Statistical bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.","Bias made can be consciously or unconsciously, and it will affect what?",Performance of a data science model.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.",What is the result of violations of external validity or internal validity of a study?,Statistical bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.",What does sampling bias undermine in a previous module?,External validity
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.",What happens when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about?,Selection Bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.",What is usually a concern of studies using convenience samples?,Selection bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",What occurs when individuals select themselves to be included in a study?,Self-selection Bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",Self-selection bias is a threat to what?,The external validity of the study
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",What is often associated with certain characteristics of the sample that induce such individuals to be in the resulting study sample?,Self-selection bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.",What can a role play in the data that is used to build your analytic solution?,"Your prior knowledge, beliefs, and values."
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.","What is a bias that occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses?",Confirmation Bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.",What is information bias also known as?,Measurement bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.",What is an example of information bias?,Misclassification of observations
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.",When is an observation with attributes similar to the stereotypical female student recorded as female?,When it is actually from a male student.
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.",How are groups under the age of 45 seen as low risk?,during a screening exercise
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.",What happens when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable?,Confounding Bias
Collecting and Understanding Data,Data Collection,Validity and Bias,,"Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.",What is an exogenous factor that causes subject matters of interest?,A potentially confounding variable
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.",What are two approaches to evaluating clustering?,Internal and External
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.",What is a ground truth classification?,Empirical evidence or data that is provable.
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.",How can clustering be evaluated?,Expert
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:",What does Internal Evaluation evaluate?,Clusters with high similarity within the cluster and high dissimilarity with other clusters.
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:",What is the cluster with the best score seen to be?,the best
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:",How can one clustering technique perform against another?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:",Why does k-means clustering only find convex?,Clusters are not convex
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).",The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).",What is the mean intra-cluster distance and the mean nearest cluster distance for each data point?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).","When the silhouette coefficient is close to 0, there is a presence of what?",overlapping clusters
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter",What is Dunn Index similar to?,Silhouette coefficient
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter",What is a good clustering?,A higher Dunn index
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter",How is the Dunn index computed?,By calculating the distance between each data point in a cluster and others in different clusters
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter",When is the compactness of a cluster measured?,max.diameter
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.",What is expected to be the diameter of the clusters?,Small
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.",What should the Dunn index be maximized?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,See https://en.wikipedia.org/wiki/Dunn_index for more details.,"For more details, see https://en.wikipedia.org/wiki/Dunn_index?",
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,External Evaluation measures the results from a clustering task based on what?,Data not used for the clustering task
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,Benchmarks are set from what set of pre-classified data?,A
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.,External evaluation techniques need what to evaluate clustering?,ground truth data
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,Rand Index tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:,What does Rand Index tell you how similar a cluster or clusters are to a set benchmark?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,Rand Index tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:,How can you calculate the Rand index?,As a result of a cluster or clusters' comparison technique.
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,(TP + TN)/(TP+FP+FN+TN),What does TP + TN mean?,TP + TN means (TP+FN+TN) or
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,(TP + TN)/(TP+FP+FN+TN),What is TP+FP+FN+TN?,(TP+FP+FN+TN)
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.",What is a no-frills technique that assigns each cluster to a class?,Purity
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.",How is the number of correctly assigned observations divided by the overall number of observations?,To determine accuracy
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.",A large number of clusters can lead to what?,a higher purity
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.",What can be used to measure and compare the quality of clustering between different clusterings?,Normalized mutual information (NMI)
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Jaccard Index is used in cluster analysis evaluation. It is defined as ""the size of the intersection divided by the size of the union of the sample sets."" The Jaccard distance measures dissimilarity between sample sets.",What is used in cluster analysis evaluation?,Jaccard Index
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Jaccard Index is used in cluster analysis evaluation. It is defined as ""the size of the intersection divided by the size of the union of the sample sets."" The Jaccard distance measures dissimilarity between sample sets.",What is defined as the size of the intersection divided by?,Union of the sample sets
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"Jaccard Index is used in cluster analysis evaluation. It is defined as ""the size of the intersection divided by the size of the union of the sample sets."" The Jaccard distance measures dissimilarity between sample sets.",The distance measured dissimilarity between sample sets?,Jaccard distance
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"F-Measure is simply computed as the \\(\\frac{2*Precision*Recall}{Precision+Recall}\\). You might remember it from the classification metrics, it is also known as the F1 score.",What is F-Measure simply computed as?,(frac2*Precision*RecallPrecision+Recall
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"F-Measure is simply computed as the \\(\\frac{2*Precision*Recall}{Precision+Recall}\\). You might remember it from the classification metrics, it is also known as the F1 score.",What is the F1 score?,
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",What is the Dice Index also known as?,Sorensen-Dice index or Dice Coefficient
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",How many samples can the Sorensen-Dice index assess?,Two
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,,"The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",What is a semi-metric version of the Jaccard index?,The dice index
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,It would be helpful to think about what you would like to showcase in your diagrams. The following classifications of diagrams can help you get started.,What would be helpful to think about in your diagrams?,What you would like to showcase
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,It would be helpful to think about what you would like to showcase in your diagrams. The following classifications of diagrams can help you get started.,What type of classification can help you get started?,The following classifications of diagrams.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules And Their Relationships - How code is structured,What are Modules and Their Relationships?,How code is structured
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules And Their Relationships - How code is structured,How is code structured?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules And Their Relationships - How code is structured,What is the purpose of module structure?,To help guide code.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Runtime Components and Connections - How data flow during runtime,How does data flow during runtime Components and Connections?,How
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Runtime Components and Connections - How data flow during runtime,What is the purpose of running time components and connections?,To help data flow during runtime components and connections.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Deployment Diagrams - What is Infrastructure for the architecture,What is Infrastructure for the architecture?,Deployment Diagrams
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Deployment Diagrams - What is Infrastructure for the architecture,What is the name of the diagram for the diagrams for the Diagrams?,Infrastructure
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"You can use these types of diagrams at any layer of abstraction, i.e., you can use these types of diagrams to describe  a high-level view of the system or a sub-system, as is apparent in the exemplar documentation.",What can you use to describe a high-level view of the system or sub-system?,Diagrams
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"You can use these types of diagrams at any layer of abstraction, i.e., you can use these types of diagrams to describe  a high-level view of the system or a sub-system, as is apparent in the exemplar documentation.",What type of diagrams can be used at any layer of abstraction?,These types of diagrams can be used at any layer of abstraction.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"These diagrams are useful if you would like to show modules and their relationships with each other. Modules represent a static way of structuring the system. In these diagrams, we do not care much about how the system behaves at runtime.",What are diagrams useful if you want to show modules and their relationships with each other?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"These diagrams are useful if you would like to show modules and their relationships with each other. Modules represent a static way of structuring the system. In these diagrams, we do not care much about how the system behaves at runtime.",What is a static way of structuring the system?,Modules
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Modules allow us to answer questions like:,,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What are the different business functions in your system?,What are the different business functions in your system?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What other modules does each business function depend on?,What other module does each business function depend on?,ecommerce module
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What other modules does each business function depend on?,What other modules does a business function use?,.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Are there any external dependencies for each module?,What are there external dependencies for each module?,yes
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Are there any external dependencies for each module?,What does each module have?,External Dependencies
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Does any module inherit behavior from another module?,What behavior does a module inherit from another module?,Yes.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 1. Execution Graph of a Completed Run of the Continuous Training Pipeline (ACAI, MCDS Capstone Project, 2020)",,nan
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Notice how model dependency is represented in this execution graph, as it is important to this project.",How is model dependency represented in this execution graph?,Notice how model dependency is represented in this execution graph.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Notice how model dependency is represented in this execution graph, as it is important to this project.",What is important to this project?,model dependency
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"When you want to showcase how a system functions during runtime, you can use Components and Connectors. Each runtime element is related to another element via a Connector, which should be adequately described in your writeup as well as in the legend.",What can you use when you want to showcase how a system functions during runtime?,Components and Connectors
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"When you want to showcase how a system functions during runtime, you can use Components and Connectors. Each runtime element is related to another element via a Connector, which should be adequately described in your writeup as well as in the legend.",What is the name of the element that is related to another element?,Connector
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Examples of runtime elements, i.e., components are:",What are some examples of runtime elements?,:
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Services,What is the name of the service that is offered by the service provider?,The name of the service that is offered by the service provider is The Name of the service that
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Peers,What is the name of the person who is a member of the group?,Peers
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Clients,What is the name of the client?,The client's name is Peter
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Clients,Who is the client who is a customer?,The client is named John.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Clients,What type of client does the client have?,A client that has a history of client is a client who has had many clients over
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Servers,What is the name of a server?,Server Name
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Servers,What type of server does the server have?,A server can be a server of any type.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Filter Systems,What is the name of the Filter System?,The Filter System Name is The name of the Filter System.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Examples of connectors also called vehicles of communication are:,What are some examples of connectors called?,Vehicles of communication
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Call-return style connector,What is a call-return style connector?,A call-return style connector.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Data pipelines,What is the name of the data pipeline?,The Data Pipeline
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Data pipelines,What are data pipelines?,"referred to as data pipelines or simply simply, data pipelines are basically anything you can"
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Process synchronization operators,What is the name of the process synchronization operator?,The name of the process synchronization operator is Y.E.D.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Process synchronization operators,What are the processes synchronized with?,The processes are synchronized with the use of the latest technologies.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,These diagrams help us answer the following type of questions:,What diagram helps us answer the following types of questions?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What are the major components of your system at runtime?,What are the major components of your system at runtime?,-
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?",What does the system have?,Shared data stores
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?",What is the nature of the shared data stores?,Yes
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,How does data progress through the system?,How does data progress through the system?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Does anything run in parallel?,What does something run in parallel?,yes
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Does anything run in parallel?,What is the name of a parallel running system?,Streaming Parallel
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,This is a good example of using components and connectors to provide a high-level view of the flow of data. We can see that connection types have been labeled separately to represent the different types of data transfer that can happen in the system.,What is a good example of using components and connectors to provide a high-level view of the flow of data?,
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,This is a good example of using components and connectors to provide a high-level view of the flow of data. We can see that connection types have been labeled separately to represent the different types of data transfer that can happen in the system.,What are connection types labeled separately to represent?,Different types of data transfer that can happen in the system.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 2. A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020)",What is a ML Workflow in ACAI?,A ML Workflow in ACAI
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 2. A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020)",What is the MCDS Capstone Project?,A Typical ML Workflow in ACAI (ACAI)
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Representing deployment models can be confusing because they look very similar to runtime diagrams.,How can deployment models look?,very similar to runtime diagrams
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Representing deployment models can be confusing because they look very similar to runtime diagrams.,What can be confusing?,Representing deployment models
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"The important distinction between the two is that deployment diagrams are meant to display the interaction of the solution with non-software structures like CPUs, file systems, networks, development teams, etc.",What is the important distinction between the two?,deployment diagrams
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"The important distinction between the two is that deployment diagrams are meant to display the interaction of the solution with non-software structures like CPUs, file systems, networks, development teams, etc.",What are deployment diagrams meant to display?,the interaction of the solution with non-software structures
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"In a deployment model, you will need to make infrastructural considerations for your solution. Your deployment model will help answer questions like:",What type of considerations will you need to make infrastructural considerations for your deployment model?,Infrastructural considerations will need to be made for your deployment model.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What cloud instance type does your solution execute on?,What type of instance type does your solution execute on?,Cloud
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What cloud instance type does your solution execute on?,What is the name of the cloud instance type?,Azure
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What type of cloud data store are you expected to use?,What type of data store are you expected to use?,Cloud
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What type of cloud data store are you expected to use?,What is the name of the cloud data store that is expected to be used?,Azure
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,Who deploys the solution?,Who deploys the solution?,Who deploys the solution?
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,What type of queuing system are you expected to use?,What type of system are you expected to use?,Queuing
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"In the following example, we know that we have docker containers that interact with a Job Monitor, a Log Server, and a Launcher.","What container interacts with a Job Monitor, Log Server, and a Launcher?",docker container
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"A Job Registry requires the use of an SQL server. In your diagrams, you can be specific about particular SQL servers like MySQL/Postgres if the requirement is clear.",What does a Job Registry require?,The use of an SQL server.
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"A Job Registry requires the use of an SQL server. In your diagrams, you can be specific about particular SQL servers like MySQL/Postgres if the requirement is clear.",What is a specific SQL server in your diagrams?,MySQL/Postgres
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.,"Figure 3. Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020)","What is the Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020)?",Figure 3.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Jitendra Malik, a computer vision pioneer, proposed the 'Three R's' as the classic problems of computational vision: reconstruction, recognition, and (re)organization. You might have come across many computer vision applications in your daily life, which are most likely attempts to solve one of the three Rs. In this section, we brief a few common applications of computer vision.",Who proposed the 'Three R's' as the classic problems of computational vision?,Jitendra Malik
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Jitendra Malik, a computer vision pioneer, proposed the 'Three R's' as the classic problems of computational vision: reconstruction, recognition, and (re)organization. You might have come across many computer vision applications in your daily life, which are most likely attempts to solve one of the three Rs. In this section, we brief a few common applications of computer vision.",What is the most likely attempt to solve one of the three Rs?,computer vision applications
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Jitendra Malik, a computer vision pioneer, proposed the 'Three R's' as the classic problems of computational vision: reconstruction, recognition, and (re)organization. You might have come across many computer vision applications in your daily life, which are most likely attempts to solve one of the three Rs. In this section, we brief a few common applications of computer vision.","In this section, we brief a few common applications of what?",computer vision
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Optical character recognition (OCR),What is Optical Character Recognition?,Optical character recognition (OCR)
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Optical character recognition (OCR),What does OCR mean?,Optical character recognition
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 4. LeNet 5.,Figure 4. LeNet 5. What does LeNet 5 do?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.",What is a technology that converts images of text into actual text?,optical character reader (OCR)
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.","What type of text can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text? What is OCR a commonly used method to digitalize printed text to reduce?",image
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Object Recognition,What is Object Recognition?,An application for users who want to recognize objects
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Object Recognition,What is the name of the object recognition?,"The name of the object recognition is The Name of the object recognition is ""Cause of"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Object recognition is a computer vision technique for identifying objects in images or videos. It might be relatively easy for a human to look at a photograph or watch a video and spot people, objects, scenes, and visual details. However, it is not as straightforward for a computer not only to recognize the items but also to understand what the items mean to the level of understanding of a human. Object recognition is a key technology behind driverless cars (Figure 5), enabling them to recognize a stop sign or to distinguish a pedestrian from a lamp post. It is also valuable for disease identification in biological imaging, industrial inspection, and robotic vision applications, just to name a few.",What is a computer vision technique for identifying objects in images or videos?,Object recognition
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Object recognition is a computer vision technique for identifying objects in images or videos. It might be relatively easy for a human to look at a photograph or watch a video and spot people, objects, scenes, and visual details. However, it is not as straightforward for a computer not only to recognize the items but also to understand what the items mean to the level of understanding of a human. Object recognition is a key technology behind driverless cars (Figure 5), enabling them to recognize a stop sign or to distinguish a pedestrian from a lamp post. It is also valuable for disease identification in biological imaging, industrial inspection, and robotic vision applications, just to name a few.","What can a human look at a photograph or watch a video and spot people, objects, scenes, and visual details?",It might be relatively easy to look at a photograph or watch a video and spot people
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 5. Object recognition technology from Teslas Autopilot and full self-driving capabilities.,What is the name of the Object Recognition technology used by Teslas Autopilot?,Figure 5
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Face Recognition,What does the recognition of face recognition do?,Does some damage to the face recognition system
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 6. Apples Face ID.,What is the name of Apple's face ID?,"The name of Apple's face ID is ""Alice""."
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.",What is a specific case of object detection?,Facial recognition
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.",What does facial recognition perform?,Rectification of the detected face
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.",How do facial recognition systems search for standard features and landmarks?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Image augmentation applications on various social media services, such as Snapchat, use an algorithm to detect faces and perform augmentation to swap faces in an image for entertainment (Figure 7).",What is the name of a social media service that uses an algorithm to detect faces?,Snap
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Image augmentation applications on various social media services, such as Snapchat, use an algorithm to detect faces and perform augmentation to swap faces in an image for entertainment (Figure 7).",What does the algorithm use to swap faces in an image for?,entertainment
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 7. Snapchats Face Swap.,What is the name of the Snapchats Face Swap?,Figure 7
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Figure 7. Snapchats Face Swap.,What does Snapchats face Swap do?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Vision-based Biometrics,What is based on vision-based biometrics?,A measure of success is based on vision-based biometrics.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,Vision-based Biometrics,What is an example of a biometric?,Vision based Biometrics
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Figure 8. Sharbat Gula, photographed in 1984 and 2002.",Who photographed Sharbat Gula in 1984 and 2002?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Sharbat Gula, one of the students in an informal school at the Nasir Bagh refugee camp in Afghanistan, was identified 18 years later from a photograph taken in 1984 when she was 12, using an analysis of her iris pattern image (Figure 8). Daugman (2004) computed IrisCodes from both of her eyes from the photograph in 1984 and 2002, respectively, and matched them using a Hamming Distance to confirm that the images are of the same person. You can read more about the story here.",What year was Sharbat Gula identified from a photograph taken at the Nasir Bagh refugee camp in Afghanistan?,1984
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Sharbat Gula, one of the students in an informal school at the Nasir Bagh refugee camp in Afghanistan, was identified 18 years later from a photograph taken in 1984 when she was 12, using an analysis of her iris pattern image (Figure 8). Daugman (2004) computed IrisCodes from both of her eyes from the photograph in 1984 and 2002, respectively, and matched them using a Hamming Distance to confirm that the images are of the same person. You can read more about the story here.",What year did Daugman compute IrisCodes from both of Gula's eyes from the photograph?,2004
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,,"Sharbat Gula, one of the students in an informal school at the Nasir Bagh refugee camp in Afghanistan, was identified 18 years later from a photograph taken in 1984 when she was 12, using an analysis of her iris pattern image (Figure 8). Daugman (2004) computed IrisCodes from both of her eyes from the photograph in 1984 and 2002, respectively, and matched them using a Hamming Distance to confirm that the images are of the same person. You can read more about the story here.",How many years later did Gula find a student in an informal school?,18
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.",What is the process of collecting and organizing data that can meet defined business and analytic objectives?,Data collection
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.",What can be compared to the data collection process in scholarly research?,The data collection process in the data science lifecycle.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.",How should data collection be conducted to ensure that the data are valid and reliable?,systematically
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:",What is a professional who works with data important to know about?,Where data comes from
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:",What are two key types of data?,: raw data and structured data.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:",How do we know where data come from?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,Organic or process data,What is the name of the data that can be found in organic or process data?,"The name of the data that can be found in organic or process data is ""Adam."""
Collecting and Understanding Data,Data Collection,Where do data come from?,,Data collected from a designed study,What is the purpose of collecting data from a study?,To analyze a study.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.","What type of data is generated by an automated computerized information system or extracted from images, video, or audio recordings?",Organic or process data
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.",What is the result of some process continuously or over a period of time?,Organic data
Collecting and Understanding Data,Data Collection,Where do data come from?,,Examples of organic or process data:,What are examples of organic or process data?,X-rays
Collecting and Understanding Data,Data Collection,Where do data come from?,,Financial or stock market exchange transactions,What type of transactions do financial or stock market exchange transactions involve?,"Transactions involve either stock market exchange, or a combination of financial or stock market exchange transactions"
Collecting and Understanding Data,Data Collection,Where do data come from?,,Web browser history,What is the history of a browser?,Web browser history
Collecting and Understanding Data,Data Collection,Where do data come from?,,Web or mobile application activity history,What is a web or mobile application activity history?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,Netflix viewing history,What is Netflix's history of viewing?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,Netflix viewing history,What is the history of Netflix viewing history?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,Surveillance camera video recordings,What is the name of surveillance camera video recordings?,"The name of surveillance camera video recordings is ""ASCAP"""
Collecting and Understanding Data,Data Collection,Where do data come from?,,Surveillance camera video recordings,What does surveillance camera recording do?,It gathers information about you.
Collecting and Understanding Data,Data Collection,Where do data come from?,,The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.,What term refers to organically produced data from automated processes over time in massive quantities?,Big datad
Collecting and Understanding Data,Data Collection,Where do data come from?,,The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.,What does cbig datad mean?,These types of datasets comprising organically produced data from automated processes over time.
Collecting and Understanding Data,Data Collection,Where do data come from?,,The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.,How do data scientists mine these data to study trends?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.",What is the name of a study that collects data from specific studies?,Designated study
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.",What are the main differences between this type of data and organic data?,The main difference between this type of data and organic data is that data collected from a designed
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.",How are data collected from a designed study collected?,based on a certain guided agenda
Collecting and Understanding Data,Data Collection,Where do data come from?,,Here are examples of data that can be collected from a designed study:,What are examples of data that can be collected from a designed study?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.",What type of questionnaires are used to collect data from a group of individuals?,Questionnaires and surveys.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.",How can questionnaires be administered on paper or online?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.",What is a consideration with questionnaire data as errors can occur?,Data cleaning
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.",What is an open-ended question-answering dialog between an interviewer and one or more interviewee?,Interviews.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.","How are interviews guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked and the space to take notes during the interview?",
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.",What is the process of collecting open-ended information by observing people and places at a research site?,Observation
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.",What can support or complement the data collected during interviews and from questionnaires?,Data collected during these observations.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.",What can be used to collect shared understanding from several individuals as well as to get views from specific people?,Focus groups
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.","What is a process of collecting data through interviews with a group of people, typically four to six?",A focus group interview
Collecting and Understanding Data,Data Collection,Where do data come from?,,"Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.",When are interviewees similar to and cooperative with each other?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.",What type of data is collected from a design data collection?,This type of data is collected from a design data collection.
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.",What is an example of an automated process that can be used to collect data?,A design data collection process
Collecting and Understanding Data,Data Collection,Where do data come from?,,"As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.",How do we want to ask a group of individuals that are sampled from all the Netflix viewers?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,Figure 1: Drawing a representative sample from the population. (Source: https://www.voxco.com/),Figure 1: Drawing a representative sample from what population?,the population
Collecting and Understanding Data,Data Collection,Where do data come from?,,"In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.","In figure 1, what is a population of interest?",
Collecting and Understanding Data,Data Collection,Where do data come from?,,"In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.",What is another way to leverage process data in a study?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.",What are the key differences in the data collected through a designed study?,
Collecting and Understanding Data,Data Collection,Where do data come from?,,"You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.",What are very rigorously designed data collections that people might be interested in?,Such data
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?",What is used to draw conclusions with inferential statistics?,data
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?",What is the purpose of developing an analytic solution?,Statistical modeling
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?",How can a mathematical model understand variables that are not numeric?,
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.",What is a numeric representation of a part of the raw data?,A feature
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.",What does Wikipedia describe a feature as?,An individual measurable property or characteristic of an observation
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.",How are features represented?,In a way that a machine learning model can use them.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.","When raw data is transformed into features, what must a data scientist consider?",The right features that are useful for the data science task.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.",What is a good feature that is appropriate to the statistical modeling technique and data science task?,Feature that is appropriate to the statistical modeling technique and data science task.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.","If you are performing a predictive task, what should your features have?",Predictive values
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.",What is an important task in the data science project life cycle?,Transforming or processing features from data
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.",What is a costly one that rears its head when you are training your model?,The price for badly selected features
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.",The snowball effect of badly selected features will end up leading decision-makers down what path?,Wrong path
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018)),,
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.",What is the process of extracting features from raw data and transforming them into suitable formats for a machine learning model?,Feature engineering
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.",What leads to higher quality models and better insights for decision-makers?,Feature engineering
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.",Feature engineering is not a one size fits all process?,It is dependent on the analytic objective and the data.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.",What does the data scientist remove from the data that does not provide task-specific information?,Features
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.",What is the term for feature selection?,Features that introduce redundancy.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.",What is a feature that is in numeric form?,Raw data
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.",What is the reason that raw data must meet the assumptions of the chosen model?,feature engineering
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","Scalar: Single numeric feature, e.g., mass.",,
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.,What is a vector defined as?,An object that has both a magnitude and direction.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.,What is an object that has a magnitude and direction?,Ordered list of scalars
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features",Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.,What is a collection of vectors that can be added or multiplied by scalars?,Vectors
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","In machine learning, the input to a model is represented as a numeric vector.",What is the input to a model represented in machine learning?,A numeric vector.
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"Raw Data to Features,Features","In machine learning, the input to a model is represented as a numeric vector.",What is a numeric vector?,input to a model
Data Science Project Planning,Developing a Vision,Developing a Vision,,"To develop a vision for the project, the project team works with stakeholders to develop a vision document and related artifacts that represent the high-level scope and purpose of the project. The vision document communicates the motivation for the project - what problems does the proposed undertaking attempt to solve, and for what data set(s)? What is novel or unique about the proposed method? How will success be measured and evaluated? What is the anticipated impact (business or research value) if the project is successful? An explicit statement of the problem, proposed solution, and high-level features of the project can establish clear expectations and reduce potential risks.",What does the project team work with to develop a vision document?,Stakeholders
Data Science Project Planning,Developing a Vision,Developing a Vision,,"To develop a vision for the project, the project team works with stakeholders to develop a vision document and related artifacts that represent the high-level scope and purpose of the project. The vision document communicates the motivation for the project - what problems does the proposed undertaking attempt to solve, and for what data set(s)? What is novel or unique about the proposed method? How will success be measured and evaluated? What is the anticipated impact (business or research value) if the project is successful? An explicit statement of the problem, proposed solution, and high-level features of the project can establish clear expectations and reduce potential risks.",What is the purpose of the project?,To help define the high-level scope and purpose of the project.
Data Science Project Planning,Developing a Vision,Developing a Vision,,"To develop a vision for the project, the project team works with stakeholders to develop a vision document and related artifacts that represent the high-level scope and purpose of the project. The vision document communicates the motivation for the project - what problems does the proposed undertaking attempt to solve, and for what data set(s)? What is novel or unique about the proposed method? How will success be measured and evaluated? What is the anticipated impact (business or research value) if the project is successful? An explicit statement of the problem, proposed solution, and high-level features of the project can establish clear expectations and reduce potential risks.",How will success be measured and evaluated?,How?
Data Science Project Planning,Developing a Vision,Developing a Vision,,"Before diving into the implementation of the project, the vision must be developed. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective. It provides context to succeeding developers about why exactly this project is necessary.","Before diving into the implementation of the project, what must be developed?",The vision
Data Science Project Planning,Developing a Vision,Developing a Vision,,"Before diving into the implementation of the project, the vision must be developed. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective. It provides context to succeeding developers about why exactly this project is necessary.","What defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution?",The vision
Data Science Project Planning,Developing a Vision,Developing a Vision,,Figure 1. Engineering Lifecycle Management (IBM),What is the name of the engineering lifecycle management system?,IBM
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,The requirements document should not only state all the hardware and software requirements of the project but also other requirements such as functional and non-functional requirements and the intended group of users that the project aims to target. The most significant sub-headings that must be covered in a requirements document have been detailed below.,What should the requirements document not only state?,All the hardware and software requirements of the project but also other requirements
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,The requirements document should not only state all the hardware and software requirements of the project but also other requirements such as functional and non-functional requirements and the intended group of users that the project aims to target. The most significant sub-headings that must be covered in a requirements document have been detailed below.,What are the most important sub-headings that must be covered in a requirements document?,
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The first section is the introduction which would either include an introduction to the project or to the upcoming sections in the document. If the project is more research-oriented, the introduction should summarize the experiments that will be conducted during the entire duration of the project. For industrial or software-oriented projects, it should describe the various interfaces that will be developed for the system. The document will also mention how testing will be done and how various modules will be run together. This would make developers think about the various sub-sections involved in the overall project. The intended users section will detail the various groups of users involved in the project and their corresponding use cases so that the requirements of each group of users can be determined beforehand. This would help develop a solution that fits all the requirements of all the various stakeholders.",What is the first section of the document?,Introduction
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The first section is the introduction which would either include an introduction to the project or to the upcoming sections in the document. If the project is more research-oriented, the introduction should summarize the experiments that will be conducted during the entire duration of the project. For industrial or software-oriented projects, it should describe the various interfaces that will be developed for the system. The document will also mention how testing will be done and how various modules will be run together. This would make developers think about the various sub-sections involved in the overall project. The intended users section will detail the various groups of users involved in the project and their corresponding use cases so that the requirements of each group of users can be determined beforehand. This would help develop a solution that fits all the requirements of all the various stakeholders.",What will be discussed in the introduction of the project?,The experiments that will be conducted during the entire duration of the project.
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The first section is the introduction which would either include an introduction to the project or to the upcoming sections in the document. If the project is more research-oriented, the introduction should summarize the experiments that will be conducted during the entire duration of the project. For industrial or software-oriented projects, it should describe the various interfaces that will be developed for the system. The document will also mention how testing will be done and how various modules will be run together. This would make developers think about the various sub-sections involved in the overall project. The intended users section will detail the various groups of users involved in the project and their corresponding use cases so that the requirements of each group of users can be determined beforehand. This would help develop a solution that fits all the requirements of all the various stakeholders.",How many modules will be run together?,various modules will be run together
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The next section in the requirements document will be the system functionality section. This section describes the goals of the project using multiple simplified diagrams, such as context diagrams and component diagrams, which can be used to illustrate these goals.",What section describes the goals of the project using multiple simplified diagrams?,System functionality
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"The next section in the requirements document will be the system functionality section. This section describes the goals of the project using multiple simplified diagrams, such as context diagrams and component diagrams, which can be used to illustrate these goals.",What can be used to illustrate these goals?,Multiple simplified diagrams.
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"While developing this document, the non-functional requirements, including various quality attributes of the system like speed, reliability, and performance, will be considered. This will help developers handpick the most important non-functional requirements for the system, which would be achievable within the time frame of the project. The use cases of the project must be kept in mind when selecting the non-functional requirements. Other constraints like hardware and software resources, monetary resources, and human effort must also be taken into account before committing to the functional and non-functional requirements of the project. The requirements document would be the preliminary attempt at defining the overall resource requirement of the project for meeting the goals of the entire project. Developers should begin thinking about a design methodology while preparing this document.",What will be considered while developing this document?,The non-functional requirements
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"While developing this document, the non-functional requirements, including various quality attributes of the system like speed, reliability, and performance, will be considered. This will help developers handpick the most important non-functional requirements for the system, which would be achievable within the time frame of the project. The use cases of the project must be kept in mind when selecting the non-functional requirements. Other constraints like hardware and software resources, monetary resources, and human effort must also be taken into account before committing to the functional and non-functional requirements of the project. The requirements document would be the preliminary attempt at defining the overall resource requirement of the project for meeting the goals of the entire project. Developers should begin thinking about a design methodology while preparing this document.",What will help developers handpick the most important non-functional requirements for the system?,This document will help developers select the most important non-functional requirements for the system.
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"While developing this document, the non-functional requirements, including various quality attributes of the system like speed, reliability, and performance, will be considered. This will help developers handpick the most important non-functional requirements for the system, which would be achievable within the time frame of the project. The use cases of the project must be kept in mind when selecting the non-functional requirements. Other constraints like hardware and software resources, monetary resources, and human effort must also be taken into account before committing to the functional and non-functional requirements of the project. The requirements document would be the preliminary attempt at defining the overall resource requirement of the project for meeting the goals of the entire project. Developers should begin thinking about a design methodology while preparing this document.",The use cases of the project must be kept in mind when selecting what?,Non-functional requirements
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,,"While developing this document, the non-functional requirements, including various quality attributes of the system like speed, reliability, and performance, will be considered. This will help developers handpick the most important non-functional requirements for the system, which would be achievable within the time frame of the project. The use cases of the project must be kept in mind when selecting the non-functional requirements. Other constraints like hardware and software resources, monetary resources, and human effort must also be taken into account before committing to the functional and non-functional requirements of the project. The requirements document would be the preliminary attempt at defining the overall resource requirement of the project for meeting the goals of the entire project. Developers should begin thinking about a design methodology while preparing this document.",Who should begin thinking about a design methodology while preparing the requirements document.,Developers
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.",What do you use to get estimates of your prediction error or loss function?,k-fold cross-validation
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.",What does k-fold cross-validation mean?,estimates of your prediction error or loss function
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.",How is it difficult to confirm that you have not learned some constant model?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.",How many features are there in a case?,Hundred
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.",What is the prediction error of a k-means cross-fold validation?,0.5
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.",Why is a trivial but cgood enoughd classifier?,Strong performance due to random associations between the features and the label.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.",What do most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to use to ensure that our systems work when deployed?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.",What can cause situations where the best-performing model on a training set might not be the best performing model on the test set?,Incertitudes
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.",What do you need to do when comparing different models and different hyper-parameters for the same model?,Take a page from statistics and do a hypothesis test on your performance measures.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.",What does a hypothesis test on your performance measures pay for?,Take a page from statistics and do a hypothesis test on your performance measures.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.",What is an example of a hypothesis test?,A statistical procedure
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.",What is a statistical procedure that allows you to test assumptions about the true distribution of your dataset?,A hypothesis test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.","Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with?",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.",What should you consider if this is the tool you need for the problem at hand?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Performing a hypothesis test involves three major steps:,How many steps are required to perform a hypothesis test?,Three
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Deciding what your Null Hypothesis, \\(H_{0}\\) and what your Alternative Hypothesis are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to cdisprove,d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.",What is the name of your Alternative Hypothesis?,(H_A)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Deciding what your Null Hypothesis, \\(H_{0}\\) and what your Alternative Hypothesis are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to cdisprove,d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.",What does (H_0 refer to?,what you wish to cdisprove
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Computing some sort of ctest-statisticd. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.",What is a measure of how unlikely the observed metric is?,ctest-statisticd
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Computing some sort of ctest-statisticd. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.",What does ctest-statisticd depend heavily on?,The distribution we assume the metric has in our problem.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.",What is the minimum likelihood threshold for failing to reject the null hypothesis?,(alpha)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.","If we are lower than (alpha), we can reject what?",The null hypothesis
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," It is important to note that crejecting the nulld does NOT mean caccepting the alternatived. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.",What does not mean caccepting the alternatived?,crejecting the nulld
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," It is important to note that crejecting the nulld does NOT mean caccepting the alternatived. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.",What does the alternative hypothesis mean?,Negation of the null
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.",What are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.",What are iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.",What can you try to separate models based on?,Their metrics
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.",What makes them useful in fields like Automated Machine Learning?,They suggest when a model is likely to perform better than another in general.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.",How do you compare models?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.",What is the first statistical test of this module?,Welchs t-test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.",What is one of the prerunners of hypothesis testing?,Welchs t-test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.",What do we want to do when we compare different metrics or other values about data or models?,Means or averages
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.",What would be nice to know if that difference is statistically significant?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.",What do we need to compare to each other?,Means
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.",What are two options?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",We could try to test if they are just different from each other with the following hypotheses:,What hypotheses could we try to test if they are different from each other?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{0}: \\mu _{\\alpha}=\\mu _{\\beta}\\),,nan
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{A}: \\mu _{\\alpha}\\neq \\mu _{\\beta}\\),What is the name of the (H_A: mu _alphaneq?,(H_A)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Or we could test that one is strictly larger than the other:,What type of test could we test that one is larger than another?,Or
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{0}: \\mu _{\\alpha}< \\mu _{\\beta}\\),(H_0: mu _alpha?,(H_0: mu__alpha)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{A}: \\mu _{\\alpha}> \\mu _{\\beta}\\),(H_A: mu _alpha>?,(H_A)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.",What is the first type of test known as?,A ctwo-tailed t-test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.",What is a cone-tailed t-test?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.",How do test statistics come with their own distribution?,Generally
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In our case, we have two averages and want to look at their differences. For the students t-test, we shall use the aptly named ct-test statisticd:",How many averages do we have in our case?,two
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In our case, we have two averages and want to look at their differences. For the students t-test, we shall use the aptly named ct-test statisticd:",What is the aptly named ct-test statisticd?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\[ T = \\frac{\\mu_\\alpha - \\mu_\\beta}{\\sqrt{\\frac{\\sigma^2_\\alpha}{n_\\alpha} + \\frac{\\sigma^2_\\beta}{n_\\beta}}} \\],What does [ T = fracmu_alpha - mum_betasqrtfracsigma2_alta?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In particular, we are going to use what is known as cWelchs t-test statistic,d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:",What is used when we have two averages with potentially different variances?,cWelchs t-test statistic
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","In particular, we are going to use what is known as cWelchs t-test statistic,d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:",What is a more conservative estimate of the normal distribution?,T-Distribution
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\[ \\nu = \\frac{(\\sigma^2_\\alpha + \\sigma^2_\\beta)^2}{\\frac{\\sigma^4_\\alpha}{n^2_\\alpha(n_\\alpha-1)} + \\frac{\\sigma^4_\\beta}{n^2_\\beta(n_\\beta-1)}} \\],,nu = (sigma2_alpha +
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.",What is the probability that our null hypothesis holds?,p-value
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.",What does the p-value mean?,The probability that our null hypothesis holds
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.",What does this test give us the first method of comparing different model performances?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.",How can we create multiple sets of test and training datasets and try this test on two models to see if they have differing performance?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.",What are some problems with this testing procedure?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.",What does a t-distribution mean about the distribution of our metrics?,It means that our metrics follow a t-distribution.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.",What can we do without heavily segmenting the dataset?,use the test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.",What does a k-fold CV do to each section?,A k-fold CV applies to each section.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To help combat some of the issues with Welchs t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\\text{accuracy}\\).",What test does the McNemar test compare?,The error of two different models
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To help combat some of the issues with Welchs t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\\text{accuracy}\\).",What is the name of the test that compares the error of two different models?,McNemar test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:",What does the error of model (alpha) be?,(E_alpha)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:",What is the result of the error in model '(beta)?,The result is that the error in model '(beta) is not enough.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{0}: E _{\\alpha}=E _{\\beta}\\),,nan
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{A}: E _{\\alpha}\\neq E _{\\beta}\\),,nan
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this test, our test metric is actually much simpler:",What is our test metric for this test?,-100 % more than a quarter of a test metric
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this test, our test metric is actually much simpler:",What is the difference between the test and the test? What is a test meter for this testing?,a test meter is actually much simpler
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\[ \\frac{(|E_\\alpha - E_\\beta|-1)^2}{E_\\alpha + E_\\beta} \\],[ frac(|E_alpha - E_beta|-1)2E_'alpha + E___?,____
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.",What is the corrected McNemar Test?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.",What test helps when we compare high-accuracy measures?,McNemar Test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.",What does this metric follow instead of following a t-distribution?,Chi-Squared distribution
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.","If you have at least 25 misclassified examples, this test is suitable for your data?",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.",What is the standard Welchs t-test?,Assumes far less about accuracy than the standard Welchs t-test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.",What does this test assume about accuracy?,Far less
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.",How does the test assume the samples you have tested are independent?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welchs or another paired t-test variety.",What does this test only work for?,accuracy values
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welchs or another paired t-test variety.",What do you need to do when you are trying to compare other loss metrics?,Use Welchs or another paired t-test variety.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","However, both of these tests do suffer a single, incredibly critical flaw.",What do both of these tests suffer from?,"A single, incredibly critical flaw"
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","However, both of these tests do suffer a single, incredibly critical flaw.","What does a single, critical flaw do?",Suffers two critical flaws.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Namely, they suffer from cp-hacking.d",Who suffers from cp-hacking.d?,"Namely, they suffer from cp-hacking.d."
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.",What is the probability that our null hypothesis implies the result we have?,p-value
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.",What does the p-value mean?,Probability that our null hypothesis implies the result we have.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.",What is one way to see the flipping of a heavily weighted coin?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.",What percentage of the time does one side of the coin come up?,(alpha) percent of the time
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.",How many times does the other side of coin come out?,100
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.",When is the null hypothesis rejected?,When
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.",What would lead to a false comparison?,Such a result would lead to a false comparison.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.",What is a result of a falsified comparison? What is the result of the false comparison in research?,A statistically significant conclusion
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For our problem, this is especially grave. Consider that, for n models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.",What type of comparison would we want to perform for n models?,(nchoose 2)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For our problem, this is especially grave. Consider that, for n models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.",What does the chances of having a poor comparison skyrocket as the number of models increases?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?",How are we going to correct this problem?,By using different models
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?",How are our models statistically different from each other?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.",What is the concept of the Friedman test?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.",How do we define a crelative rank between algorithms?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.",What does a simple linear classifier get on the first dataset?,A rank of 1
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.","If there are ties, you will need to change the rank slightly to compensate?",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.",What is one of the many ways you can handle this case?,changing the rank
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.",How can we define the Friedman test?,"In terms of the average rank of the (ith) algorithm, "
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.",What is the average rank of the (ith algorithm?,(r_i) algorithm
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",The hypotheses are the following:,What are the hypotheses?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{0}: r_{1}=r_{i}=...=r_{k}\\),(H_0: r__1=r_i=...?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\(H_{1}:\\) They are not all equal.,(H_1:) They are not all equal?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",and the associated statistic is:,What is the name of the statistic associated with the statistic?,The name of the statistic is:
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",\\[ \\frac{12n}{k(k+1)} \\sum_{i}^{k}(r_i - \\frac{k+1}{2})^2 \\],[ frac12nk(k+1) sum_ik(r_i - 'frack+12]2?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.",How many datasets does the Friedman test have?,15
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.",How many algorithms do you need to use to check statistical significance?,4
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.",What is the main problem with a null test?,What happens after you reject the null?
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.",What does cthere seem to be between the ranks of each algorithmd?,A difference
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.",How is cpost hoc testd called?,"Cpost hoc testd is called ""lazy"""
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:",How many types of tests are there?,Two
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:",What are the two types of post hoc tests?,: retrospective and a retrospective
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.",What is the name of the test that performs all pairwise comparisons?,The pairwise comparison test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.",What does the test perform?,All pairwise comparisons
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.",Which tests perform better than the paired-test?,The comparison test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.",What are tests that compare with a baseline?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.",What is a problem of cwhich of the models Ive tested?,Those models Ive tested are better than the baselined.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.",How many comparisons are made on the number of algorithms used?,linear
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.",What does statistical tests help us make more sense of?,model evaluation strategies
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.",What are statistical tests considered to be cost-effective and relatively difficult to run?,Multiple-comparisons testing
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.",How are these statistical tests important to a model evaluation strategy?,They are computationally expensive and relatively difficult to run.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:",What is the number one skin health concern for Black females ages 18-45?,Hyper-pigmentation
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:",What is one of the leading manufacturers of skin care products?,Skincare Co.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:",How many days is Skincare Co. looking to develop a skincare line to target this population?,120
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:",Who is the data scientist assigned to the project investigating the use of hydroquinone in the product for the treatment of hyperpigmentation?,you
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Identify the population parameter of interest.,Identify the population parameter of interest?,Identify the population parameter of interest.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Determine whether you will be conducting a one-tailed or two-tailed test.,What test does a one-tailed test do?,A one-tailed test does not.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.",What is often denoted as a null hypothesis?,(H__0)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.",What is considered the status quo?,A null hypothesis
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.",How long does hydroquinone take on the skin of black females?,90 days
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","...then define an alternative hypothesis, denoted as \\(H_{A}\\). This would be the opposite of the null hypothesis.",What is the opposite of the null hypothesis?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?",What does the example above not cover?,Identifying your null and alternative hypotheses
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?",What is a call to action if you reject your hypothesis?,Your alternative hypothesis.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?",How do we test our hypothesis statistically?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.",What are the two types of tests that are not error-proof?,Null hypothesis and rejecting the alternative hypothesis
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.",What type of tests do you want to avoid?,Hypothesis testing
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Type I error occurs when you reject the null hypothesis when it should be accepted.,What type of error occurs when you reject the null hypothesis when it should be accepted?,Type I error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.,What happens when you accept the null hypothesis?,It should be rejected.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.,What error occurs when you fail to reject the hypothesis when it should be rejected?,Type II error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.,What does a Type I error mean that the company does not include in their skincare line when they should have been able to do so?,Hydroquinone
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.,What does the company stand to lose to companies with products that include hydroquinone that is effective in treating this condition with no side effects?,customers
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.",What is the p-value of a test?,The smallest value at which the test would reject the null hypothesis.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.",The smallest (alpha_z) value at which the test would reject the null hypothesis is what?,The p-value.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)",What is the p-value for a test statistic with z-score?,-2.878
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)",What is a two-tailed test?,To determine if the null hypothesis is acceptable.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Here, given a z-score of -2.878, we can calculate the p-value as,",What is a z-score?,-2.878
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Here, given a z-score of -2.878, we can calculate the p-value as,",How can we calculate the p-value?,As z-score of -2.878
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",p-value = \\(2\\times P(z< -2.878)\\),p-value = (2times P(z -2.878)?,= 2 times P(z -2.878)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," Note: Since were conducting a two-tailed test, we can then multiply this value by 2.",What is the value of a two-tailed test?,Multiply this value by 2
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking"," Note: Since were conducting a two-tailed test, we can then multiply this value by 2.",How can we multiply this value?,By 2
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","If you locate -2.878 in a z-score table, you get a value of 0.002.",What is the value of -2.878 in a z-score table?,0.002
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",p-value = \\(2\\times 0.002\\),What is the p-value?,= (2times 0.002)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking",p-value = 0.004,What is the p-value?,0.004
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","So, we have our p-value <  \\(\\alpha_z\\)",What is our p-value?,(alpha_z)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","So, we have our p-value <  \\(\\alpha_z\\)",What is the p value?,(alpha_z)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"McNemar Test,P-Hacking","Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.",What is considered to be statistically significant?,A p-value less than 0.05 is typically considered to be statistically significant.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Module 17 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists have virtually unlimited access to data and analytical techniques with which to analyze that data. As data scientists, we should be thinking about whether we should do something just because it is technically possible.",Who has virtually unlimited access to data and analytical techniques with which to analyze that data?,Data scientists
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists have virtually unlimited access to data and analytical techniques with which to analyze that data. As data scientists, we should be thinking about whether we should do something just because it is technically possible.",What should we think about as data scientists?,Whether we should do something just because it is technically possible.
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",Informed consent is based on what principles?,Fair Information Practice Principles (FIPPs)
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",When was the Privacy Act created?,1974
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",What does informed consent state when a study is being done on a subject?,
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data governance defines how data is accessed and managed within an organization. It is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, reduces the scourge of data silos, and improves data management overall.",What defines how data is accessed and managed within an organization?,Data governance
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data governance defines how data is accessed and managed within an organization. It is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, reduces the scourge of data silos, and improves data management overall.",What does data governance provide a consistent view of?,enterprise-wide data
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Accountability is about all the little decisions made by a group of people who created a system at each step of the way. There is both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong - to design a system for someone whom we are responsible for.",What is accountability about?,Little decisions made by a group of people.
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Accountability is about all the little decisions made by a group of people who created a system at each step of the way. There is both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong - to design a system for someone whom we are responsible for.",What is the term for anticipatory versus remedial accountability?,Ex-ante and post-hoc accountability
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most ""normal"" about a given dataset.",What do data scientists try their best to make predictions about the future based on?,Information in the present
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most ""normal"" about a given dataset.",What does data science involve making predictions and classifications and separating one group from another?,
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,,"Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most ""normal"" about a given dataset.",The prevalence of different subgroups in data will directly impact how much each of those groups affects the final result?,
Data Science Project Planning,Developing a Vision,Overview,,"The vision document is the starting point of your documentation set. Besides giving an overview of the project, this document helps remove ambiguities and puts everyone, including your collaborators and others, on the same page. Documenting these details helps direct every effort in the future toward the same goal. This is essentially helpful when there are many collaborators, which is usually the case in the industry. Without a clear vision, the project can go off the rails with changing collaborators or leadership. Take the example of the Virtual Case File study (needs CMU log-in to access the link). The FBI blew more than $100 million on a case-management software it will never use. One of the reasons for failure was an unclear vision for the project.",What is the starting point of your documentation set?,The vision document
Data Science Project Planning,Developing a Vision,Overview,,"The vision document is the starting point of your documentation set. Besides giving an overview of the project, this document helps remove ambiguities and puts everyone, including your collaborators and others, on the same page. Documenting these details helps direct every effort in the future toward the same goal. This is essentially helpful when there are many collaborators, which is usually the case in the industry. Without a clear vision, the project can go off the rails with changing collaborators or leadership. Take the example of the Virtual Case File study (needs CMU log-in to access the link). The FBI blew more than $100 million on a case-management software it will never use. One of the reasons for failure was an unclear vision for the project.",What does the vision document help remove?,ambiguities
Data Science Project Planning,Developing a Vision,Overview,,"The vision document is the starting point of your documentation set. Besides giving an overview of the project, this document helps remove ambiguities and puts everyone, including your collaborators and others, on the same page. Documenting these details helps direct every effort in the future toward the same goal. This is essentially helpful when there are many collaborators, which is usually the case in the industry. Without a clear vision, the project can go off the rails with changing collaborators or leadership. Take the example of the Virtual Case File study (needs CMU log-in to access the link). The FBI blew more than $100 million on a case-management software it will never use. One of the reasons for failure was an unclear vision for the project.",How much money did the FBI blow on a case-management software it will never use?,$100 million
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.",What can not be used directly as model inputs?,textual data
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.",What does applying the text processing tasks mentioned in the previous section help streamline textual data into a form that can be easily constructed to numerical features?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts.  It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document.  Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.",What is a primitive feature construction method that can be employed for simple problems to obtain quick results?,Bag-of-Words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts.  It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document.  Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.",What does Bag-of-Words assume is fixed and the size of the vocabulary determines?,Size of the vector
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts.  It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document.  Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.",How many words would be used in a document?,A small number
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Instead of representing the entire corpus as a one-dimensional list of numbers indicating word counts, term frequency takes into account the word frequencies for each member document in the corpus.",What does term frequency take into account for each member document?,Word frequencies
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Consider the corpus = [ cJack ate an appled, can apple on the tabled, cJack likes the appled ]",What does cJack eat?,An appled
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Consider the corpus = [ cJack ate an appled, can apple on the tabled, cJack likes the appled ]",What does the corpus mean?,"cJack ate an appled, can apple on the tabled, c"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Consider the corpus = [ cJack ate an appled, can apple on the tabled, cJack likes the appled ]",Who likes the appled?,cJack
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Bag-of-words will create a one-dimensional vector for this entire corpus:,What will create a one-dimensional vector for this entire corpus?,Bag-of-words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Bag-of-words will create a one-dimensional vector for this entire corpus:,What is the name of the vector created by Bag-of-words?,A One dimensional vector created by Bag-of-words.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Jack,What is Jack's name?,Jack's name is Jack.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,What did you eat?,a lot of meat
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,What was the name of the meal?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,What is an example of a problem?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,What type of problem does a person have?,An individual has a problem relating to their health.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",apple,What is the name of the apple?,The apple is named Apple of the apple name is Apple
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",on,What is the name of the person who is responsible for determining the status of a person?,Mr./Joshua
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,What is the name of the question that relates to a person's life?,The Question of Life
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,What type of question does the question raise?,The type of question raised is an admission of innocence.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",table,What is a table based on?,on concrete
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",likes,What is the name of the person who likes to be like?,She Likes to be like.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",3,How many questions do you have?,3
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","However, the term-frequency matrix will have a row corresponding to each of the three documents:",What will the term-frequency matrix have a row corresponding to?,Each of the three documents
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Jack,What is Jack's name?,Jack's name is Jack.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,What did you eat?,a lot of meat
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",ate,What was the name of the meal?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,What is an example of a problem?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",an,What type of problem does a person have?,An individual has a problem relating to their health.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",apple,What is the name of the apple?,The apple is named Apple of the apple name is Apple
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",on,What is the name of the person who is responsible for determining the status of a person?,Mr./Joshua
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,What is the name of the question that relates to a person's life?,The Question of Life
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",the,What type of question does the question raise?,The type of question raised is an admission of innocence.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",table,What is a table based on?,on concrete
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",likes,What is the name of the person who likes to be like?,She Likes to be like.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Document #1,What document does the document #1 contain?,A single document
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Document #2,What document does #2 contain?,Document #2 contains a document #2 containing the document #
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Document #3,What document does #3 contain?,Document #3 contains a document #3 containing the document #3
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to cnormalized these high-frequency values. In tf-idf, for any word not in the corpus, we can either ignore it or consider its frequency under another/foreign word column.",What does tf-idf treat for each document of a corpus distinctly like in?,Term Frequency
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to cnormalized these high-frequency values. In tf-idf, for any word not in the corpus, we can either ignore it or consider its frequency under another/foreign word column.",What does the Tf method assign an importance value to all the words purely based on?,Their frequency
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to cnormalized these high-frequency values. In tf-idf, for any word not in the corpus, we can either ignore it or consider its frequency under another/foreign word column.",How can we cnormalize these high-frequency values?,tf-idf
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Each word has an inverse document frequency associated with it. Hence,",What is an inverse document frequency associated with a word?,Each word has an inverse document frequency associated with it.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",\\[IDF_{j}=log(\\frac{\\text{number of documents}}{\\text{number of documents with the word j}})\\],What is the name of the word j?,Number of documents
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Intuitively, the IDF of a word that appears in fewer documents is higher. (Now, using the above formula, what is the IDF of a word that appears in every document? What could be the highest IDF value a word could get?)",What is the IDF of a word that appears in fewer documents?,Higher
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Intuitively, the IDF of a word that appears in fewer documents is higher. (Now, using the above formula, what is the IDF of a word that appears in every document? What could be the highest IDF value a word could get?)",What could be the highest IDF value?,A word that appears in every document.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","Finally, tf-idf for a word j in document i is calculated by multiplying the IDF score for word along j with the Tf for word j in document i :",What is tf-idf for a word in document i calculated by multiplying the IDF score for word along j with the Tf for word j?,j
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",\\[ TF - ID_{ij}= TF_{i}\\times IDF_{j} \\],[ TF - ID_ij= TF_itimes IDF_j?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","All the above feature construction methods are easy to visualize and understand. However, it represents individual words as dimensions and thus tends to suffer from the curse of dimensionality. Word embeddings are representations of words in a meaningful low-dimensional space whose dimensionality is a fixed number independent of the word count in the corpus. Intuitively, Words that are placed closer in this space are expected to be similar in meaning. For instance, the position of cSeattled is closer to cBostond than it is to ctalkd in the below illustration of word embeddings.",What does cSeattled stand for?,CSeattled stands for Contextual Awareness.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","All the above feature construction methods are easy to visualize and understand. However, it represents individual words as dimensions and thus tends to suffer from the curse of dimensionality. Word embeddings are representations of words in a meaningful low-dimensional space whose dimensionality is a fixed number independent of the word count in the corpus. Intuitively, Words that are placed closer in this space are expected to be similar in meaning. For instance, the position of cSeattled is closer to cBostond than it is to ctalkd in the below illustration of word embeddings.",What is the dimensionality of cBostond?,cBostond is dimensionality of cBostond.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Figure 1. Word Embeddings (Source: IBM Research Blog),What is the source of Word Embedding?,IBM Research Blog
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings",Figure 1. Word Embeddings (Source: IBM Research Blog),What is a source of the IBM Research Blog?,Word Embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","As discussed in an earlier module, word embeddings can be generated by training a model from scratch or through pre-trained models like BERT (introduced later in the course), which brings down the training time significantly. Due to the advantages of word embeddings over other methods in capturing context and minimizing memory used for feature representation, it is increasingly used for deep learning and advanced NLP tasks, some of which will be discussed in the next section.",What can be generated by training a model from scratch or through pre-trained models like BERT?,Word embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings","As discussed in an earlier module, word embeddings can be generated by training a model from scratch or through pre-trained models like BERT (introduced later in the course), which brings down the training time significantly. Due to the advantages of word embeddings over other methods in capturing context and minimizing memory used for feature representation, it is increasingly used for deep learning and advanced NLP tasks, some of which will be discussed in the next section.",What is the advantage of word embeddings over other methods in capturing context and minimize memory used for feature representation?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"A quick survey of both scholarly and practitioner literature shows that errors can make or break the analytic development process and render your analytic solution of little use to your client. Errors can start from the business understanding phase when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding to bloated costs and scope creep (changes, continuous or uncontrolled growth in a projects scope, at any point after the project begins). A data science team might also encounter errors during the data understanding phase, such as issues with data that is not prepared adequately or, even worse, collecting data that is not relevant to the analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team and an attempt to deliver a solution prematurely.",What can errors make or break the analytic development process?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"A quick survey of both scholarly and practitioner literature shows that errors can make or break the analytic development process and render your analytic solution of little use to your client. Errors can start from the business understanding phase when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding to bloated costs and scope creep (changes, continuous or uncontrolled growth in a projects scope, at any point after the project begins). A data science team might also encounter errors during the data understanding phase, such as issues with data that is not prepared adequately or, even worse, collecting data that is not relevant to the analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team and an attempt to deliver a solution prematurely.",Why can errors start from the business understanding phase?,A data science team does not set the appropriate analytic objectives.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"A quick survey of both scholarly and practitioner literature shows that errors can make or break the analytic development process and render your analytic solution of little use to your client. Errors can start from the business understanding phase when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding to bloated costs and scope creep (changes, continuous or uncontrolled growth in a projects scope, at any point after the project begins). A data science team might also encounter errors during the data understanding phase, such as issues with data that is not prepared adequately or, even worse, collecting data that is not relevant to the analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team and an attempt to deliver a solution prematurely.",What can a data science team encounter during the data understanding phase due to?,Inexperience
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data-related problems. Errors in this phase may show up when training and validating models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will encounter throughout your career as a data scientist. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.",What phase presents errors that we should explore as we learn about different data science patterns and techniques that can be used to solve data-related problems?,model understanding phase
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data-related problems. Errors in this phase may show up when training and validating models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will encounter throughout your career as a data scientist. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.",Errors in the model understanding phase may show up when training and validating what?,models
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data-related problems. Errors in this phase may show up when training and validating models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will encounter throughout your career as a data scientist. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.",What are the models below that you will encounter throughout your career as a data scientist?,Errors
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,Training Error is derived by computing the classification error of a model on the exact data that was used to train the model. The training error is defined as the average loss that occurs  at the end of  the training process. It should be noted that the training error will usually be lower  than  the test error.,What is the classification error of a model?,Training Error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,Training Error is derived by computing the classification error of a model on the exact data that was used to train the model. The training error is defined as the average loss that occurs  at the end of  the training process. It should be noted that the training error will usually be lower  than  the test error.,What is defined as the average loss that occurs at the end of the training process?,The training error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error.  As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.",What is the term for the loss of a model on the test set?,Classification error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error.  As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.",What is used to determine the number of errors to expect when making future predictions?,Test Error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error.  As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.",No part of the training data set should be part of what?,Test data
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error.  As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.",How can the test error decrease?,As the model complexity increases up to a certain point.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Reducible Error is the error resulting from a mismatch between the ground truth and the model estimation, or the estimate of the true relationship between x and y. The reducible error is the element that we can improve. It is the quantity that we reduce when the model is learning on a training dataset, and we try to get this number as close to zero as possible.",What is the error resulting from a mismatch between the ground truth and the model estimation?,Reducible Error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Reducible Error is the error resulting from a mismatch between the ground truth and the model estimation, or the estimate of the true relationship between x and y. The reducible error is the element that we can improve. It is the quantity that we reduce when the model is learning on a training dataset, and we try to get this number as close to zero as possible.",What does the estimate of the true relationship between x and y mean?,It means that they will be in a relationship between x and y.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Irreducible Error is the noise term in the true relationship that cannot fundamentally be reduced by any model. When x can not determine y because there are other predictors that might improve the prediction error, you can incorporate those variables. The irreducible error is the error that we can not remove with our model or with any model. The error is caused by elements outside our control, such as statistical noise in the observations.",What is the noise term in a true relationship that cannot fundamentally be reduced by any model?,Irreducible Error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,,"Irreducible Error is the noise term in the true relationship that cannot fundamentally be reduced by any model. When x can not determine y because there are other predictors that might improve the prediction error, you can incorporate those variables. The irreducible error is the error that we can not remove with our model or with any model. The error is caused by elements outside our control, such as statistical noise in the observations.",What does the irreducible error mean?,The error that we cannot remove with our model or with any model.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Have you come across the term ""lazy learning""? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.",What is a method in which training data is generalized?,lazy learning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Have you come across the term ""lazy learning""? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.",What is an application of a lazy learner?,A recommendation system
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Have you come across the term ""lazy learning""? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.",A recommendation system relies on what variables?,"Certain variables such as ratings, pricing, and country of origin"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. , An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.",What is the opposite of a lazy learning method?,An eager learning method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. , An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.",An eager learner usually requires less space than what?,A lazy learner
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. , An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.",A lazy learner will typically learn for a short time but take a longer time to classify data?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.",What is the name of a well-known lazy learner?,k-Nearest Neighbor (kNN) method.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.",What can be used to solve both classification and regression problems?,k-Nearest Neighbor (kNN) method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.",How is kNN considered simple yet useful?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"kNN will perform its task just in time as it does not learn a  model but keeps its training observations in an explicit form, so it spends less time learning and more time classifying or predicting.",What does kNN keep its training observations in an explicit form?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"kNN will perform its task just in time as it does not learn a  model but keeps its training observations in an explicit form, so it spends less time learning and more time classifying or predicting.",How much time is spent learning and classifying?,kNN
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are three steps involved in making predictions with kNN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.",How many steps are involved in making predictions with kNN?,Three
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are three steps involved in making predictions with kNN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.",Compute the Euclidean distance of the new observation to previous classified observations?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,Reading: k-Nearest Neighbors: From Global to Local,What is k-Nearest Neighbor Reading?,From Global to Local
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,Reading: k-Nearest Neighbors: From Global to Local,What is the name of the book that reads from Global to Local?,k-Nearest Neighbors
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The kNN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The kNN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the Euclidean Distance. The formula for the Euclidean distance between observations (\\(x_{i}\\) and \\(y_{i}\\)) is shown below:",What method begins by identifying the observations that are similar to a new observation that should be classified?,kNN
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The kNN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The kNN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the Euclidean Distance. The formula for the Euclidean distance between observations (\\(x_{i}\\) and \\(y_{i}\\)) is shown below:",What can be used to classify the new observation into its class?,neighboring observations
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"The kNN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The kNN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the Euclidean Distance. The formula for the Euclidean distance between observations (\\(x_{i}\\) and \\(y_{i}\\)) is shown below:",The most popular distance measure used in kNN is what?,Euclidean Distance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,Euclidean Distance Function. (Source: Sayad (2010)),What is the Euclidean Distance Function?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are other distance measures, including Manhattan (distance between vectors using the sum of their absolute differences) and Minkowski distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the Hamming Distance, which can be used as long as you do not have more than two (2) classes.",What is the name of the distance measure used in Manhattan?,Hamming Distance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are other distance measures, including Manhattan (distance between vectors using the sum of their absolute differences) and Minkowski distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the Hamming Distance, which can be used as long as you do not have more than two (2) classes.",What is an alternative distance measure?,Hamming Distance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"There are other distance measures, including Manhattan (distance between vectors using the sum of their absolute differences) and Minkowski distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the Hamming Distance, which can be used as long as you do not have more than two (2) classes.",How many classes can the Hamming Distance be used?,two (2)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Once distances are computed, the class that a new observation will be assigned is based on the classes of its neighbors. Now, you can see why kNN is considered a similarity function.",What is the class that a new observation will be assigned?,based on the classes of its neighbors
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Once distances are computed, the class that a new observation will be assigned is based on the classes of its neighbors. Now, you can see why kNN is considered a similarity function.",What is kNN considered a similarity function?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.",How do you determine the number of neighbors to assess?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.",How can you determine that the new data should be classified in the same class as its single closest/nearest neighbor?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.",What can you do if you guessed that you would face overfitting?,?
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.","If you assign k = n, what will be over-smoothing?",Number of observations
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",What is the main difference between predicting a categorical response and a continuous value?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",The algorithm will use the average (usually weighted) response of the neighbors of what data point to determine the accurate prediction?,new
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,,"Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",What is determined using the root mean square error?,Best k
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Feature engineering is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering.",What is the process of using domain knowledge to extract features from raw data?,Feature engineering
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Feature engineering is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering.",What does Feature engineering ensure your dataset is compatible with your algorithm?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course),What is the name of a Google Developer Course?,Mapping Raw Data to ML Features
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course),What does the mapping of Raw Data to ML Features mean?,It means you can now access the mapping of Raw Data to ML Features.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.",What is an example of a simple but problematic solution?,Dropping rows or columns.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.",What is a preferred solution for missing values in a row?,Imputation
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Numerical Data Imputation. If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with "" - "" or ""NA"" can be replaced with a ""0"". Other methods used include using the median or mean values of that variable.",What method allows you to replace missing values intuitively?,Numerical Data Imputation
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Numerical Data Imputation. If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with "" - "" or ""NA"" can be replaced with a ""0"". Other methods used include using the median or mean values of that variable.","What can be replaced with a ""0""?","A column with numbers and some with ""-"" or ""NA"""
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Categorical Data imputation. In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the cmost frequently  occurring value,d and you can impute cotherd in a situation where there is no dominant value in the categorical column.",What does replacing missing values with a zero make sense to the dataset?,"In some cases, replacing missing values with a zero will not make sense to the dataset."
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Categorical Data imputation. In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the cmost frequently  occurring value,d and you can impute cotherd in a situation where there is no dominant value in the categorical column.",What can you replace values in a categorical column with?,"The cmost frequently occurring value,d"
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.",What can be applied to numerical and categorical data?,Binning
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.","What makes a model more robust, but there is a trade-off between performance and overfitting?",Binning
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.",How can binning be used to capture noisy data when you have values that have variance?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.",What is the procedure of combining a cluster of pixels into a single pixel?,Binning
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.",How many pixels becomes a larger pixel in 2x2 binning?,4
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the ""Exploratory Data Analysis"" module.",How did you learn how to visualize your data to detect outliers in an earlier module?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the ""Exploratory Data Analysis"" module.",What is more error-prone to using statistical and visualization methods?,To detect and handle outliers
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.",What is another term for logarithm transform?,Log transform
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.",What is the term used to handle skewed data and make the distribution of data less?,Log transform
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.",Why is it widely used?,It is easy to use
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called dummy variables.,What is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions?,Hot encoding
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called dummy variables.,What are the Boolean variables called?,Dummy variables
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.,What is one hot encoding easy to implement?,It will retain all information of the categorical variable
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.,What does this method do?,No
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable chigh school,d which becomes one if the person has high school or 0 if the person has less than high school.",What is a categorical variable education with labels less than high school and high school?,Assume
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable chigh school,d which becomes one if the person has high school or 0 if the person has less than high school.","What is the Boolean variable chigh school,d which becomes one if the person has what?",High school
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information.",What method can be used to create a smaller number of variables without losing information?,Binary encoding
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information.",What type of encoding can increase dimensionality of your data?,One-hot
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.",What can you do when you have analytic data that is useful to your solution?,Transform those features into integer values.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.",What do we convert string labels to?,Integer values
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.","If you have a variable with string values that are satisfied, dissatisfied, highly satisfied, not applicable, and somewhat satisfied, what will encoding map the values?",An ordinal feature
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Highly Satisfied,What type of people are highly satisfied with their services?,Forecast people
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Highly Satisfied,What kind of person is highly satisfied?,Any kind of person
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",1,How many questions do you have?,1
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",1,What is the number of questions that you have answered?,1
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Satisfied,What was Satisfied?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Satisfied,What is Satisfaction?,It is the sign of Satisfaction.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",2,How many questions do you have?,Two
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Somewhat Satisfied,What was somewhat Satisfied?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",3,How many questions do you have?,3
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Not Applicable,What is not applicable?,The above is not applicable.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Not Applicable,What is the name of the person who is not able to apply?,Janet
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",4,How many questions do you have?,4
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Dissatisfied,What is the reason for the dissatisfaction?,The customer was unhappy because of the satisfaction.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Dissatisfied,What is a reason for a person to be unhappy with a job?,Unsatisfied job leads to a person being unhappy with a job.
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",5,How many questions do you have?,5
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Highly Dissatisfied,What kind of dissatisfaction does a person feel is not satisfied with?,highly
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Highly Dissatisfied,What type of person is unhappy with a job?,Highly Dissatisfied
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",6,How many questions do you have?,6
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.",What happens when you split features?,The features become easier to bin
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.",What does the variable address affect?,The way you split features
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.",How do you run the risk of increasing dimensions?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.",What is not necessary for most algorithms?,Scaling
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.",Scaling can make continuous features identical with respect to what?,range
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","There are instances that require the use of scaled data, including algorithms that use gradient descent",What type of data is required to be scaled?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","There are instances that require the use of scaled data, including algorithms that use gradient descent",What algorithm uses gradient descent?,Scaled data
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.",What are some examples of Neural Networks and Linear Regression?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.",What type of algorithms would need data that is scaled before being fed to?,Distance-based algorithms
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Quick thought: How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.,What type of algorithms are not affected by tree-based algorithms?,Decision trees and random forests
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Quick thought: How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.,What is the name of the algorithm that is not based on distances?,Tree based algorithm
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Normalization. This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled.","What technique involves values between 0 and 1. Before normalization, what should be handled before normalization?",Outliers
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Standardization. Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks.",What is standardization also known as?,z-score normalization
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.","Standardization. Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks.",What is z-score normalization useful for?,"feature engineering in logistic regression, artificial neural networks, and support vector machine tasks"
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.",Reading: Data Preprocessing in scikit-learn.,Reading: Data Preprocessing in scikit-learn?,
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Algorithms represent data and relations between data items using a variety of abstract data types. The most commonly used abstract data types are the following:,What do algorithms represent?,Data and relations between data items
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Algorithms represent data and relations between data items using a variety of abstract data types. The most commonly used abstract data types are the following:,What are the most commonly used abstract data types?,:
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.",What is the term for one-dimensional arrays?,Sequences
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.",What type of arrays are typically fixed size?,one-dimensional
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.",How are items identified by positions or indices starting with?,0.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.",When can old items be replaced with new values?,At the end of an array
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sets: Sets let one represent the equivalent of finite sets in mathematics, with elements coming from some domain where equality between the elements in the domain is defined in some way. The operations on sets are the typical operations one does on mathematical sets:",What is the equivalent of finite sets in mathematics?,Sets
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Sets: Sets let one represent the equivalent of finite sets in mathematics, with elements coming from some domain where equality between the elements in the domain is defined in some way. The operations on sets are the typical operations one does on mathematical sets:",What are the typical operations on sets?,
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Intersection and union of sets,",What is the intersection and union of sets?,
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Intersection and union of sets,",What does the union and intersection of sets mean?,They mean the union and intersection of sets mean the union and intersection of sets mean the union and
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Subtraction of one set from another set,","Subtraction of one set from another set, subtraction of another set from what set?",another set
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Inserting elements into a set or removing an element from a set,",What does removing an element from a set do?,Simplifies the removal.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Checking if a given element of the domain is a member of a set,",What does checking if a given element of the domain is a member of a set?,Yes.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Computing the size of a set.,What is the size of a set?,.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"In ordered sets, the elements are assumed to be orderable based on a \\(<\\) relationship. With sorted sets, we can also do operations such as",What is assumed to be orderable in order sets?,Elements
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"In ordered sets, the elements are assumed to be orderable based on a \\(<\\) relationship. With sorted sets, we can also do operations such as",What does sorted sets mean?,They mean they are orderable based on a () relationship.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Find the \\(i^{th}\\) smallest element of a set,",What is the smallest element of a set?,i(i)th
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Find the ordered position in the set of a specific value \\(x\\) (same as finding the number of elements in the set less than a given \\(x\\)).,What is the order position in the set of a specific value?,(x)
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Find the ordered position in the set of a specific value \\(x\\) (same as finding the number of elements in the set less than a given \\(x\\)).,What does finding the number of elements in a set less than a given value mean?,"Similarly, finding the number of elements in a set less than a given value means"
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Tables: Tables are an extension of sets. Each element in a table \\((k, v)\\) consists of a key \\(k\\) and an associated value \\(v\\). Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value \\(v\\) associated with a key. Similarly, tables can be ordered based on a key if there is a need.",Tables are an extension of what?,Sets
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Tables: Tables are an extension of sets. Each element in a table \\((k, v)\\) consists of a key \\(k\\) and an associated value \\(v\\). Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value \\(v\\) associated with a key. Similarly, tables can be ordered based on a key if there is a need.","What is a key in a table (k, v)) and an associated value?",(v)
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Tables: Tables are an extension of sets. Each element in a table \\((k, v)\\) consists of a key \\(k\\) and an associated value \\(v\\). Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value \\(v\\) associated with a key. Similarly, tables can be ordered based on a key if there is a need.",Tables can be ordered based on what if a need exists?,Key
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Graphs: Graphs are the most versatile abstract data types. They are typically used to represent a set of items (called nodes) along with asymmetric or symmetric relations between those items (called edges). For example, graphs can be used to represent",What are the most versatile abstract data types?,Graphs
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Graphs: Graphs are the most versatile abstract data types. They are typically used to represent a set of items (called nodes) along with asymmetric or symmetric relations between those items (called edges). For example, graphs can be used to represent",What are asymmetric or symmetric relations between a set of items?,Nodes
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Social networks with nodes representing people and edges representing cfriendshipd or cfollowsd relations between them.,Social networks with nodes representing people and edges representing what?,cfriendshipd or cfollowsd relations between them.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Social networks with nodes representing people and edges representing cfriendshipd or cfollowsd relations between them.,What are cfollowsd relations?,Social networks with nodes representing people and edges representing cfriendshipd or cfollow
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Transportation networks with nodes representing cintersectionsd and edges representing the roads between the intersections, with a distance measure associated with each road.",What are nodes representing?,Cintersectionsd
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Transportation networks with nodes representing cintersectionsd and edges representing the roads between the intersections, with a distance measure associated with each road.",What is a distance measure associated with each road?,A distance measure is associated with each road.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Neural networks with nodes representing neurons and edges representing weighted connections.,Neural networks with nodes representing neurons and edges representing what?,Weighted connections
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,There is a whole set of operations one can do on graphs that can compute all kinds of useful information about the data and the relations. Here are some of such operations:,What is a graph that can compute all kinds of useful information about?,data and relations
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,What is the shortest distance between any two intersections?,What is the distance between a two intersections?,shortest distance
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Who are the people on a social network with more that 100 connections?,Who are the people on a social network with more than 100 connections?,John and Jane
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Trees: Trees are special cases of graphs and are used to represent hierarchical relations such as parentchild relations. This restriction usually allows for more memory-efficient representations or time-efficient operations for specific classes of operations.,What type of graphs are used to represent hierarchical relations?,Trees
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Trees: Trees are special cases of graphs and are used to represent hierarchical relations such as parentchild relations. This restriction usually allows for more memory-efficient representations or time-efficient operations for specific classes of operations.,What does this restriction usually allow for more memory-efficient representations?,Trees
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Priority Queues: Priority queues are essentially sets where each element consists of a value, and a priority, and the only operations one can do are",What are Priority Queues?,
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"Priority Queues: Priority queues are essentially sets where each element consists of a value, and a priority, and the only operations one can do are",What are the only operations one can do?,Priority Queues
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Inserting a value with its priority,What is the priority of a value?,Inserting a value
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Inserting a value with its priority,What type of value is a priority?,Inserting a value with its priority is a priority.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,Finding or deleting a value with the maximum priority.,What is the maximum priority for a value?,Finding or deleting a value with the maximum priority.
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"While we listed these as abstract data types, some of these types are used to implement others, usually in a nested way. For instance, sets and tables can be used to implement graphs, while trees (specifically their balanced binary variants) can be used to efficiently implement sets, tables, or sequences. More details on these are beyond the scope of this section and are typically the topic of an introductory course and book on data structures and algorithms.",What are some of the abstract data types used to implement?,"Some of these types are used to implement others, usually in a nested way."
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,"While we listed these as abstract data types, some of these types are used to implement others, usually in a nested way. For instance, sets and tables can be used to implement graphs, while trees (specifically their balanced binary variants) can be used to efficiently implement sets, tables, or sequences. More details on these are beyond the scope of this section and are typically the topic of an introductory course and book on data structures and algorithms.","What can be used to effectively implement sets, tables, or sequences?",trees
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,,One can also refer to https://en.wikipedia.org/wiki/Abstract_data_ type for more details on abstract data types.,What type of data type can be found at https://en.wikipedia.org/wiki/Abstract_data_?,Abstract data type
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Natural Language Processing (NLP hereafter) is a subfield of computer science aiming to  build  systems that:,,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Enable human-computer communication,What do you need to do with your computer?,Enable human-computer communication.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Enable/enrich human-human communication,What type of communication can be activated/enriched?,Human-human communication
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Perform tasks requiring the use of human language faculty, more efficiently and more correctly.",What does a human language faculty need to do?,Perform tasks
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Here are some applications of NLP in these three areas:,What are some applications of NLP?,In these three areas
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Human-computer communication,What type of communication does human-computer communication have?,Human-computer communication has a human-computer type of communication.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Human-computer communication,What kind of communication do humans use?,Computer
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Dictating email messages,What type of messages do you send?,Email
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Dictating email messages,What kind of messages are you sending?,Email messages.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Listening to email messages,What do you listen to?,Email messages
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Listening to email messages,What does listening to emails do?,helps you remember what they are.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"With proper pronunciation, tone, and stress!","What does proper pronunciation, tone, and stress mean?",Means a lot of things
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Using human language to give commands to the operating system,What language is used to give commands to the operating system?,Human language
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Human-human communication,What type of communication does human-human communication have?,Human-human communication has a type of communication that is called human-human communication.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Real-time text and speech translation,What is real-time text and speech translation?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Summarizing meeting conversations,What is the summarizing of meeting conversations?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Enabling communication with people with hearing/vision impairments,What is the purpose of enabling communication with people with hearing/vision impairments?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Improving the efficiency of tasks requiring human language faculty,What is the purpose of enhancing the efficiency of tasks requiring human language faculty?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Improving the efficiency of tasks requiring human language faculty,What does the improvement of the effectiveness of tasks require?,Human language faculty
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Correcting typing/grammatical errors,What type of errors did the correction of typing/grammatical errors cause?,Caused by typing/grammatical errors
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Question answering,What is the answer to questions?,Yes
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Answering e-mails automatically,How do you respond to e-mails automatically?,You respond to e-mails automatically.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Searching large document databases,Searching large document databases generates what?,Death
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Here are some (and definitely not a comprehensive set of) applications of  NLP in a diverse set of areas:,What are some applications of NLP?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Information extraction (IE), especially copend IE:  Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.",What is IE?,Information extraction
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Information extraction (IE), especially copend IE:  Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.",What does IE mean?,Information extraction
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.",What is a classification of a document?,Topic areas
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.",What is the classification of an email?,Spam or not
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Question answering beyond finding the documents relevant to the question and instead delivering the actual answer.,Question answering beyond finding the documents relevant to the question and instead providing the actual answer?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.",What are Conversational Agents called?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.",What is the name of the conversational agent that holds a goal-driven conversation with a user?,"Siri, Alexa, Google Assistant"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Image (and eventually video) understanding:  Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).",What is the term for expressing the pictorial or video content in human language?,Image
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Image (and eventually video) understanding:  Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).",What is a term for verbalizing a football match?,Image captioning
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.",What does text include?,Images
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.",What does real-time lecture translation produce?,Speech output
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).,What is a sign language translation and scene-to-speech for visually impaired?,A sign language translation and scene-to-speech for the visually impaired
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).,What would be similar to video understanding?,Sign language translation and scene-to-speech
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,Summarization:  Generating a short summary of the salient points of a document or a set of documents.,What is a short summary of a document or a set of documents?,Summarization
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Opinion and sentiment analysis:  Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.","What type of analysis is used to extract political or personal sentiments from news pieces, tweets, product reviews, or movie reviews?",opinion and sentiment analysis
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.",What is an example of an SAT exam?,An essay
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.",What is the purpose of an essay evaluation?,"For proper structure, sentence, and vocabulary use"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Fake news or fake review detection:  Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.",What is a fake news detection?,A fake review detection.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Fake news or fake review detection:  Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.",What is an automatic fact-checking of news in real-time viral settings on social media?,Fake news or fake review detection
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,The following figure gives a birds eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.,What does the following figure give a bird eye view of?,NLP
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,From Eduard Hovys cThe Past and 3\xbd Futures of NLPd presentation.,What is the name of Eduard Hovys cThe Past and 3xbd Futures of NLPd presentation?,The name of Eduard Hovys cThe Past and 3xbd Futures
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,There are three high-level functions that NLP tries to automate:,How many high-level functions does NLP try to automate?,Three
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.",What is cunderstandingd or cprocessingd?,Analyze
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.",What is the output of a language that supports useful action?,A representation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).",What is input a representation of?,An utterance
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).",What is the output of text or speech that captures the semantics and the intent encoded in the input representation?,A document
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Acquisition: obtaining the representation and necessary algorithms from data (e.g., learning to translate from aligned translated sentences).",,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"The representation that these employs depends on the actual task being solved:  they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.",What does a neural machine translation system compute as it analyzes an input sentence?,sentence embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"The representation that these employs depends on the actual task being solved:  they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.","What can morphological analyses of words, syntax trees, or part of-speech symbol sequences of sentences capture?",the linguistic structure of words or sentences
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.",What is NLP closely related to?,Computer science
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.",What is the NLP related to in other fields?,"Machine Learning, Deep Learning, Artificial Intelligence, Software Engineering,"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.",Who is closely related in NLP?,"Machine learning, Deep Learning, Artificial Intelligence"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",What type of learning does machine learning involve?,Deep learning
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,,"Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",What types of data sources do NLP applications require to train?,"Annotated and unannotated text, speech corpora, and large-scale"
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,The Transformer model was introduced in the famous paper Attention is All You Need in 2017.,When was the Transformer model introduced?,2017
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,The Transformer model was introduced in the famous paper Attention is All You Need in 2017.,What was the name of the famous paper that introduced the Transformer?,Attention is All You Need
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?",What is a good way to understand Transformers?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?",What are the hidden states supposed to represent?,Input embeddings
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?",How is attention alignment supposed to define the input step?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.,How do Transformers do this?,By replacing the sequential processing performed by RNNs in Sequence2Sequence models
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.,What does RNNs do in Sequence2Sequence?,Reparate sequential processing
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,Figure 5: Interactions within components in different architectures.,Figure 5: Interactions within components in different architectures?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.",What do Transformers use instead of using attention to connect the encoder and decoder blocks?,self-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.",What are Transformers using instead of deriving hidden states?,RNNs
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.",What is self-attention also known as?,Intro-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.",Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence?,Intro-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.","What has been shown to be useful in machine reading, abstractive summarization, or image description generation?",Self-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.",What do Transformers use to generate a richer representation of a given input step?,Self-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.",What can be done for all input steps in parallel?,This can be done in parallel.
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.",On what side is an attention-based decoder used?,Decoder side
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.",There are no hidden states anymore and no computation of what?,A separate context vector
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,,"On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.",Self-attention on all outputs generated till that point is used to generate (y_i)?,(y_i)
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As we explore the different methods, we will dive deeper into calculating the error rate for the models trained using those methods.",What method will be used to calculate the error rate for the models trained using?,Various methods
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The bias is a measure of how closely the model can capture the mapping function between inputs and outputs and measures the average accuracy of the model arising from erroneous assumptions in the learning algorithm. A high bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting). The bias is always positive.,What is a measure of how closely the model can capture the mapping function between inputs and outputs?,The bias
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The bias is a measure of how closely the model can capture the mapping function between inputs and outputs and measures the average accuracy of the model arising from erroneous assumptions in the learning algorithm. A high bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting). The bias is always positive.,What is the average accuracy of the model arising from erroneous assumptions in the learning algorithm?,A high bias
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Low Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are weak.,Low Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are what?,weak
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,High Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are strong.,What indicates that assumptions regarding the functional form of the mapping of inputs to outputs are strong?,High Bias
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.,What is the variance of the model?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.,What is an error from sensitivity to small fluctuations in the training set?,The variance of the model
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.,How can an algorithm model the random noise in training data?,High variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Low Variance indicates that changes to the training dataset cause  small changes to the model.,Low Variance indicates that changes to the training dataset cause what?,small changes to the model
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,High Variance indicates that  changes to the training dataset cause large changes to the model.,High Variance indicates that changes to the training dataset cause large changes to what?,the model
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this. It is a property of the data.,How big is the data-intrinsic noise?,100
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this. It is a property of the data.,What measures ambiguity due to your data distribution and feature representation?,This error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Variance refers to the amount by which \\(\\hat{f}\\) would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of f.",What refers to the amount by which (hatf) would change if you estimated it using a different training data set?,Variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Variance refers to the amount by which \\(\\hat{f}\\) would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of f.",What is introduced by approximating a real-life problem?,Bias
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Variance refers to the amount by which \\(\\hat{f}\\) would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of f.",How does real life do not present scenarios that have a simple linear relationship?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"When you decompose bias-variance, you will analyze an algorithm's ability to predict outcomes for data that your model has not seen. The bias-variance tradeoff is encountered while working with some supervised learning techniques. The premise is that your model will adequately learn the training data, and it should properly generalize well to new data.","When you decompose bias-variance, you will analyze an algorithm's ability to predict outcomes for data that your model has not seen?",
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"When you decompose bias-variance, you will analyze an algorithm's ability to predict outcomes for data that your model has not seen. The bias-variance tradeoff is encountered while working with some supervised learning techniques. The premise is that your model will adequately learn the training data, and it should properly generalize well to new data.",What is a tradeoff encountered while working with some supervised learning techniques?,Bias-variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.",What is a supervised learning method that can represent training data well but experiences overfitting?,A high variance method
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.",What is an example of a method with high bias that can not adequately learn training data?,High variance method
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Bias-Variance Diagram.,What is a Bias-Variance Diagram?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Let us look at a very well deconstructed mathematical representation of Bias-Variance by IBM's Aditya Prasad:,What is the name of IBM's mathematical representation of Bias-Variance?,Aditya Prasad
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Let us look at a very well deconstructed mathematical representation of Bias-Variance by IBM's Aditya Prasad:,What was Aditya Prasad?,IBM
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Note that the bias and variance of an estimator are mathematically related to each other and also to the performance of the estimator. Let us define an estimators error at a test point as the cexpectedd squared difference between the true value and the estimators estimate.,How are the bias and variance of an estimator mathematically related to each other?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Note that the bias and variance of an estimator are mathematically related to each other and also to the performance of the estimator. Let us define an estimators error at a test point as the cexpectedd squared difference between the true value and the estimators estimate.,What is the cexpectedd squared difference between the true value and the estimators estimate?,Expected square difference
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Whenever an expected value is referenced, this means the expectation over all the possible models, trained individually over all the possible data samples. For any unseen test point \\(x_{0}\\), you will have:","When an expected value is referenced, what means the expectation over all the possible models?",
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Whenever an expected value is referenced, this means the expectation over all the possible models, trained individually over all the possible data samples. For any unseen test point \\(x_{0}\\), you will have:",What is the test point for an unseen test point?,x_0
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,\\[\\operatorname{Err}\\left(x_0\\right)=E\\left[\\left(Y-g\\left(x_0\\right)\\right)^2 \\mid X=x_0\\right]\\],What is the name of the operatornameErrleft?,The operator name is Errleft.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Referring to \\(f\\left(x_0\\right)\\) and \\(g\\left(x_0\\right)\\) as f and g, respectively and skipping the conditional on X:",(fleft(x_0right)) and g as what?,f
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Referring to \\(f\\left(x_0\\right)\\) and \\(g\\left(x_0\\right)\\) as f and g, respectively and skipping the conditional on X:",What is the conditional on X?,Y:
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,\\[\\begin{aligned}&\\operatorname{Err}\\left(\\mathrm{x}_0\\right)=\\mathrm{E}\\left[\\left(\\mathrm{Y}-\\mathrm{g}\\left(\\mathrm{x}_0\\right)\\right)^2\\right] \\\\&=\\mathrm{E}\\left[(\\mathrm{f}+\\epsilon-\\mathrm{g})^2\\right] \\\\&=\\mathrm{E}\\left[\\epsilon^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{g})^2\\right]+2 \\cdot \\mathrm{E}[(\\mathrm{f}-\\mathrm{g}) \\epsilon] \\\\&=\\mathrm{E}\\left[(\\epsilon-\\mathrm{o})^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\\right]+2 \\cdot \\mathrm{E}[\\mathrm{f} \\epsilon]-2 \\cdot \\mathrm{E}[\\mathrm{g} \\epsilon] \\\\&=\\mathrm{E}\\left[(\\epsilon-\\mathrm{E}[\\epsilon])^2\\right]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\\right]+\\mathrm{o}-\\mathrm{o} \\\\&=\\operatorname{Var}(\\epsilon)+\\mathrm{E}\\left[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])^2\\right]+\\mathrm{E}\\left[(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})^2\\right]+2 \\cdot \\mathrm{E}[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})] \\\\&=\\operatorname{Var}(\\epsilon)+\\operatorname{Var}(\\mathrm{g})+\\mathrm{Bias}(\\mathrm{g})^2+2 \\cdot\\left\\{\\mathrm{E}[\\mathrm{g}]^2-\\mathrm{E}[\\mathrm{gf}]-\\mathrm{E}[\\mathrm{g}]^2+\\mathrm{E}[\\mathrm{gf}]\\right\\} \\\\&=\\sigma^2+\\operatorname{Var}(\\mathrm{g})+\\operatorname{Bias}(\\mathrm{g})^2\\end{aligned}\\],[beginaligned&operatornameErrrleft(mathrmx_0right)?,mathrmx_0right
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,\\[\\textit{Generalization Error} = \\textit{Bias}^2 + \\textit{Variance} + \\textit{Irreducible Error}\\],What is a textitGeneralization Error?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,\\[\\textit{Generalization Error} = \\textit{Bias}^2 + \\textit{Variance} + \\textit{Irreducible Error}\\],What is an example of an error in textit?,> > > > > > > > > > > > > > > > > > >
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"So, the error of the estimator at an unseen data sample \\(x_{0}\\) can be decomposed into the variance of the noise in the data, bias, and the variance of the estimator. This implies that both bias and variance are the sources of error in an estimator.",How can the error of the estimator be decomposed into the variance of the noise in the data?,Bias and variance of the estimator.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"So, the error of the estimator at an unseen data sample \\(x_{0}\\) can be decomposed into the variance of the noise in the data, bias, and the variance of the estimator. This implies that both bias and variance are the sources of error in an estimator.",What is the source of error in an estimate?,Bias and variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Reading: Bias-Variance Tradeoff.,Reading: Bias-Variance Tradeoff?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,bbThe bias and the variance of a models performance are connected.,The bias and the variance of a model performance are connected what?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.",What is the goal of applied machine learning for a given predictive modeling problem?,A model with low bias and low variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.",How can reducing the bias easily be achieved?,increasing the variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.",What relationship is generally referred to as?,The bias-variance trade-off.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,The variation of Bias and Variance with the model complexity.,What is the variation of Bias and Variance with the model complexity?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,This is similar to the concept of overfitting and underfitting. More complex models overfit while the simplest models underfit.,What concept is similar to the concept of overfitting and underfitting?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,This is similar to the concept of overfitting and underfitting. More complex models overfit while the simplest models underfit.,What type of models are more complex?,simplest models underfit
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, Cross-Validation. The figure below gives you a visual representation of bias-variance with the training dataset.",What is an ideal and balanced model that has a low bias and low variance?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, Cross-Validation. The figure below gives you a visual representation of bias-variance with the training dataset.",How can you work against overfitting?,With dimensionality reduction techniques
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,"As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, Cross-Validation. The figure below gives you a visual representation of bias-variance with the training dataset.",What can be optimized using a technique that will be discussed on the next page?,Trade-offs
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Bias-Variance and Training-Test Data.,What data does Bias-Variance and Training-Test Data generate?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,,Additional Reading: Bias-Variance Tradeoff.,What is a Bias-Variance Tradeoff?,
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"It is necessary to establish a design for the project as it can help identify various bottlenecks of the project. This documentation makes the developers begin thinking about the various risks and challenges involved in the data science project. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, and the system environment. A clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams. The developers are also prompted to consider the many risks and difficulties that the data science project may present.",What is required to establish a design for the data science project?,It is necessary to establish a design for the project.
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"It is necessary to establish a design for the project as it can help identify various bottlenecks of the project. This documentation makes the developers begin thinking about the various risks and challenges involved in the data science project. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, and the system environment. A clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams. The developers are also prompted to consider the many risks and difficulties that the data science project may present.",What can help identify various bottlenecks of the project? What are some key design considerations that developers should address?,A design
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:",What does every business make sure the software development life cycle is maintained to start the project because it is very crucial?,
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:",How can each of the tasks be assigned to certain team members?,
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:",What does Agile Scrum mean?,Agile Scrum means Agile Scrum means Agile Agile Scrum means Agile Agile Scrum means Agile
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,User stories: A high-level definition of a work request.,What is a high-level definition of a work request?,User stories
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Sprints: Short spans of work where teams work on tasks determined in the sprint planning meeting. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service.",What is a short span of work where teams work on tasks determined in the sprint planning meeting?,Sprints
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Sprints: Short spans of work where teams work on tasks determined in the sprint planning meeting. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service.",What is the goal of a sprint?,To review the product or service.
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Stand-up meetings:  These are daily stand-up meetings where participants are required to stay standing, helping to keep the meetings short and to the point.",What are stand-up meetings called?,Daily
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,"Stand-up meetings:  These are daily stand-up meetings where participants are required to stay standing, helping to keep the meetings short and to the point.",What is the purpose of stand up meetings?,"Participants are required to stay standing, to keep the meetings short and to the point."
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Agile board: This is a tracking system that helps your team track the progress of your project.,What is a tracking system that helps your team track your progress?,Agile board
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Backlog: This is the set of outstanding project requests that are added through your intake system. Managing your backlog is a vital role for project managers in an Agile environment.,What is the set of outstanding project requests that are added through your intake system?,Backlog
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Backlog: This is the set of outstanding project requests that are added through your intake system. Managing your backlog is a vital role for project managers in an Agile environment.,What is a vital role for project managers in an Agile environment?,Managing your backlog
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Sprint retrospective: This is a meeting where the team gathers to discuss what went well and what didn't.,What is a meeting where the team gathers to discuss what went well and what didn't?,Sprint retrospective
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Sprint retrospective: This is a meeting where the team gathers to discuss what went well and what didn't.,What is the purpose of the meeting?,To discuss what went well and what didn't.
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Demonstration: This is when the team demonstrates a working product to the stakeholder.,What happens when the team demonstrate a working product to the stakeholder?,Demonstration
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,,Demonstration: This is when the team demonstrates a working product to the stakeholder.,What is the name of the team that demonstrated a work product?,Demonstration
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"This module focused on formulating the analytic objective, which is also considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances.",What was the focus of the module on formulating the analytic objective?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"This module focused on formulating the analytic objective, which is also considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances.",What is considered a research hypothesis?,Analytic objective
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,A well-framed analytic objective will include the following elements:,What is a well-framed analytic objective?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Understanding the business objective,",What is the purpose of the business?,To help businesses grow.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Identifying the problem,","Identifying the problem, identifying the problem and identifying what?",What actions are necessary to identify the problem.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Focusing on a well-defined task,",What is the focus of a task?,Having a well-defined task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,Checking the proposed method against the target insight,When is the proposed method checked against the target insight?,During the proposal method's checkout
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Proposing data collection and curation methods, and",Proposing data collection and curation methods?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Proposing data collection and curation methods, and",What are two methods of curation and data collection?,- Several data collection methods
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,Framing the analytic objective.,What is the purpose of the analytic objective?,Framing
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Analytic objectives can be reframed to focus on specific tasks, in which case an incremental step would become the primary goal.",What can be reframed to focus on specific tasks?,Analytic objectives
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"Analytic objectives can be reframed to focus on specific tasks, in which case an incremental step would become the primary goal.",What would become the primary goal?,An incremental step
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"An analytic objective should state what specific functionality, insight, or resource is gained from leveraging that data and methods you propose, relative to the current situation, and assuming the project is successful as proposed.",What should an analytic objective state?,"What specific functionality, insight, or resource is gained from leveraging that data and methods you propose"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,,"An analytic objective should state what specific functionality, insight, or resource is gained from leveraging that data and methods you propose, relative to the current situation, and assuming the project is successful as proposed.",What is an example of a project that is successful?,One that is successful as proposed.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data. Data is typically categorized as numeric or categorical. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.","Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data?",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data. Data is typically categorized as numeric or categorical. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.","Quantitative data is represented as continuous or discrete values, while categorical data can be what?",nominal or ordinal values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.",Structured Data has organized facts that are presented in fixed formats and are what?,easy to extract
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.",Unstructured Data does not fit neatly in what row?,row
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.",Structured data cannot be maintained in what format?,uniform
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Unlike structured data, unstructured data can be stored without a predefined schema. New-generation database frameworks, also known as NoSQL databases, have been developed specifically to handle this type of data.",What type of data can be stored without a predefined schema?,Unstructured data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Unlike structured data, unstructured data can be stored without a predefined schema. New-generation database frameworks, also known as NoSQL databases, have been developed specifically to handle this type of data.",What are noSQL databases also known as?,New-generation database frameworks
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Internal data is data collected and/or controlled by an organization, and external data is data that is collected from sources outside of an organization.",What is internal data collected and/or controlled by an organization?,Internal data is data collected and/or controlled by an organization.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Internal data is data collected and/or controlled by an organization, and external data is data that is collected from sources outside of an organization.",What is external data collected from sources outside of a company?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. Data wrangling is sometimes referred to as feature engineering. Feature engineering involves selecting the right features from the data to further improve the performance of your models.","What is the process of cleaning, formatting, and enriching raw data to make it usable for analysis?",Data Wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. Data wrangling is sometimes referred to as feature engineering. Feature engineering involves selecting the right features from the data to further improve the performance of your models.",What is sometimes referred to as feature engineering?,Data wrangling
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Dummy variables are a technique that allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary/dummy variables.",What is a technique that allows for categorical data to be transformed into 0s and 1s?,Dummy variables
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Dummy variables are a technique that allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary/dummy variables.",What can a dataset containing customer spending data have?,"A categorical variable, gender"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Dummy variables are a technique that allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary/dummy variables.",How can the gender variable be converted to?,Binary/dummy variables
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.,The purpose of descriptive statistics is to make it easier to assimilate what?,information
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.,What is the purpose of the exploratory data analysis?,To make it easier to assimilate information.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.,How does EDA use non-graphical techniques?,To explore the data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables.",What does Correlation or correlation coefficient describe?,"How strong the association between two variables, x, and y, is in terms of"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,,"Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables.",What is a different measure than covariance because it describes the direction and strength of the linear relationship between variables?,Correlation coefficient
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Similar to traditional software development projects, data science projects are also guided by requirements gathering principles. Figure 1 lists the steps that are followed during the process. Requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s).",What are data science projects guided by?,Requirements gathering principles
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Similar to traditional software development projects, data science projects are also guided by requirements gathering principles. Figure 1 lists the steps that are followed during the process. Requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s).",What are the steps that are followed during a data science project?,Requirements gathering and requirements gathering.
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Figure 1. Requirement Gathering Process,What is the process for a Requirement Gathering Process?,Figure 1.
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project.,What process involves eliciting user and system needs and defining data and analytic requirements?,The requirements-gathering process
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Gather Information: The first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business's need, as well as decision-makers within the business. Once stakeholders are identified, the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives. Later on in this unit, we will discuss the techniques used to gather information.",What is the first step in gathering information?,Identifying the stakeholders
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Gather Information: The first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business's need, as well as decision-makers within the business. Once stakeholders are identified, the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives. Later on in this unit, we will discuss the techniques used to gather information.",What are the stakeholders in the business?,"Individuals, decision-makers, and decision-makers."
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Gather Information: The first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business's need, as well as decision-makers within the business. Once stakeholders are identified, the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives. Later on in this unit, we will discuss the techniques used to gather information.",Who will elicit information to determine what?,What the solution should do to meet the defined business and analytic objectives.
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Define and Prioritize Requirements: Stakeholders will provide information according to their view of the business needs; it is the job of the business analyst to lead the effort in defining and prioritizing requirements. It is important to document ""complete"" requirements that capture the needs of the stakeholders, as this will guide the project team in developing the right solution(s). Stakeholders might provide information that can be used for future projects related to the proposed solution; that information should not be discarded; it is prioritized as a low-priority requirement and considered for future implementation.",What is the job of the business analyst to lead the effort in defining and prioritizing requirements?,
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Define and Prioritize Requirements: Stakeholders will provide information according to their view of the business needs; it is the job of the business analyst to lead the effort in defining and prioritizing requirements. It is important to document ""complete"" requirements that capture the needs of the stakeholders, as this will guide the project team in developing the right solution(s). Stakeholders might provide information that can be used for future projects related to the proposed solution; that information should not be discarded; it is prioritized as a low-priority requirement and considered for future implementation.","What is important to document ""complete"" requirements that capture the needs of the stakeholders?",
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Define and Prioritize Requirements: Stakeholders will provide information according to their view of the business needs; it is the job of the business analyst to lead the effort in defining and prioritizing requirements. It is important to document ""complete"" requirements that capture the needs of the stakeholders, as this will guide the project team in developing the right solution(s). Stakeholders might provide information that can be used for future projects related to the proposed solution; that information should not be discarded; it is prioritized as a low-priority requirement and considered for future implementation.",When should information be used for future projects related to the proposed solution?,Low priority requirement
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Evaluate Requirements: The project team must verify and validate all documented requirements. This additional step in the requirements-gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.,What must the project team verify and validate?,Documented requirements
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Evaluate Requirements: The project team must verify and validate all documented requirements. This additional step in the requirements-gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.,What step will ensure that the solution meets the business needs and meets the expectations of stakeholders?,The requirements-gathering step.
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Receive Sign-Off: This is an indication that the requirements have been approved and agreed upon by the client. Requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution.,What is an indication that the requirements have been approved and agreed upon by the client?,Receive Sign-Off
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,Receive Sign-Off: This is an indication that the requirements have been approved and agreed upon by the client. Requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution.,How many times are the requirements signed off during the development lifecycle?,Twice
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"A requirements management plan can be used to document the requirements-gathering process. This document is made available to the client and the project team as it contains information that affects both parties. There is no standard template for this document, but it is in good practice to include the following sections:",What can be used to document the requirements-gathering process?,A requirements management plan
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"A requirements management plan can be used to document the requirements-gathering process. This document is made available to the client and the project team as it contains information that affects both parties. There is no standard template for this document, but it is in good practice to include the following sections:",What document is made available to the client and the project team as it contains information that affects both parties?,A requirements management plan
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,The project description is an overview of your project. This section describes the purpose of your project.,What is an overview of your project?,The project description
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,The project description is an overview of your project. This section describes the purpose of your project.,What is the purpose of the project description?,
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Team Responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements-gathering process. Data science project team members might take on duties outside of their normal roles, e.g., a data analyst on the data science team might serve in the role of Scribe during joint application development sessions.",What is defined in this plan to designate who will be involved in managing activities during the requirements-gathering process?,Team Responsibilities
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Team Responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements-gathering process. Data science project team members might take on duties outside of their normal roles, e.g., a data analyst on the data science team might serve in the role of Scribe during joint application development sessions.",What might a data analyst on the data science team serve in?,Scribe
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture, manage, and track requirements through the requirements-gathering process and throughout the project lifecycle.",What are some of the tools used to manage the requirements?,Project management tools and word processing or other dedicated systems
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture, manage, and track requirements through the requirements-gathering process and throughout the project lifecycle.","What type of system is used to capture, manage, and track requirements through the requirements-gathering process?",Dedicated
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Requirements Gathering Process should be defined in this plan. This section will describe the techniques used in eliciting user and system needs, defining the requirements, and evaluating the success of the requirements-gathering process (these techniques are covered later in this unit).",What should be defined in this plan?,Requirements Gathering Process
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Requirements Gathering Process should be defined in this plan. This section will describe the techniques used in eliciting user and system needs, defining the requirements, and evaluating the success of the requirements-gathering process (these techniques are covered later in this unit).",What are the techniques used in eliciting user and system needs?,Requirements Gathering Process
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Change control and requirements gathering: Modern-day collaboration tools will support change control. However, a change control process should be documented to ensure that changes to requirements are formally managed.",Change control and requirements gathering: Modern-day collaboration tools will support what?,change control
Data Science Project Planning,Requirements Gathering,Overview,Requirements Management Plan,"Change control and requirements gathering: Modern-day collaboration tools will support change control. However, a change control process should be documented to ensure that changes to requirements are formally managed.",What should be documented to ensure that changes to requirements are officially managed?,A change control process
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets.",What is the ultimate goal of any supervised machine learning problem?,To find a model or function that predicts a target or label.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets.",What means the function must be able to generalize and make accurate predictions on unseen inputs?,Minimizing error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Regularization is a general approach to help select a balanced model to trade off between a high bias and a high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve,  hence, the need for the trade-off to minimize the model's total error.",What is regularization a general approach to help select a balanced model to trade off between?,a high bias and a high variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Regularization is a general approach to help select a balanced model to trade off between a high bias and a high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve,  hence, the need for the trade-off to minimize the model's total error.",What is the ideal goal of generalization in terms of bias and variance?,A low bias and a low variance
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"There are three popular regularization techniques, each of them aiming at decreasing the total size of the parameters of the model (e.g., coefficients in a regression):",How many popular regularization techniques are there?,Three
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"There are three popular regularization techniques, each of them aiming at decreasing the total size of the parameters of the model (e.g., coefficients in a regression):",What is one technique that aims at decreasing the total size of the parameters of a model?,Regularization
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Ridge Regression, which penalizes the sum of squares of the coefficients (L2 penalty).",What penalizes the sum of squares of the coefficients?,Ridge Regression
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).",What penalizes the sum of absolute values of the coefficients?,Lasso Regression
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).",What penalty does Lasso Regression punish?,L1 penalty
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Elastic Net, a convex combination of Ridge and Lasso.",What is the convex combination of Ridge and Lasso?,Elastic Net
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,"Elastic Net, a convex combination of Ridge and Lasso.",What is Elastic Net?,A convex combination of Ridge and Lasso.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,,L1 and L2 Regularizations.,What are L1 and L2 Regularizations?,
Data Science Project Planning,Design and Plan Overview,Overview,,"After the project design has been developed, it can be divided into sub-tasks. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. For industry-oriented projects, it might be useful to include budget plans for every individual task. This would help judiciously distribute temporal, human, and monetary resources. The milestone plan developed in this section could be fairly specific, involving every minor update to the project. This could be communicated via a table of tasks, a flow diagram, or even a Gantt chart. These useful tools can further simplify the understanding process for future researchers and developers who would continue working in this domain.",What can be divided into sub-tasks after the project design has been developed?,Tasks
Data Science Project Planning,Design and Plan Overview,Overview,,"After the project design has been developed, it can be divided into sub-tasks. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. For industry-oriented projects, it might be useful to include budget plans for every individual task. This would help judiciously distribute temporal, human, and monetary resources. The milestone plan developed in this section could be fairly specific, involving every minor update to the project. This could be communicated via a table of tasks, a flow diagram, or even a Gantt chart. These useful tools can further simplify the understanding process for future researchers and developers who would continue working in this domain.",What can each of the tasks be assigned to certain team members?,Sub-tasks
Data Science Project Planning,Design and Plan Overview,Overview,,"After the project design has been developed, it can be divided into sub-tasks. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. For industry-oriented projects, it might be useful to include budget plans for every individual task. This would help judiciously distribute temporal, human, and monetary resources. The milestone plan developed in this section could be fairly specific, involving every minor update to the project. This could be communicated via a table of tasks, a flow diagram, or even a Gantt chart. These useful tools can further simplify the understanding process for future researchers and developers who would continue working in this domain.","For industry-oriented projects, it might be useful to include budget plans for every individual task?",
Data Science Project Planning,Design and Plan Overview,Overview,,"After the project design has been developed, it can be divided into sub-tasks. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. For industry-oriented projects, it might be useful to include budget plans for every individual task. This would help judiciously distribute temporal, human, and monetary resources. The milestone plan developed in this section could be fairly specific, involving every minor update to the project. This could be communicated via a table of tasks, a flow diagram, or even a Gantt chart. These useful tools can further simplify the understanding process for future researchers and developers who would continue working in this domain.",The milestone plan developed in this section could involve what?,every minor update to the project
Data Science Project Planning,Design and Plan Overview,Overview,,Preparing such a comprehensive collection of documentation for any data science project. Diagrams in the documentation of a data science project are very useful as they help translate requirements and system design much more efficiently.,What is the purpose of preparing a comprehensive collection of documentation for a data science project?,
Data Science Project Planning,Design and Plan Overview,Overview,,Preparing such a comprehensive collection of documentation for any data science project. Diagrams in the documentation of a data science project are very useful as they help translate requirements and system design much more efficiently.,What are diagrams in the documentation of a project that help translating requirements and system design more efficiently?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.",What is a data science pattern that can be used to solve different data science tasks from machine translation to information retrieval?,Ranking
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.",What is the technique used to train a model for ranking tasks?,Learning-to-rank
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:,What type of queries do users pose to retrieve documents matching a query?,Queries
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:,What can we use?,One of the following approaches
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.",What is the assumption that each we can compute a numerical score that captures how much a document is relevant to a query?,Pointwise Approach
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.",What can be cast as a regression problem given a (training set of ) query-document pair?,Learning-to-rank problem
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.,What does Pairwise Approach seek to reduce?,Number of wrongly ordered rankings
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.,What is the term for the average number of wrongly ordered rankings?,Average
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.,How is each pair of documents classified by a binary classifier?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.",Listwise Approach reviews the list of documents and produces an optimal ordering?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.",What does Listwise approach try to directly optimize the value of?,One of the above evaluation measures
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,"Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.",How many evaluation measures are not continuous functions?,most
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:,What is the name of the three known learnings that Microsoft has developed to rank algorithms that all use what?,Pairwise ranking
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:,How many learnings has Microsoft Research developed?,Three
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.,What algorithm uses gradient descent to update the weights or model parameters for a learning-to-rank task?,RankNet
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.,What algorithm seeks to minimize the number of wrong orderings?,RankNet
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.,What function does LambdaRank use to train a RankNet?,Cost
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.,What results in speed and accuracy improvements?,RankNet
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.,What is an implementation of the gradient tree boosting methods for regression and classification?,Multiple Additive Regression Trees (MART)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.,What does LambdaRank use to solve a ranking task?,Multiple Additive Regression Trees
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Learning to Rank Algorithms (Source: Lucidworks),What is Learning to Rank Algorithms?,(Source: Lucidworks)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Additional Reading: Application of LTR - Bayesian Product Ranking at Wayfair,What is the Bayesian Product Ranking at Wayfair?,LTR
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Additional Reading: From RankNet to LambdaRank to LambdaMART,What is RankNet's name?,Lambda Rank
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,,Additional Reading: From RankNet to LambdaRank to LambdaMART,What is LambdaMART?,A measure to measure RankNet to LambdaRank
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:",What type of processing tasks begin with converting textual to numerical data that machines can understand?,Natural language
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:",What is critical for the success of downstream tasks?,A good representation
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:",The NLP module provided an introduction to what techniques?,Text representation
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.",What were the individual items in a vocabulary represented as dimensions?,Terms
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.",What was the curse of dimensionality?,
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.,What can be useful for the downstream tasks?,Context and word order
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.,What could be encoded within the numerical representations?,No useful information like context and word order that can be useful for downstream tasks
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, learn, and study will be closer than the pair learn and eat. Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.",What is a term used for the representation of words?,Word embeddings
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, learn, and study will be closer than the pair learn and eat. Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.",What does embeddings encode the semantics of words essentially by capturing the contexts in which they appear?,Word embeddings are a fixed-size real-valued vector.
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, learn, and study will be closer than the pair learn and eat. Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.","Words that are closer in the vector space of the word embedded vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have what?",a smaller distance than dissimilar words
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, its a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.",What are embeddings often learned automatically from large text corpora?,
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, its a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.",What is the idea that contextual information alone can help in generating a viable representation of linguistic items?,These embeddings are often learned automatically from large text corpora.
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, its a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.",Why are the semantics captured solely using raw text data?,Because it is generated solely using raw text data.
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Word2Vec (2013) and GloVe (dGlobal Vectors for Word Representationd) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.",Word2Vec and GloVe are two early models to generate what?,Word embeddings
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Word2Vec (2013) and GloVe (dGlobal Vectors for Word Representationd) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.",What are Word2vec embeddings based on?,Training a shallow feedforward neural network
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Word2Vec (2013) and GloVe (dGlobal Vectors for Word Representationd) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.",How are Glove embeddeds learned?,matrix factorization techniques
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.",What does neither of the models handle polysemy very well?,
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.",How many embeddings do these models output for each word?,One
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.",What did cHe go to the prison cell with his cell phone to extract from inmates?,Blood cell samples
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"This problem was solved by the ELMo (""Embeddings from Language Model"") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.",When was the ELMo model developed?,2018
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"This problem was solved by the ELMo (""Embeddings from Language Model"") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.",What is the name of a task that is trained to predict the next word in a sequence of words?,Language Modeling
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.,What is another issue with contextualized word embeddings?,They are often not enough to get a good performance in all kinds of NLP tasks.
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.,What is the main problem with contextualization?,
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.,How do representations need task-specific fine-tune?,To obtain better results.
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"ULM-FiT (cUniversal Language Model Fine-tuningd), also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:",What was ULM-FiT introduced in 2018?,
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"ULM-FiT (cUniversal Language Model Fine-tuningd), also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:",What is a three-stage process that can be applied to any NLP task?,ULM-FiT
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.",What is a general domain LM Pre-Training?,A general domain LM Pre-Training is a general domain LM Pre-
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.",What is the language model that is trained on?,A general-domain corpus
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.,Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using what?,discriminative fine-tuning and a changing learning rate schedule
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,Target task Classifier Fine-Tuning where the classifier is fine-tuned on the target task using gradual unfreezing (unfreezing weights from the last to the first layer in different learning epochs) and repeating stage 2. This helps the network to preserve low-level representations and adapt to high-level ones.,Target task Classifier Fine-Tuning where the classifier is fine-tuned on the target task using what?,Progressive unfreezing
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. Well discuss the  Transformer model and the motivation behind its architecture in detail next.",What year was the Transformer architecture released?,2017
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. Well discuss the  Transformer model and the motivation behind its architecture in detail next.",What was the name of the first model that revolutionized the NLP field?,Transformer
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,,"Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. Well discuss the  Transformer model and the motivation behind its architecture in detail next.",How did the Transformer model replace LSTM models?,
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Quiz 4,,,What does nan do?,He is a nurse
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Quiz 4,,,What is the name of the nnan?,The nan name is Peter Durning.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Module 14 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"The process of discovering the requirements of stakeholders and systems is called Requirements Gathering.  The requirements gathering process involves eliciting user and system needs, defining the formal requirements from those needs, and evaluating the success of the requirements gathering process. The data science project team will understand the needs of the system and stakeholders set by the business team and end-users (elicit needs/expectations), and then analyze and align the expectations to the business and analytic objectives to define the requirements for the project.",What is the process of discovering the requirements of stakeholders and systems called?,Requirements Gathering
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"The process of discovering the requirements of stakeholders and systems is called Requirements Gathering.  The requirements gathering process involves eliciting user and system needs, defining the formal requirements from those needs, and evaluating the success of the requirements gathering process. The data science project team will understand the needs of the system and stakeholders set by the business team and end-users (elicit needs/expectations), and then analyze and align the expectations to the business and analytic objectives to define the requirements for the project.","What process involves eliciting user and system needs, defining the formal requirements from those needs and evaluating the success of the requirements gathering process?",Requirements Gathering
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"The process of discovering the requirements of stakeholders and systems is called Requirements Gathering.  The requirements gathering process involves eliciting user and system needs, defining the formal requirements from those needs, and evaluating the success of the requirements gathering process. The data science project team will understand the needs of the system and stakeholders set by the business team and end-users (elicit needs/expectations), and then analyze and align the expectations to the business and analytic objectives to define the requirements for the project.",Who will understand the needs of the system and stakeholders set by the business team and end-users?,The data science project team
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,Requirements gathering should be performed systematically to ensure that information is extracted from diverse sources. This will ensure that little to no bias is introduced during the process and that all user and system needs are represented and met. It is important to note that different scenarios and circumstances call for a specific type of requirements-gathering technique.,What should be done to ensure that information is extracted from diverse sources?,Requirements gathering
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,Requirements gathering should be performed systematically to ensure that information is extracted from diverse sources. This will ensure that little to no bias is introduced during the process and that all user and system needs are represented and met. It is important to note that different scenarios and circumstances call for a specific type of requirements-gathering technique.,What is important to note that different scenarios and circumstances require a specific type of requirements gathering technique?,Requirements gathering technique
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Interviews. Interviews allow the project team to thoroughly investigate the needs of the stakeholder. Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles. Bear in mind that this can be a time-consuming process. In order to ensure that an interview elicits the right information, an interviewer should develop questions that address the right issues and, in certain cases, probe for answers to get useful information related to the business, system, and user. Interviews can be conducted in a one-on-one setting or in a group setting.",Interviews allow the project team to thoroughly investigate what?,The needs of the stakeholder
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Interviews. Interviews allow the project team to thoroughly investigate the needs of the stakeholder. Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles. Bear in mind that this can be a time-consuming process. In order to ensure that an interview elicits the right information, an interviewer should develop questions that address the right issues and, in certain cases, probe for answers to get useful information related to the business, system, and user. Interviews can be conducted in a one-on-one setting or in a group setting.","Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles?",
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Interviews. Interviews allow the project team to thoroughly investigate the needs of the stakeholder. Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles. Bear in mind that this can be a time-consuming process. In order to ensure that an interview elicits the right information, an interviewer should develop questions that address the right issues and, in certain cases, probe for answers to get useful information related to the business, system, and user. Interviews can be conducted in a one-on-one setting or in a group setting.",What can an interviewer develop to ensure that an interview elicits the right information?,Questions
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Brainstorming. Brainstorming sessions involve gathering ideas from multiple stakeholders at once. In a brainstorming session, a facilitator monitors the session to ensure all possible solutions are identified, and all parties contribute to the process in a reasonable amount of time.",What do brainstorming sessions involve gathering ideas from multiple stakeholders at once?,
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Brainstorming. Brainstorming sessions involve gathering ideas from multiple stakeholders at once. In a brainstorming session, a facilitator monitors the session to ensure all possible solutions are identified, and all parties contribute to the process in a reasonable amount of time.",What does a facilitator monitor in a brainstorming session to ensure all possible solutions are identified?,
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"This technique can yield new ideas and themes that would otherwise be difficult for the analyst to produce. A brainstorming session should have five to eight representatives from each shareholder group, including management, users, and support staff.",How many representatives should an analyst have in a brainstorming session?,Five to Eight
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"This technique can yield new ideas and themes that would otherwise be difficult for the analyst to produce. A brainstorming session should have five to eight representatives from each shareholder group, including management, users, and support staff.",How many members should a meeting have in each group?,Five to Eight
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Questionnaires. The project team prepares and distributes a questionnaire with predefined questions to stakeholders in the company. The responses from these questionnaires are then analyzed and used to define the project requirements. A well-designed questionnaire can be completed by non-technical system users and stakeholders with little or no guidance from the project team. This technique can be used when interviews are not possible due to budget, time, and scheduling constraints.",What is the name of the project team that prepares and distributes a questionnaire with predefined questions to stakeholders in the company?,Questionnaires
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Questionnaires. The project team prepares and distributes a questionnaire with predefined questions to stakeholders in the company. The responses from these questionnaires are then analyzed and used to define the project requirements. A well-designed questionnaire can be completed by non-technical system users and stakeholders with little or no guidance from the project team. This technique can be used when interviews are not possible due to budget, time, and scheduling constraints.",What can a well-designed questionnaire be completed by non-technical system users and stakeholders with little or no guidance from the team?,
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,Document Analysis. This is an information-gathering technique that involves reviewing existing documentation to elicit needs. Document analysis can be the first step in the requirements gathering process and can be used to create questions for questionnaires or interview sessions. Document analysis is considered to be a supporting technique and can serve as a completeness check for requirements.,What is the first step in the requirements gathering process?,Document Analysis
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,Document Analysis. This is an information-gathering technique that involves reviewing existing documentation to elicit needs. Document analysis can be the first step in the requirements gathering process and can be used to create questions for questionnaires or interview sessions. Document analysis is considered to be a supporting technique and can serve as a completeness check for requirements.,What is document analysis considered to be?,A supporting technique
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",What is a joint application design session called?,A Workshop
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",What is the purpose of a Joint Application Design session?,To collect information from stakeholders.
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",How can a successful executed workshop result in the development of requirements?,
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,Requirements Gathering Techniques,"Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",Who is the Executive Sponsor?,Scribe
Deep Learning and Model Deployment,CPU vs. GPU,Module 21 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Quiz 11,,,What does nan do?,He is a nurse
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Quiz 11,,,What is the name of the nnan?,The nan name is Peter Durning.
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance defines how data is accessed and managed within an organization. It is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis. An organization that handles data efficiently understands that data governance impacts data quality and the decisions made from the data available to the organization. This unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and its decision-making. Any organization that stores and utilizes data should have a data governance strategy for internal data, or data stored within the organization, and external data.",What defines how data is accessed and managed within an organization?,Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance defines how data is accessed and managed within an organization. It is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis. An organization that handles data efficiently understands that data governance impacts data quality and the decisions made from the data available to the organization. This unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and its decision-making. Any organization that stores and utilizes data should have a data governance strategy for internal data, or data stored within the organization, and external data.","What is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis?",Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance defines how data is accessed and managed within an organization. It is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis. An organization that handles data efficiently understands that data governance impacts data quality and the decisions made from the data available to the organization. This unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and its decision-making. Any organization that stores and utilizes data should have a data governance strategy for internal data, or data stored within the organization, and external data.",An organization that handles data efficiently understands that data governance impacts what?,Data quality and the decisions made from the data available to the organization.
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, maps the location of data in the enterprise, reduces the scourge of data silos, and improves data management overall.",What is beneficial because it provides a reliable and consistent view of enterprise-wide data?,Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, maps the location of data in the enterprise, reduces the scourge of data silos, and improves data management overall.","What is a plan for improved quality of data, maps the location of data in the enterprise?",
Collecting and Understanding Data,Ethics of Data Science,Governance,,Reading: Data Governance in the Cloud.,What does Reading: Data Governance in the Cloud do?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,Reading: Data Governance in the Cloud.,What does reading: data governance in the cloud?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is the responsibility of an entire organization; although it is often administered by the data management team, all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that makes or is affected by data-driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as to determine the requirements for the analytic solution. So why is data governance important to a member of a data science project team?",What is the responsibility of an entire organization?,Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is the responsibility of an entire organization; although it is often administered by the data management team, all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that makes or is affected by data-driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as to determine the requirements for the analytic solution. So why is data governance important to a member of a data science project team?",What does the Data Governance Institute define an individual or group that makes or is affected by data-driven decisions within an organization? What is data governance important to a member of a data science project team?,Stakeholder
Collecting and Understanding Data,Ethics of Data Science,Governance,,Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).,What is more than just policymaking for data?,Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).,What influences business strategy?,Iit
Collecting and Understanding Data,Ethics of Data Science,Governance,,Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).,An organization that has embedded data governance principles into its data infrastructure or data management framework is able to follow what?,Data standards
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance best practices for organizations are met when data has integrity, and data-related decisions and controls are transparent and can be audited. Another best practice gaining ground in the industry is for organizations to collect and store data that is unbiased. Unbiased data means something different to each organization and industry, but the general idea is that the data represent all members of a population that could be served by an organization. This best practice will positively influence the development of ethical models and algorithms for analytic solutions.",What is one way organizations can collect and store data that is unbiased?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance best practices for organizations are met when data has integrity, and data-related decisions and controls are transparent and can be audited. Another best practice gaining ground in the industry is for organizations to collect and store data that is unbiased. Unbiased data means something different to each organization and industry, but the general idea is that the data represent all members of a population that could be served by an organization. This best practice will positively influence the development of ethical models and algorithms for analytic solutions.",What does unbiased data mean to each organization and industry?,Something different
Collecting and Understanding Data,Ethics of Data Science,Governance,,Reading: Data Governance and Its Implications for Ethical Models.,Reading: Data Governance and its Implications for Ethical Models?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is necessary to ensure that data is safe, secure, private, usable, and in compliance with both internal and external data policies. Data governance allows setting and enforcing controls that allow greater access to data, gaining security and privacy from the controls on data. Some common use cases include:","What is necessary to ensure that data is safe, secure, private, usable, and in compliance with internal and external data policies?",Data governance
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data governance is necessary to ensure that data is safe, secure, private, usable, and in compliance with both internal and external data policies. Data governance allows setting and enforcing controls that allow greater access to data, gaining security and privacy from the controls on data. Some common use cases include:",What does data governance allow setting and enforcing controls that allow greater access to data?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.d,What does data governance often mean?,Giving accountability and responsibility for both the data itself and the processes that ensure its proper use to 
Collecting and Understanding Data,Ethics of Data Science,Governance,,Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.d,What does cdata stewards.d appoint?,Responsible use of the data.
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.",What does data governance refer to?,Activities or techniques designed to make sure data is suitable to be used
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.",How many dimensions is data quality judged?,Six
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.",What is data governance also used to ensure?,Data quality
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data management. This is a broad concept encompassing all aspects of managing data as an enterprise asset, from collection and storage to usage and oversight, making sure that data  are leveraged securely, efficiently, and cost-effectively before they are disposed of.",What is a broad concept of data management?,
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data management. This is a broad concept encompassing all aspects of managing data as an enterprise asset, from collection and storage to usage and oversight, making sure that data  are leveraged securely, efficiently, and cost-effectively before they are disposed of.",What does data management encompass?,"Data management encompasses all aspects of managing data as an enterprise asset, from collection and storage"
Collecting and Understanding Data,Ethics of Data Science,Governance,,"Data management. This is a broad concept encompassing all aspects of managing data as an enterprise asset, from collection and storage to usage and oversight, making sure that data  are leveraged securely, efficiently, and cost-effectively before they are disposed of.",How is data leveraged?,"Succeeds, efficiently, and cost-effectively"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.",What is considered to be among the simpler methods for prediction and classification?,Tree-based methods
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.",What is rated highly as an interpretable method?,tree method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.",Some data science practitioners and thought leaders favor the simplicity of what?,tree-based models
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.",What is one of the most popular methods that we can use to create a decision tree?,Decision Trees.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.",What is a scikit-learn example of?,Decision Trees
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.",What is the end goal of a decision tree method?,Predict the value of a target variable based on several predictors.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.",What is an outcome response that contains a categorical value?,Classification Tree
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Additional Reading: Decision Trees for Decision Making,What are Decision Trees for Decision Making?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Additional Reading: Decision Trees for Decision Making,What is another example of a Decision Tree?,Another example of a Decision Tree is also included in a Decision Tree.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.,What are two concepts used to ensure the model has a low error rate and that overfitting is not an issue?,Partitioning and pruning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.",What is one of the popular algorithms that employ recursive partitioning?,C4.5
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.",What is done by repeatedly splitting and creating subsets until the tree is pure?,Partitioning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.",How does partitioning split each node on the decision tree?,Recursive
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.",What is another technique for building decision trees?,Chi-square automatic interaction detection (CHAID)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.",What is the Chi-square automatic interaction detection (CHAID) used for?,Classification and prediction
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.",Who is the most useful when you have a large dataset?,CHAID
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.",How can CHAID help Capital One's marketing firm predict?,"How your age, income, and credit score will affect your response to the interest rate offered."
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.",What are measures of impurity?,Entropy and the Gini index
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.",What is a measure of uncertainty within a model?,Entropy
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.",Which index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen?,The Gini index
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Reading: Gini Index and Impurity Measures,What are Gini Index and Impurity Measures?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.",What is the term for pruning?,pruning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.",What is one of the solutions to avoid overfitting the training dataset?,Pruning decision trees
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.",How can you prune the weakest branches?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.",What type of pruning will generate a series of trees?,Cost complexity
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.",What is the name of the tree created by removing a subtree and replacing it with a leaf node?,The Tree
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.",The best tree is chosen by what?,Generalized accuracy
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Reduced error pruning is done by replacing each node with the node's most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.",What is done by replacing each node with the most popular class of the node?,Reduced error pruning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Reduced error pruning is done by replacing each node with the node's most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.",What is an efficient technique for pruning?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Application: Decision Trees and NLP: A Case Study in POS Tagging.,What are Decision Trees and NLP?,A Case Study in POS Tagging
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Application: Decision Trees and NLP: A Case Study in POS Tagging.,What is a Case Study in POS Tagging?,Decision Trees and NLP
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.","When a full tree is built, what will result in a fully grown decision tree?",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.",What is the least complex tree with the smallest validation error called a Minimum Error Tree?,Best Pruned Tree
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.",What dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree?,Validation
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.",What is a performance measure for classification trees?,Misclassification rate
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",What are decision trees more explainable than linear regression models?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",A smaller tree can easily be interpreted by someone who is not in what field?,field
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",What is the impurity measure for a regression tree?,Sum of the squared deviations from the mean of the terminal nodes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).",The predictive accuracy of CART models is not as robust as other methods?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.",What can be used to improve prediction accuracy and performance?,"Random Forests, Bagging, and Boosting"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.",What is the purpose of Random Forests and Boosting?,Improve prediction accuracy and performance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.",What reduces variance in a decision tree method?,Bagging
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.",What does Bagging do to reduce overfitting issues?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.",How is Out-of-Bag Error Estimation calculated?,On data that was not used in the analysis of a model.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Additional Reading: History of Random Forest Algorithm,What is the history of Random Forest Algorithm?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).",What is an extension of bagging and makes some changes to bagged trees?,Random Forests
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).",What is the focus of a random forest?,The low correlation between trees.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).",A random forest will build several decision trees and then merge them for better accuracy and predictive value?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).",The random forest method will evaluate the importance of what?,Features
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.",What can be used to improve the predictive accuracy of certain methods including decision trees?,Boosting
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.",What does boosting differ from?,Bagging
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.",How is each tree created iteratively?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.",What can occur in boosting if the number of trees becomes too large?,Overfitting
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.",What is one of the most popular techniques used in Boosting?,AdaBoost
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees","Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.",AdaBoost is used to improve the performance of what?,models
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.,What method was represented as an extension of the tree method?,Ensemble methods
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.,What method is used for other methods?,Ensemble methods
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Reading: Ensemble Methods-General Use,Reading: Ensemble Methods-General Use What?,A dictionary
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Building Classification Trees,Building Regression Trees",Reading: Ensemble Methods-General Use,What is the purpose of Reading?,To general use of Ensemble Methods-General Use
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there werent many rules to follow, so people did and took what they could and werent exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome  that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.",What did people do for many of human civilizations?,They did everything they could to thrive under challenging circumstances.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there werent many rules to follow, so people did and took what they could and werent exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome  that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.",What was necessary to survive difficult times?,Ethics
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there werent many rules to follow, so people did and took what they could and werent exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome  that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.",How did people see the tragedy of the commons mentioned in the module?,Through some people took what they could and weren't exactly ethical.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"We have virtually unlimited access to data as data scientists today, and we have unprecedented analytical techniques with which to analyze that data. So the question we should be thinking about is whether we should do something just because it is technically possible. Are there things that are possible to do but which we can agree would not be right to do? This may sound strange, but it is a question that modern science has already begun considering and continues to do so.",What type of data do we have as data scientists today?,virtually unlimited
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"We have virtually unlimited access to data as data scientists today, and we have unprecedented analytical techniques with which to analyze that data. So the question we should be thinking about is whether we should do something just because it is technically possible. Are there things that are possible to do but which we can agree would not be right to do? This may sound strange, but it is a question that modern science has already begun considering and continues to do so.",What is a question that modern science has already begun considering?,whether we should do something just because it is technically possible
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences peoples lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.",What does data science create?,Impacts
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences peoples lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.",What is the power that you have as a data scientist and how it influences people's lives?,
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences peoples lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.",Who is responsible for exercising that great power?,It is crucial as data scientists that we be responsible for exercising that great power.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,The difficult thing about being an ethical data scientist is not about understanding ethics. It is the connection to how one applies ethical principles to data science practice; it is about doing cgoodd or ethical data science.,What is the difficult thing about being an ethical data scientist?,It is not about understanding ethics.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,The difficult thing about being an ethical data scientist is not about understanding ethics. It is the connection to how one applies ethical principles to data science practice; it is about doing cgoodd or ethical data science.,What does cgoodd or ethical data science do?,Does cgoodd or ethical data science?
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"By Rijksdienst voor het Cultureel Erfgoed, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=23391637","What does Rijksdienst voor het Cultureel Erfgoet, CC BY-SA4.0, https://commons.wikimedia.org/w/index.php?",Wrong=23391637
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.",What was the name of the Amsterdam Civil Registry Office?,It was named Amsterdam Civil Registry Office
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.",When did the Nazis take over the registry office?,World War II
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.",What was used to identify members of the Dutch resistance during World War II?,Records about their birth
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.",Who saw that the existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine?,The Dutch resistance
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.",When did the Dutch resistance plan a bombing of the Office?,1943
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.",What was the name of the plaque that lists the names of the people in the Dutch Resistance who bombed the office?,Figure 2
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"By Design: Willem Sandberg. Photographer: Frans Willemsen - Own work, CC BY-SA 3.0 nl, https://commons.wikimedia.org/w/index.php?curid=32468279",What is the name of the photographer who is a photographer?,Frans Willemsen
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"By Design: Willem Sandberg. Photographer: Frans Willemsen - Own work, CC BY-SA 3.0 nl, https://commons.wikimedia.org/w/index.php?curid=32468279",What is CC BY-SA 3.0 nl?,Photographer
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"We hope this is a good place to start the conversation about doing good data science and the motivation to do so. In the rest of the module, we hope to convince you, if youre not already convinced, that the story of the 1943 bombing of the Amsterdam civil registry office is highly relevant to the work you do as a data scientist. While the data science work you do may be mundane by comparison and not involve a life-or-death decision, ethical data science practices matter to everyday things that affect all of us. The goal of this module is to help you participate in the ethical debates that you will face as a data scientist, infuse your data science work with ethical principles, and inform you to be thoughtful, deliberate, and ethical  the kind of data scientists that we all hope that youre going to be.",What was the bombing of the Amsterdam civil registry office in 1943?,It was a bomb.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"We hope this is a good place to start the conversation about doing good data science and the motivation to do so. In the rest of the module, we hope to convince you, if youre not already convinced, that the story of the 1943 bombing of the Amsterdam civil registry office is highly relevant to the work you do as a data scientist. While the data science work you do may be mundane by comparison and not involve a life-or-death decision, ethical data science practices matter to everyday things that affect all of us. The goal of this module is to help you participate in the ethical debates that you will face as a data scientist, infuse your data science work with ethical principles, and inform you to be thoughtful, deliberate, and ethical  the kind of data scientists that we all hope that youre going to be.",What is the purpose of this module?,To help you participate in the ethical debates that you will face as a data scientist.
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,[Required Reading],What type of reading is required?,Required Reading
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Please read chapter 3 from Loukides, Mason, H., & Patil, D. (2018). Ethics and Data Science (1st edition). OReilly Media, Inc.","What is the title of the chapter 3 from Loukides, Mason, H., & Patil, D. (2018)?",Ethics and Data Science
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Please read chapter 3 from Loukides, Mason, H., & Patil, D. (2018). Ethics and Data Science (1st edition). OReilly Media, Inc.","What does OReilly Media, Inc. do?",It helps you navigate the complex data science process
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Note: When prompted to select institution, select ""Not listed? Click here"" and enter your CMU email address to access content.",What is the name of the institution you are prompted to select?,Not listed? Click here
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,,"Note: When prompted to select institution, select ""Not listed? Click here"" and enter your CMU email address to access content.",What is your CMU email address?,Not listed? Click here
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need  Business Objective  Analytic Objective including Problem Statement  Task Definition  Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:",What is the purpose of the conceptual progression Business Need Business Objective Analytic Objective including Problem Statement Task Definition Method & Data Statement?,To refining our understanding of what the client needs until we arrive at a technical project
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need  Business Objective  Analytic Objective including Problem Statement  Task Definition  Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:",How many elements are important to the statement of methods to be applied?,three
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",The statement  must be precise enough so that the data science team understands it as a concise summary of the technical approach.,What must the data science team understand as a concise summary of the technique?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","At the same time, it should allow for testing different techniques around the main conceptual idea.",What should allow for testing different techniques around the main conceptual idea?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",The methods should in principle be suitable to produce the target functionality/insight for the task given the available data.,What should the methods be used to produce the target functionality/insight?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","As you become more proficient in the discipline of data science (e.g., after having completed the more advanced units of this course), you will develop refined intuitions about which method to propose in which context. The most general categories of methods one can identify in this statement typically include:",What do you develop as you become more proficient in the discipline of data science?,Raffine intuitions
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","As you become more proficient in the discipline of data science (e.g., after having completed the more advanced units of this course), you will develop refined intuitions about which method to propose in which context. The most general categories of methods one can identify in this statement typically include:",What are the most general categories of methods one can identify in this statement?,:
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Supervised learning methods which involve learning to predict a target variable (typically through regression or classification) by training on ctrued example data points whose target variable has manually been labeled or is available by other means.,What do supervised learning methods involve learning to predict a target variable?,Regression or classification
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Supervised learning methods which involve learning to predict a target variable (typically through regression or classification) by training on ctrued example data points whose target variable has manually been labeled or is available by other means.,What are ctrued example data points whose target variable has been manually labeled or is available by other means?,Non-standard
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Unsupervised learning methods which deal with finding patterns in unlabeled data without an explicit prediction target.,Unsupervised learning methods deal with finding patterns in unlabeled data without what?,An explicit prediction target
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",Semi-Supervised learning methods which encompass hybrid methods that combine supervised and unsupervised learning in different ways.,What are semi-supervised learning methods that combine supervised and unsupervised learning in different ways?,Hybrid methods
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","This course is very focused on the application of machine learning, which has a large overlap with various kinds of statistical methods. It is possible to phrase your method statement around the use of statistics, but it is recommended that one qualifies this rather broad term as something adequate for the project.",What is the focus of the course on machine learning?,Application of machine learning
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","This course is very focused on the application of machine learning, which has a large overlap with various kinds of statistical methods. It is possible to phrase your method statement around the use of statistics, but it is recommended that one qualifies this rather broad term as something adequate for the project.",What is a large overlap with different types of statistical methods?,machine learning
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","In many contexts, the method will need to be stated much more precisely than that, especially if the problem domain is already well-studied in data science or there is some prior work that should be extended. The method can range from specifying a particular family of models (e.g., linear vs. non-linear), using a new feature set, testing the explainability of a particular models predictions, optimizing hyperparameters for faster training and inference in neural networks, and more. We can illustrate the specific vs. general statement in the context of an example:",What can the method range from specifying a particular family of models?,linear vs. non-linear
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","In many contexts, the method will need to be stated much more precisely than that, especially if the problem domain is already well-studied in data science or there is some prior work that should be extended. The method can range from specifying a particular family of models (e.g., linear vs. non-linear), using a new feature set, testing the explainability of a particular models predictions, optimizing hyperparameters for faster training and inference in neural networks, and more. We can illustrate the specific vs. general statement in the context of an example:",How can we illustrate the specific vs. general statement in a context of an example?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the quality control personnel do not reliably find all potential defects (problem). Your model should be able to predict product failure within one month after the sale (task 1) and identify predictors measured at quality control time (task 2). In a pilot project, you propose to apply ctraditional supervised learning methods to train a classifier and examine the model for predictive variables.d You then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation.",What does your client ask you to help improve?,Quality control
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the quality control personnel do not reliably find all potential defects (problem). Your model should be able to predict product failure within one month after the sale (task 1) and identify predictors measured at quality control time (task 2). In a pilot project, you propose to apply ctraditional supervised learning methods to train a classifier and examine the model for predictive variables.d You then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation.",What should your model be able to predict product failure within one month after the sale?,Task 1
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the quality control personnel do not reliably find all potential defects (problem). Your model should be able to predict product failure within one month after the sale (task 1) and identify predictors measured at quality control time (task 2). In a pilot project, you propose to apply ctraditional supervised learning methods to train a classifier and examine the model for predictive variables.d You then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation.","In a pilot project, you propose to use supervised learning methods to train a classifier and examine the model for what?",Predictive variables
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Variation: Your pilot project was a success, and your models are being used to better inform quality control personnel. However, it turns out that it still produces a high false-positive rate (problem) and causes shipping delays, which the client wants to minimize (business objective). You are asked to improve the models performance by reducing its false-positive rate (task). You are further given access to quality control diagnostic equipment readings, which the engineers believe are useful to discern whether a product is defective or just cneeds to be broken in.d You hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model.",What is a problem that the client wants to minimize?,Shipping delays
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Variation: Your pilot project was a success, and your models are being used to better inform quality control personnel. However, it turns out that it still produces a high false-positive rate (problem) and causes shipping delays, which the client wants to minimize (business objective). You are asked to improve the models performance by reducing its false-positive rate (task). You are further given access to quality control diagnostic equipment readings, which the engineers believe are useful to discern whether a product is defective or just cneeds to be broken in.d You hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model.",What is the purpose of reducing the false-positive rate of a product?,To improve the models performance
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","Variation: Your pilot project was a success, and your models are being used to better inform quality control personnel. However, it turns out that it still produces a high false-positive rate (problem) and causes shipping delays, which the client wants to minimize (business objective). You are asked to improve the models performance by reducing its false-positive rate (task). You are further given access to quality control diagnostic equipment readings, which the engineers believe are useful to discern whether a product is defective or just cneeds to be broken in.d You hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model.",How are you asked to improve the models performance?,By reducing its false-positive rate (task).
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","The method and data are the heart of a data science project. Finding the best tool for the task is, of course, one of the core skills of being a good data scientist. This course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects. As you gain deeper knowledge and experience, you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with state-of-the-art solutions. At the same time, thinking through different approaches and deciding on a set of methods and datasets benefit from teamwork, open discussion, and active seeking of advice and feedback from your peers, mentors, and relevant specialists.",What is the heart of a data science project?,method and data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","The method and data are the heart of a data science project. Finding the best tool for the task is, of course, one of the core skills of being a good data scientist. This course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects. As you gain deeper knowledge and experience, you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with state-of-the-art solutions. At the same time, thinking through different approaches and deciding on a set of methods and datasets benefit from teamwork, open discussion, and active seeking of advice and feedback from your peers, mentors, and relevant specialists.",What is one of the core skills of being a good data scientist?,Finding the best tool for the task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","The method and data are the heart of a data science project. Finding the best tool for the task is, of course, one of the core skills of being a good data scientist. This course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects. As you gain deeper knowledge and experience, you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with state-of-the-art solutions. At the same time, thinking through different approaches and deciding on a set of methods and datasets benefit from teamwork, open discussion, and active seeking of advice and feedback from your peers, mentors, and relevant specialists.",How will you become better at analyzing problems and tasks in different domains?,By researching data science literature and libraries effectively.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",A good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results.,What is a good starting strategy?,To check your proposed method against the target functionality or insight.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.",A good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results.,What is an example of a strategy that can be used to check your proposed method against the target functionality?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","In the above electronics scenario, you imagine training a logistic regression model on the features in the data to predict product failure. Once trained, influential features should receive a high weight in the regression equation, allowing you to identify them easily, similar to correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, you may encounter the objection that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommends you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.",What type of model can you train to predict product failure with maximum accuracy?,A complex random forest model
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","In the above electronics scenario, you imagine training a logistic regression model on the features in the data to predict product failure. Once trained, influential features should receive a high weight in the regression equation, allowing you to identify them easily, similar to correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, you may encounter the objection that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommends you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.",What would be an unsuitable method for task 2 in the pilot stage?,Random forests
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms.","In the above electronics scenario, you imagine training a logistic regression model on the features in the data to predict product failure. Once trained, influential features should receive a high weight in the regression equation, allowing you to identify them easily, similar to correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, you may encounter the objection that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommends you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.",How can a colleague suggest that the signal encoding algorithm you plan to use is outdated?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"For our purposes, before we get into the differences between CPUs and GPUs, it is important to take some time to first understand memory and its role in computation.",What is important to understand before we get into the differences between CPUs and GPUs?,Memory and its role in computation.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"For our purposes, before we get into the differences between CPUs and GPUs, it is important to take some time to first understand memory and its role in computation.",What role does memory play in computation?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"While it is incredibly difficult, if not impossible, to know the relative speeds of different operations on a computer, accessing and loading data from memory tends to be one of the slowest if poorly done. To better use memory, you need to understand that memory is not a singular shelf from which you can pull and place data. Instead, it operates as a series of increasingly higher shelves, with those shelves at the bottom being much more expensive but way faster to reach than the shelves at the top. This is known as a memory hierarchy; it serves as a great basic model of memory for performance-intensive applications like those found in data science.",What is one of the slowest if poorly done?,accessing and loading data from memory
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"While it is incredibly difficult, if not impossible, to know the relative speeds of different operations on a computer, accessing and loading data from memory tends to be one of the slowest if poorly done. To better use memory, you need to understand that memory is not a singular shelf from which you can pull and place data. Instead, it operates as a series of increasingly higher shelves, with those shelves at the bottom being much more expensive but way faster to reach than the shelves at the top. This is known as a memory hierarchy; it serves as a great basic model of memory for performance-intensive applications like those found in data science.",What is a memory hierarchy known as?,a memory hierarchy
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"The memory hierarchy can frequently be pictured as a pyramid, where at the top lie the bits of memory that are the fastest to access but the most expensive to get, while those at the bottom are the slowest to access but the least expensive to buy:",What is often seen as a pyramid?,Memory hierarchy
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"The memory hierarchy can frequently be pictured as a pyramid, where at the top lie the bits of memory that are the fastest to access but the most expensive to get, while those at the bottom are the slowest to access but the least expensive to buy:",What are the bits of memory that are the fastest to access but most expensive to get?,At the top
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datums location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. Youve implemented this partially in P1.",What is the hierarchy of memory that most computer systems try to find?,Top level
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datums location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. Youve implemented this partially in P1.",What will the system do once the data is found?,Note the datum / the datums location in higher levels
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datums location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. Youve implemented this partially in P1.",In what way did you implement this?,Partially in P1
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"As most of this management is done outside of your control, it can be frustrating to try to figure out how to make data access code faster. However, while the systems managing the data are opaque, it pays to instead focus on two simple facts that underlie all attempts at managing this hierarchy:",What can be frustrating to try to make data access code faster?,Most of this management is done outside of your control
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"As most of this management is done outside of your control, it can be frustrating to try to figure out how to make data access code faster. However, while the systems managing the data are opaque, it pays to instead focus on two simple facts that underlie all attempts at managing this hierarchy:",What are the two simple facts that support all attempts at managing this hierarchy?,-
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Temporal Locality: When you access some data on your computer once, youre probably going to access it again in the near future. Thus, if you can keep the data youre accessing to one small section at a time, you can receive better performance.",What does Temporal Locality mean when you access some data on your computer once?,You're likely going to access it again in the near future.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Temporal Locality: When you access some data on your computer once, youre probably going to access it again in the near future. Thus, if you can keep the data youre accessing to one small section at a time, you can receive better performance.",What do you need to do if you can keep the data to one small section at a time?,get better performance
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Spatial Locality: When you access one element in an array or another complex data structure, you are probably going to access data next to it. Thus, the management system will not just store the data at your location, but also the data near your requested location for future quick access. Thus, if you can access data sequentially in memory, you can receive better performance.",What does the management system store?,"Data at your location, but also the data near your requested location for future quick access."
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Spatial Locality: When you access one element in an array or another complex data structure, you are probably going to access data next to it. Thus, the management system will not just store the data at your location, but also the data near your requested location for future quick access. Thus, if you can access data sequentially in memory, you can receive better performance.",What is the name of the system that stores the data at your location?,The Management System
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Following these two principles can help guide you towards faster code in general. For example, consider the following two pseudocode sections:",What two principles can help guide you towards faster code?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"Following these two principles can help guide you towards faster code in general. For example, consider the following two pseudocode sections:",What are two sections of code that can help you in achieving faster code in general?,: :Br :Always check the following sections of code
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"The first code section accesses the elements of <![CDATA[array]]> in order, from 1 to 100, while the second section accesses the elements of <![CDATA[array]]> randomly. While both tasks perform essentially the same task, namely that of summing the elements of an array, the code in section 1 will be much faster than that of section 2, as when we access <![CDATA[array[1]]]> The computer stores the elements near it in the cache, allowing for their rapid access compared to the access pattern in section 2. A good general rule of thumb here is simple and direct access patterns beat complex and potentially insufficient algorithms here. In fact, it is for this reason that the traditional \\(O(n^3)\\) matrix-multiplication algorithm tends to beat fancier algorithms in benchmark tests. While there are analytically faster algorithms for the problem, the simple access pattern, combined with ease of tuning, ensures that competitive performance is maintained for longer than more complicated algorithms.",How does the first code section access the elements of![CDATA[array]]>?,In order.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,,"The first code section accesses the elements of <![CDATA[array]]> in order, from 1 to 100, while the second section accesses the elements of <![CDATA[array]]> randomly. While both tasks perform essentially the same task, namely that of summing the elements of an array, the code in section 1 will be much faster than that of section 2, as when we access <![CDATA[array[1]]]> The computer stores the elements near it in the cache, allowing for their rapid access compared to the access pattern in section 2. A good general rule of thumb here is simple and direct access patterns beat complex and potentially insufficient algorithms here. In fact, it is for this reason that the traditional \\(O(n^3)\\) matrix-multiplication algorithm tends to beat fancier algorithms in benchmark tests. While there are analytically faster algorithms for the problem, the simple access pattern, combined with ease of tuning, ensures that competitive performance is maintained for longer than more complicated algorithms.",What is a good general rule of thumb for simple and direct access patterns?,beat complex and potentially insufficient algorithms
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Module 15 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Following are some of the key deep learning architecture/algorithms:,What are some of the key deep learning architecture/algorithms?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a multi-layer perceptron.",What are Artificial neural networks composed of?,Node layers
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a multi-layer perceptron.",What is the general name for a multi-layer perceptron?,ANN architecture
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks,What are multiple hidden layers?,Deep Neutral Networks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks,What is the name of the source for Deep Neutral Networks?,Ibm
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks,How many hidden layers are there?,Multiple
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.",What is a multilayer neural network that was biologically inspired by the animal visual cortex?,A CNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.",Who created the first CNN?,Yann LeCun
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.",What is the name of the architecture that focuses on handwritten character recognition?,CNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.",How do early layers in a CNN recognize features?,As a deep network
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 2. LeNet CNN. Source: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures,What is the source of LeNet CNN?,cc-machine-learning-deep-learning-architectures
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.",What has CNN been successfully applied to?,video recognition and various tasks within natural language processing
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.",What is the name of a deep learning neural network?,CNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).",What is one of the foundational network architectures from which other deep learning architectures are built?,The RNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).",What is the primary difference between a typical multi-layer network and a recurrent network?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).",How can RNNs maintain memory of past inputs and model relationships?,Through feedback from previous layers.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been  developed and are described below:",What type of architectures suffer from vanishing and exploding gradient problems?,RNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been  developed and are described below:",What are LSTM and GRU architectures described below?,RNN architectures
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.",What architecture did Hochreiter and Schmidhuber create the LSTM?,Long Short-Term Memory (LSTM) Networks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.",What is the SLTM's name?,Long-Term Memory (LSTM) Networks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.",How long does the memory cell retain its value?,Short or long time
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.",How many gates does the LSTM memory cell contain?,Three
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.",What does the input gate control when an existing piece of information is forgotten?,The forget gate controls when an existing piece of information is forgotten.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.",The output gate controls when the information that is contained in the cell is used in the output from the cell?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.",When were GRUs proposed as a simplification of the LSTM?,2014
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.",What is an update gate and a reset gate?,Gates
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.",How much of the previous cell contents to maintain?,how much
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.",What is easier than the LSTM?,The GRU
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,"The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.",What can be more efficient in the execution of the GRU?,LSTM
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/,What is the name of the RNN?,Recurrent Neural Networks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,,Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/,Where is the source of the Recurrent Neural Network?,clay-atlas.com/blog/2020/06/02/mach
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?",What is one primary goal of data science?,To generate specific conclusions based on the presentation of evidence or data.
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?",What is the central role of information we have about the past in making inferences?,cdata-drivend
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?",How do we try our best as data scientists to make predictions or prescriptions about the future in the present?,We cannot capture information about the future in the present.
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"For example, lets say you have a longitudinal dataset about income and education and you want to make some inferences from it. If the data go back far enough, you will get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didnt take that into account or consider it when creating your framing questions, you could end up with conclusions like cgirls arent smart enough to go to colleged or cwomen dont like high-paying jobs.d",What is a longitudinal dataset about income and education that you want to make some inferences from?,
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"For example, lets say you have a longitudinal dataset about income and education and you want to make some inferences from it. If the data go back far enough, you will get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didnt take that into account or consider it when creating your framing questions, you could end up with conclusions like cgirls arent smart enough to go to colleged or cwomen dont like high-paying jobs.d",What was often difficult for women to get into many professions and their incomes were often correspondingly low?,
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The importance of recognizing that these conclusions, while technically possible but maybe problematic to assume, is that in history, there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.",What is the importance of recognizing that these conclusions are technically possible but perhaps problematic to assume?,"That in history, there may have been situations where discrimination existed between girls or women getting an"
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The importance of recognizing that these conclusions, while technically possible but maybe problematic to assume, is that in history, there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.",What may have been situations where discrimination existed against girls or women getting an education or against them working in certain fields?,
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.",What does the nature of data science mean?,"Making predictions and classifications, and separating one group from another"
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.","What does data science entail making predictions, classifications, and separating one group from another?",
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.",The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result?,
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.","If we are trying to infer or optimize some parameter based on our dataset, what will be inherently skewed toward?",members of group A
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most cnormal,d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.",What is important to watch out for?,"Bias toward what is most prevalent, or most cnormal,d about a"
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most cnormal,d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.",What is the most common bias about a given dataset under analysis?,cnormal
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most cnormal,d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.",Why do we need to be aware of these possibilities?,Because many data science techniques can be adversely affected by this data imbalance.
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"Reading: Introduction section of ""Raw Data"" Is An Oxymoron by Lisa Gitleman and Virginia Jackson.","What section of ""Raw Data"" is an Oxymoron by Lisa Gitleman and Virginia Jackson?",Introduction
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The location, tier level, and capacity of all 6th grades in Boston for the 2016-17 academic year. Illustration by the Boston Area Research Initiative.","What is the location, tier level, and capacity of all 6th grades in Boston for 2016-17?",
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The location, tier level, and capacity of all 6th grades in Boston for the 2016-17 academic year. Illustration by the Boston Area Research Initiative.",What was the name of the Boston Area Research Initiative?,
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.",What is the goal of the school assignment algorithm?,To increase access to high-quality schools while reducing the commute.
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.",What did the BPS use to assign students to good schools?,school assignment policy
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,,"The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.",How did BPS reduce the commute to and from school?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as cactivatedd or cinhibited,d thus affecting the transformations in the subsequent layers.",What is key to neural networks' learning capabilities?,The nonlinearity introduced in them.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as cactivatedd or cinhibited,d thus affecting the transformations in the subsequent layers.",What is responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers?,activation functions
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as cactivatedd or cinhibited,d thus affecting the transformations in the subsequent layers.",The neuron can be considered what?,cactivatedd or cinhibited
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,Following are some common activation functions:,What are some common activation functions?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The sigmoid function transforms an input to a value between 0 and 1. Assuming the \\(s\\) represents the weighted sum of the inputs, the logistic function, which is an example of a sigmoid function, computes",What function transforms an input to a value between 0 and 1?,The sigmoid function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"The sigmoid function transforms an input to a value between 0 and 1. Assuming the \\(s\\) represents the weighted sum of the inputs, the logistic function, which is an example of a sigmoid function, computes",What is an example of a sigmoid function?,logistic function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,\\[\\frac{1}{1+e^{-s}}\\],[frac11+e-s?,[frac1+e-s]
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.",What is a disadvantage of the sigmoid function?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.",What does the gradient approach for very small or very large input values?,zero
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.",What is a piecewise linear function that will output the input directly if it is positive?,ReLU stands for Rectified Linear Unit.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.",What does ReLU have the advantage of being faster to compute than some other activation functions?,It has the advantage of being faster to compute than some other activation functions.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.",The ReLU activation function does have the disadvantage of dying out for what?,Negative values
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.",What is a type of activation function based on?,ReLU
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.",What is the slope coefficient determined before training?,Hypoparameter
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.",The small slope for negative values prevents what?,Stalling problem
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Tanh, the hyperbolic tangent function is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the function is closer to having a zero mean therefore, data is more centered, which makes the learning for the next layer easier and faster. The tanh function shares the same disadvantage as that of the sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.",What is a shifted version of the sigmoid function?,Tanh
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,"Tanh, the hyperbolic tangent function is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the function is closer to having a zero mean therefore, data is more centered, which makes the learning for the next layer easier and faster. The tanh function shares the same disadvantage as that of the sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.",What is the mean of the activations that come out of the function closer to having a zero mean?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.,What function is used in neural networks when we want to interpret the outputs of a multi-class classifier as a probability distribution?,Softmax activation function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.,What makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two?,The softmax activation function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,,The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.,The softmax ensures that the sum of outputs for each class is equal to what?,1
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Above, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.",What is the best algorithm in terms of the time it takes to solve an instance of a problem?,
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Above, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.",How much memory does the algorithm need during execution?,Maximum memory
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.",What does time depend on?,Hardware parameters
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.",What is the clock speed of the CPU?,3.5 GHz
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.",The speed of what is the CPU speed?,3.5 GHz
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.",What has theoretical computer science developed to compare algorithms?,mathematical tools
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.",What is the term for asymptotic analysis?,asynchronous analysis
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).",What is the basic idea of the asymptotic analysis?,
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).",How do the growth rate of two functions compare to large input?,
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).","What does one grow faster, equally as fast, or slower than another?",Fast
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The most important tool is based on the big-O notation. We say a function \\(f(n)\\) is \\(O(g(n)\\) if there is are positive constants \\(c\\) and \\(n_0\\) such that for \\(n \\geq n_0\\) \\(f(n) \\leq c\\cdot g(n)\\). That is, beyond \\(n_0\\), \\(f(n)\\) grows at most as fast as \\(c\\cdot g(n)\\), that is, \\(c\\cdot g(n)\\) always dominates \\(f(n)\\) in growth, as shown below.",What is the most important tool based on?,big-O notation
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"The most important tool is based on the big-O notation. We say a function \\(f(n)\\) is \\(O(g(n)\\) if there is are positive constants \\(c\\) and \\(n_0\\) such that for \\(n \\geq n_0\\) \\(f(n) \\leq c\\cdot g(n)\\). That is, beyond \\(n_0\\), \\(f(n)\\) grows at most as fast as \\(c\\cdot g(n)\\), that is, \\(c\\cdot g(n)\\) always dominates \\(f(n)\\) in growth, as shown below.",What is a function (f(n)) if there are positive constants?,O(g(n)
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"So when analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of the input. Based on the definition above, we express the number of steps with a function \\(f(n)\\), which captures the size of the \\(n\\). We then try to find the function \\(g(n)\\) such that \\(f(n)\\) is \\(O(g(n)\\).","When analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of what?",input
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"So when analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of the input. Based on the definition above, we express the number of steps with a function \\(f(n)\\), which captures the size of the \\(n\\). We then try to find the function \\(g(n)\\) such that \\(f(n)\\) is \\(O(g(n)\\).",What does the function (f(n) capture?,The size of the (n)(n)(n))
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Let's give a very simple example to explain this. Suppose we have an unsorted array of \\(n\\) integers, and we want to search if a given integer \\(x\\) appears in the array or not. A simple algorithm for this would a simple loop that searches if the next entry in the array is equal to \\(x\\). So in the worst case, we will need to look at each of the \\(n\\) integers until we know what the result is. Assume of these steps take, say, 3 operations; our function \\(f(n)\\) will be \\(3n\\). Now with the choice of \\(c=4\\) and \\(n_0 = 0\\) you can easily convince that \\(f(n)\\) is \\(O(n)\\). That is, the number of steps of our algorithm grows linearly as \\(n\\). While this is a very simple example, not every algorithm analysis will be simple as that. You can try and come up with an algorithm that searches \\(x\\) in a sorted array whose number of steps \\(f(n)\\) is \\(O(\\log n)\\). So ignoring the cost of initial sorting (which can be amortized over many searches if necessary), you can see that this latter algorithm uses much less time than the former one, as the function \\(\\log n\\) grows much much slower than the function \\(n\\).",What is a simple algorithm that searches if the next entry in the array is equal to (x)?,A simple loop.
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Let's give a very simple example to explain this. Suppose we have an unsorted array of \\(n\\) integers, and we want to search if a given integer \\(x\\) appears in the array or not. A simple algorithm for this would a simple loop that searches if the next entry in the array is equal to \\(x\\). So in the worst case, we will need to look at each of the \\(n\\) integers until we know what the result is. Assume of these steps take, say, 3 operations; our function \\(f(n)\\) will be \\(3n\\). Now with the choice of \\(c=4\\) and \\(n_0 = 0\\) you can easily convince that \\(f(n)\\) is \\(O(n)\\). That is, the number of steps of our algorithm grows linearly as \\(n\\). While this is a very simple example, not every algorithm analysis will be simple as that. You can try and come up with an algorithm that searches \\(x\\) in a sorted array whose number of steps \\(f(n)\\) is \\(O(\\log n)\\). So ignoring the cost of initial sorting (which can be amortized over many searches if necessary), you can see that this latter algorithm uses much less time than the former one, as the function \\(\\log n\\) grows much much slower than the function \\(n\\).",What is the number of steps of our algorithm that grows linearly?,(n)
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,The following are some additional important concepts regarding asymptotic analysis:,What are some additional important concepts regarding asymptotic analysis?,
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Upper bound is the asymptotically maximum time that a given algorithm needs for all inputs of size \\(n\\). Algorithms have upper bounds. For example, we say, ""mergesort has an upper bound time complexity of \\(f(n) = c_1 n \\log n\\)"" or ""selection sort has an upper bound time complexity of \\(f(n) = c_2 n^2\\)"".",What is the asymptotically maximum time that a given algorithm needs for all inputs of size (n)?,Upper bound
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Upper bound is the asymptotically maximum time that a given algorithm needs for all inputs of size \\(n\\). Algorithms have upper bounds. For example, we say, ""mergesort has an upper bound time complexity of \\(f(n) = c_1 n \\log n\\)"" or ""selection sort has an upper bound time complexity of \\(f(n) = c_2 n^2\\)"".",What does mergesort have an upper bound time complexity of?,(f(n) = c_1 nlog n
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Lower bound is the minimum asymptotically the minimum time that any algorithm for a problem needs for all inputs of size \\(n\\). For example, it can be shown that no sorting algorithm that works by comparing elements can take less than \\(c_3n \\log n\\) steps.",What is the minimum asymptotically the minimum time that any algorithm for a problem needs for all inputs of size (n)?,Lower bound
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"Lower bound is the minimum asymptotically the minimum time that any algorithm for a problem needs for all inputs of size \\(n\\). For example, it can be shown that no sorting algorithm that works by comparing elements can take less than \\(c_3n \\log n\\) steps.",What can be shown that no sorting algorithm that works by comparing elements can take less than what?,(c_3n log n) steps
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"For more details on asymptotic analysis one can refer to any book on algorithm analysis, or to https://en.wikipedia.org/wiki/Asymptotic_analysis",What book can be found on asymptotic analysis?,Acrobatus
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,,"For more details on asymptotic analysis one can refer to any book on algorithm analysis, or to https://en.wikipedia.org/wiki/Asymptotic_analysis",What is the name of a book on algorithm analysis that is available on wikipedia?,Asymptotic_analysis_page
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.",What is the second type of clustering technique called?,Hierarchical Clustering technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.",How are hierarchies of clusters presented?,With a Dendrogram
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.",What is a Dendrogram?,A reading
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.",What technique involves starting the clustering process?,aggregation
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.",How are clusters formed by combining or agglomerating the nearest clusters?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.",What technique will combine clusters with the smallest distance from each other?,Agglomerative technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","The technique can take advantage of any distance measure, but you will find that most studies will use the Euclidean distance as a distance metric.",What technique can take advantage of any distance measure?,The technique can take advantage of any distance measure.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","The technique can take advantage of any distance measure, but you will find that most studies will use the Euclidean distance as a distance metric.",Most studies will use the Euclidean distance as what?,Distance metric
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Raw Data-Source1,What is Raw Data-Source1?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","The first round of merges finds clusters with observations/clusters b and c merged to form one cluster, and d and e are also merged into one. Now we have clusters a, bc, de and f.",The first round of merges finds clusters with observations/clusters b and c merged to form what?,One cluster
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","The first round of merges finds clusters with observations/clusters b and c merged to form one cluster, and d and e are also merged into one. Now we have clusters a, bc, de and f.","What are clusters a, bc, de and f merged into?",One
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Next, de and f are combined to form cluster a, bc, and def.","What are clusters a, bc, and def combined to form?",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Clusters are further combined to form a and bcdef.,What clusters are combined to form a and bcdef?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Finally, abcdef is formed.",What is abcdef formed?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Hierarchical Clustering Technique-Source1,What is the Hierarchical Clustering Technique-Source1?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single Linkage Method is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.",What method is based on grouping clusters using the agglomerative method?,Single Linkage Method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single Linkage Method is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.",How many clusters are merged at each step?,two
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single Linkage Method is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.",What is the distance between the nearest pair of observations in the two clusters used to determine?,the best clusters to combine
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.",What does chaining mean?,Single linkage suffers from chaining.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.",How many groups need to be close?,one pair
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.",Why can clusters be too large and not enough?,Because only one pair of points needs to be close.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Complete Linkage Method uses the maximum distance between data points within each cluster, also known as the farthest neighbor method. Clusters are combined into larger clusters until all data points are in the same cluster. The distance between clusters is the distance between two data points (i.e., one per cluster) that are farthest from each other. Complete linkage avoids chaining but suffers from crowding; because its score is based on the worst-case dissimilarity between pairs, a point can be closer to points in other clusters than to points in its own cluster.",What method uses the maximum distance between data points within each cluster?,Complete Linkage Method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Complete Linkage Method uses the maximum distance between data points within each cluster, also known as the farthest neighbor method. Clusters are combined into larger clusters until all data points are in the same cluster. The distance between clusters is the distance between two data points (i.e., one per cluster) that are farthest from each other. Complete linkage avoids chaining but suffers from crowding; because its score is based on the worst-case dissimilarity between pairs, a point can be closer to points in other clusters than to points in its own cluster.",What is the distance between two data points that are farthest from each other?,The Distance between clusters
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Average Linkage Method uses the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity overall points in opposite groups.,What method uses the average distance between data points within each cluster?,Average Linkage Method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Average Linkage Method uses the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity overall points in opposite groups.,What is the average dissimilarity between clusters using the average linkage method?,Overall points in opposite groups.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Centroid Linkage Method is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.",What method uses the centroid distance between clusters?,Centroid Linkage Method
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Centroid Linkage Method is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.",What is the mean for data points in a cluster?,centroid
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Centroid Linkage Method is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.","In complete linkage, what is the largest dissimilarity between two points?",opposite groups
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Divisive Clustering. This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.",What is the opposite of the agglomerative method?,Divisive Clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Divisive Clustering. This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.",What is clustered using a top-down approach?,Data
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters","Divisive Clustering. This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.",The largest cluster is split until each observation is in what?,its own cluster
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Agglomerative Clustering,Merging Clusters",Reading: Hierarchical Clustering of Words and Application to NLP Tasks,Reading: Hierarchical Clustering of Words and Application to NLP Tasks?,Reading
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.",What is the process of collecting and organizing data that can meet defined business and analytic objectives?,Data collection
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.","What does data collection consist of attending to issues of validity, reliability, and ethics?",
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.",What is the need for a sound study design?,Data scientists
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.",What is an exploratory analysis of data that is organically available?,Highly planned efforts
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.",The way in which data is collected is more important than what?,availability of the data itself
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Validity is the development of sound evidence to demonstrate the intended test interpretation.  In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.",What is the development of sound evidence to demonstrate the intended test interpretation?,Validity
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Validity is the development of sound evidence to demonstrate the intended test interpretation.  In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.",What is a threat to validity in an observational study?,Whether the observed changes can be attributed to the exposure or intervention.
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Threats to internal validity are problems in drawing correct inferences about whether the covariation between the presumed treatment variable and the outcome reflects a causal relationship.,What is a problem in drawing correct inferences about the covariation between the presumed treatment variable and the outcome?,A causal relationship
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.","What are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures?",Threats to external validity
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Statistical bias results from violations of external validity or internal validity of a study. Common statistical biases that you need to be aware of and take into account during the data understanding process are:,What does a Statistical bias result from?,Violations of external validity or internal validity of a study.
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Statistical bias results from violations of external validity or internal validity of a study. Common statistical biases that you need to be aware of and take into account during the data understanding process are:,What is a common statistical bias that you need to be aware of during the data understanding process?,
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Selection Bias: a threat to internal validity occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about.,What happens when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about?,a threat to internal validity
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Self-selection Bias: If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",What is the answer to a survey that is not perfect?,Yes
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Self-selection Bias: If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.",What are some characteristics of a person related to?,Reason why they responded to the survey
Collecting and Understanding Data,Data Collection,Module 7 Summary,,"Confirmation Bias: This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.","What type of bias occurs when we favor evidence that confirms our beliefs, values, and hypotheses?",Confirmation Bias
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Information Bias: The data collected has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.,What does the data collected have?,A misclassification bias and is not accurate.
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Information Bias: The data collected has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.,What is one way to control information bias?,Blinding
Collecting and Understanding Data,Data Collection,Module 7 Summary,,Confounding Bias: occurs when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable.,What happens when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable?,Confounding Bias
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Quiz 7,,,What does nan do?,He is a nurse
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Quiz 7,,,What is the name of the nnan?,The nan name is Peter Durning.
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.",What is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem?,The data science team
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.",What can organizations provide the structure and support for the different responsibilities within the data science process?,
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.",Who can wear multiple hats to ensure the efficient execution of tasks and the development of solutions?,personnel in the data science process
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Data Scientist. This role involves solving business tasks using machine learning model development and statistical techniques. This individual identifies trends and patterns within the data and makes predictions based on trends. The data scientist will write code to support the data analysis and model-building process.,What is the role of Data Scientist?,
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Data Scientist. This role involves solving business tasks using machine learning model development and statistical techniques. This individual identifies trends and patterns within the data and makes predictions based on trends. The data scientist will write code to support the data analysis and model-building process.,What does Data Scientist do to solve business tasks?,Machine learning model development and statistical techniques
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data Engineer. The Data Engineer specializes in data structures and algorithms, as well as in working with data in databases and other large repositories.",What does the Data Engineer specialize in?,Data structures and algorithms.
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data Engineer. The Data Engineer specializes in data structures and algorithms, as well as in working with data in databases and other large repositories.",What is the main focus of Data Engineer?,Data structures and algorithms.
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Solutions Architect. This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.",What is a customer-facing role that ensures end-to-end customer deployment for company-related data services?,Solutions Architect
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Solutions Architect. This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.","What does Solutions Architect interact with clients to design, coordinate, and execute solution prototypes?",end-to-end customer deployment
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.,What is the name of the machine learning engineer?,ML
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.,What does the ML Engineer do?,Models modeling tasks
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.,How much time is required for ML engineers?,A considerable amount of time
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.",What skills does a data analyst have?,"Data gathering, analysis, and visualization"
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.",What do data scientists use to inform decision-making?,Business intelligence and analytics tools
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.",How are data analysts rooted in?,The business domain
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Software Engineer. The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.,Who handles the alignment between business objectives and solutions?,Software Engineer.
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,Software Engineer. The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.,Who is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise?,The Software Engineer.
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",What are domain experts also known as?,Subject matter experts
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",Who are the actors who know the most about the problem on the business side?,Domain Experts
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,,"Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",What is the role of domain experts?,Domain experts are actors who know the most about the problem on the business side.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:",How can we reserve a random portion of our dataset for testing?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:",How can the model selection process be expressed?,As follows
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: candidate models M1, M2, , Ml","What are the candidate models M1, M2, and Ml?","candidate models M1, M2, and Ml are the candidate models M1, M2, and M"
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: candidate models M1, M2, , Ml",What is the candidate model M1?,M2
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What is the procedure?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What type of procedure does the procedure follow?,A purely cosmetic procedure.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Split the dataset into train set and test set.,What is split into train set and test set?,Dataset
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Split the dataset into train set and test set.,What does split the dataset into?,Train set and test set
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is the name of a candidate model?,Mi
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is a model model for Mi?,Y
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,Which candidate model model is Mi for?,For each candidate model Mi:
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Train Mi on the train set.,i) Train Mi on the train set?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Train Mi on the train set.,What is the name of the train that trains Mi?,(i) Train Mi is the name of the train that trains Mi.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Train Mi on the train set.,How does the train Mi train train?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(ii) Evaluate Mis performance on the test set,ii) Evaluate Mis performance on the test set?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Output: the model Mj with the best performance on the test set.,What is the model Mj with the best performance on the test set?,Output
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.",What type of tuning is part of the model selection process?,Hyperparameter
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.",What can be split into a subset and validation subset?,train set
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: candidate models M1, M2, , Ml and hyperparameter space S.","What are the candidate models M1, M2,, Ml and hyperparameter space S?","candidate models M1, M2, Ml and hyperparameter space S."
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What is the procedure?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What type of procedure does the procedure follow?,A purely cosmetic procedure.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Split the dataset into train subset, validation subset and test set.","What subset is split into train subset, validation subset and test set?",Train subset
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is the name of a candidate model?,Mi
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is a model model for Mi?,Y
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,Which candidate model model is Mi for?,For each candidate model Mi:
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Pick the hyperparameters that give the best performance on the validation subset when Mi is trained on the train subset. We call these the best hyperparameters.,What are the hyperparameters that give the best performance on the validation subset?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Pick the hyperparameters that give the best performance on the validation subset when Mi is trained on the train subset. We call these the best hyperparameters.,What is the name of the best hypermeters?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Pick the hyperparameters that give the best performance on the validation subset when Mi is trained on the train subset. We call these the best hyperparameters.,When is Mi trained?,On the train subset
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(ii) Retrain a new model Mi using the best hyperparameters on the combined data from the train subset and validation subset.,What is the name of the new model Mi?,Retrain
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(ii) Retrain a new model Mi using the best hyperparameters on the combined data from the train subset and validation subset.,What are the best hyperparameters on the combined data?,(ii) Retrain a new model Mi
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(iii) Evaluate Mis performance on the test set.,iii) Evaluate Mis performance on the test set?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Output: the model Mj and the associated best hyperparameters with the best performance on the test set.,What is the model Mj and the associated best hyperparameter with the best performance on the test set?,Output
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.",What can be done by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem?,step (i)
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.",What type of search is preferred when there are multiple hyperparmeters?,Random
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"While the above procedure is sufficient to demonstrate the prediction of a  models validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative is to perform k-fold cross-validation as in the inner loop of the model selection procedure.",How many random splits does the above procedure depend on?,two
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"While the above procedure is sufficient to demonstrate the prediction of a  models validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative is to perform k-fold cross-validation as in the inner loop of the model selection procedure.",What is a more rigorous alternative to k-fold cross-validation?,Inner loop
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,"Input: number of folds K, candidate models M1, M2, , Ml and hyperparameter space S.",,nan
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What is the procedure?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Procedure:,What type of procedure does the procedure follow?,A purely cosmetic procedure.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Split the dataset into train set and test set. Split the train set into K folds.,What is the name of the dataset that is split into train set and test set?,The dataset that is split into train set and test set.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Split the dataset into train set and test set. Split the train set into K folds.,What folds are split into K folds?,train set
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is the name of a candidate model?,Mi
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,What is a model model for Mi?,Y
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,For each candidate model Mi:,Which candidate model model is Mi for?,For each candidate model Mi:
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(i) Pick the hyperparameters that give the best cross-validated performance for Mi on the train set. We call these the best hyperparameters.,What are the hyperparameters that give the best cross-validated performance for Mi on the train set?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(ii) Retrain a new model Mi using the best hyperparameters on the train set.,What is a new model Mi using?,The best hyperparameters on the train set
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(ii) Retrain a new model Mi using the best hyperparameters on the train set.,What is the best hyperparameter on the train set?,(ii) Retrain a new model Mi.
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,(iii) Evaluate Mis performance on the test set.,iii) Evaluate Mis performance on the test set?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,,Output: the model Mj and the associated best hyperparameters with the best performance on the test set.,What is the model Mj and the associated best hyperparameter with the best performance on the test set?,Output
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.",What do traditional sequence2sequence models transform an input sequence to a new one?,Target
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.",What are the recurrent neural networks with LSTM or GRU units?,Encoder and decoder
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.",How does the encoder process the input sequence?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.",What is a sentence embedding or cthoughtd vector?,A context vector.
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.",When is the chidden state produced?,At a particular timestep
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.",When is the Decoder initialized?,With the context vector
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.",What does the decoder take at a particular timestep to generate the result?,One-word embedding
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,Figure 1: Traditional Sequence2Sequence model architecture.,Figure 1: Traditional Sequence2Sequence model architecture?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.,What is an apparent disadvantage of a fixed length context vector design?,incapability to remember long sentences
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.,What is a critical disadvantage of the fixed-length context vector?,incapability to remember long sentences
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.,"When it completes processing the whole input, what may it forget the first part of?",A long sequence
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,Figure 2: Limitation of the traditional Sequence2Sequence model architecture.,Figure 2: Limitation of what model architecture?,Sequence2Sequence
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this cbottleneck problemd.","The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve what problem?",cbottleneck problemd
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.","What allows a model to focus on specific, most important parts of the sequence in the case of natural language processing?",Attention
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.",What does a vision model focus on?,Different regions of an image
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.",What is the color term for ceating?,Dgreend
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.",What does greend mean?,green food
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,Figure 3: Attention between words in a sequence.,Figure 3: Attention between words in a sequence?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of cattentiond to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.",What does the attention mechanism in models try to imitate in NLP?,This behavior
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of cattentiond to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.",What is the relationship between words in one sentence or in a close context?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",What can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element?,Attention in deep learning
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",What does the attention vector take as the approximation of the target?,Sum of their values
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.","In sequence2sequence models with attention, what can the model decide at each decoder step?",Which parts of the source are more important
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.",What does the encoder give representations for all source tokens by passing the intermediate hidden states to?,the decoder
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.",How does the model learn which input words to cattend tod at each step without the need to manually provide this information?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,Figure 4: Attention in Seq2Seq RNN architectures.,Figure 4: Attention in Seq2Seq RNN architectures?,
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",What can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element?,Attention in deep learning
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,,"Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",What does the attention vector take as the approximation of the target?,Sum of their values
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Throughout this section, we will make use of the following matrix as an example:",What is an example of a matrix that we will use in this section?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.",What is a dense matrix format?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.",What does a sparse matrix aim to store?,Non-zero (non-empty) values
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.",How do we use zero-based indexing?,Refering to row and column indices.
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):",What is a straightforward that stores a matrix as three lists?,COO
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):",What is the matrix A represented as a tuple of?,Three lists
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.",What do![CDATA[data[i]]] represent?,"The actual value, row index, and column index of the i-th non-zer"
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.",What is the i-th non-zero value in the matrix?,"Row index, and column index"
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.",Where can the non zero values be arranged?,![CDATA[data]]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.",How can new entries be appended to the end of the three lists?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.",What does COO support?,efficient arithmetic operations
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.",How can COO be quickly converted to other sparse formats?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,",What does DOK use to map the location of every non-zero element to its value?,A dictionary representation
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,",What is the example matrix A?,Non-zero element
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its DOK representation is,What is the DOK representation?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.",What format allows for fast element access by row and column index?,COO format
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.",What format can be quickly converted to and from?,COO format
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",The compressed sparse row format stores a matrix as three lists:,What format stores a matrix as three lists?,The compressed sparse row format
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",The compressed sparse row format stores a matrix as three lists:,How many lists does the compressed sparse row format store?,three
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[data]]>: a list of the non-zero values in the matrix,What is a list of the non-zero values in the matrix?,CDATA[data]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col]]>: a list of the column indices of the non-zero values,What is a list of the column indices of the non-zero values?,CDATA[col]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.",What is a list of m+1 values?,CDATA[row]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.",What is the number of rows in the original matrix?,m
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","<![CDATA[row[i]]]> denotes the number of non-zero entries that appear in the rows above the i-th row in the original matrix, where row indexes start from 0, and <![CDATA[row[m]]]> denotes the number of non-zero entries in the entire matrix.",![CDATA[row[i]]> denotes the number of non-zero entries that appear in the rows above what row in the original matrix?,i-th row
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With the example matrix A,",What is the example matrix A?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With the example matrix A,",What is a example matrix of a matrix?,A
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its CSR representation is,What is the CSR representation?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:",What are the![CDATA[data]] lists?,COO
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:",What does the row column contain?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","0 at index 0, as there are no non-zero entries above the first row.","What is 0 at index 0, as there are no non-zero entries above the first row?",0
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","2 at index 1, as there are 2 non-zero entries above the second row ( 7 and 5 ).",How many non-zero entries are there above the second row?,2
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","2 at index 2, as there are still only 2 non-zero entries above the third row.",How many non-zero entries are there at index 2?,2
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","2 at index 2, as there are still only 2 non-zero entries above the third row.",How many entries above the third row are there?,2
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).",How many non-zero entries are there at index 3?,4
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).",How many entries above the fourth row are there?,4
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","6 at index 4, as there are 6 non-zero entries in the matrix.",How many non-zero entries are there in index 4?,Six
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.",What are the number of non-zero entries in the matrix?,![CDATA[data] and![CDATA[col]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.",What is the length of![CDATA[data]]>?,![CDATA[data]]>
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).",What does cCSRd give its name?,A compressed row representation
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).",What is the name of the compressed row representation?,cCSRd
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).",How are the rows compressed?,To save space.
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:",What is a matrix-vector product that involves computing the dot product between every row of M and x?,Mx
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",\\[M x=\\left(M_{1} \\cdot x M_{2} \\cdot x \\quad \\ldots M_{m} \\cdot x\\right)^{\\top}\\],What does [M x=left(M_1 cdot x M_2?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.",What are the non-zero entries in row Mi?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.",What is the name of the indices that are at x?,CDATA
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?",How is column access slow with CSR?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?",How is conversion to other sparsity formats?,expensive
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?",What is the time-complexity of conversion to COO?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:",What format behaves similarly to CSR?,CSC
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:",How many lists does the CSC format consist of?,Three
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[data]]>: a list of the non-zero values in the matrix,What is a list of the non-zero values in the matrix?,CDATA[data]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.",What is a list of n+1 values?,CDATA[col]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.",What is the number of columns in the original matrix?,n
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix,What does![CDATA[col[i]]> denote?,The number of non-zero entries that appear in the columns before (to the left of
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix,What is the name of the column in the original matrix?,i-th
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",<![CDATA[row]]>: a list of the row indices of the non-zero values.,What is a list of the row indices of the non-zero values?,CDATA[row]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With the example matrix A,",What is the example matrix A?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","With the example matrix A,",What is a example matrix of a matrix?,A
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",its CSC representation is,What is the CSC representation?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.",What is the order that is important in CSR?,Order of importance
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.",What are entries that appear in earlier columns that need to be listed before those in later columns?,columns
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:",What format allows for efficient column access and arithmetic operations?,Arithmetic
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:",What is faster for elementwise matrix operations and matrix-vector products?,CSR
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:",Mx can be expressed as what?,A linear combination of columns of M
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",\\[ M x=x_{1} M_{(1)}+x_{2} M_{(2)}+\\ldots+x_{n} M_{(n)} \\],[ M x=x_1 M_(1)+x__2 What does M_i(2)+ldots+n?,M___2+ M___2+ldots
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.",What are the non-zero entries in column M(j)?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.",What is the name of the indices in![CDATA[x]]?,![CDATA[x]]
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?",How is row access slow with CSC?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?",How is conversion to other sparsity formats cgenerallyd expensive?,Row access is slow with CSC
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?","In some cases, conversions might be readily easy to do?",yes
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling",Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.,How are sparse matrices used in different data science domains?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.",What is a crucial component of natural language processing?,Converting text data into numerical features
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.",How can text data be converted to numerical features?,
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.",What does the bag-of-word technique transform an input document into?,A binary vector
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \\times n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \\times k}, V \\in R^{k \\times n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.",What is an example of a very large but sparse data structure?,User-movie rating matrix
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \\times n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \\times k}, V \\in R^{k \\times n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.",What is a standard technique for performing such predictions?,collaborative filtering
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \\times n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \\times k}, V \\in R^{k \\times n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.",How many lower-ranked matrices are U and V?,Two
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.",What format is suitable for storing not only input data but also model parameters in certain domains?,Sparse format
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","In computational biology, we typically need to build predictive models over a large number of features?",
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Natural Language Processing,Recommender Systems,Sparse Modeling","The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.",What format can we opt to store model weights in?,Sparse vector/matrix format
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Finally, the analytic objective should state what specific functionality, insight, or resource is gained from leveraging the data and methods you propose relative to the current situation and assuming the project is successful as proposed.",What is the analytic objective of the project?,"To state what specific functionality, insight, or resource is gained from leveraging the data and methods"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Finally, the analytic objective should state what specific functionality, insight, or resource is gained from leveraging the data and methods you propose relative to the current situation and assuming the project is successful as proposed.","What is an example of a specific functionality, insight or resource gained from leveraging the data and methods you propose relative to?","Example of a specific functionality, insight, or resource gained from leveraging the data and methods"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Functionality: In many practical scenarios, this may simply be that, for example, a predictive model can successfully solve the task and contribute to the problem solution, as stated. In more complex situations, the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task. For example, the task may be to link passages in news text that state false information about certain historical events to a reference database of those events. Initially, one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address. This detector, if implemented successfully, would add valuable functionality to the linking task.",What can a predictive model solve in many practical scenarios?,A task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Functionality: In many practical scenarios, this may simply be that, for example, a predictive model can successfully solve the task and contribute to the problem solution, as stated. In more complex situations, the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task. For example, the task may be to link passages in news text that state false information about certain historical events to a reference database of those events. Initially, one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address. This detector, if implemented successfully, would add valuable functionality to the linking task.",What is the purpose of the predictive model to solve the problem solution?,To help provide useful functionality to the linking task.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Functionality: In many practical scenarios, this may simply be that, for example, a predictive model can successfully solve the task and contribute to the problem solution, as stated. In more complex situations, the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task. For example, the task may be to link passages in news text that state false information about certain historical events to a reference database of those events. Initially, one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address. This detector, if implemented successfully, would add valuable functionality to the linking task.",How can the benefit gained be in more complex situations?,An incremental yet necessary step towards producing a model that solves the task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"It should be noted that analytic objectives can, of course, be reframed to focus on specific tasks, in which case an incremental step would become the main goal. In the example just given, one can change the analytic objective so that historic text detection becomes the main task in the first phase of the project. In fact, if the circumstances and the client allow for such a rescoping, then this may even be preferable. One of the main points of this unit is that explicitly formulating, discussing, and committing to analytic objectives supports a productive data science project lifecycle and ensures a proper alignment of the work with the business objective.",What can be reframed to focus on specific tasks?,analytic objectives
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"It should be noted that analytic objectives can, of course, be reframed to focus on specific tasks, in which case an incremental step would become the main goal. In the example just given, one can change the analytic objective so that historic text detection becomes the main task in the first phase of the project. In fact, if the circumstances and the client allow for such a rescoping, then this may even be preferable. One of the main points of this unit is that explicitly formulating, discussing, and committing to analytic objectives supports a productive data science project lifecycle and ensures a proper alignment of the work with the business objective.",What is the main goal of analytic objectives in the first phase of a project?,Historic text detection
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Insight: In some project contexts, the client may not need a piece of software that automates analytic functionality but rather requires insight from data in order to make decisions. This area of data science blends into what is commonly referred to as cbusiness intelligence.d",What is cbusiness intelligence.d referred to as in some contexts?,Valuable Insight
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Insight: In some project contexts, the client may not need a piece of software that automates analytic functionality but rather requires insight from data in order to make decisions. This area of data science blends into what is commonly referred to as cbusiness intelligence.d",What does the client need to automate?,Analytic functionality
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Resource: Data science projects can also produce resources to be used by the organization or further projects. For example, the project may be intended to collect and curate a competition dataset and publish it along with some baseline results to facilitate research on a certain topic.",What is the purpose of a data science project?,To collect and curate a competition dataset and publish it along with some baseline results to facilitate
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Valuable Resource: Data science projects can also produce resources to be used by the organization or further projects. For example, the project may be intended to collect and curate a competition dataset and publish it along with some baseline results to facilitate research on a certain topic.",What can a project do to collect and curate?,Competition dataset
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state-of-the-art results. This can be done via a survey of related work and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.",What should the project improve over the current situation?,"Functionality, information, and resources"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state-of-the-art results. This can be done via a survey of related work and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.",What is usually obvious in industry settings?,"Improvement over the currently available functionality, information, and resources"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state-of-the-art results. This can be done via a survey of related work and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.",How can you characterize your project as improving over the state-of-the-art results?,Through a survey of related work and possibly some exploration of existing methods and datasets.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Assumption of project success: While it is natural that one would propose a project with confidence to succeed, it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees. The dataset may contain too much noise on top of the interesting patterns, or the computational effort may be too large. One should be mindful of what can be done to distill some value-added from the data, even if the main objective may fail due to factors beyond ones control.",What is an example of a project with confidence?,Explanation
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Assumption of project success: While it is natural that one would propose a project with confidence to succeed, it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees. The dataset may contain too much noise on top of the interesting patterns, or the computational effort may be too large. One should be mindful of what can be done to distill some value-added from the data, even if the main objective may fail due to factors beyond ones control.",What are many data science projects that are exploratory to different degrees?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,,"Assumption of project success: While it is natural that one would propose a project with confidence to succeed, it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees. The dataset may contain too much noise on top of the interesting patterns, or the computational effort may be too large. One should be mindful of what can be done to distill some value-added from the data, even if the main objective may fail due to factors beyond ones control.",The dataset may contain too much noise on top of what?,interesting patterns
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Reminder: The data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client. The requirements gathering process will involve identifying the stakeholders, eliciting needs, and defining requirements. The difference between the requirements of a traditional IT project and those of a data science project is the focus on the requirements for the analytic solution.",What is the focus of the data science process?,Understanding business needs and objectives.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Reminder: The data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client. The requirements gathering process will involve identifying the stakeholders, eliciting needs, and defining requirements. The difference between the requirements of a traditional IT project and those of a data science project is the focus on the requirements for the analytic solution.",What are the requirements of a traditional IT project and data science project?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.",What defines the functions of a system and how users will interact with the system?,Functional requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.",What is derived from the user and system requirements that are needed to satisfy the business requirements?,Functional requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.",How are user requirements captured in use cases?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.",Who defines the interaction between the system and its users?,Goals
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.,Why are functional requirements organized by priority?,Not all functional requirements are implemented in the first iteration of solution development.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.,High priority functional requirements must be implemented to meet what?,the business objectives
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.,What are requirements that will not affect the current business objectives?,Medium and low priority functional requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Traditional functional requirements considered in the software and application development process include Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Requirements, Administration Functions, Authorization Requirements, and Data Management.",What are the traditional functional requirements considered in software and application development?,"Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Re"
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Traditional functional requirements considered in the software and application development process include Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Requirements, Administration Functions, Authorization Requirements, and Data Management.",What are some of the functional requirements that are considered in the software development process?,"Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Re"
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Reading: IEEE ANSI 830 Documentation on Functional Requirements.,What is IEEE ANSI 830 Documentation on Functional Requirements?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.",What format can be captured in different formats?,story format
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.",What is the purpose of the unit to focus on functional requirements?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.",How are initial user requirements often written?,Too broadly
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.",What is the danger of a system that may not meet the business objectives?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.","How are different forms of use case analysis typically used to capture, discuss, and verify the details of the solution with the customer?",
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:",What is a textual description of a use case?,The use case specification
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:",What will decompose a user requirement into functional requirements?,The use case specification
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:",The use case specification details the steps involved in a goal or action?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Figure 1. Use Case Specification,What is a Case Specification?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","A business analyst (BA) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers. The BA analyzed the information from the questionnaire and defined some user requirements. One of the user requirements is customers ability to update their billing address in the new system. This requirement describes what customers can do with the solution, but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement. We will illustrate how we can simply decompose that user requirement into functional requirements.",What is the purpose of a business analyst?,To elicit the needs of stakeholders for a proposed system for their customers.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","A business analyst (BA) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers. The BA analyzed the information from the questionnaire and defined some user requirements. One of the user requirements is customers ability to update their billing address in the new system. This requirement describes what customers can do with the solution, but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement. We will illustrate how we can simply decompose that user requirement into functional requirements.",What is one of the user requirements that customers can update their billing address in a new system?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Use Case/User Requirement: Update billing address.,What is the use case/user requirement?,Update billing address
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Use Case/User Requirement: Update billing address.,What is a billing address?,UPDATED
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(1) The user shall be able to view the billing addresses in the system.,Who must be able to view billing addresses?,The user
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(2) The user shall be able to update a billing address in the system.,Who must be able to update the billing address in the system?,The user
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(2) The system shall display updated customer service and billing addresses.,What is the name of the system that displays customer service and billing addresses?,:
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",(2) The system shall display updated customer service and billing addresses.,What does the system display?,Updated customer service and billing addresses.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.",What are some functional requirements considered in software development?,"The business rules, user and system authorization levels, authentication, and regulatory requirements"
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.","What are the business rules, user and system authorization levels, authentication and regulatory requirements?",
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.",What are NFRs for a traditional IT project called?,Operating requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.",What does NFR describe the performance and behavior of a system?,Non-functional requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Availability,What is the name of the source of information?,The Internet
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Availability,What does Availability have?,A reputation for reliability.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Refers to a property of software that is there and ready to carry out its task when you need it to.,What is a property of software that is there and ready to carry out its task when you need it?,.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Interoperability,What is interoperability?,A term for interchangeability.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Interoperability,What is the interoperability of interoperablerability? What does interoperaperability do?,It allows for easy comparison between different types of software.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Interoperability refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context.,What refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context?,Interoperability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modifiability,What is Modifiability?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modifiability refers to the ease of modifying the system with minimal changes to the architecture.,What refers to the ease of modifying the system with minimal changes to the architecture?,Modifiability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Performance,What does the performance of a performer do?,Does things like performer performer performer can be called a performance that does the opposite
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Performance,What does a performance perform?,Performs a number of times.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.",What does performance refer to?,software systems ability to meet timing requirements
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.","When events occurinterrupt, messages, requests from users or other systems, or clock events marking the passage of timethe system must respond to them in time?",
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Security,What is the name of security?,''
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Security,What does security do?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Security refers to the systems ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized.,What refers to the system ability to protect data and information from unauthorized access while still providing access to what?,people and systems that are authorized.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Testability,What is the testability of a test?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Software testability refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing. Specifically, testability refers to the probability that the software will fail on its next test execution, assuming that it has at least one fault.",What refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing?,Software testability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Software testability refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing. Specifically, testability refers to the probability that the software will fail on its next test execution, assuming that it has at least one fault.",What does testability refer to the probability that the software will fail on its next test execution?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Usability,What does Usability mean?,Usability means in many ways.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Usability,What is the name of the person who uses Usability?,The name of the person who uses Usability is Her.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Usability is concerned with how easy it is for the user to accomplish the desired task and the kind of user support the system provides. Over the years, a focus on usability has shown itself to be one of the cheapest and easiest ways to improve a systems quality (or, more precisely, the users perception of quality).",What is the focus of Usability?,How easy it is for the user to accomplish the desired task and the kind of user support the
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","Usability is concerned with how easy it is for the user to accomplish the desired task and the kind of user support the system provides. Over the years, a focus on usability has shown itself to be one of the cheapest and easiest ways to improve a systems quality (or, more precisely, the users perception of quality).",What is one of the cheapest and easiest ways to improve a system quality?,Usability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Scalability,What is Scalability?,A component of any business
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Scalability refers to the ease of adding new resources to a system to cope with increasing demands on its use.,What refers to the ease of adding resources to a system to cope with increasing demands on its use?,Scalability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Observability / Monitorability,Observability / Monitorability?,Observability / Monitorability
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",These refer to the ability of the operations staff to monitor the system while it is in operation.,What does the operation staff have to do to monitor the system while it is in operation?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Portability and Compatibility,What is Portability and Compatibility?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","These refer to the compatibility of the hardware, systems, application software, and solution with other applications and processes within the existing environment.","What does the compatibility of hardware, systems, application software and solution refer to?","The compatibility of hardware, systems, application software and solution with other applications and processes within the"
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modules and Architecture,What are modules and architecture?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Modules and Architecture,What are Modules and Architecture?,Modules and Architecture are modules and architecture are a module in the building code.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","These refer to the technical considerations for the system, including operating system compatibility and the programming development environment employed by the developers.",What are the technical considerations for the system?,
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements","These refer to the technical considerations for the system, including operating system compatibility and the programming development environment employed by the developers.",What are some of the technical aspects of the system related to operating system compatibility?,The operating system compatibility and the programming development environment employed by the developers.
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules. Framing a non-functional requirement for a data science project will include the above-mentioned attributes and additional requirements that are related to machine learning models and AI analytic solution(s).,What do non-functional requirements focus on?,the user experience
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements",Non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules. Framing a non-functional requirement for a data science project will include the above-mentioned attributes and additional requirements that are related to machine learning models and AI analytic solution(s).,What are the requirements for a data science project that focus on the user experience?,Non-functional
Analytic Algorithms and Model Building,Model Selection,Quiz 6,,,What does nan do?,He is a nurse
Analytic Algorithms and Model Building,Model Selection,Quiz 6,,,What is the name of the nnan?,The nan name is Peter Durning.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,A Confusion Matrix (contingency table) shows how well a classifier performs compared to the ground truth labels. It is often used in a binary classification setting and has the following components:,What does a Confusion Matrix show?,How well a classifier performs compared to the ground truth labels
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,A Confusion Matrix (contingency table) shows how well a classifier performs compared to the ground truth labels. It is often used in a binary classification setting and has the following components:,What is a confusion matrix called?,Contingency table
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,True Positives (TP) is the number of positive data points that are correctly predicted as positive.,What is the number of positive data points that are correctly predicted as positive?,True Positives (TP)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,True Negatives (TN) is the number of negative data points that are correctly predicted as negative.,What is the number of negative data points that are correctly predicted as negative?,True Negatives (TN)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,False Positives (FP) is the number of negative data points that are incorrectly predicted as positive. This is also called the Type I error in statistics.,What is the number of negative data points that are incorrectly predicted as positive?,False Positives (FP)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,False Positives (FP) is the number of negative data points that are incorrectly predicted as positive. This is also called the Type I error in statistics.,What type of error is also called in statistics?,Type I error
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,False Negatives (FN) is the number of positive data points that are incorrectly predicted as negative. This is also called the Type II error in statistics.,What is the number of positive data points that are incorrectly predicted as negative?,False Negatives (FN)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,False Negatives (FN) is the number of positive data points that are incorrectly predicted as negative. This is also called the Type II error in statistics.,What is FN also called?,Type II error
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).",What is a trick for remembering these definitions?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).",What does the second term denote?,what is predicted
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Ground truth \\(y = 1\\),(y = 1) What is ground truth?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Ground truth \\(y = 0\\),What is ground truth (y = 0)?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Prediction \\(\\hat y = 1\\),Prediction (hat y = 1?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,TP,What is the name of the TP?,The TP is the name of the TP.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,FP,What is the name of the FP?,The FP is the name of the FP.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Prediction \\(\\hat y = 0\\),Prediction (hat y = 0?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,FN,What is the name of FN?,FN stands for FN in the name of FN.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,TN,What is the name of TN?,TN stands for Tennessee Service.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"In a multi-class classification setting with \\(K\\) categories, a similar confusion matrix with \\(K\\) rows and \\(K\\) columns can be constructed, where the entry at row \\(i\\) and column \\(j\\) denotes the number of instances that have ground-truth label \\(j\\) and are predicted as having label \\(i\\). In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.",What can be constructed in a multi-class classification setting with (K) categories?,A similar confusion matrix with (K) rows and (K) columns
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"In a multi-class classification setting with \\(K\\) categories, a similar confusion matrix with \\(K\\) rows and \\(K\\) columns can be constructed, where the entry at row \\(i\\) and column \\(j\\) denotes the number of instances that have ground-truth label \\(j\\) and are predicted as having label \\(i\\). In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.",What does the entry at row i and column j denote?,The number of instances that have ground-truth label (j)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"In a multi-class classification setting with \\(K\\) categories, a similar confusion matrix with \\(K\\) rows and \\(K\\) columns can be constructed, where the entry at row \\(i\\) and column \\(j\\) denotes the number of instances that have ground-truth label \\(j\\) and are predicted as having label \\(i\\). In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.","How are the metrics TP, TN, FP, and FN computed?","By treating A as positive and B, C together as negative."
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Accuracy is the number of correctly classified instances, divided by the total number of instances in the dataset.",What is the number of correctly classified instances divided by?,Total number of instances
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Accuracy is the number of correctly classified instances, divided by the total number of instances in the dataset.",How many instances are in the dataset?,There are total number of instances in the dataset.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\(\\text{Accuracy} = \\frac{TP+TN}{TP+FP+FN+TN}\\),(textAccuracy = fracTP+TNTP +FP+FN+TN?,(textAccuracy = fracTP+TNP+FN+TN)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"bbThis is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.","What is an intuitive measure, but should only be used when there is an even distribution of ground truth labels?",bb
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"bbThis is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.",What can easily be inflated even by simple models?,Accuracy
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"bbThis is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.","If a dataset has 90% negative instances and 10% positive instances, what can a naive model that always predicts negative labels achieve?",an accuracy of 0.9
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall, also known as sensitivity or true positive rate, denotes the fraction of all positive instances that are correctly classified as such:",What is Recall also known as?,Sensibility or true positive rate
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall, also known as sensitivity or true positive rate, denotes the fraction of all positive instances that are correctly classified as such:",What is the fraction of all positive instances that are correctly classified as such?,Recall
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\(\\text{Recall} = \\frac{TP}{TP+FN}\\),,nan
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).",What is used when you want to optimize your model to detect positive instances as best as possible?,Recall
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).",What can a model aim for high recall values?,Cancer detection models
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).",Why is predicting healthy people as having cancer less expensive?,False positive
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Specificity, also known as true negative rate, denotes the fraction of all negative instances that are correctly classified as such:",What is also known as true negative rate?,Specificity
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Specificity, also known as true negative rate, denotes the fraction of all negative instances that are correctly classified as such:",What is the fraction of all negative instances that are correctly classified as such?,True negative rate
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\(\\text{Specificity} = \\frac{TN}{TN + FP}\\),,nan
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"It can be interpreted similarly as recall, but in this case, you prioritize detecting negative instances as best as possible.",What type of recall can be interpreted as?,A recall
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"It can be interpreted similarly as recall, but in this case, you prioritize detecting negative instances as best as possible.",What is the best way to detect negative instances?,Prioritize
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Prediction denotes the fraction of positive predictions whose ground truth label is also positive:,Prediction denotes what fraction of positive predictions?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Prediction denotes the fraction of positive predictions whose ground truth label is also positive:,What is the ground truth label of prediction?,Positive
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\(\\text{Precision} = \\frac{TP}{TP + FP}\\),,nan
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.",What is a good idea for optimizing the confidence of positive predictions in your model?,Prediction
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.",What does the government decide to cover the health care cost of anyone who has cancer?,A cancer prediction model with high precision
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.",Who will choose a cancer prediction model with?,The government
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,F1 score is the harmonic mean of precision and recall:,What is the harmonic mean of precision and recall?,F1 score
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,F1 score is the harmonic mean of precision and recall:,What is F1 score?,The harmonic mean of precision and recall
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\(\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\),How many cdot fractextPrecision?,2
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Like accuracy, F1  provides a general measure of model performance without a bias for or against a certain type of error. The differences between accuracy and F1 score are as follows:",What does F1 provide a general measure of model performance without a bias for or against?,A certain type of error
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Like accuracy, F1  provides a general measure of model performance without a bias for or against a certain type of error. The differences between accuracy and F1 score are as follows:",What is the difference between accuracy and F1 score?,
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Accuracy is good for optimizing true positive and true negative, while F1 is good for optimizing false positive and false negative.",What is good for optimizing true positive and false negative?,Accuracy
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Accuracy is good for optimizing true positive and true negative, while F1 is good for optimizing false positive and false negative.",F1 is good to optimizing false negative and false positive?,F1 is good for optimizing false positive and false negative.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Accuracy is good for balanced datasets (with even distribution of the ground truth labels) while F1 is good for imbalanced datasets.,What is good for balanced datasets with even distribution of the ground truth labels?,Accuracy
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,Accuracy is good for balanced datasets (with even distribution of the ground truth labels) while F1 is good for imbalanced datasets.,What does F1 stand for?,Good for imbalanced datasets
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: MCC in skikit-learn.",What is Matthews Correlation Coefficient?,A correlation coefficient between the observed and predicted binary classification.
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: MCC in skikit-learn.",What is a correlation coefficient between observed and predicted binary classification?,Matthews Correlation Coefficient
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: MCC in skikit-learn.","A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label?",0
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,\\[\\textrm {MCC} = \\frac{TP\\times TN - FP\\times FN}{\\sqrt{\\left(TP+FP\\right)\\left(TP+FN\\right)\\left(TN+FP\\right)\\left(TN+FN\\right)}}\\],[textrm MCC = fracTPtimes TN - FPTimes FNsqrtleft?,fracTPtimes TN - FPrightleft(TP
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"If your model outputs a probability value \\(\\hat y\\) that an input data point has a positive label, it can be evaluated by the logistic loss",What is the probability value that an input data point has a positive label?,(hat y)
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"If your model outputs a probability value \\(\\hat y\\) that an input data point has a positive label, it can be evaluated by the logistic loss",What can be evaluated by the logistic loss?,An input data point has a positive label
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"\\(L(\\hat y, y) = y \\log(\\hat y) + (1 - y) \\log (1 - \\hat y)\\)",,nan
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"where \\(y\\) is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.",What is the ground truth label?,y
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"where \\(y\\) is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.",What is a value between 0 and 1?,Logistic loss
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the ""cost of different errors,"" instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.",What is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds?,ROC curve
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the ""cost of different errors,"" instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.",What is Area Under the ROC Curve also known as?,AUC
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,,"The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the ""cost of different errors,"" instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.",The AUC measures how well predictions are ranked and the quality of what?,Prediction
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.",What is the study of how to equip computers with human-level perception?,Computer vision
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.",What are input feature matrices and output scalars?,Vectors
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.","What is the power of neural networks, particularly multilayer perceptrons, in?",The ability to automatically extract a hierarchy of features at different levels of abstraction.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.",What does neural networks preclude the need for feature engineering as standard machine learning methods require?,feature engineering
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 1. Rivian Pickup Truck.,What is a Rivian Pickup Truck?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 1. Rivian Pickup Truck.,What is the name of the truck?,Rivian Pickup Truck
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of image understanding in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer's view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).",What is the task of image understanding in computer vision?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of image understanding in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer's view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).",What is a big matrix with a number at every pixel?,An image
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 2. What a person sees.,What does a person see?,Figure 2
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,Figure 3. What a computer sees.,What does a computer see?,Figure 3
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"A straightforward approach would be to flatten the image's pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).",What is a simple approach to flatten the image's pixel values and feed them as inputs to a multilayer perceptron?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"A straightforward approach would be to flatten the image's pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).",What is the key idea to preserve and use while learning useful visual features in computer vision?,spatial structure of the image
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"A straightforward approach would be to flatten the image's pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).",Which algorithm was formalized as the convolutional neural network?,CNN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"In this module, we will introduce the basic CNN architecture and classic architectures such as LeNet, AlexNet, VGGNet, and ResNet, which you will be implementing during the Computer Vision task in one of the Projects during the course. We will also brief the current state of CNN research and a few contemporary applications of computer vision.",In what module will you introduce the basic CNN architecture?,Module 216
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,,"In this module, we will introduce the basic CNN architecture and classic architectures such as LeNet, AlexNet, VGGNet, and ResNet, which you will be implementing during the Computer Vision task in one of the Projects during the course. We will also brief the current state of CNN research and a few contemporary applications of computer vision.",What is the name of the classic architecture that you will be implementing during the Computer Vision task in one of the Projects?,"LeNet, AlexNet, VGGNet, and ResNet"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Just as the methods proposed must be suitable to achieve the analytic goal, so must the data on which the methods operate. The most important criterion in this regard is, of course, that the data contains patterns that are informative for the analytic objective and allow one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way. Beyond this main requirement, and depending on the project context and proposed methods, specific data sources can be more or less suitable for the project. Possible considerations include:",What must be suitable to achieve the analytic goal?,The methods proposed
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Just as the methods proposed must be suitable to achieve the analytic goal, so must the data on which the methods operate. The most important criterion in this regard is, of course, that the data contains patterns that are informative for the analytic objective and allow one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way. Beyond this main requirement, and depending on the project context and proposed methods, specific data sources can be more or less suitable for the project. Possible considerations include:",What is the most important criterion in this regard?,That the data contains patterns that are informative for the analytic objective.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Just as the methods proposed must be suitable to achieve the analytic goal, so must the data on which the methods operate. The most important criterion in this regard is, of course, that the data contains patterns that are informative for the analytic objective and allow one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way. Beyond this main requirement, and depending on the project context and proposed methods, specific data sources can be more or less suitable for the project. Possible considerations include:",How can a specific data source be more or less suitable for the project?,Dependent on the project context and proposed methods
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?,What is the dataset of the appropriate size for the proposed method?,Yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?,How many data points does the dataset contain?,Nine
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,What distribution do the individual phenomena in the data follow?,What does the distribution of the individual phenomena in the data follow?,The distribution of the individual phenomena in the data follows a linear distribution.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?,What is the dataset raw or readily processable?,raw
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?,What is preprocessing necessary and how will it influence relevant patterns?,Raw or readily processable?
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the data complete or are some parts of it missing?,What is the completeness of the data?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the data complete or are some parts of it missing?,What are some parts missing?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset clean or noisy? Does measurement error play a role?,Is the dataset clean or noisy?,clean
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Is the dataset clean or noisy? Does measurement error play a role?,Does measurement error play a role?,yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?",Are there access restrictions to the data?,yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?",Are the data subject to licensing restrictions?,yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?",What is it subject to?,Licence restrictions
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.",What are well-studied benchmark datasets available to the data science community for many tasks?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.",Prior work on a dataset is a good indicator of what?,its utility for a task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.",What is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"While statistical analysis and machine learning are, of course, core pillars of data science, effective collection, curation, and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects. As such, the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis.",What are the core pillars of data science?,"Effective collection, curation, and interaction with data"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"While statistical analysis and machine learning are, of course, core pillars of data science, effective collection, curation, and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects. As such, the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis.",What is the core method and data component of an analytic objective?,Collecting data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"When proposing data collection as part of an analytical objective, the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource. Data collection, especially involving human annotators, is its own research field. Relevant considerations include:",What is the purpose of collecting data as an analytical objective?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"When proposing data collection as part of an analytical objective, the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource. Data collection, especially involving human annotators, is its own research field. Relevant considerations include:",What is a research field that involves human annotators?,Data collection
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?,How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?,How well
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Does the collection require human annotators? Can it be done using crowdworkers?,Does the collection require human annotators?,Yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Does the collection require human annotators? Can it be done using crowdworkers?,Can it be done using crowdworkers?,yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,What qualifications do the human annotators need to possess? How can they be effectively trained for the task?,What qualifications do human annotators need to possess?,How can they be effectively trained for the task?
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,What qualifications do the human annotators need to possess? How can they be effectively trained for the task?,How can they be effectively trained for the task?,Qualifications
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,Do the human annotators need to be examined/tested before and/or after collection?,What do human annotators need to be examined before and/or after collection?,Yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?,How should the annotation task be designed to ensure productive and correct annotation?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?,How will agreement between annotators be measured?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?,How much data should be gathered to enable progress towards the analytic objective?,500
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?,What can be collected given the budget?,How much data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Once the data has been gathered, what cleaning and curation needs to happen?",What kind of cleaning and curation needs to be done after the data has been collected?,A different cleaning and curation needs to be done.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Once the data has been gathered, what cleaning and curation needs to happen?",What type of curation and cleaning needs to happen after a data collection?,A data collection.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"Are there any approvals/permissions that need to be obtained before the collection can proceed (e.g., human subject research)?",Are there any approvals/permissions that need to be obtained before the collection can proceed?,yes
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"This course includes an introduction to data collection methods. For purposes of this unit, the main takeaway is that collecting good datasets requires an amount of skill, care, and attention to detail comparable to those needed for doing good data analysis. Like core machine learning efforts, data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template.",What is an introduction to data collection methods?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"This course includes an introduction to data collection methods. For purposes of this unit, the main takeaway is that collecting good datasets requires an amount of skill, care, and attention to detail comparable to those needed for doing good data analysis. Like core machine learning efforts, data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template.",What is the main takeaway of a data collection method?,"That collecting good datasets requires skill, care, and attention to detail comparable to those needed for"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,Proposing Datasets & Collection Methods,"This course includes an introduction to data collection methods. For purposes of this unit, the main takeaway is that collecting good datasets requires an amount of skill, care, and attention to detail comparable to those needed for doing good data analysis. Like core machine learning efforts, data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template.",How are data collection projects conducted?,With specific analytic objectives in mind.
Analytic Algorithms and Model Building,Model Selection,Module 18 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Once the team has identified the precise problem for which data should be leveraged, it is advisable to explicitly characterize the task to be done at the analytical level. To do this, one casts the cheartd of the problem into one or more categories of typical data science tasks:",What is the purpose of identifying the exact problem for which data should be leveraged?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Once the team has identified the precise problem for which data should be leveraged, it is advisable to explicitly characterize the task to be done at the analytical level. To do this, one casts the cheartd of the problem into one or more categories of typical data science tasks:",What type of tasks does one cast the cheartd of the problem into?,Types of data science tasks
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Classification: Individual instances in the dataset have a categorical (i.e., non-numerical) label associated with them or will be labeled as such. The goal of the project is to develop a system capable of categorizing data points using these labels. A common variant of classification is sequence labeling, where individual data points are not independent but form a series. Typical examples of classification are sentiment analysis of text and labeling images as depicting certain objects (animals, cars, etc.).",What is the goal of the project?,To develop a system capable of categorizing data points using these labels.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Classification: Individual instances in the dataset have a categorical (i.e., non-numerical) label associated with them or will be labeled as such. The goal of the project is to develop a system capable of categorizing data points using these labels. A common variant of classification is sequence labeling, where individual data points are not independent but form a series. Typical examples of classification are sentiment analysis of text and labeling images as depicting certain objects (animals, cars, etc.).",What is a common variant of classification?,Sequence labeling
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Regression: Individual instances in the dataset have a numerical label associated with them whose magnitude carries a specific meaning. The goal of the project is to develop a model capable of predicting this target score for individual data instances. Like classification, regression is often applied to dependent series of data points. Typical examples of regression include predicting measurements in medical or demographic data over time.",What is the goal of the project?,To develop a model capable of predicting this target score.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Regression: Individual instances in the dataset have a numerical label associated with them whose magnitude carries a specific meaning. The goal of the project is to develop a model capable of predicting this target score for individual data instances. Like classification, regression is often applied to dependent series of data points. Typical examples of regression include predicting measurements in medical or demographic data over time.",What is often applied to dependent series of data points?,Regression
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Retrieval & Ranking: The dataset can be thought of as one or more collections of data points and queries. In response to the query, one or more cideald data points should be retrieved and presented in ccorrectd order. A typical example is a  search engine returning a ranked list of results in response to a query.",What can be thought of as one or more collections of data points and queries?,The dataset
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Retrieval & Ranking: The dataset can be thought of as one or more collections of data points and queries. In response to the query, one or more cideald data points should be retrieved and presented in ccorrectd order. A typical example is a  search engine returning a ranked list of results in response to a query.",What is a typical example of a search engine returning a ranked list of results?,A query
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Recommendation: The dataset consists of users and items, as well as information about the preferences of users for specific items. The task is to find items to recommend to users towards the maximization of some utility. Typical examples are movie recommendations on Netflix and product recommendations on Amazon.",What is the task of the dataset?,To find items to recommend to users towards the maximization of some utility.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Recommendation: The dataset consists of users and items, as well as information about the preferences of users for specific items. The task is to find items to recommend to users towards the maximization of some utility. Typical examples are movie recommendations on Netflix and product recommendations on Amazon.",What are some examples of movie recommendations on Netflix?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,Clustering: The dataset is assumed to have some latent structure which should be discovered by partitioning the data points into groups that are close to each other in variants of the feature space. A typical example is the exploratory analysis of unlabeled data.,What type of structure is the dataset assumed to have?,Latent
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,Clustering: The dataset is assumed to have some latent structure which should be discovered by partitioning the data points into groups that are close to each other in variants of the feature space. A typical example is the exploratory analysis of unlabeled data.,What is a typical example of the exploratory analysis of unlabeled data?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,Anomaly Detection: The dataset is assumed to have some latent structure that should be discovered in order to identify instances that do not adhere to the pattern. A typical example is the detection of fake customer reviews in online retail data.,Anomaly Detection is assumed to have some latent structure that should be discovered to identify instances that do not adhere to what pattern?,The pattern
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,Anomaly Detection: The dataset is assumed to have some latent structure that should be discovered in order to identify instances that do not adhere to the pattern. A typical example is the detection of fake customer reviews in online retail data.,What is a typical example of the detection of fake customer reviews?,online retail data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score. Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an cintersection over uniond threshold.",What are some domains that have developed specific task patterns?,Domain-specific tasks
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score. Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an cintersection over uniond threshold.",What is a specialized metric called?,BLEU score
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score. Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an cintersection over uniond threshold.",How can models that segment a part of an image containing a specific object be evaluated?,Average precision
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"It should be noted that some of these categories overlap somewhat in that the methods associated with them can be used in different ways. For example, ranking problems can be solved using classification or regression methods. Still, if the primary nature of the problem is that items must be ranked, it should be evaluated using ranking experiments and metrics, even if it is doing classification cunder the hood.d",What can be solved using classification or regression methods?,Ranking problems
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"It should be noted that some of these categories overlap somewhat in that the methods associated with them can be used in different ways. For example, ranking problems can be solved using classification or regression methods. Still, if the primary nature of the problem is that items must be ranked, it should be evaluated using ranking experiments and metrics, even if it is doing classification cunder the hood.d",What should the problem be evaluated using?,Ranking experiments and metrics
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"The purpose of identifying the type of task at this stage is not to firmly restrict the project to a narrow instrumentarium, but to characterize it for purposes of planning and communication. Specifically, a possible quantitative evaluation and the metrics used therein will typically closely correspond to the problem's primary nature.",What is the purpose of identifying the type of task at this stage of the project?,To characterize it for purposes of planning and communication.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,,"The purpose of identifying the type of task at this stage is not to firmly restrict the project to a narrow instrumentarium, but to characterize it for purposes of planning and communication. Specifically, a possible quantitative evaluation and the metrics used therein will typically closely correspond to the problem's primary nature.",What is a possible quantitative evaluation and the metrics used therein typically correspond to?,The problem's primary nature
Exploratory Data Analysis,Feature Engineering,Quiz 3,,,What does nan do?,He is a nurse
Exploratory Data Analysis,Feature Engineering,Quiz 3,,,What is the name of the nnan?,The nan name is Peter Durning.
Deep Learning and Model Deployment,Model Deployment,Module 23 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistics is the science of using data to learn about the world around us. In this course, we use the term ""statistics"" in the broad sense to refer to methods for obtaining and analyzing data. Specifically, statistics provides methods for:",What is the science of using data to learn about the world around us?,Stats
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistics is the science of using data to learn about the world around us. In this course, we use the term ""statistics"" in the broad sense to refer to methods for obtaining and analyzing data. Specifically, statistics provides methods for:",What term refers to methods for obtaining and analyzing data?,Statistics
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design: Planning how to gather data for research studies,",What is planning how to gather data for research studies?,Design
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design: Planning how to gather data for research studies,",What is the purpose of planning to collect data?,Research studies
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description: Summarizing the data, and",What is the purpose of summarizing the data?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description: Summarizing the data, and",What does the description of the data mean?,Summarizing the data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Inference: Making predictions based on the data.,Inference: Making predictions based on the data?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Inference: Making predictions based on the data.,What is the purpose of making predictions?,Inference
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.",What refers to planning how to obtain the data?,Design
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.",What type of survey would specify how to select the people to interview and construct the questionnaire?,A survey
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, lets say 3, or the most common response, lets say 2. Graphs, tables, and numerical summaries are called descriptive statistics.",What refers to summarizing data to help understand the information they provide?,Description
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, lets say 3, or the most common response, lets say 2. Graphs, tables, and numerical summaries are called descriptive statistics.",What is an analysis of the number of siblings based on a survey?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, lets say 3, or the most common response, lets say 2. Graphs, tables, and numerical summaries are called descriptive statistics.","The raw data are a complete listing of observations, person-by-person?",Person-by-person
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.",What does inference refer to making predictions based on?,data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.",What percentage of adults in the U.S. reported having no siblings?,15.6%
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.",How are predictions made using data called?,statistical inferences
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, ""Is changing the website layout associated with an increase in visitors?"" or ""Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?""",What are the two types of statistical analysis?,Description and inference
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, ""Is changing the website layout associated with an increase in visitors?"" or ""Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?""",What do data scientists use to answer questions about data?,Descriptive and inferential statistics
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, ""Is changing the website layout associated with an increase in visitors?"" or ""Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?""",How does a student perform in schools?,"Depends on the amount of money spent per student, the size of the classes, or the"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.",What does Statistical methods help us determine?,The factors that explain the variability among subjects.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.",What is the name of a characteristic we can measure for each subject?,A variable
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.","In data science, what is the term feature used interchangeably with?",variable
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.",What are the values the variable takes to form the measurement scale?,Different subjects may have different values of variables.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.",What do these measurement scales result in?,Data exhibiting different types
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.",The valid values for a feature depend on what?,its data type
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.","When discussing data types in data science, data is typically classified as what?",Numeric or categorical
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.",Quantitative data is represented as continuous or discrete values?,Continuous or discrete values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.",What can categorical data be?,Nominal or ordinal values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Discrete: I have one sibling.,How many siblings do i have?,one
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Discrete: I have one sibling.,What is one sibling?,Discrete
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Ordinal: I can rate my customer service experience at the grocery store as Good.,What is the name of my customer service experience at the grocery store?,Good
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Continuous: It takes 1 hour and 20 minutes to get to school.,How long does it take to get to school?,1 hour and 20 minutes
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Nominal: What is your hair color? Brown.,What is the color of your hair?,Brown
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",Nominal: What is your hair color? Brown.,What color is your hair color?,Brown
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data is quantitative when the measurement scale has numerical values. The values represent different magnitudes of the variable. Examples of quantitative variables are income, the number of siblings, age, the number of years of education completed, etc.",What is quantitative when the measurement scale has numerical values?,Data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data is quantitative when the measurement scale has numerical values. The values represent different magnitudes of the variable. Examples of quantitative variables are income, the number of siblings, age, the number of years of education completed, etc.",What are the values that represent different magnitudes of the variable?,Number of units
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.",When is data categorical?,When the measurement scale is a set of categories.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.",What is a set of categories?,Measurement scale
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.",Where is the province of residence classified?,categorical
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.",What are categorical and quantitative data often called?,Qualitative
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.",What is a statistical summary of a quantitative variable?,The average
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.",What is organized facts that are presented in fixed formats and are easy to extract?,Structured Data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.",What type of format can unstructured data be stored in?,Row and column
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.",How is structured data most difficult to use?,It is stored in fixed formats and is easy to extract.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.","Text, multimedia files, and log files from servers are examples of what type of data?",unstructured data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.",What type of data can be classified as internal data?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources","Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.",What is an example of internal data collected and stored by an organization?,Personnel data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.,What are the different types of data explored earlier?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.,What are primary data sources?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources",The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.,Where are secondary data sources collected?,External to an organization
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"The difference between a traditional IT project and a Data Science project is the focus on the analytical data, model, and operations requirements.",What is the difference between a traditional IT project and a Data Science project?,"The focus on the analytical data, model, and operations requirements."
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Requirements are capabilities needed to achieve an objective. Requirements are gathered from stakeholders of a project, including the client, end-users, and the data science team. Good requirements are correct, complete, unambiguous, verifiable, measurable, and traceable.",What are requirements needed to achieve an objective?,Capabilities
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Requirements are capabilities needed to achieve an objective. Requirements are gathered from stakeholders of a project, including the client, end-users, and the data science team. Good requirements are correct, complete, unambiguous, verifiable, measurable, and traceable.",What are requirements collected from stakeholders of a project?,capabilities
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Requirements analysis involves activities that identify business needs, evaluate the feasibility of solutions, and establish constraints.","What type of analysis involves activities that identify business needs, evaluate feasibility of solutions, and establish constraints?",Requirements
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Requirements management is an iterative set of activities to help ensure that elicitation, documentation, refinement, and changes of requirements are done during the data science project life cycle.","What is an iterative set of activities to help ensure that elicitation, documentation, refinement, and changes of requirements are done during the data science project life cycle?",Requirements management
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,Requirements gathering involves fact-finding that describes the current state of the business and the business objectives that inform the proposed solution. Requirements gathering is done to ensure that business needs and solutions are well defined.,What involves fact-finding that describes the current state of the business and the business objectives that inform the proposed solution?,Requirements gathering
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,Requirements gathering involves fact-finding that describes the current state of the business and the business objectives that inform the proposed solution. Requirements gathering is done to ensure that business needs and solutions are well defined.,What is done to ensure that business needs and solutions are well defined?,Requirements gathering
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are three types of requirements highlighted in this unit. The business, the system, and user, and the solution requirements.",What are the three types of requirements highlighted in this unit?,"The business, the system, and user"
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are three types of requirements highlighted in this unit. The business, the system, and user, and the solution requirements.","What is the business, the system, and the user requirements?","The business, the system, and the user requirements are highlighted in this unit."
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,Poorly documented requirements lead to assumptions and miscommunications between the project team and client stakeholders. Communication issues can lead to defects in solutions and unmet expectations. These are issues that a project team should avoid. This can be achieved by defining requirements in the early stages of solution development. Requirements are well-documented statements that define the needs of the users and systems that should be implemented to address a business need. The requirements gathering process can become complex if it is not managed properly. This process is managed by the business analyst.,What are issues that a project team should avoid?,defects in solutions and unmet expectations
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,Poorly documented requirements lead to assumptions and miscommunications between the project team and client stakeholders. Communication issues can lead to defects in solutions and unmet expectations. These are issues that a project team should avoid. This can be achieved by defining requirements in the early stages of solution development. Requirements are well-documented statements that define the needs of the users and systems that should be implemented to address a business need. The requirements gathering process can become complex if it is not managed properly. This process is managed by the business analyst.,What are requirements that define the needs of the users and systems that should be implemented to address a business need?,Well-documented statements
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are different types of requirements including business and solution requirements. Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.",What type of requirements describe the cwhyd behind the implementation of a solution?,Business and solution requirements
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are different types of requirements including business and solution requirements. Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.",What are the requirements that describe the tasks that users will be able to perform with the system?,User requirements
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Similar to a traditional IT project, it is a best practice to collect requirements for analytic projects. The requirements for an analytic project include determining how data, models, and results or outputs of the models will support meeting the analytic and business objectives.",What is a best practice to collect requirements for an analytic project?,
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"Similar to a traditional IT project, it is a best practice to collect requirements for analytic projects. The requirements for an analytic project include determining how data, models, and results or outputs of the models will support meeting the analytic and business objectives.","What is an example of determining how data, models, and outputs of the models will support?",meeting the analytic and business objectives
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are techniques that are used to gather requirements including conducting interviews, brainstorming sessions, and facilitated workshops, other techniques used in addition to the above listed include document analysis and observations. Once requirements are elicited, they are analyzed and translated into written requirements for understanding. Finally, requirements are validated by the project and client team to signal the commencement of solution development.",What are some techniques that are used to gather requirements?,"Conducting interviews, brainstorming sessions, and facilitated workshops"
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are techniques that are used to gather requirements including conducting interviews, brainstorming sessions, and facilitated workshops, other techniques used in addition to the above listed include document analysis and observations. Once requirements are elicited, they are analyzed and translated into written requirements for understanding. Finally, requirements are validated by the project and client team to signal the commencement of solution development.",What are other techniques used?,Document analysis and observations
Data Science Project Planning,Requirements Gathering,Module 5 Summary,,"There are techniques that are used to gather requirements including conducting interviews, brainstorming sessions, and facilitated workshops, other techniques used in addition to the above listed include document analysis and observations. Once requirements are elicited, they are analyzed and translated into written requirements for understanding. Finally, requirements are validated by the project and client team to signal the commencement of solution development.",How are requirements elicited?,analyzed and translated into written requirements for understanding
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Throughout the course of this module, you have learned a lot about the underlying hardware that makes or breaks data science operations. While choosing underlying hardware goes a long way to ensuring that your system works within the budgets you allocated for yourself, you should also be prepared also to optimize your code accordingly. Given that times for jobs can range from overnight to multiple days, even a meager 10% speedup can provide more time to tune your model and extract bigger insights into your problem.",What is a good way to ensure that your system works within the budgets you allocated for yourself?,Choosing underlying hardware
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Throughout the course of this module, you have learned a lot about the underlying hardware that makes or breaks data science operations. While choosing underlying hardware goes a long way to ensuring that your system works within the budgets you allocated for yourself, you should also be prepared also to optimize your code accordingly. Given that times for jobs can range from overnight to multiple days, even a meager 10% speedup can provide more time to tune your model and extract bigger insights into your problem.",What percentage of speedup can provide more time to tune your model and extract bigger insights into your problem?,10%
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Even if you have experience optimizing C or C++ code, data science code can be much trickier to optimize. To better understand why, lets consider a couple of notable factors at play.",What can be much trickier to optimize if you have experience optimizing C or C++ code?,data science code
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Even if you have experience optimizing C or C++ code, data science code can be much trickier to optimize. To better understand why, lets consider a couple of notable factors at play.",What are some factors at play?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Most languages in which data science is done are interpreted rather than compiled. When you interpret code, you do not have the same ability to statically optimize the code as a compiled language. As a result, some of the automatic optimizations you might expect to happen will not, and this will result in slowdowns compared to C++.",What do most languages in which data science is done are interpreted rather than compiled?,interpreted
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Most languages in which data science is done are interpreted rather than compiled. When you interpret code, you do not have the same ability to statically optimize the code as a compiled language. As a result, some of the automatic optimizations you might expect to happen will not, and this will result in slowdowns compared to C++.",What does a compiled language do when you interpret code?,statically optimize the code
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"For most data science stacks, you will need to use matrix-manipulation libraries instead of implementing every bit of an algorithm from scratch. These libraries provide fast implementations of specific operations in a pre-compiled binary, ensuring that the actual code is much faster than what is possible in the interpreted language as is.",What libraries provide fast implementations of specific operations in a pre-compiled binary?,matrix-manipulation libraries
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"For most data science stacks, you will need to use matrix-manipulation libraries instead of implementing every bit of an algorithm from scratch. These libraries provide fast implementations of specific operations in a pre-compiled binary, ensuring that the actual code is much faster than what is possible in the interpreted language as is.",What does matrix-manipulation libraries provide for most data science stacks?,Fast implementations of specific operations in a pre-compiled binary
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Sometimes, the code you are looking at will have layers upon layers of code underneath it that could be the source of your performance issues. Even simple data science projects will have libraries for linear algebra operations linked in, along with the tree of libraries your project requires. This added complexity can make it difficult to understand where the potential problems are and make the relationship between code and performance harder to reason with.",What could be the source of your performance issues?,Layer upon layers of code underneath it.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Sometimes, the code you are looking at will have layers upon layers of code underneath it that could be the source of your performance issues. Even simple data science projects will have libraries for linear algebra operations linked in, along with the tree of libraries your project requires. This added complexity can make it difficult to understand where the potential problems are and make the relationship between code and performance harder to reason with.",What can make it difficult to understand where the potential problems are?,Addition complexity
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Despite these factors, however, there are principles that can be followed to improve performance actively. These are not going to be surefire ways to optimize code, but tend to lead to better performance more often than not.",What are some principles that can be followed to improve performance actively?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Despite these factors, however, there are principles that can be followed to improve performance actively. These are not going to be surefire ways to optimize code, but tend to lead to better performance more often than not.",What are these principles not going to be surefire ways to optimize code?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.",What is the process of writing code in the language of a data science library?,Vectorization
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.",What does vectorization mean?,Writing code in the language of a library.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.",How do libraries you work with can optimize performance?,By default
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"In particular, your goal should be to do at least the following:",What should your goal be to do?,At least the following
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Remove for-loops and replace them with maps: For-loops in interpreted code are much slower than for-loops in pre-compiled code. If you can replace a for-loop with a map function or a function without any side-effects that take in each element of the matrix as input, applies some operation without any side-effect, and returns that element, then your code will speed up accordingly.",What are for-loops in interpreted code much slower than in pre-compiled code?,maps
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Remove for-loops and replace them with maps: For-loops in interpreted code are much slower than for-loops in pre-compiled code. If you can replace a for-loop with a map function or a function without any side-effects that take in each element of the matrix as input, applies some operation without any side-effect, and returns that element, then your code will speed up accordingly.",What does a map function take in each element of the matrix as input?,Side-effects
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to make some function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.",What do you use instead of if-statements?,conditional indexing
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to make some function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.",What does conditional indexing mean?,To express what you want
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to make some function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.",How do you make some functions that associate some element of a matrix with a Boolean without side effects?,Conditional indexing
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.",What is the main goal of vectorization?,Utilizing the resources involved as effectively as possible
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.",What does vectorization aim to use?,resources
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.",Which library is more efficient?,whichever library you have access to.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments which share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.",What is the main bottleneck of your program?,Parallelization disk operations
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments which share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.",What do you need to make your code friendly to?,Multiprocessing or multithreading libraries
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments which share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.",How do you create separate copies of your programs?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the cMap-Reduced framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:",What does Spark use for computation?,cMap-Reduced framework
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the cMap-Reduced framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:",What does cMap-Reduce do?,It uses the cMap-Reduced framework for computation.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the cMap-Reduced framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:",How do you deconstruct the pipeline you wish to parallelize?,In separate phases
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Mapping: Here, you separately process each line of a data file, and apply some operation to turn it into a key, value pair.",How do you process each line of a data file?,Mapping
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Mapping: Here, you separately process each line of a data file, and apply some operation to turn it into a key, value pair.","What is a key, value pair?",A data file
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Reducing: Here, you take all of the data for a given key, and process the values together, producing some output to then map again.",What does Reducing mean?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"Reducing: Here, you take all of the data for a given key, and process the values together, producing some output to then map again.",What do you process together for a given key?,Values
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets, later on, youll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.",What can be challenging to construct a pipeline in this manner?,
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets, later on, youll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.",What is a part of your job?,rephrase the calculations you want to go into into these varying programming frameworks
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets, later on, youll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.","When you take courses on cloud computing or ML on Large Datasets, you will be exposed to what?",These tools.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"These frameworks can automatically parallelize the job with those functions given, allowing you to process more data faster.",What can parallelize the job with the functions given?,Frameworks
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"These frameworks can automatically parallelize the job with those functions given, allowing you to process more data faster.",How can you process more data faster?,These frameworks can automatically parallelize the job with the functions given.
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"If the above steps are not enough, you could write sections of your code in a compiled language and then create functions that use that code in your interpreted language. While this is generally not advised unless you know the code is the main bottleneck, it can provide large speedups at the cost of technical complexity.",What could you write in a compiled language and then create functions that use that code in your interpreted language?,Sections of code
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,,"If the above steps are not enough, you could write sections of your code in a compiled language and then create functions that use that code in your interpreted language. While this is generally not advised unless you know the code is the main bottleneck, it can provide large speedups at the cost of technical complexity.",What is usually not advised unless you know the code is the main bottleneck?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied  Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.",What is unsupervised learning also referred to as?,self-organizing
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied  Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.",What type of analysis techniques do we use to identify patterns in a dataset?,Unsupervised learning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.",What can you categorize data according to characteristics using a technique called Cluster Analysis?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.","What are memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors?",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.",How can clustering data identify similarities as a method of exploring data?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Clustering (Source: www.pyarmy.com),What is clustering?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Clustering (Source: www.pyarmy.com),What does clustering mean?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.",How many groups does Hard Clustering divide data into?,A number of
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.",What can only belong to one cluster?,Data points
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.",How many clusters are independent of each other?,All
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.",What groupes data into clusters?,Soft Clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.",How can a data point belong to more than one cluster to a degree?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Overlapping Clustering allows data to belong to more than one cluster.,What does Overlapping Clustering allow data to belong to?,More than one cluster
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Overlapping Clustering allows data to belong to more than one cluster.,How many clusters do overlapping clustering allow?,More than one cluster
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Hierarchical Clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.,What organizes data in a hierarchical manner so that the hierarchies are represented by what?,A dendrogram
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Reading: An Expansion on Clustering.,What is Reading: An Expansion on Clustering?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:",What method involves identifying the number of clusters k that the dataset will be grouped into?,This method involves identifying the number of clusters k that the dataset will be grouped
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:",What will the data in each cluster share similarities to the other data points within its cluster?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:",The data within clusters adhere to what to ensure dispersion is minimized?,Distance measures
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.",What is chosen to capture the mean of all observations within the cluster?,A set of k means
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.",What is the purpose of a set of k means?,"To capture the mean of all the observations within the cluster, in general."
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Each data point is then assigned to the cluster with the nearest mean.,Which data point is then assigned to the cluster?,Each
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Each data point is then assigned to the cluster with the nearest mean.,What is the nearest mean?,Cluster
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"After a pass, using the points assigned to a cluster, new means are computed.",What is computed after a pass?,New means
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"After a pass, using the points assigned to a cluster, new means are computed.",What are the points assigned to a cluster?,New means are computed
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,The last two steps are repeated.,How many steps are repeated?,The last two steps are repeated.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,The last two steps are repeated.,What are the last two steps?,Replicated
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.,What happens when the assignments to cluster no longer change?,The algorithm converges
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.,What is the algorithm not guaranteed to find?,optimum
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.",How do we decide k?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.",What are empirically studied recommendations for the best k to select?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.",How can you choose k based on previous knowledge?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.",The k-Means cluster method is good practice to run?,By using different values for k.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.,What is WCSS?,Within Cluster Sum of Squares
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.,What is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster?,Within Cluster Sum of Squares (WCSS)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.",How many observations do we have in a dataset?,1000
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.",What should the WCSS be?,zero (0)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.",Why are all the observations considered centroids?,They are considered centroids.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.",How much data is there in the cluster?,"100,000"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the ""elbow"" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.","When you randomly initialize with a range of k values for the 1,000 observations mentioned above, between 2-10, you can use what method to find the optimum value for k?",The Elbow method.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the ""elbow"" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.","What method produces a graph that shows k at the ""elbow"" of the line?",The Elbow method.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the ""elbow"" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.",The figure below shows that after 5 the decrease in what is quite small?,WCSS
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Elbow Method,What is the Elbow Method?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Reading: k-Means Clustering-sklearn.,Reading: k-Means Clustering-sklearn?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,Additional Reading: K-Means Clustering Algorithm,What is the K-Means Clustering Algorithm?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.",What is the main difference between k-Means Clustering and kNearest Neighbors?,It is unsupervised technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.",What is a supervised classification method?,kNN
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.",What does K-means not provide to the model for learning purposes?,A labeled dataset
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.",What is kNN best with?,Data that is of the same scale
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,,"K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.",How does k-mean cope with noise in training?,tends to deal with noise in the training dataset better than a lazy learner.
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"Metrics are employed to objectively evaluate the performance of machine learning models. When selecting a metric, you should always have the end goal of the machine learning application in mind.",What is used to evaluate the performance of machine learning models?,Metrics
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"Metrics are employed to objectively evaluate the performance of machine learning models. When selecting a metric, you should always have the end goal of the machine learning application in mind.","When selecting a metric, you should always have what in mind?",The end goal of the machine learning application
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"In practice, we are usually interested not just in making accurate predictions but also in using these predictions as part of a larger decision-making process. Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. The consequences of choosing a particular algorithm for a machine learning application are called the business impact. Despite being called cbusiness impactd, not losing track of the end goal is important in any scientific domain. This can be the high-level goal of avoiding traffic accidents or decreasing the number of hospital admissions. It could also be getting more users for your website, or having users spend more money in your shop. When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production in a real-life system.",What are the consequences of choosing a particular algorithm for a machine learning application called?,The business impact
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"In practice, we are usually interested not just in making accurate predictions but also in using these predictions as part of a larger decision-making process. Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. The consequences of choosing a particular algorithm for a machine learning application are called the business impact. Despite being called cbusiness impactd, not losing track of the end goal is important in any scientific domain. This can be the high-level goal of avoiding traffic accidents or decreasing the number of hospital admissions. It could also be getting more users for your website, or having users spend more money in your shop. When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production in a real-life system.",What is the high-level goal of avoiding traffic accidents or decreasing the number of hospital admissions?,reducing the number of hospital admissions or increasing the number of hospital admissions.
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidance capabilities of a self-driving car by just letting it drive around, without verifying it first; if your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using an evaluation metric that is easier to compute. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation and selection. The result of this evaluation might not be a single numberthe consequence of your algorithm could be that you have 10% more customers, but each customer will spend 15% lessbut it should capture the expected business impact of choosing one model over another.",What is often infeasible to put models into production just for testing purposes?,
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidance capabilities of a self-driving car by just letting it drive around, without verifying it first; if your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using an evaluation metric that is easier to compute. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation and selection. The result of this evaluation might not be a single numberthe consequence of your algorithm could be that you have 10% more customers, but each customer will spend 15% lessbut it should capture the expected business impact of choosing one model over another.",What is an example of an evaluation metric that is easier to compute?,a surrogate evaluation procedure
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidance capabilities of a self-driving car by just letting it drive around, without verifying it first; if your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using an evaluation metric that is easier to compute. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation and selection. The result of this evaluation might not be a single numberthe consequence of your algorithm could be that you have 10% more customers, but each customer will spend 15% lessbut it should capture the expected business impact of choosing one model over another.",How many customers will each customer spend less?,15%
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"This section covers a number of common metrics used in classification, regression, and clustering problems. It is imperative that you understand which metrics are suitable for which use cases because your choice of metric (which should be decided prior to model training) will have an impact on the entire subsequent pipeline. or example, you may get different best models in the model selection process with different metrics.","What are some of the common metrics used in classification, regression, and clustering problems?",
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"This section covers a number of common metrics used in classification, regression, and clustering problems. It is imperative that you understand which metrics are suitable for which use cases because your choice of metric (which should be decided prior to model training) will have an impact on the entire subsequent pipeline. or example, you may get different best models in the model selection process with different metrics.",What should be decided prior to model training?,metric
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,,"This section covers a number of common metrics used in classification, regression, and clustering problems. It is imperative that you understand which metrics are suitable for which use cases because your choice of metric (which should be decided prior to model training) will have an impact on the entire subsequent pipeline. or example, you may get different best models in the model selection process with different metrics.",How can you get different models with different metrics?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.",What can be complex in nature and require the input and efforts of many stakeholders?,Data Science projects
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.",What is important to a well-defined workflow?,It is followed
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.",How will all stakeholders be on the same page?,A well-defined workflow
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.",What will a data scientist achieve with the end goal of meeting a business need?,Business and analytic objectives
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.",What is the data science life cycle similar to?,System development life cycle
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.",How will the data scientist stay on task?,The lifecycle will give structure to the process.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Figure 2. Data Science Life Cycle (courtesy: Microsoft ),What is the Data Science Life Cycle courtesy of Microsoft?,Figure 2
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"You can draw similarities between the CRISP-DM lifecycle and the data science lifecycle. Perusing the Internet, you will also find that different data science solution vendors have adapted the lifecycle to fit their products and the solutions supported by their tools. The data science lifecycle is briefly explained below:",What is the CRISP-DM lifecycle?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"You can draw similarities between the CRISP-DM lifecycle and the data science lifecycle. Perusing the Internet, you will also find that different data science solution vendors have adapted the lifecycle to fit their products and the solutions supported by their tools. The data science lifecycle is briefly explained below:",What are the data science solutions vendors that have adapted the lifecycle to fit their products?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Business Understanding. cWe fail more often because we solve the wrong problem than because we get the wrong solution to the right problem.d  Professor Russell L. Ackoff.,What do we do more often because we solve the wrong problem than because we get the wrong solution to the right problem?,Fail
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Business Understanding. cWe fail more often because we solve the wrong problem than because we get the wrong solution to the right problem.d  Professor Russell L. Ackoff.,What is Professor Russell L. Ackoff?,Business Understanding
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.",What is tasked with providing solutions to difficult business problems?,Data scientists
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.",What should be supported by factual data?,Solutions to difficult business problems
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.","Before solving a problem, it is important to understand what?",The context of the business and the problem.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Data Acquisition. This process involves obtaining data from various sources and may also require setting up a data collection task and infrastructure.,What process involves obtaining data from various sources?,Data Acquisition
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Data Acquisition. This process involves obtaining data from various sources and may also require setting up a data collection task and infrastructure.,What is another way to collect data?,Using a data collection task and infrastructure.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Subsequently, you will perform data preparation to ensure the data is ready for analysis.",What is the purpose of the data preparation?,To ensure the data is ready for analysis.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Preparation. This is the process of cleaning and transforming raw data prior to processing and analysis. This needs to be done carefully as assumptions made here may influence, or even limit, the use of the data during analysis.",What is the process of cleaning and transforming raw data prior to processing and analysis?,Data Preparation
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Preparation. This is the process of cleaning and transforming raw data prior to processing and analysis. This needs to be done carefully as assumptions made here may influence, or even limit, the use of the data during analysis.",What does Data Preparation mean?,Cleaning and transforming raw data before processing and analysis.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Exploration and Cleaning. The quality of your dataset will determine your success in meeting your business objectives. Data exploration includes identifying variables, conducting univariate and multivariate analyses, identifying outliers, anomalies, and missing values, as well as feature creation and selection. We will cover these topics in future units.",What will determine your success in meeting your business objectives?,The quality of your dataset
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Data Exploration and Cleaning. The quality of your dataset will determine your success in meeting your business objectives. Data exploration includes identifying variables, conducting univariate and multivariate analyses, identifying outliers, anomalies, and missing values, as well as feature creation and selection. We will cover these topics in future units.",What does Data Exploration and Cleaning include?,Identifying variables
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.",What type of model will you learn later in the course?,Modeling
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.",What is the balance between generalizability and performance?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.",How do you want your model to learn and perform well?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Feature Engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features.,What is needed to prepare datasets that are compatible with the appropriate algorithms?,Feature engineering
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,Feature Engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features.,What is required to improve the performance of models by using domain knowledge?,Feature Engineering
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.",What is the purpose of Model Training?,maximizing performance and finding a balance between performance and generalization.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.",What does Model Training involve maximizing performance?,Finding a balance between performance and generalization.
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.","When a Data Scientist has collected millions of records, what should be considered and treated as?",A scarce resource
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.",How are Models trained on dedicated training data and evaluated on dedicated test data?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.",What is the ability to match the training performance on unseen test data referred to as?,The model's ability to generalize
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.",How is a validation data set often sampled from the test data to allow an estimation of test performance during training?,
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Model Evaluation is an essential step in the lifecycle. Typically, analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced. Depending on the nature of the task (as stated in the analytic objective), model evaluation will follow corresponding metrics and techniques that will be explored in this course.",What is an essential step in the lifecycle?,Model Evaluation
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Model Evaluation is an essential step in the lifecycle. Typically, analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced. Depending on the nature of the task (as stated in the analytic objective), model evaluation will follow corresponding metrics and techniques that will be explored in this course.",What are analytical solutions meant to provide when fitted with different datasets or when new data is introduced?,results
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Deployment. Once you have evaluated your models to ensure accuracy and performance, you will deploy the model to an environment for application consumption.",What is the purpose of evaluating your models to ensure accuracy and performance?,Deployment
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,,"Deployment. Once you have evaluated your models to ensure accuracy and performance, you will deploy the model to an environment for application consumption.",What environment will you deploy to?,Application consumption
Advanced Natural Language Processing,Language Representation and Transformers,Module 24 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:",What involves formulating a hypothetical mapping from the input features to the output space?,Machine learning
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:","What can be done by logistic regression, support vector machines, or k-nearest neighbors?",Classification
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search",What is a set of assumptions you make about your data?,A model
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search",What defines the hypothesis space over which learning performs its search?,A model of assumptions you make about your data
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The model parameters are the numeric values or structures that are derived from the learning process.,What are the numeric values or structures that are derived from?,The learning process
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.,What are model hyperparameters?,Numeric values or structures
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.,What are the numeric values or structures that impact the learning process?,The model hyperparameters
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,The learning algorithm specifies the way in which model parameters are updated or derived from the input data.,What algorithm specifies the way in which model parameters are updated or derived from input data?,The learning algorithm
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.","What is the process of identifying the learning algorithm, hyperparameters, and pre-processing steps that yield the best-fitting model for your data?",model selection
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.",What are two candidate models for binary classification?,Table below
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Component,What is the component of the component?,The component is called The component of the component is called The component of the component is called The
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Component,What component does the component component have?,Has a component component that has the component name is ''
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Model 1,What is the Model 1 model?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Model 2,What is the Model 2 model?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Hyperparameters,What are hyperparameters?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Hyperparameters,What is the name of the Hyperparameter?,"The Name of the Hyperparameter is ""Cause of Parameters""."
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Learning rate \\(\\alpha =0.1\\), regularizer \\(\\lambda =1\\), number of iterations \\(N = 1000\\)",,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)",What is the learning rate?,"(alpha =0.5), regularizer(albda"
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)",What is a regularizer?,Learning rate
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Learning algorithm,What is a learning algorithm?,A learning algorithm is a learning algorithm.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Gradient descent over the logistic loss function with L2 regularization,What is a gradual descent over the logistic loss function with L2 regularization?,A gradual descent over the logistic loss function with L2 regularization.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Gradient descent over the logistic loss function with L2 regularization,What is a gradual descent over the logistic loss function with L2 regularization?,A gradual descent over the logistic loss function with L2 regularization.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Pre-processing,Pre-processing involves what type of process?,a chemical process
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,What is the name of the person who is a member of the group?,Nick
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Normalize the data to have zero mean and unit variance,What does normalize the data to have zero mean and unit variance?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Post-processing,What is the post-processing process?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,What is the name of the person who is a member of the group?,Nick
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,None,What is the name of the person who is a member of the group?,Nick
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.",What do both models involve using to perform classification?,Regularized logistic regression
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.",What do Model 1s higher regularizer value correspond to?,The assumption that the dataset may contain outliers which the model should not overfit to.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.",How do Model 2s pre-processing step compare a set of candidate models?,We will introduce ways to compare the models.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.",What is the process of developing models from available data to predict outcomes from new and unseen data?,Prediction
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.",What is an example of a prediction problem?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.",How are predictive models evaluated against data they have not been trained on?,
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.,What is the process of identifying relationships between independent variables and dependent variables?,Inference
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.,Inference models are evaluated on both their goodness of fit and what?,simplicity
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.,What is an example of inference problem?,Inferring people's political inclinations based on their demographic information.
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).",What are the best prediction models typically very different from?,the best inference models
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).",Prediction models are fitted on what?,The training set
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,"Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).",What are inference models fitted on?,dataset
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,,Optional Reading: Integrating explanation and prediction in computational social science,,nan
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.,What are the encoder and decoder units in a Transformer made up of?,Multiple individual encoders and decoders.
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.,What is the structure of the Transformer?,Identical
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Figure 6: Transformer architecture.,Figure 6: What is the name of the Transformer architecture?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.",What is a method to account for the order of words in the input sequence?,A vector.
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.",How does the transformer begin by adding a vector to each input embedding?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.",What does the vectors follow?,A specific pattern
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Figure 7: Transformer Encoder Inputs.,How many Transformer Encoder Inputs are shown in Figure 7?,Two
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.",How many sub-layers is each encoder in the stack broken down into?,Two
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.",What helps the encoder look at other words in the input sentence?,self-attention layer
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.",How many vectors does the self-attention layer create for each word?,Three
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.",How many matrices are trained during the training process?,Three
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.",What is the key/value/query concept analogous to?,Retrieving systems
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.","When you search for videos on Youtube, the search engine will map your query against a set of keys associated with what?",Candidate videos
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.",What can be thought of as a retrieval process?,The attention operation
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.",The outputs of the self-attention layer are then fed into what?,feed-forward neural network
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.",The exact same feed-forward network is independently applied to each position of the input sequence?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.",What is followed by the final results of the first encoder?,Layer normalization
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Figure 8: Transformer Encoder Architecture.,What is the name of the Transformer Encoder Architecture?,Figure 8
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Figure 9: Transformer Encoder-Decoder.,What is the Transformer Encoder-Decoder?,Figure 9
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.",What is shown in Figure 9?,Decoding
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.",What does decoding look very similar to?,Encoding
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.",How does it work?,Sequencing
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.","When we have predicted the first three target words, we give them to what?",Deccoders
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.","In the figure, what are the embeddings of the output sequence already generated?",The input of the first decoder
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.",What is the self-attention layer allowed to do for the first timestamp?,attend to earlier positions in the output sequence
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.",How are the masks removed?,One by one in successive iterations.
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.",What is the result of a decoder-decoder attention layer that is different from the encoder stack?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.",What is a set of attention vectors K and V for use in this layer?,The encoder's output.
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.",How does the cEncoder Attentiond layer work?,Just like multiheaded self-attention
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.",The final decoded result is then fed into what?,A linear layer
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.",What do the following steps repeat until a special symbol is reached?,At a high level
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,"At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.",What is the output of each step fed to the bottom decoder in the next step?,A token
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,Here are some additional sources for more details on Transformers.,What are some additional sources for more details on Transformers?,Here
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,The Annotated Transformer,What is the Annotated Transformer?,
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,The Annotated Transformer,What type of Transformer is annotated?,Transformer is annotated as a part of an annotated transformer.
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,,The Illustrated Transformer,What is the Illustrated Transformer?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language processing has numerous practical applications within todays world, such as question-answering systems or language translators. Here are a few of the more well-known applications:",What is natural language processing?,It is question-answering systems or language translators.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language processing has numerous practical applications within todays world, such as question-answering systems or language translators. Here are a few of the more well-known applications:",What are some of the more well-known applications?,Question-answering systems or language translators
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","As the name suggests, sentiment analysis is used to identify the sentiments in a fragment of a natural language text. Expressions like sarcasm, threat, exclamation, etc., are often very hard to be recognized by the computer. A use case of sentiment analysis is companies identify the opinion and sentiments of their customers based on online reviews and feedback.",What is used to identify the sentiments in a fragment of a natural language text?,Sentiment analysis
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","As the name suggests, sentiment analysis is used to identify the sentiments in a fragment of a natural language text. Expressions like sarcasm, threat, exclamation, etc., are often very hard to be recognized by the computer. A use case of sentiment analysis is companies identify the opinion and sentiments of their customers based on online reviews and feedback.",What type of expressions are often very hard to be recognized by the computer?,"sarcasm, threat, exclamation, etc."
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Information Extraction is the task of extracting pre-specified types of facts from written texts or speech transcripts and converting them into structured representations (e.g., databases). An example is when your email extracts only the relevant data of a meeting invite for you to add to your Calendar.",What is the task of extracting pre-specified types of facts from written texts or speech transcripts?,Information Extraction
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Information Extraction is the task of extracting pre-specified types of facts from written texts or speech transcripts and converting them into structured representations (e.g., databases). An example is when your email extracts only the relevant data of a meeting invite for you to add to your Calendar.",What is an example of an email extracting only the relevant data of a meeting invite for you to add to your Calendar?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Machine Translation is the procedure of automatically converting the text from one language to another language while keeping the meaning intact. In earlier days, machine translation systems were dictionary- and rule-based systems, and they saw very limited success. However, starting in mid 199os first due to statistical models based on large parallel corpora and then to deep neural networks coupled with special processing hardware such as GPUs and TPU (Tensor Processing Units), machine translation has become fairly accurate in converting text from one language to another. I",What is the process of automatically converting text from one language to another?,Machine Translation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Machine Translation is the procedure of automatically converting the text from one language to another language while keeping the meaning intact. In earlier days, machine translation systems were dictionary- and rule-based systems, and they saw very limited success. However, starting in mid 199os first due to statistical models based on large parallel corpora and then to deep neural networks coupled with special processing hardware such as GPUs and TPU (Tensor Processing Units), machine translation has become fairly accurate in converting text from one language to another. I",In what year did machine translation begin?,199os
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Machine Translation is the procedure of automatically converting the text from one language to another language while keeping the meaning intact. In earlier days, machine translation systems were dictionary- and rule-based systems, and they saw very limited success. However, starting in mid 199os first due to statistical models based on large parallel corpora and then to deep neural networks coupled with special processing hardware such as GPUs and TPU (Tensor Processing Units), machine translation has become fairly accurate in converting text from one language to another. I",What is machine translation?,Automatic conversion of text from one language to another language.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real time.",What is the process of producing a human language text response based on some data input?,Natural language generation (NLG)
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real time.",What can be converted into a speech format through text-to-speech services?,Natural language text response
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real time.",How can text summarization capabilities generate summaries from one or more documents?,NLG
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Topic modeling refers to the task of identifying topics that best describe a set of documents. These topics are not predefined and will only emerge during the topic modeling process, which makes it an unsupervised approach. We will delve into the implementation of one of the popular topic modeling techniques known as Latent Dirichlet Allocation (LDA) in Project 5.",What is the task of identifying topics that best describe a set of documents?,Topic modeling
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling","Topic modeling refers to the task of identifying topics that best describe a set of documents. These topics are not predefined and will only emerge during the topic modeling process, which makes it an unsupervised approach. We will delve into the implementation of one of the popular topic modeling techniques known as Latent Dirichlet Allocation (LDA) in Project 5.",What is one of the popular topic modeling techniques known as?,Latent Dirichlet Allocation (LDA)
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.",What does a data science project not begin with?,Building models
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.",What must a scientist approach a project with a methodology?,
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.",A data science team will work with the client to understand what?,the business need(s)
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.",What will be defined by the company to help meet business needs?,Business objectives
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.",What is considered to be achieved in case of success?,Not reached
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.",How can data science methods be used to facilitate the company's efforts?,
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.",What team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts?,A data science team
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.","Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with what?",The client to develop a solution vision.
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.",What will the data science team identify if achieved?,Analytic objectives
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.,What is the EVP?,A framework that can be used to ensure that defined business objectives are met.
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.,What is a framework that can be used to determine business objectives?,EVP
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,,"When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.","When engaging with potential clients seeking analytical solutions, it is important to assess what?",The organizations readiness.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Let us assume that you have a medical dataset that contains observations with features including patient age, weight, height, sex, and race, and you have been tasked with identifying whether a patient is diabetic or not). It would take a long time for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes. Using a data science approach, you can assign a diagnosis to each observation based on the historical data for that diagnosis. You would be a classification problem. Classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new, previously unseen dataset. Below, you will find the different types of classification problems. Later in the course, we will explore methods that can be used to solve classification problems.",How long would it take for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes?,a long time
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Let us assume that you have a medical dataset that contains observations with features including patient age, weight, height, sex, and race, and you have been tasked with identifying whether a patient is diabetic or not). It would take a long time for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes. Using a data science approach, you can assign a diagnosis to each observation based on the historical data for that diagnosis. You would be a classification problem. Classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new, previously unseen dataset. Below, you will find the different types of classification problems. Later in the course, we will explore methods that can be used to solve classification problems.",How can you assign a diagnosis to each observation based on the historical data for that diagnosis?,Using a data science approach
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).",What will classify observations in a dataset into two defined categories?,Classification tasks that are binary
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).",What are the observations grouped based on?,Preparation of characteristics unique to one of the two categories.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).",A decision on a credit card application would be made on what?,approve/deny
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.",What classifies observations into one of three or more classes?,Multi-class classification
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.",What can an observation not be labeled as belonging to?,imore than one class
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.",How would a classifier classify each new image into one type of fruit?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.",What does multi-class classification assume that each observation belongs to a class?,One class
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.",What is the term for multi-label classification?,Multiple-label classification
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?,What is a scenario where observations can belong to multiple classes at once?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?,What type of class can be labeled under?,Multiple class
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document  though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.",What type of classification can be applied to textual data?,Multi-label
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document  though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.",What is the task of determining the topic of a document?,Document classification
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document  though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.",How can a multi-label classification task work?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.",What is the name of the movie that you are tasked to classify based on?,Romance and Comedy
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.",What does Movie A belong to when you classify the observations in this dataset?,Romance and Comedy
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.",How can you look at different classification techniques?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Multi-label classification does not have constraints on the labels that observation can have, and this makes it difficult to learn. Using the OneVsRest Technique, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes.",What technique makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes?,OneVsRest Technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.",What technique trains a separate single-label binary classifier for each class?,Binary Relevance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.",What does the Binary Relevance technique ignore?,Correspondence between classes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.",What is a popular example of a multi-label version of the k-Nearest Neighbors?,kNN
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.",What is the supervised learning technique that makes the assumption that data points are always close together?,k-Nearest Neighbors (kNN)
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Example: scikit-multilearn for MLkNN.,What is a scikit-multilearn for MLkNN?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",You can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier.,How can you transform your task into a multi-class task?,by training all unique class combinations on one multi-class classifier
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X,What is X X?,A Human being.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y1,What is Y1?,A type of baby
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y2,Y2?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y3,What is Y3?,A type of dementia
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X1,What is the name of X1?,The name of X1 is X1.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X2,What is the name of the X2?,The X2 is the name of the X2.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X3,What is the name of the X3?,The X3 is the name of the X3.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X4,What is the name of X4?,The name of X4 is X4
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",0,What is the name of the number of questions that are asked?,0
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Here we see that observations X2 and X3 belong to the same classes. This technique will transform our task into a single multi-class task and give a unique class to all possible combinations in your training data set.,X2 and X3 belong to what class?,Identical classes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Here we see that observations X2 and X3 belong to the same classes. This technique will transform our task into a single multi-class task and give a unique class to all possible combinations in your training data set.,What technique will transform our task into a single multi-class task?,This technique will transform our task into a single multi-class task.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X,What is X X?,A Human being.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",Y1,What is Y1?,A type of baby
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X1,What is the name of X1?,The name of X1 is X1.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,How many questions do you have?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",1,What is the number of questions that you have answered?,1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X2,What is the name of the X2?,The X2 is the name of the X2.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X3,What is the name of the X3?,The X3 is the name of the X3.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",2,How many questions do you have?,Two
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",X4,What is the name of X4?,The name of X4 is X4
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques",3,How many questions do you have?,3
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.","What does oneVsRest, Binary Relevance, and Label Powerset technique not consider?",Correspondences between classes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.",What will the Classifier Chains technique build to take into account?,Any correlations between classes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.",How many classifiers will we have if we have classes?,Three
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.",What is an important classification technique?,Logistic Regression
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.",What does logistic regression use to model the probability of a class or event?,A logistic function
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques","We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.",How many college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in 2024?,18 to 35
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?",What are methods that allow for your model to be fit multiple times with different subsets of a dataset?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?",What are key concepts of importance to every data scientist?,Model Assessment and Model Selection
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?",How can you test and validate your model?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.",What is a method that can be used to validate your model?,Holding out
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.",What is the method that is used to evaluate the error rate of the validation set approach?,Mean squared error (MSE)
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.",How can you estimate the test error rate for quantitative outputs?,Using the mean squared error (MSE)
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.,What is the test error rate for the validation data set?,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.,What will depend on the observations included in the validated data set and not on the training data set.,The test error rate for the validation data set.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.,What might be overestimated when this approach is applied?,The validation data set test error rate
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,k-Fold Cross Validation,What is the k-Fold Cross Validation?,A step towards k-Fold Cross Validation.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.",What is the number of groups that are formed as a result of splitting your dataset?,k
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.",What should the dataset be shuffled randomly and split into groups according to the chosen value of k?,k-fold cross-validation
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.",How can k-fold cross-validation be applied to most learning methods?,It is not costly to implement.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of k=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.",What is not a random process?,Selecting k
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of k=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.",What is a balanced model with low bias and low variance?,A balanced model with low bias and low variance.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of k=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.",How has a fixed value of k=10 been empirically tested to show that the resulting model will be a what?,Balanced model
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.",What does the LOOCV technique offer?,Less bias
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.",What is the cost of using LOOV to make the cost similar to that of fitting a model?,Polynomial regression
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,LOOCV can be used with any kind of predictive modeling.,What can be used with any kind of predictive modeling?,LOOCV
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,LOOCV can be used with any kind of predictive modeling.,What type of modeling can LOOCV be used?,Predictive modeling
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Leave One Out Cross Validation (LOOCV),What does LOOCV mean?,Leave one out cross validation
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.",What will have a higher variance than the k-fold CV?,LOOCV
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.",What is the difference between the outputs of LOOCV and the LOOV models?,The outputs of LOOCV are not as correlated with each other.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Classification Problems: When Y is qualitative, we use the number of misclassified observations as a measure of the model's test error.","When Y is qualitative, we use the number of misclassified observations as a measure of what?",The model's test error.
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,"Regression Problems: When Y is quantitative, the MSE is used to measure the test error.","When Y is quantitative, the MSE is used to measure what?",The test error
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,,Reading: Cross-Validation: Python,Reading: Cross-Validation: Python?,Python
Data Science Project Planning,Design and Plan Overview,Quiz 1,,,What does nan do?,He is a nurse
Data Science Project Planning,Design and Plan Overview,Quiz 1,,,What is the name of the nnan?,The nan name is Peter Durning.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Quiz 9,,,What does nan do?,He is a nurse
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Quiz 9,,,What is the name of the nnan?,The nan name is Peter Durning.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,A few samples of completely fake faces generated by a generative adversarial network (GAN). Source: ThisPersonDoesNotExist.com,What is the name of the network that generates fake faces?,GAN
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,A few samples of completely fake faces generated by a generative adversarial network (GAN). Source: ThisPersonDoesNotExist.com,What is a source of fake faces generated by the GAN?,ThisPersonDoesNotExist.com
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Deep learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep learning is an area of machine learning that draws inspiration from how the human brain functions as a model of computation. Like most machine learning algorithms, deep learning also involves a transformation of data from an input to an output: for example, using speech samples as inputs to predict the speaker or taking some text in one language and translating the text into another language. As the problem becomes more and more complex, classical machine learning approaches fail to adequately learn such transformation or mapping of input to output from the data. Deep learning algorithms overcome this using many successive transformations of input data, thus the deep in deep learning.",Deep learning is an area of what?,Machine learning
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Deep learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep learning is an area of machine learning that draws inspiration from how the human brain functions as a model of computation. Like most machine learning algorithms, deep learning also involves a transformation of data from an input to an output: for example, using speech samples as inputs to predict the speaker or taking some text in one language and translating the text into another language. As the problem becomes more and more complex, classical machine learning approaches fail to adequately learn such transformation or mapping of input to output from the data. Deep learning algorithms overcome this using many successive transformations of input data, thus the deep in deep learning.",Deep learning draws inspiration from how the human brain functions as a model of computation?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Deep learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep learning is an area of machine learning that draws inspiration from how the human brain functions as a model of computation. Like most machine learning algorithms, deep learning also involves a transformation of data from an input to an output: for example, using speech samples as inputs to predict the speaker or taking some text in one language and translating the text into another language. As the problem becomes more and more complex, classical machine learning approaches fail to adequately learn such transformation or mapping of input to output from the data. Deep learning algorithms overcome this using many successive transformations of input data, thus the deep in deep learning.",What type of machine learning algorithms fail to adequately learn?,Classical
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"At a very high-level abstraction, we can view neurons as aggregating signals from their inputs and sending processed signals to their outputs. Such input signals can be visual signals detected by the retina or acoustic signals detected by the ear. Again at an abstract level, the neuron can be considered computing a weighted summation of the inputs weighing each input by a synaptic weight and doing a final non-linear transformation on the sum.",At what level can we view neurons as aggregating signals from their inputs and sending processed signals to their outputs?,High-level abstraction
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"At a very high-level abstraction, we can view neurons as aggregating signals from their inputs and sending processed signals to their outputs. Such input signals can be visual signals detected by the retina or acoustic signals detected by the ear. Again at an abstract level, the neuron can be considered computing a weighted summation of the inputs weighing each input by a synaptic weight and doing a final non-linear transformation on the sum.",What can input signals be detected by the retina or acoustic signals detected by ear?,Visual signals
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"At a very high-level abstraction, we can view neurons as aggregating signals from their inputs and sending processed signals to their outputs. Such input signals can be visual signals detected by the retina or acoustic signals detected by the ear. Again at an abstract level, the neuron can be considered computing a weighted summation of the inputs weighing each input by a synaptic weight and doing a final non-linear transformation on the sum.",How can the neuron be considered computing a weighted summation of the inputs weighing each input?,By a synaptic weight.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.",What are neurons connected to each other via weights?,neural networks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.",What is summed and passed through a nonlinearity function called?,activation function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.",How is the resulting output passed on to neurons in the next layer?,Through a nonlinearity function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,,"In this module, we discuss some of the key concepts in deep learning.",In what module do we discuss some of the key concepts in deep learning?,Module
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"This module focuses on the foundation of a documentation set which is the vision document. This document not only provides a summary of the project but also clears up any confusion and ensures that you, your colleagues, and others are all on the same page.",What module focuses on the foundation of a documentation set?,This module focuses on the foundation of a documentation set.
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"This module focuses on the foundation of a documentation set which is the vision document. This document not only provides a summary of the project but also clears up any confusion and ensures that you, your colleagues, and others are all on the same page.",What document provides a summary of the project?,Vision document
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"This module focuses on the foundation of a documentation set which is the vision document. This document not only provides a summary of the project but also clears up any confusion and ensures that you, your colleagues, and others are all on the same page.",How does this document clear up confusion?,
Data Science Project Planning,Developing a Vision,Module 4 Summary,,The project's high-level scope and purpose are represented in the vision document. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective.,In what document is the project's scope and purpose represented?,Vision document
Data Science Project Planning,Developing a Vision,Module 4 Summary,,The project's high-level scope and purpose are represented in the vision document. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective.,What is the purpose of the project?,To address the domain of the problem that needs to be addressed.
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"To develop a vision for the project, the project team develops a vision document and related artifacts. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution.",What is the vision for the project?,
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"To develop a vision for the project, the project team develops a vision document and related artifacts. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution.",What does the project team develop to develop a vision document?,The project team develops a vision document and related artifacts.
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"To develop a vision for the project, the project team develops a vision document and related artifacts. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution.",The vision defines what?,The high-level objective of the entire project.
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?",What is the real-world problem that you are trying to address?,
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?",What does the vision document aim to do?,Answer questions
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?",How do you come up with an overarching framework for your proposed solution?,
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Next, the vision document focuses on the traceability part of the solution by highlighting the system features you plan to implement from the proposed solution to the problem. Finally, the scope will outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline.",What is the focus of the vision document?,The traceability part of the solution
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Next, the vision document focuses on the traceability part of the solution by highlighting the system features you plan to implement from the proposed solution to the problem. Finally, the scope will outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline.",What part of the solution is focused on the traceability part of?,The vision document
Data Science Project Planning,Developing a Vision,Module 4 Summary,,"Next, the vision document focuses on the traceability part of the solution by highlighting the system features you plan to implement from the proposed solution to the problem. Finally, the scope will outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline.",How will the scope outline the boundaries of your project?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:",What do we need to represent when we design algorithms?,Data items
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:",What is a formal abstract or mathematical sense of data?,A set of data.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:",How can we represent our data as a set?,In mathematics
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,inserting an element into a set,What does insert an element into a set?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,deleting an element from a set,What does deleting an element from a set do?,Removes an element from a set.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"intersecting, unionizing two sets",How many sets of sets are there?,Two
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"intersecting, unionizing two sets",What is the difference between the two sets?,Unionizing
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,subtracting a set from another set,What is subtracting a set from another set?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,testing whether a set is equal to another one or is a subset of another one,What does testing if a set is equal to another?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,testing if a set contains a certain element,What is testing if a set contains a certain element?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Note that we have intentionally not said anything about what kind of an element a set contains. In fact, sets do not really care what types of elements are stored in them, except that they expect at least that you can test if two such elements are equal or not. In a way, we have abstracted sets from what is contained in them.",What have we intentionally not said about a set?,What kind of element a set contains
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Note that we have intentionally not said anything about what kind of an element a set contains. In fact, sets do not really care what types of elements are stored in them, except that they expect at least that you can test if two such elements are equal or not. In a way, we have abstracted sets from what is contained in them.",What do sets expect to test if two elements are equal or not?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Modern computers provide standard representations for data such as integers, and floating point numbers, which are approximations to their mathematical analogs of mathematical integers and real numbers, differing only in the range or the precision of the numbers that can be represented. Our sets can be sets of integers or real numbers or any other structured data we can build from these, e.g., complex numbers, etc.",What are standard representations for data such as integers and floating point numbers?,Modern computers
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Modern computers provide standard representations for data such as integers, and floating point numbers, which are approximations to their mathematical analogs of mathematical integers and real numbers, differing only in the range or the precision of the numbers that can be represented. Our sets can be sets of integers or real numbers or any other structured data we can build from these, e.g., complex numbers, etc.",What are approximations to their mathematical analogs of mathematical integers?,"Standard representations for data such as integers, and floating point numbers."
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Modern computers provide standard representations for data such as integers, and floating point numbers, which are approximations to their mathematical analogs of mathematical integers and real numbers, differing only in the range or the precision of the numbers that can be represented. Our sets can be sets of integers or real numbers or any other structured data we can build from these, e.g., complex numbers, etc.",How can we build from these sets?,Structured data
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What is important for an abstract data type description are the following:,What is important for an abstract data type description?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the mathematical descriptions of each of the operations one can do?,What are the mathematical descriptions of each of the operations one can do?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the types of data items that are input to each operation?,What are the types of data items that are input to each operation?,Arrays
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,What are the types of data items that are output from each operation?,What are the types of data items that are output from each operation?,X
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"For instance, for the first operation above, we input an element and set and get a new set which is guaranteed to contain the given element after the operation. For intersection, we get two sets and return a set of only those elements that are in both of the given sets. Finally, for the last two operations, the output is of a binary-valued boolean type whose values can be true or false. Note that these specifications are independent of what the type elements are in the sets or how the sets are represented, or how each operation is implemented in code.",How many sets do we get for intersection?,two
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"For instance, for the first operation above, we input an element and set and get a new set which is guaranteed to contain the given element after the operation. For intersection, we get two sets and return a set of only those elements that are in both of the given sets. Finally, for the last two operations, the output is of a binary-valued boolean type whose values can be true or false. Note that these specifications are independent of what the type elements are in the sets or how the sets are represented, or how each operation is implemented in code.",What is the output of a binary valued boolean type whose values can be true or false?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"So ideally, the programmer decides on the abstract data types that will be used in the algorithmic solution to a problem by concentrating on the operations that will be needed. At this point, she does not need to worry about how those data structures are represented in detail and how the operations are implemented.",Who decides on the abstract data types that will be used in the algorithmic solution to a problem?,The programmer.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"So ideally, the programmer decides on the abstract data types that will be used in the algorithmic solution to a problem by concentrating on the operations that will be needed. At this point, she does not need to worry about how those data structures are represented in detail and how the operations are implemented.",What does the programmer not need to worry about?,How those data structures are represented in detail and how the operations are implemented.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is communicated through typically an API that names the operations and the input-output data to each call in the API along with some description of what function that call provides.",What is the most important idea one should remember about abstract data types?,That abstract data types determine functionality
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is communicated through typically an API that names the operations and the input-output data to each call in the API along with some description of what function that call provides.",What is an important aspect of an abstract data type that any user needs to know?,Functionality
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is communicated through typically an API that names the operations and the input-output data to each call in the API along with some description of what function that call provides.",How does an API name the operations and output data to each call in the API?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"A data structure specifies how data of an abstract data type is represented and how the operations are implemented. For example, a set can be represented in many different ways:",What does a data structure specify?,How data of an abstract data type is represented and how the operations are implemented
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"A data structure specifies how data of an abstract data type is represented and how the operations are implemented. For example, a set can be represented in many different ways:",What can a set be represented in many different ways?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as an array of elements. This is an efficient representation in terms of the memory required. On the other hand, many of the operations would be very inefficient. inefficiently. For example, to decide if the set contains a certain element, we have to start at the beginning and systematically compare it to every element in the array until we either locate it or exhaust the array. Other operations, such as the intersection of two sets or the insertion of a new element, would have additional complications in terms of steps required or memory allocated to represent the set. Yet, with suitable coding, all the operations can be implemented, but in this case, almost all operations would require a number of steps that is proportional either to the number of elements in the set (e.g., insertion or search) or is proportional to the product of the sizes of each set (e.g., intersection).",How can we represent a set as an array of elements?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as an array of elements. This is an efficient representation in terms of the memory required. On the other hand, many of the operations would be very inefficient. inefficiently. For example, to decide if the set contains a certain element, we have to start at the beginning and systematically compare it to every element in the array until we either locate it or exhaust the array. Other operations, such as the intersection of two sets or the insertion of a new element, would have additional complications in terms of steps required or memory allocated to represent the set. Yet, with suitable coding, all the operations can be implemented, but in this case, almost all operations would require a number of steps that is proportional either to the number of elements in the set (e.g., insertion or search) or is proportional to the product of the sizes of each set (e.g., intersection).",What is an efficient representation in terms of the memory required?,An array of elements
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as an array of elements. This is an efficient representation in terms of the memory required. On the other hand, many of the operations would be very inefficient. inefficiently. For example, to decide if the set contains a certain element, we have to start at the beginning and systematically compare it to every element in the array until we either locate it or exhaust the array. Other operations, such as the intersection of two sets or the insertion of a new element, would have additional complications in terms of steps required or memory allocated to represent the set. Yet, with suitable coding, all the operations can be implemented, but in this case, almost all operations would require a number of steps that is proportional either to the number of elements in the set (e.g., insertion or search) or is proportional to the product of the sizes of each set (e.g., intersection).",How can many of the operations be very inefficient?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,We leave it as an exercise to see how much time these operations would take if one implemented a set as a sorted array.,How long would it take for a set to be implemented?,time
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,We leave it as an exercise to see how much time these operations would take if one implemented a set as a sorted array.,What is the purpose of a sorted array?,To calculate how much time these operations would take if one implemented a set.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as a hash table. Hash tables consume additional memory in addition to the memory required to store the data but allow for a search for any element based on its key in expected constant time. So if you plan to do a lot of searching most of the time, that will be desirable. You can also insert new elements or delete old elements from a set in the expected constant time. However, the intersection of two sets would take a comparatively longer time, and so is the operation of finding the element with the minimum value.",What can we represent as a hash table?,A set
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as a hash table. Hash tables consume additional memory in addition to the memory required to store the data but allow for a search for any element based on its key in expected constant time. So if you plan to do a lot of searching most of the time, that will be desirable. You can also insert new elements or delete old elements from a set in the expected constant time. However, the intersection of two sets would take a comparatively longer time, and so is the operation of finding the element with the minimum value.",What does a haveh table consume in addition to the memory required to store the data?,Memory
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"We can represent a set as a hash table. Hash tables consume additional memory in addition to the memory required to store the data but allow for a search for any element based on its key in expected constant time. So if you plan to do a lot of searching most of the time, that will be desirable. You can also insert new elements or delete old elements from a set in the expected constant time. However, the intersection of two sets would take a comparatively longer time, and so is the operation of finding the element with the minimum value.",How long would the intersection of two sets take?,a comparatively longer time
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,There are many other concrete data structures that one can use to implement the abstract data type for sets.,What type of data structures can one use to implement abstract data types for sets?,Concrete
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,There are many other concrete data structures that one can use to implement the abstract data type for sets.,What are some examples of concrete data structures that one can use to create abstract data?,.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Similarly, a specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement:",What can be used to implement multiple abstract data sets?,A specific concrete data structure
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Similarly, a specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement:",What can a concrete data structure implement?,Multiple abstract data sets
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Complex numbers, where the pair either encodes the real and imaginary parts of a cartesian representation OR the magnitude and the argument of a polar representation of a complex number. All operations on complex numbers (e.g., exponentiation of a complex number to a complex number) would then be implemented in the said representations.",What type of representation encodes the real and imaginary parts of a cartesian representation?,Complex numbers
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Complex numbers, where the pair either encodes the real and imaginary parts of a cartesian representation OR the magnitude and the argument of a polar representation of a complex number. All operations on complex numbers (e.g., exponentiation of a complex number to a complex number) would then be implemented in the said representations.",What kind of representation is a complex number encoded by a pair?,Cartesian representation
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,Two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. One can then implement operations such as vector addition or the dot product of two vectors using this representation.,What are two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes?,Two-dimensional vectors in a cartesian vector space where the two numbers represent the components
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,Two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. One can then implement operations such as vector addition or the dot product of two vectors using this representation.,How can one implement vector addition or the dot product of two vectors using this representation?,
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Note that it is the abstract data type that determines what operations to be done on the concrete representations are sensible or not. For instance, exponentiation of a complex number to a complex number makes sense in the domain of complex numbers but not in the domain of two-dimensional vectors, and similarly, the dot product of two vectors does not make sense in the domain of complex numbers, even though in both cases the very underlying concrete representations are the same.",What is the abstract data type that determines what operations to be done on concrete representations?,Whether they are sensible or not.
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Note that it is the abstract data type that determines what operations to be done on the concrete representations are sensible or not. For instance, exponentiation of a complex number to a complex number makes sense in the domain of complex numbers but not in the domain of two-dimensional vectors, and similarly, the dot product of two vectors does not make sense in the domain of complex numbers, even though in both cases the very underlying concrete representations are the same.",What makes sense in the domain of complex numbers?,exponentiation of a complex number to a complex number
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"Note that it is the abstract data type that determines what operations to be done on the concrete representations are sensible or not. For instance, exponentiation of a complex number to a complex number makes sense in the domain of complex numbers but not in the domain of two-dimensional vectors, and similarly, the dot product of two vectors does not make sense in the domain of complex numbers, even though in both cases the very underlying concrete representations are the same.",How does the dot product of two vectors make sense?,No
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The important point to remember about concrete data structures is that they are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.",What is the important point to remember about concrete data structures?,That they are used to implement abstract data types correctly
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,,"The important point to remember about concrete data structures is that they are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.",What determines the cost of the abstract data?,This implementation
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",AI projects are data intensive. Data can be,What are AI projects that are data intensive?,AI projects that are data intensive.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",AI projects are data intensive. Data can be,What can data be?,Inferent
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",public (from government websites and open data),What type of data does public use?,Open data
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","paid (from the marketplace, brokers, and other services)","What is paid from the marketplace, brokers, and other services?",
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",gathered (from customers using the product platform),What is gathered from customers using the product platform?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",owned (from employees like annotators who manually create data),What is owned by employees like annotators?,owned
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","We get the data mentioned above periodically to build models. Therefore, we need to analyze the quality of data and model performance every time new data is available.",How do we get the data mentioned above?,Periodically.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","We get the data mentioned above periodically to build models. Therefore, we need to analyze the quality of data and model performance every time new data is available.",What do we need to analyze every time new data is available?,Quality of data and model performance
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","It is critical to track data efficiently. For example, let's say we are working with a customer who provides us with newly collected data weekly. Let us also assume we found a drift in data distribution, and we want to roll back to the model version and keep tuning the model from that checkpoint. The previous model can be reproduced by model parameters (weights) and hyper-parameters. However, we might want to reproduce the model using another library that optimizes the performance, which requires retraining or using old data to build another model. Therefore, we need a tool to version model weights and training data efficiently.",What is critical to track data efficiently?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","It is critical to track data efficiently. For example, let's say we are working with a customer who provides us with newly collected data weekly. Let us also assume we found a drift in data distribution, and we want to roll back to the model version and keep tuning the model from that checkpoint. The previous model can be reproduced by model parameters (weights) and hyper-parameters. However, we might want to reproduce the model using another library that optimizes the performance, which requires retraining or using old data to build another model. Therefore, we need a tool to version model weights and training data efficiently.",What is a tool to version model weights and training data effectively?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Sometimes, after our initial engagement, we return to old projects based on customer requirements. We might want to reproduce the previous models built by other developers to deliver predictions to customers. However, the performance of models depends on the data used to train the model. Hence, data needs to be tracked periodically.",When do we return to old projects based on customer requirements?,Sometimes
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Sometimes, after our initial engagement, we return to old projects based on customer requirements. We might want to reproduce the previous models built by other developers to deliver predictions to customers. However, the performance of models depends on the data used to train the model. Hence, data needs to be tracked periodically.",What does the performance of models depend on?,The data used to train the model
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Sometimes, after our initial engagement, we return to old projects based on customer requirements. We might want to reproduce the previous models built by other developers to deliver predictions to customers. However, the performance of models depends on the data used to train the model. Hence, data needs to be tracked periodically.",How does data need to be tracked periodically?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","The Git system (e.g., in Assembla / Atlassian) can only track comparatively small files (e.g., the source code) used for the project. Because git contains a complete history of file changes, disk and memory requirements will grow significantly if we commit data files. Hence, DVC (Data Versioning Control) aims to bring git to projects that use a lot of data and helps to track and version data efficiently. DVC is also easy to learn as it runs on top of git and uses the same git vocabulary.",What system can only track comparatively small files?,Git
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","The Git system (e.g., in Assembla / Atlassian) can only track comparatively small files (e.g., the source code) used for the project. Because git contains a complete history of file changes, disk and memory requirements will grow significantly if we commit data files. Hence, DVC (Data Versioning Control) aims to bring git to projects that use a lot of data and helps to track and version data efficiently. DVC is also easy to learn as it runs on top of git and uses the same git vocabulary.",What is the purpose of DVC?,To bring git to projects that use a lot of data and helps to track and version
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","The Git system (e.g., in Assembla / Atlassian) can only track comparatively small files (e.g., the source code) used for the project. Because git contains a complete history of file changes, disk and memory requirements will grow significantly if we commit data files. Hence, DVC (Data Versioning Control) aims to bring git to projects that use a lot of data and helps to track and version data efficiently. DVC is also easy to learn as it runs on top of git and uses the same git vocabulary.",How does DVC work?,It runs on top of git.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","git-lfs can be a solution for data versioning using pointers and remote storage. However, one of the main advantages of DVC over git-lfs is it doesn't require installing a dedicated server, and it can be used with cloud storage like AWS S3, GCP, Azure, etc. We can also assign tags for each data file version, which allows us to track necessary metadata, such as who gave us the data and when we got it.",What is one of the main advantages of DVC over git-lfs?,It doesn't require installing a dedicated server.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","git-lfs can be a solution for data versioning using pointers and remote storage. However, one of the main advantages of DVC over git-lfs is it doesn't require installing a dedicated server, and it can be used with cloud storage like AWS S3, GCP, Azure, etc. We can also assign tags for each data file version, which allows us to track necessary metadata, such as who gave us the data and when we got it.","What can be used with cloud storage like AWS S3, GCP, Azure, etc?",git-lfs
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Managing the Data Quality,What is the purpose of Managing the Data Quality?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Automating testing and deployment,What automates testing and deployment?,A automates testing and deployment.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Easy integration with MLflow, which helps in tracking experiments","Easy integration with MLflow, which helps in tracking experiments?",
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",1. Install Anaconda,1. Install Anaconda?,1. Install Anaconda.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",2. Define the project folder structure,What is the name of the project folder structure?,Def
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Example,What is an example of an example?,A sentence that requires a specific type of example is provided in a given example.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Clone the above sample repo here.,What type of repo do you use?,a sample
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",Clone the above sample repo here.,What is a sample repo?,Clone
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",3. Create a virtual environment for DVC in the respective project folder and activate it,What is a virtual environment for DVC in the project folder?,Create
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",4. Install dvc,4. Install dvc cc?,4. Install dvc cc.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",4. Install dvc,What is the name of the cvc installed?,4.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",5. Install boto3 for pushing data to AWS S3,5. Install boto3 for pushing data to what?,AWS S3
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",6. Install dvc[s3],6. Install dvc[s3]?,6. Install dvc[s3]
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","7. Initiate DVC which creates .dvc/.gitignore, .dvc/config and .dvcignore files","Initiate DVC which creates.dvc/.gitignore, what is the name of the file created by DVC?",.dvc/.gitignore
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",8. Commit the above dvc files,How many dvc files do you have?,1.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",8. Commit the above dvc files,What is the name of the file that you have compiled above?,Dvc Files
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",9. Avoid tracking the data folder for git,9. Avoid tracking the data folder for git?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",10. DVC tracks the data files by adding a data folder to the DVC cache. It prevents adding to GIT by implicitly adding the data folder to .gitignore.,How does DVC track the data files?,by adding a data folder to the DVC cache
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",10. DVC tracks the data files by adding a data folder to the DVC cache. It prevents adding to GIT by implicitly adding the data folder to .gitignore.,What prevents adding to GIT?,
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",11. Git add and commit data.dvc,What does Git add and commit data.dvc?,11
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",12. Create a bucket on S3,How many buckets do you create on S3?,12.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",13. DVC Remote Adda-acreates a remote section in DVC's config file,What does DVC Remote Adda create in DVC's config file?,A remote section
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",14. Set AWS credentials,What do you set AWS credentials?,14.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",14. Set AWS credentials,What is the name of a set of credentials that you set up AWS?,The Set of AWS credentials is named The Set AWS Credentials is the name of
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",15. Push the data folder to S3,How do you push the data folder to S3?,15.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",16. Remove AWS credential information in .dvc/config before pushing to git,How do you remove AWS credential information in.dvc/config before pushing to git?,16.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)",17. Push the updates to git,How do you push the updates to git?,17.
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)","Kudos, you completed the hands-on Data Versioning tutorial with DVC and AWS S3. Now you have the version history of the data and can revisit the respective files in the future.",How did you complete the hands-on Data Versioning tutorial with DVC and AWS S3. Now you have what history of the data and can revisit the files in future?,Kudos
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,Figure 1. Kidney Cancer Rate in Each U.S. County (2007-2011). (Source: https://dataremixed.com),What is the Kidney Cancer Rate in Each U.S. County?,Figure 1
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbersit is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.",What is the law of small numbers called?,Law of small numbers
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbersit is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.",What is used as a satire?,Law of small numbers
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbersit is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.","When the sample size gets large, the mean of the sample will be near what?",Mean of the overall population
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.","What is one of Daniel Kahneman's examples of cognitive biases in his book Thinking, Fast and Slow?",Drawing naive conclusions and making inferences about the population from a small sample
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.",What kind of conclusions are drawn from a small sample size?,naive
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.",The book provides a great illustration of what?,The dangers of acting as if the law of small numbers is actually a law.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"A study of the incidence of kidney cancer in the 3,141 counties of the United States reveals a remarkable pattern. You may find out that the counties in the United States with the lowest incidence of kidney cancer are mostly rural rather than urban. Now, you can probably imagine why that's true if you're healthy or living out of the countryside with better air, or maybe you're eating from the food that you're growing, and you're getting better nutrition. You can come up with some interesting reasons why this rural lifestyle would lead to a lower rate of kidney cancer.",How many counties in the United States have the lowest incidence of kidney cancer?,3141
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"A study of the incidence of kidney cancer in the 3,141 counties of the United States reveals a remarkable pattern. You may find out that the counties in the United States with the lowest incidence of kidney cancer are mostly rural rather than urban. Now, you can probably imagine why that's true if you're healthy or living out of the countryside with better air, or maybe you're eating from the food that you're growing, and you're getting better nutrition. You can come up with some interesting reasons why this rural lifestyle would lead to a lower rate of kidney cancer.",What is the most common type of cancer in the US?,Kidney
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"A study of the incidence of kidney cancer in the 3,141 counties of the United States reveals a remarkable pattern. You may find out that the counties in the United States with the lowest incidence of kidney cancer are mostly rural rather than urban. Now, you can probably imagine why that's true if you're healthy or living out of the countryside with better air, or maybe you're eating from the food that you're growing, and you're getting better nutrition. You can come up with some interesting reasons why this rural lifestyle would lead to a lower rate of kidney cancer.",How many of the counties that have the highest incidence are rural?,three
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.d",What are the counties in which the incidence of kidney cancer is highest?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.d",Where are kidney cancer ailing counties located?,"Midwest, the South, and the West"
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.d",What is the reason for kidney cancer?,"Poor diet, high-fat diet, and too much alcohol and tobacco."
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"None of these explanations is correct. What's really going on is that the rural counties have fewer people. So we have a smaller sample size and, therefore, more variation in our observed kidney cancer rate even though there isn't any difference in the actual cancer rate because we have more variation for the small counties. Both the lowest and the highest rates come from the counties with a small population, the rural counties.",What is the name of the county that has fewer people?,Rural counties
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"None of these explanations is correct. What's really going on is that the rural counties have fewer people. So we have a smaller sample size and, therefore, more variation in our observed kidney cancer rate even though there isn't any difference in the actual cancer rate because we have more variation for the small counties. Both the lowest and the highest rates come from the counties with a small population, the rural counties.",What type of cancer rate does the rural county have?,Kidney
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,,"None of these explanations is correct. What's really going on is that the rural counties have fewer people. So we have a smaller sample size and, therefore, more variation in our observed kidney cancer rate even though there isn't any difference in the actual cancer rate because we have more variation for the small counties. Both the lowest and the highest rates come from the counties with a small population, the rural counties.",Which county has the lowest and highest rates?,Rural counties
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.",What are the people who want to pursue skills required for data roles?,
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.",What is a lot of overlap between the fields?,Because the fields are relatively new.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.",How are the definitions of the roles and skills required?,Different for different organizations.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","This chapter introduces three different environments for AI professionals and focuses on different tasks and skills required for each environment. In general, a good AI professional needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment. Based on her interest and expertise in tasks of environments, she can further pursue skills in depth and expand their skill set. Awareness of these environments can help individuals avoid confusion while making career decisions in the dynamic data world.",How many different environments does this chapter introduce for AI professionals?,Three
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","This chapter introduces three different environments for AI professionals and focuses on different tasks and skills required for each environment. In general, a good AI professional needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment. Based on her interest and expertise in tasks of environments, she can further pursue skills in depth and expand their skill set. Awareness of these environments can help individuals avoid confusion while making career decisions in the dynamic data world.",What is an example of a good AI professional who needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment?,example of a good AI professional who needs to be aware of the basics of all three environments
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","AI professionals first collaborate with stakeholders and domain experts to understand and define the business problem. They might also present and validate assumptions related to the problem. Once the problem is defined, and assumptions are validated, AI professionals can start working in the research environment.",Who first collaborates with stakeholders and domain experts to understand and define the business problem?,AI professionals
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","AI professionals first collaborate with stakeholders and domain experts to understand and define the business problem. They might also present and validate assumptions related to the problem. Once the problem is defined, and assumptions are validated, AI professionals can start working in the research environment.",What can AI professionals start working in?,research environment
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.",What is the name of a research environment where AI professionals define experiments?,The Exploratory Data Analysis (EDA)
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.","What do AI professionals use to collect data, clean it, perform Exploratory Data Analysis (EDA) and present findings to the team?",Jupyter Notebook and Jupyter Lab
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.",How are metrics defined to evaluate and select models across different experiments? What might be difficult for others to run your code and/or reproduce your results successfully on their machines?,
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",The different steps in the research environment include:,What are the different steps in the research environment?,
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Define Experiments: Different experiments can be defined based on the definition of the problem. For example, suppose we have a classification problem. In that case, experiments might be defined based on different approaches like conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training, etc. Experiments are prioritized based on the type, project timeline, and quantity and quality of data.",What can be defined based on the definition of the problem?,Different experiments
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Define Experiments: Different experiments can be defined based on the definition of the problem. For example, suppose we have a classification problem. In that case, experiments might be defined based on different approaches like conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training, etc. Experiments are prioritized based on the type, project timeline, and quantity and quality of data.",What are examples of experiments that might be defined by different approaches?,"Conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training"
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.",What does the AI professional do for a given experiment?,
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.",What type of scripts do AI professionals use to extract data?,Python
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.",What are the steps in the cleaning of data?,Data Cleaning
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.",What type of data can AI professionals do to classify?,Structured data
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.",How do AI professionals normalize extreme values?,
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.,What is the purpose of EDA?,To find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.,What is EDA's goal?,To find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Feature Engineering: The knowledge from domain experts and EDA patterns help AI professionals create new features that might increase the performance of models in the experiment. Remember that generating relevant new features from existing features is called feature engineering.,What is the term for generating relevant new features from existing features?,feature engineering
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Feature Engineering: The knowledge from domain experts and EDA patterns help AI professionals create new features that might increase the performance of models in the experiment. Remember that generating relevant new features from existing features is called feature engineering.,What do domain experts and EDA patterns help AI professionals create?,New features
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Data Modeling: The goal of a model is to try to replicate domain experts decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.,What is the goal of a model?,To try to replicate domain experts decision-making process
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Data Modeling: The goal of a model is to try to replicate domain experts decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.,What do AI professionals build to automate the decision-making process?,Models
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Tuning and Evaluation: Optimal hyperparameters can be found to maximize the model performance by comparing the metrics of each version of the model in an experiment on evaluation data.,What can be found to maximize the model performance by comparing the metrics of each version of the model in an experiment on evaluation data?,Optimal hyperparameters
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Experiments Tracking and Evaluation:  Steps 2 to 7 are repeated for each experiment and evaluated at the end. Experiment tracking tools like Neptune AI and Weights and Biases can efficiently track experiment information with a good user interface.,How are Experiment Tracking and Evaluation repeated for each experiment?,Steps 2 to 7
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Experiments Tracking and Evaluation:  Steps 2 to 7 are repeated for each experiment and evaluated at the end. Experiment tracking tools like Neptune AI and Weights and Biases can efficiently track experiment information with a good user interface.,What tools can efficiently track experiment information with a good user interface?,Neptune AI and Weights and Biases
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",The model in an experiment with the highest performance is selected for working further in the development environment.,What model is selected for working in a development environment?,The model with the highest performance is selected for working further in a development environment.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.",What is a component that performs one step like data extraction in the AI/ML pipeline?,"An organized, modular, maintainable, and reusable code"
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.","What is an organized, modular, maintainable, and reusable code?",A component
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","In applied machine learning, the AI/ML Pipeline automates performing a sequence of steps in components and interaction between the components defined by the AI/ML system design. The components include data collection, data preprocessing, model development and fine-tuning, post-processing on predictions, model evaluation, model deployment, maintenance, and monitoring.",What does the AI/ML Pipeline automate in applied machine learning?,Performing a sequence of steps in components and interaction between the components defined by the AI/
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","In applied machine learning, the AI/ML Pipeline automates performing a sequence of steps in components and interaction between the components defined by the AI/ML system design. The components include data collection, data preprocessing, model development and fine-tuning, post-processing on predictions, model evaluation, model deployment, maintenance, and monitoring.",What is the name of the component that automates a sequence of steps?,AI/ML Pipeline
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a version control tool. Version control plays a crucial role in the development environment. Version control tools like perforce and assembla make the processes like creating a GIT repository, defining the code repository structure, and branching strategy easy.",What does version control play a crucial role in?,The development environment
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a version control tool. Version control plays a crucial role in the development environment. Version control tools like perforce and assembla make the processes like creating a GIT repository, defining the code repository structure, and branching strategy easy.",What do versions control tools like perforce and assembla make?,Create a GIT repository
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Install IDE like PyCharm to automatically create virtual environments for projects and allow easy integration with GIT.,What does PyCharm use to create virtual environments?,Install IDE
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Install IDE like PyCharm to automatically create virtual environments for projects and allow easy integration with GIT.,What is the name of the IDE that allows easy integration with GIT?,PyCharm
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Convert Jupyter Notebook code into object-oriented code and save in .py files. Have appropriate variable names, add comments, and organize different files into components with proper hierarchy.",How do Jupyter Notebook code convert into object-oriented code?,Save in.py files
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Convert Jupyter Notebook code into object-oriented code and save in .py files. Have appropriate variable names, add comments, and organize different files into components with proper hierarchy.",What is the name of a component that can be organized into components with proper hierarchy?,Theme Tree
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.",What do config files contain?,Standard information across multiple components
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.",What makes adding new variables easy for all components?,Config files
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).,How do tests for multiple components work?,Write and automate them.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).,How do modules test each component individually?,Unit testing
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).,What does integration testing measure?,Interaction between components
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",What makes debugging easy?,Logging
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",What is an essential message to log?,Critical
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",How does a logging message have a minimum level?,"Critical, error, warning, info, debug, or notset"
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.",When is clevel?,When logging.warningd
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.",What are some open-source data versioning tools?,"DVC, Delta Lake, and LakeFS"
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.","What type of software does DVC, Delta Lake, and LakeFS have?",open-source data versioning
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Often based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.",When is a server built?,Often based on the requirement.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Often based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.",What is the name of a web framework that delivers predictions to other software components?,Django
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",The packaged code is further used in the production environment.,What is the name of the code that is used in the production environment?,Packet Code
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.","What is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers?",Production environment
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Design Optimization: In general, there is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment. Hence, if required, the AI/ML system design created before in the development environment needs to be optimized and redesigned for production.","What is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment?",Design Optimization
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Design Optimization: In general, there is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment. Hence, if required, the AI/ML system design created before in the development environment needs to be optimized and redesigned for production.",What should the AI/ML system design created before in the development environment be redesigned for?,Production
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Containerization using Docker: Developers might use multiple components like Data Extractor, Elastic Search, Rest API, Messaging Queues, etc. Each component has its respective dependency libraries. Having components with different versions of a library in the same environment might lead to conflict. With the help of Docker, AI Professionals can standardize environments and run different containers for different components in isolation, where each container has dependent libraries for the respective component. An environment can be created by Docker using a DockerFile. DockerFile contains instructions like navigating to a respective folder, installing dependencies, setting environment variables, loading configuration parameters for the model, etc. Scaling is easy with containers because AI professionals can spin up new containers for the same component in seconds to satisfy the scaling requirements.",What is the name of a container that can be created using Docker?,DockerFile
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Containerization using Docker: Developers might use multiple components like Data Extractor, Elastic Search, Rest API, Messaging Queues, etc. Each component has its respective dependency libraries. Having components with different versions of a library in the same environment might lead to conflict. With the help of Docker, AI Professionals can standardize environments and run different containers for different components in isolation, where each container has dependent libraries for the respective component. An environment can be created by Docker using a DockerFile. DockerFile contains instructions like navigating to a respective folder, installing dependencies, setting environment variables, loading configuration parameters for the model, etc. Scaling is easy with containers because AI professionals can spin up new containers for the same component in seconds to satisfy the scaling requirements.",What is a simple way to scale containers for the same component in seconds?,Using DockerFile
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Continuous Integration and Continuous Delivery (CI/CD): CI/CD enables AI professionals to work together in a shared code repository where updates to a part of code by an individual are automatically pushed, built, tested, delivered, and deployed to the shared code repository, and code issues can be tracked and resolved respectively",What does CI/CD allow AI professionals to work together in a shared code repository?,Continuous Integration and Continuous Delivery (CI/CD)
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Continuous Integration and Continuous Delivery (CI/CD): CI/CD enables AI professionals to work together in a shared code repository where updates to a part of code by an individual are automatically pushed, built, tested, delivered, and deployed to the shared code repository, and code issues can be tracked and resolved respectively",What can be tracked and resolved?,Code issues
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Googles Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Googles Kubeflow and Netflixs Metaflow, which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.",What tools can quickly spin up multiple containers on different machines on demand?,Googles Kubernetes and Red Hat's Openshift
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Googles Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Googles Kubeflow and Netflixs Metaflow, which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.",What is the name of Googles Kubernetes and Red Hat's Openshift?,Openshift
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Googles Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Googles Kubeflow and Netflixs Metaflow, which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.",Who owns the infrastructure of the workflow orchestration tool?,DevOps or AI professionals
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.",What are AI/ML models that are dynamic and degraded over time?,Traditional software
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.",How many metrics can the models health be measured?,Three
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintains good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).","What measure incoming traffic, CPU/GPU memory usage or utilization?",Resource Metrics
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintains good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).",What measure does server handle requests quickly?,prediction latency
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintains good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).",How does server maintain throughput and scales?,Yes
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:",Data Metrics: It is essential to check if the input data format is correct first instead of debugging the entire pipeline.,What is essential to check if the input data format is correct first?,Data Metrics
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Anomaly Checks: Simple checks like having max and minimum values for each feature (age cannot be negative or 100000) can identify and validate extreme or anomalous data points in input data. Later, the team can brainstorm and find root causes for receiving these anomalies from users.",What type of checks can identify and validate extreme or anomalous data points in input data?,Anomaly Checks
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Anomaly Checks: Simple checks like having max and minimum values for each feature (age cannot be negative or 100000) can identify and validate extreme or anomalous data points in input data. Later, the team can brainstorm and find root causes for receiving these anomalies from users.",Who can brainstorm and find root causes for receiving anomalies from users?,The team
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Quality Issues: Users might give synonyms (cGirld for cFemaled) or incorrect values (cMaild instead of cMaled) as input to the pipeline. In these cases, the model might fail to recognize the value in the feature cGenderd (data might be absent while training the model) and assign NaN (not a number) for the feature. Even though the model doesnt break, the predictions produced by the model might be wrong. Hence, testing new data that the model hasnt seen before is essential.",What do users give as input to the pipeline?,Synonyms or incorrect values
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Quality Issues: Users might give synonyms (cGirld for cFemaled) or incorrect values (cMaild instead of cMaled) as input to the pipeline. In these cases, the model might fail to recognize the value in the feature cGenderd (data might be absent while training the model) and assign NaN (not a number) for the feature. Even though the model doesnt break, the predictions produced by the model might be wrong. Hence, testing new data that the model hasnt seen before is essential.",What might the model fail to recognize in the feature cGenderd?,the value
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Drift: When we train a model with some static data, it assumes specific patterns based on the distribution of provided data. However, real-world data is dynamic. Because of these changes, the assumptions made by the model might no longer be valid, and the model might get biased, which leads to bad performance in real-time model evaluation. For example, water consumption in hospitals during COVID-19 was very high compared to historical data. Hence, we cannot use a model built on historical water consumption data during COVID-19. This phenomenon is called cData Drift.d Periodically detecting changes in the distribution of data using statistical tests can help to detect data drift.",What does cData Drift.d mean when we train a model with some static data?,It assumes specific patterns based on the distribution of provided data.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Data Drift: When we train a model with some static data, it assumes specific patterns based on the distribution of provided data. However, real-world data is dynamic. Because of these changes, the assumptions made by the model might no longer be valid, and the model might get biased, which leads to bad performance in real-time model evaluation. For example, water consumption in hospitals during COVID-19 was very high compared to historical data. Hence, we cannot use a model built on historical water consumption data during COVID-19. This phenomenon is called cData Drift.d Periodically detecting changes in the distribution of data using statistical tests can help to detect data drift.",What does the model assume based on the distribution of provided data? What is the reason that the model might not be valid?,Specific patterns
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Model Metrics: It is crucial to estimate the expected performance of models before deploying them into production and periodically check if expected KPIs are met. If model predictions or expected KPI values are bad compared to benchmarks, AI professionals might consider the Model Drift issue. Model Drift is a phenomenon where the relationship between features changes, and the model no longer gives accurate predictions. For example, the relationship between the births and deaths ratio changed during COVID-19 causing model drift. Model drift can be detected by periodically analyzing feedback from feedback loops and correlating it with what is affecting the business.",What is crucial to estimate the expected performance of models before deploying them into production?,Model Metrics
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Model Metrics: It is crucial to estimate the expected performance of models before deploying them into production and periodically check if expected KPIs are met. If model predictions or expected KPI values are bad compared to benchmarks, AI professionals might consider the Model Drift issue. Model Drift is a phenomenon where the relationship between features changes, and the model no longer gives accurate predictions. For example, the relationship between the births and deaths ratio changed during COVID-19 causing model drift. Model drift can be detected by periodically analyzing feedback from feedback loops and correlating it with what is affecting the business.",What can be detected by periodically analyzing feedback from feedback loops and correlating it with what?,Model drift
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring: Based on Model Metrics and Data Metrics, if re-training is required, the AI professional might start repeating the research, development, and production environment to deploy and monitor the new model. Often, retraining is also a way to improve the model's performance to reflect the change in data over time. Some of the Data Monitoring tools include SuperwiseAI, ArtherAI , and VertaAL.",What are some of the Data Monitoring tools?,"SuperwiseAI, ArtherAI, and VertaAL"
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","Monitoring: Based on Model Metrics and Data Metrics, if re-training is required, the AI professional might start repeating the research, development, and production environment to deploy and monitor the new model. Often, retraining is also a way to improve the model's performance to reflect the change in data over time. Some of the Data Monitoring tools include SuperwiseAI, ArtherAI , and VertaAL.",What is retraining often used to improve the model's performance?,Retraining is often used to improve the model's performance.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.",What is good for anyone who wants to make a career in AI to be aware of?,The basics of all three environments
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.",What can help individuals identify the skills required to work in an environment?,Having a basic understanding of the basics of all three environments.
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:","It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.",How can individuals choose to specialize in one or more of these three environments?,Based on their interest.
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a users movie rating history, which movie would they want to watch next?)",What is a common data structure that encodes the relationship between elements stored in rows and columns?,A matrix (2D array)
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a users movie rating history, which movie would they want to watch next?)",Where can a movie review site store the rating history of its users in a matrix?,
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a users movie rating history, which movie would they want to watch next?)",What is the rating score of user i for movie j?,Row i and column j
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).",What is the disadvantage of the matrix format?,It is that matrices can be very sparse in certain domains.
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).",What does sparsity refer to?,the fact that the majority of entries are unknown or missing
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).",How many non-empty cells are in a matrix?,Five
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources:",What can be assigned to a placeholder value?,Empty cells
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources:",What is the primary issue with sparsity?,Waste of memory and computational resources
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"The placeholder values still consume actual memory. For example, storing a 10000 x 10000 sparse matrix of integers takes about 381MB, even if most entries are 0 and do not carry actual meaning.  There is no point in representing data that does not exist!",What does storing a 10000 x 10000 sparse matrix of integers take about 381MB?,
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"The placeholder values still consume actual memory. For example, storing a 10000 x 10000 sparse matrix of integers takes about 381MB, even if most entries are 0 and do not carry actual meaning.  There is no point in representing data that does not exist!",What is no point in representing data that does not exist?,
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Many computations on sparse matrices yield trivial results due to addition/subtraction or multiplication with 0s; however, they still need to be carried out by the computer.",How do many computations on sparse matrices yield trivial results?,Addition/subtraction or multiplication with 0s
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Many computations on sparse matrices yield trivial results due to addition/subtraction or multiplication with 0s; however, they still need to be carried out by the computer.",How do computations need to be carried out?,By the computer.
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.",What has been devised to reflect the underlying sparsity and avoid the above issues?,Alternate matrix representations
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.","In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses?",
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,,"Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.",What do we provide general pointers to?,Applications of sparse matrices
Exploratory Data Analysis,Feature Engineering,Module 13 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.",What technique is used when we want to study the hidden structure of data?,Cluster Analysis technique
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.",What is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,There are different types of clustering techniques. We will briefly define the general idea at this point and fully explore it in an upcoming module.,What are different types of clustering techniques?,There are different types of clustering techniques.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,There are different types of clustering techniques. We will briefly define the general idea at this point and fully explore it in an upcoming module.,What is the general idea of the clustering technique?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.",What is a widely used clustering technique that computes the distance between data points in a group and the center of the group?,K-means clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.",The number of clusters (k) is decided before what?,the process begins
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.",What does K-means clustering use as a similarity measure?,Euclidean distance
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"It is not uncommon to begin by randomly selecting a number of observations from the data as the initial cluster center, the remaining observations will then be assigned to the nearest cluster center. The algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids (the middle of a cluster). The reassignment is done to minimize dispersion within clusters.",What algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids?,The algorithm continues to assign and reassign observations to their closest clusters.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"It is not uncommon to begin by randomly selecting a number of observations from the data as the initial cluster center, the remaining observations will then be assigned to the nearest cluster center. The algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids (the middle of a cluster). The reassignment is done to minimize dispersion within clusters.",What is done to minimize dispersion within clusters?,The reassignment
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.",What does hierarchical clustering connect to form?,Clusters
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.",What is also known as connectivity-based clustering?,Hierarchical clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.",When is each data point considered its own?,At the start of the process
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,There is another approach of hierarchical clustering that puts all data points in one cluster and then separates them based on dissimilarity until different clusters are formed. This is called divisive clustering.,What is another method of hierarchical clustering called?,Divisive clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,There is another approach of hierarchical clustering that puts all data points in one cluster and then separates them based on dissimilarity until different clusters are formed. This is called divisive clustering.,What is the name of the method that puts all data points in one cluster?,Hierarchical clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.",How are hierarchical clusters formed and represented?,Using a dendrogram
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.",What marks the distance where clusters merge?,The y-axis of a dendrogram.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.",Where are data points placed?,On the x-axis.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the kind of clustering technique used to meet your analytic objective. The next sections will focus on the different types of clustering techniques and the evaluation techniques that apply to each technique.",What type of techniques should be evaluated?,Clustering techniques
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,"Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the kind of clustering technique used to meet your analytic objective. The next sections will focus on the different types of clustering techniques and the evaluation techniques that apply to each technique.",What is the type of technique used to meet your analytic objective?,Clustering
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,Example of a Dendrogram. (Source: Mathlab).,Reading: Evaluating Clustering Results,Reading: Evaluating Clustering Results?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.,What will a language processing system use for capturing relevant aspects of the input and output?,Different representation choices
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.,What depends on the task and what is needed in downstream processing in the pipeline?,Representations
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A typical classical NLP pipeline uses at least the representation levels, as shown in the following figure.",What type of pipeline uses at least the representation levels?,A typical classical NLP pipeline
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Phonetics is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while phonology is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc.",What is the study of speech sounds as physical entities?,Phonetics
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Phonetics is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while phonology is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc.",What is phonology a study of?,The organization and function of speech sounds
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.",What type of system analyzes waves of air pressure generated by a human speaking?,Speech recognition
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.",What is transcribed into orthographic symbols making up words taken into context?,Phonemes
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Morphology is the study of word structures, especially how morphemes, which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morphemes).",What is the study of word structures?,Morphology
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Morphology is the study of word structures, especially how morphemes, which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morphemes).",What are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence?,Morphemes
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).",What can morphemes combine in many different ways to build words?,"suffixation, prefixation, infixation, interdigitation, etc"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).",What does a typical morphological take in an orthographical representation of a word and generates a representation of all possible morphology interpretations of the word?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).",A word such as books can be segmented into what?,morphemes
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).",Book+Verb+Pres+3PSg is a third person singular form of what form of the present verb?,to book
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.",What does a morphological representation not necessarily capture?,all the information in a word
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.",What adds additional information to a word representation?,The lexeme representation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.","When we use the word cbanks, are we referring to c banks on the Wall Streetd?",cbanks on the Wall Streetd
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:",What is a valid sentence in a natural language?,Not every sequence of words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:",What is one example of an example of a sentence that is not valid in natural languages?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:",How many sentences are there?,Six
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:",In what language does not every sequence of words constitute?,natural language
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I want a flight to Tokyo,What is the name of the flight I want to take to Tokyo?,I want a flight to Tokyo because I want a flight to Tokyo.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I want to fly to Tokyo,How do I fly to Tokyo?,I want to fly to Tokyo by flying to Tokyo in a single aircraft
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found a flight to Tokyo,Where did I find a flight to?,Tokyo
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found a flight to Tokyo,What was the name of the flight I found?,Tokyo
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",I found to fly to Tokyo,How did I fly to Tokyo?,I found
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.",What does ctod go with in the first sentence?,cTokyo
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.",What is the main verb of the sentence called?,Cwant
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.",How can linguistic computational formalism be captured?,Grammars
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.",What does Grammars assign structure to valid sentences in a language?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.",What is the meaning of a sentence?,Validity is about the structure and not the meaning of a sentence.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.",How does cColorless green ideas sleep furiously?,a syntactically perfectly valid sentence
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.,What is the syntactic representation of sentences?,Hierarchical
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.,What are two commonly used representations of sentences based on?,Grammar and dependency trees
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For example, the following tree representation captures the structure of the sentence, cA boy with a flower sees a girl with a telecope.d The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.",What does a tree representation capture?,the structure of a sentence
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","For example, the following tree representation captures the structure of the sentence, cA boy with a flower sees a girl with a telecope.d The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.",What is the name of a girl with a telecope?,A boy with a flower
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.,What is the structure for the interpretation of this sentence?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.,What does the boy use to see the girl?,Telescope
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The sentence can also have the following tree representation:,What can be found in a sentence?,Tree representation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",This is for the interpretation where the girl is carrying a telescope!,What is the name of the interpretation where the girl is carrying what?,a telescope
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of cbooksd above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.",What is the hardest problem in NLP?,Rerouting such ambiguities at every level of linguistic representation.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of cbooksd above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.",What are the two possible interpretations of cbooksd?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.",What type of representation relies on dependency relationships between lexical items?,syntactic
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.",What does a label denote?,the relation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.",What is the main meaning carrier of the sentence?,Csawd
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.",What is csawd related to?,cbirds.d
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.",cbirds.d is related to what?,Cbirds.d is related to cbirdsd as a prepositional
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Loosely speaking, this level represents the cmeaningd of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a cworld modeld and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as cEverybody has something they  like.d would be represented by a logical form such as \\(\\forall x \\exists y\\  likes(x, y)\\).  The true value of such a sentence can then be computed based on the description of the world model.",What is the definition of the cmeaningd of a sentence?,clevel
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Loosely speaking, this level represents the cmeaningd of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a cworld modeld and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as cEverybody has something they  like.d would be represented by a logical form such as \\(\\forall x \\exists y\\  likes(x, y)\\).  The true value of such a sentence can then be computed based on the description of the world model.",What do early approaches to semantic representation use to capture what?,truth-conditional semantics of a sentence
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Loosely speaking, this level represents the cmeaningd of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a cworld modeld and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as cEverybody has something they  like.d would be represented by a logical form such as \\(\\forall x \\exists y\\  likes(x, y)\\).  The true value of such a sentence can then be computed based on the description of the world model.",A sentence such as cEverybody has something they like.d would be represented by what form?,logical form
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event.  For example, all these sentences:",What is a less formal but potentially more useful approach to semantics?,flatter but still hierarchical representations using semantic roles
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event.  For example, all these sentences:",What do representations assign to syntactically different sentences if they express essentially the same event?,a semantic representation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Warren bought the stock.,Warren bought what stock?,Warren bought the stock.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Warren bought the stock.,What stock did Warren buy?,Warren bought the stock.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Someone sold the stock to Warren.,Who sold the stock to Warren?,Someone
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",Someone sold the stock to Warren.,What was the name of Warren's stock?,Valdez
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The stock was bought by Warren.,Who bought the stock?,Warren
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models",The stock was bought by Warren.,What was the stock bought by Warren?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","are describing the same csellingd event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent.  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.",What is the name of the csellingd event that the buyer is?,Warren
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","are describing the same csellingd event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent.  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.",What does the seller not know or not express?,Ingenious
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.",What is the most recent approach to semantic representation?,rely on embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.",How do embeddings of individual words be computed?,Through an encoder
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.",What does a machine translation system do?,embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual information also includes intonation, physical gestures, and social identity.  For example, an utterance such as cCan you pass the salt? c in a dinner set is really not a question of someones ability to pass the salt but is rather interpreted as a gentle request.",What does Pragmatics deal with understanding how the context in an utterance is made?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual information also includes intonation, physical gestures, and social identity.  For example, an utterance such as cCan you pass the salt? c in a dinner set is really not a question of someones ability to pass the salt but is rather interpreted as a gentle request.","What is a contextual information that includes intonation, physical gestures, and social identity?",A sentence
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual information also includes intonation, physical gestures, and social identity.  For example, an utterance such as cCan you pass the salt? c in a dinner set is really not a question of someones ability to pass the salt but is rather interpreted as a gentle request.",How is c in a dinner set interpreted?,As a gentle request
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.",What requires representation of all aspects of the context?,Practica
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.",What is the set of all propositions that all discourse participants agree on?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.",What does the discourse model mean?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.",What does a sequence of natural language sentences describe?,A local model of entities and the (evolving) relations between them.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.",How do we interpret linguistic expressions in the sentences?,With respect to the discourse model
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:",What does not every possible sequence of sentences constitute?,a meaningful discourse
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:",What two sequences of sentences are considered to be meaningful?,-
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Eric is a pathetic programmer. He only knows Java. Worse still, he always optimizes the outermost loop first. However, the incompetence of his managers ensures him a steady, six-figure income.",What is Eric a pathetic programmer?,He only knows Java.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Eric is a pathetic programmer. He only knows Java. Worse still, he always optimizes the outermost loop first. However, the incompetence of his managers ensures him a steady, six-figure income.",What does Eric do when he optimizes his loop first?,"He has a steady, six-figure income."
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income.  He only knows Java.",What does Eric always optimize first?,outermost loop
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income.  He only knows Java.",What does the incompetence of Eric's managers ensure?,"A steady, six-figure income"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income.  He only knows Java.",Who knows Java?,Eric
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Clearly, only the first of these cmakes sensed; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not cmake sense.d",What does the first cmake sense?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Clearly, only the first of these cmakes sensed; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not cmake sense.d",What is the second sentence that we are likely to see?,Not Cmakes sensed
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Clearly, only the first of these cmakes sensed; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not cmake sense.d",How does the second feel?,Not cmakes sensed
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have cmeaningful relationships.d",What does cohesion and coherence mean?,The degree to which two passages of speech/text are cheld together.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have cmeaningful relationships.d",What is the degree to which two passages of speech/text are cheld togetherd by formal devices?,Cohesion
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics.  The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s.  This hypothesis basically states that cWords that occur in similar contexts tend to have similar meanings.d",What is a representational paradigm based on?,A real vector representation of words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics.  The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s.  This hypothesis basically states that cWords that occur in similar contexts tend to have similar meanings.d",What represents the identity of words as a lexicon would?,A real vector representation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics.  The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s.  This hypothesis basically states that cWords that occur in similar contexts tend to have similar meanings.d",When was the distributional hypothesis first developed?,1950's
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as cbook,d cbank,d or cdown,d get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input.  These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.",What can be computed directly from the distributions of words in large amounts of text using a variety of algorithms?,embeddings
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as cbook,d cbank,d or cdown,d get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input.  These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.",What do NLP algorithms make use of?,The meanings of words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models","Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as cbook,d cbank,d or cdown,d get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input.  These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.",How can basic embeddings be static?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Now that you have studied the elements of a properly framed analytical objective, we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science. They are provided here as purely illustrative example instances of the general template on which you can base your own formulations.\r",How many basic archetypes of hypotheses will cover a fair amount of projects one encounters in data science?,three
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Now that you have studied the elements of a properly framed analytical objective, we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science. They are provided here as purely illustrative example instances of the general template on which you can base your own formulations.\r",What is the general template on which you can base your own formulations?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.\r",What type of objectives will not include every individual element?,analytic
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.\r",What may be evident from a model that performs its function well?,Additive functionality
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.\r","In academic settings, the overarching interest may be that of advancing what?",state of the art
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","A constructive analytical objective states that it is, in principle, possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet. One can think of it as a proof-of-concept or prototyping endeavor.\r",What does a constructive analytical objective state that it is possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","A constructive analytical objective states that it is, in principle, possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet. One can think of it as a proof-of-concept or prototyping endeavor.\r",What is a proof of concept or prototyping endeavor?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to increase sales from the companys online store\r (Business objective)\r,What is the purpose of a company's online store?,To increase sales
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",...we work towards increasing the click-through rate of its advertising through targeted content\r (Problem),What do we do to increase click-through rate of its advertising?,Through targeted contentr (Problem)
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","...by classifying website visitors into youth, middle-age, and senior demographics (Task)\r","What is the name of the group that classifies visitors into youth, middle-age, and senior demographics?",Tap
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","...by classifying website visitors into youth, middle-age, and senior demographics (Task)\r",What are the demographics that classify visitors into?,"Youth, middle-age, and senior"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",...using supervised learning models on curated internal datasets\r (Method)\r,What do supervised learning models use on curated internal datasets?,Method
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",...using supervised learning models on curated internal datasets\r (Method)\r,What is the name of a curated dataset?,Metahod
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,What is omitted due to project being primarily research?,Business objective
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to enable more effective search of audio collections\r (Problem),What is the purpose of a search for audio collectionsr?,To enable more effective search of audio collectionsr.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to enable more effective search of audio collections\r (Problem),What is a term used to describe the search of audio collections?,Problem
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds\r (Task)\r,What is the name of a system that retrieves audio pieces from short natural language descriptions of their sounds?,Tapk
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",using neural models on a dataset of short clips of classical music and their descriptions\r (Method),What do neural models use on a dataset of short clips of classical music?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",towards developing suitable multi-modal audio-textual encoding\r (valuable functionality),What is a multi-modal audio-textual encodingr?,Valable functionality
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","In scenarios where the feasibility of an analytical task has been established, projects may be targeted toward improvement over the state-of-the-art in some performance metrics by using innovative methods/features/data. This is typically the case if one works on leaderboard-type datasets where there are models.\r",What can be used to improve performance metrics?,Innovative methods/features/data.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","In scenarios where the feasibility of an analytical task has been established, projects may be targeted toward improvement over the state-of-the-art in some performance metrics by using innovative methods/features/data. This is typically the case if one works on leaderboard-type datasets where there are models.\r",What type of dataset does one work on?,Leaderboard-type
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client is a logistics company that wants to speed up its automatic package sorting\r (Business objective)\r,What is a logistics company that wants to speed up its automatic package sorting?,The client
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We focus on the problem of handwritten address recognition from shipping label scans \r(Problem and Task),What is the problem of handwritten address recognition from shipping label scans r(Problem and Task?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to combine neural image recognition with language models on company-internal data\r (Method and Data)\r,What do we want to combine neural image recognition with language models on company-internal data?,Method and Data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",To improve performance beyond the current model based on standard convolutional neural networks without language information\r (Valuable functionality),What is the goal of enhancing performance beyond the current model based on?,Standard convolutional neural networks without language informationr
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,What is omitted due to project being primarily research?,Business objective
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",For the task of span-based question answering from text\r (problem and task merged because span-based question answering is a common leaderboard task) \r,What is a common leaderboard task for span-based question answering?,span-based question answering
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to combine graph-based knowledge bases with neural attention models\r (Method),What do we want to combine graph-based knowledge bases with neural attention models?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",To improve over state of the art performance on realistic news text\r (valuable functionality and data)\r,What is an example of a realistic news text?,Valable functionality and data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",To improve over state of the art performance on realistic news text\r (valuable functionality and data)\r,What is a real news text tool?,Valable functionality and data
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",Exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives.\r,What is typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in what?,projects pursuing constructive or benchmarking objectives
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient\r (Business objective)\r,Who runs a complex semi-automatic manufacturing pipeline?,The client
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient\r (Business objective)\r,What does the client want to do?,Make it more efficient
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r","Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified\r (Problem and Task)\r",What does he want to see if some parts of the process are statistically interdependent so that bottlenecks and critical components can be identified?,Problem and Task
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client \r(Methods and Data)\r,What do we want to do on a dataset of production machinery sensor readings provided by the client r(Methods and Data)?,We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",Towards identifying correlating events across the production process that can be used for process optimization\r.,Towards identifying correlating events across the production process that can be used for what?,process optimizationr
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",(Business objective omitted due to project being primarily research)\r,What is omitted due to project being primarily research?,Business objective
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bots replies\r (Problem)\r,The development of AI dialogue systems suffers from a lack of what?,clear training signal of how satisfied the user is with the chat bots
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bots replies\r (Problem)\r,How satisfied the user is with the chat bots repliesr?,(Problem)
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols\r (Methods and Data)\r,What do we want to produce for a dataset of chat protocols?,Basic topic models
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",We want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols\r (Methods and Data)\r,What does the labeling of conversation quality mean?,It indicates that the labeling of conversation quality means the user can easily change the conversation quality.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Constructive,Benchmarking,Exploratory\r",In order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset. \r(Task/Valuable Insight)\r,What is a per-topic quality scoring rubric for the eventual annotation of a larger dataset?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.",What is a data science pattern that makes assumptions about an algorithm and the data that is used to construct it?,Active Learning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.",What is the term for the learning methods you have used so far when you sample and gather data and transform it to train a model?,Active learning
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.,Active learning presents Scenarios that allow a learner to query what?,The labels of observations in a dataset.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).",What is a scenario that allows a learner to generate an observation that is similar to one or more in the dataset?,Membership Query Synthesis
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).",What can be labeled by the oracle?,An information source or teacher
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.",What scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded?,Stream-based Selective Sampling scenario
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.",What is the information that a classifier has when classifying data points?,An informativeness measure
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1,What is the Pool Based Active Learning Cycle-Source?,Settles Active Learning Survey1
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1,What is Settles Active Learning Survey?,Pool Based Active Learning Cycle
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.,How does the algorithm decide on the most informative measures?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.,What are some strategies used to evaluate the informativeness of unlabeled data?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.,What is an approach that allows the active learner to query the observations about which it is not able to label?,Uncertainty Sampling
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.",What does Query-by-committee involve?,Using a group or committee of models that have been trained on a labeled
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.",What are the competing hypotheses of each model in the committee?,They will vote on the labels.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies","Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.",Who will vote on the labels?,Each model in the committee.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.,What approach would be used to select the observation that would introduce the most change to a current model if it was known?,Expected Model Change
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).,What does Expected Error Change mean?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).,What is a measure of how accurate your learner can make predictions on new data?,Out-of-sample error
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Additional Reading: Survey of Active Learning. This report gives an in depth review of active learning in machine learning and artificial intelligence.,What does this report give an in depth review of?,Active learning in machine learning and artificial intelligence.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies",Additional Reading: Survey of Active Learning. This report gives an in depth review of active learning in machine learning and artificial intelligence.,What is an example of a study of active learning?,Machine learning and artificial intelligence
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Statistical Inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision-making purposes.,What is the process of drawing an informed conclusion about an aspect of your entire dataset?,Statistical Inference
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Statistical Inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision-making purposes.,What does Statistical Inference typically use to draw conclusions about your data?,Exploratory data analysis or summary statistics
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Statistical Inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision-making purposes.,The goal of this process is to use probability theory to make inferences about the attributes of your population?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","If you recall from a previous unit, you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise. You can use statistical inference to draw scientific conclusions and test hypotheses. The significance of a sample data set or descriptive statistics is often in question during the EDA process, but using statistical inference techniques can give significance to your conclusions from EDA. Statistical inference techniques are categorized under Estimation and Hypothesis Testing.",What can you use to draw scientific conclusions and test hypotheses?,Statistical inference
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","If you recall from a previous unit, you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise. You can use statistical inference to draw scientific conclusions and test hypotheses. The significance of a sample data set or descriptive statistics is often in question during the EDA process, but using statistical inference techniques can give significance to your conclusions from EDA. Statistical inference techniques are categorized under Estimation and Hypothesis Testing.",What are Statistical inference techniques categorized under?,Estimation and Hypothesis Testing
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Voter preference is a variable that varies among voters. Likewise, the sample proportion voting for a given candidate is a variable. If a sample was randomly drawn from a larger population, the act of random sampling makes the sample itself a random variable. Before the sample is obtained, its value is unknown, and that value varies from sample to sample. If several random samples of size n=2705 each were selected, a certain predictable amount of variation would occur in the sample proportion values. This distribution is called a sampling distribution. The sampling distribution of a statistic is the probability distribution that specifies probabilities for the possible values the statistic can take.",What is a variable that varies among voters?,Voter preference
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Voter preference is a variable that varies among voters. Likewise, the sample proportion voting for a given candidate is a variable. If a sample was randomly drawn from a larger population, the act of random sampling makes the sample itself a random variable. Before the sample is obtained, its value is unknown, and that value varies from sample to sample. If several random samples of size n=2705 each were selected, a certain predictable amount of variation would occur in the sample proportion values. This distribution is called a sampling distribution. The sampling distribution of a statistic is the probability distribution that specifies probabilities for the possible values the statistic can take.",What is the probability distribution that specifies probabilities for the possible values the statistic can take?,The sampling distribution of a statistic
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.",What is the sampling distribution of a sample mean?,A sampling distribution of a sample mean is a sampling distribution of a sample mean.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.",What does a sampling distribution define?,Probabilities of the possible values.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Suppose a student decides to record her commuting times on various days. She selects these days at random from the school year, and her daily commuting time has the cumulative distribution function in Figure 1.",What is the cumulative distribution of a student's daily commuting time?,Figure 1
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Figure 1. Cumulative Distribution Function of Commuting Time.,What is the Cumulative Distribution Function of Commuting Time?,Figure 1.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.",What does knowing the value of the commuting time on one of these randomly selected days provide?,No information about the commuting time on another of the days.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.",What is the reason that the days were selected at random?,To provide no information about the commuting time on one of the days.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.",The values of each of the different days are independently distributed what?,Random variables
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The situation described is an example of the simplest sampling scheme used in statistics, called simple random sampling, in which n objects are selected at random from a population (the population of commuting days) and each member of the population (each day) is equally likely to be included in the sample.",What is an example of the simplest sampling scheme used in statistics called?,Simple random sampling
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The situation described is an example of the simplest sampling scheme used in statistics, called simple random sampling, in which n objects are selected at random from a population (the population of commuting days) and each member of the population (each day) is equally likely to be included in the sample.",What is the population of commuting days that n objects are selected at random?,The population of commuting days
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The n observations in the sample are denoted \\(Y_{1}\\), , \\(Y_{n}\\), where \\(Y_{1}\\) is the first observation, \\(Y_{2}\\) is the second observation, and so forth. In the commuting example, \\(Y_{1}\\) is the commuting time on the first of her n randomly selected days, and \\(Y_{i}\\) is the commuting time on the \\(i^{th}\\) of her randomly selected days.",What are the n observations in the sample?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The n observations in the sample are denoted \\(Y_{1}\\), , \\(Y_{n}\\), where \\(Y_{1}\\) is the first observation, \\(Y_{2}\\) is the second observation, and so forth. In the commuting example, \\(Y_{1}\\) is the commuting time on the first of her n randomly selected days, and \\(Y_{i}\\) is the commuting time on the \\(i^{th}\\) of her randomly selected days.",What is the commuting time on the first of her n randomly selected days?,Y_1
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.",What are the values of the observations (Y_1)?,Y_1
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.",What does the act of random sampling mean?,"That (Y_1), (Y_n)"
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.",How can a random variable be treated?,By random sampling
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because \\(Y_{1}\\), , \\(Y_{n}\\) are randomly drawn from the same population, the marginal distribution of \\(Y_{i}\\) is the same for each i = 1,.., n; this marginal distribution is the distribution of Y in the population being sampled. When \\(Y_{i}\\) has the same marginal distribution for i = 1,..., n, then \\(Y_{1}\\), , \\(Y_{n}\\), are said to be identically distributed.",When are (Y_1) randomly drawn from the same population?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because \\(Y_{1}\\), , \\(Y_{n}\\) are randomly drawn from the same population, the marginal distribution of \\(Y_{i}\\) is the same for each i = 1,.., n; this marginal distribution is the distribution of Y in the population being sampled. When \\(Y_{i}\\) has the same marginal distribution for i = 1,..., n, then \\(Y_{1}\\), , \\(Y_{n}\\), are said to be identically distributed.",What is the distribution of Y in the population being sampled?,Marginal
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Under simple random sampling, knowing the value of \\(Y_{1}\\) provides no information about \\(Y_{2}\\), so the conditional distribution of \\(Y_{2}\\) given \\(Y_{1}\\), is the same as the marginal distribution of \\(Y_{2}\\). In other words, under simple random sampling, \\(Y_{1}\\) is distributed independently of \\(Y_{2}\\), , \\(Y_{n}\\).",What does knowing the value of (Y_1 provide no information about?,(Y_2)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Under simple random sampling, knowing the value of \\(Y_{1}\\) provides no information about \\(Y_{2}\\), so the conditional distribution of \\(Y_{2}\\) given \\(Y_{1}\\), is the same as the marginal distribution of \\(Y_{2}\\). In other words, under simple random sampling, \\(Y_{1}\\) is distributed independently of \\(Y_{2}\\), , \\(Y_{n}\\).",What is the conditional distribution of Y__2 given?,(Y__2)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","When \\(Y_{1}\\), , \\(Y_{n}\\) are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed (or i,i.d.).","When are (Y_1) drawn from the same distribution and are independently distributed, they are said to be what?",Independent and identically distributed
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The sample mean, \\(\\bar{y}\\), is a variable because its value varies from sample to sample. In practice, when we analyze data and find \\(\\bar{y}\\), we don't know how close it falls to the population mean \\(\\mu\\) because we do not know the value of \\(\\mu\\). Using information about the spread of the sampling distribution, though, we can predict how close it falls. For example, the sampling distribution might tell us that with high probability, \\(\\bar{y}\\) falls within 10 units of \\(\\mu\\).",What is the sample mean?,(bary)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The sample mean, \\(\\bar{y}\\), is a variable because its value varies from sample to sample. In practice, when we analyze data and find \\(\\bar{y}\\), we don't know how close it falls to the population mean \\(\\mu\\) because we do not know the value of \\(\\mu\\). Using information about the spread of the sampling distribution, though, we can predict how close it falls. For example, the sampling distribution might tell us that with high probability, \\(\\bar{y}\\) falls within 10 units of \\(\\mu\\).",What is a variable because its value varies from sample to sample?,The sample mean
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","The sample mean, \\(\\bar{y}\\), is a variable because its value varies from sample to sample. In practice, when we analyze data and find \\(\\bar{y}\\), we don't know how close it falls to the population mean \\(\\mu\\) because we do not know the value of \\(\\mu\\). Using information about the spread of the sampling distribution, though, we can predict how close it falls. For example, the sampling distribution might tell us that with high probability, \\(\\bar{y}\\) falls within 10 units of \\(\\mu\\).","When we analyze data and find (bary), we don't know how close it falls to what?",Population mean (mu)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For random samples, it fluctuates around the population mean \\(\\mu\\), sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of \\(\\bar{y}\\) equals \\(\\mu\\). If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean \\(\\mu\\). The spread of the sampling distribution of \\(\\bar{y}\\) is described by its standard deviation, which is called the standard error of \\(\\bar{y}\\). The standard error of \\(\\bar{y}\\) is denoted by \\(\\sigma _{\\bar{y}}\\).",What is the mean of the sampling distribution of (bary)?,=(mu)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For random samples, it fluctuates around the population mean \\(\\mu\\), sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of \\(\\bar{y}\\) equals \\(\\mu\\). If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean \\(\\mu\\). The spread of the sampling distribution of \\(\\bar{y}\\) is described by its standard deviation, which is called the standard error of \\(\\bar{y}\\). The standard error of \\(\\bar{y}\\) is denoted by \\(\\sigma _{\\bar{y}}\\).",What is a standard deviation of the spread of sampling distribution?,The standard error of (bary)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For random samples, it fluctuates around the population mean \\(\\mu\\), sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of \\(\\bar{y}\\) equals \\(\\mu\\). If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean \\(\\mu\\). The spread of the sampling distribution of \\(\\bar{y}\\) is described by its standard deviation, which is called the standard error of \\(\\bar{y}\\). The standard error of \\(\\bar{y}\\) is denoted by \\(\\sigma _{\\bar{y}}\\).",How is the standard deviation described?,It is described as the standard error of (bary).
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For random samples, it fluctuates around the population mean \\(\\mu\\), sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of \\(\\bar{y}\\) equals \\(\\mu\\). If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean \\(\\mu\\). The spread of the sampling distribution of \\(\\bar{y}\\) is described by its standard deviation, which is called the standard error of \\(\\bar{y}\\). The standard error of \\(\\bar{y}\\) is denoted by \\(\\sigma _{\\bar{y}}\\).",The standard error is denoted by what?,(sigma _bary)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For a random sample of size n, the standard error of \\(\\bar{y}\\) depends on n and the population standard deviation \\(\\sigma\\) by \\(\\sigma _{\\bar{y}}=\\frac{\\sigma }{\\sqrt{n}}\\).",What is the standard error for a random sample of size n?,bary
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","For a random sample of size n, the standard error of \\(\\bar{y}\\) depends on n and the population standard deviation \\(\\sigma\\) by \\(\\sigma _{\\bar{y}}=\\frac{\\sigma }{\\sqrt{n}}\\).",What depends on n and the population standard deviation?,The standard error of (bary)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because of random sampling error, it is impossible to learn the exact value of the population mean of Y using only the information in a sample. However, it is possible to use data from a random sample to construct a set of values that contains the true population mean \\(\\mu _{y}\\) with a certain prespecified probability. Such a set is called a confidence set, and the prespecified probability that \\(\\mu _{y}\\) is contained in this set is called the confidence level. The confidence set for \\(\\mu _{y}\\) turns out to be all the possible values of the mean between a lower and an upper limit so that the confidence set is an interval, called a confidence interval.",What is a set of values that contains the true population mean of Y?,Confidentiality set
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because of random sampling error, it is impossible to learn the exact value of the population mean of Y using only the information in a sample. However, it is possible to use data from a random sample to construct a set of values that contains the true population mean \\(\\mu _{y}\\) with a certain prespecified probability. Such a set is called a confidence set, and the prespecified probability that \\(\\mu _{y}\\) is contained in this set is called the confidence level. The confidence set for \\(\\mu _{y}\\) turns out to be all the possible values of the mean between a lower and an upper limit so that the confidence set is an interval, called a confidence interval.",What is the prespecified probability that (mu _y) is contained in this set called?,The confidence level
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Because of random sampling error, it is impossible to learn the exact value of the population mean of Y using only the information in a sample. However, it is possible to use data from a random sample to construct a set of values that contains the true population mean \\(\\mu _{y}\\) with a certain prespecified probability. Such a set is called a confidence set, and the prespecified probability that \\(\\mu _{y}\\) is contained in this set is called the confidence level. The confidence set for \\(\\mu _{y}\\) turns out to be all the possible values of the mean between a lower and an upper limit so that the confidence set is an interval, called a confidence interval.",How does the confidence set turn out to be?,An interval
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",Consider this example:,What is the example of an example of a similar example?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Consider that we are measuring the heights of 40 randomly selected male soccer players, our sample mean is 175cm. We calculate the standard deviation of the athletes' heights to be 20cm. Let us calculate the CI.",How many randomly selected male soccer players do we measure?,40
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Consider that we are measuring the heights of 40 randomly selected male soccer players, our sample mean is 175cm. We calculate the standard deviation of the athletes' heights to be 20cm. Let us calculate the CI.",What is the standard deviation of the athletes' heights?,20 cm
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","n = 40, mean = 175, s = 20.","What is n = 40, mean = 175, s = 20?","Mean = 175, s = 20"
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.,What percentage of the confidence intervals will contain the true mean value?,38
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.,What is the z-value for the selected CI?,95%)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.,How much does a CI mean?,95 %
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",The z-value for 95% CI is 1.960,What is the z-value for 95% CI?,1.960
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",We calculate the 175  1.960  20/\\(\\sqrt{40}\\),How many 1.960 20/(sqrt40)?,We calculate the 175 1.960 20/(sqrt40)
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom",175cm  6.20cm,,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Usually, the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate of the true value of the standard deviation, the sample mean follows the t-distribution with mean and standard deviation. The t-distribution is also described by its degrees of freedom. For a sample of size n, the t-distribution will have n-1 degrees of freedom. The notation for a t-distribution with k degrees of freedom is t(k). As the sample size n increases, the t-distribution becomes closer to the normal distribution since the standard error approaches the true standard deviation for large n.",When is the standard deviation for the population of interest not known?,Usually
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Usually, the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate of the true value of the standard deviation, the sample mean follows the t-distribution with mean and standard deviation. The t-distribution is also described by its degrees of freedom. For a sample of size n, the t-distribution will have n-1 degrees of freedom. The notation for a t-distribution with k degrees of freedom is t(k). As the sample size n increases, the t-distribution becomes closer to the normal distribution since the standard error approaches the true standard deviation for large n.",What is the estimated standard deviation s also known as?,The standard error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom","Usually, the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate of the true value of the standard deviation, the sample mean follows the t-distribution with mean and standard deviation. The t-distribution is also described by its degrees of freedom. For a sample of size n, the t-distribution will have n-1 degrees of freedom. The notation for a t-distribution with k degrees of freedom is t(k). As the sample size n increases, the t-distribution becomes closer to the normal distribution since the standard error approaches the true standard deviation for large n.",The sample mean follows the t-distribution with what degrees of freedom?,Mean and standard deviation
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Regression tasks will predict the state of a target variable based on other input variables. As a quick reminder, the target variables in these tasks are continuous values. Let us discuss the metrics that are used to evaluate the outcome of regression tasks:",What will predict the state of a target variable based on other input variables?,Regression tasks
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Regression tasks will predict the state of a target variable based on other input variables. As a quick reminder, the target variables in these tasks are continuous values. Let us discuss the metrics that are used to evaluate the outcome of regression tasks:",What are the target variables in these tasks?,Continuous values
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.",What is R-squared also known as?,Coefficient of Determination
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.",What is the proportion of the variance in the outcome variable that can be predicted using the predictor variables?,R-squared
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.",How are well-observed outcomes replicated by a model based on?,The proportion of total variation of outcomes explained by the model.
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.","If R2 is 0.5, what would mean that 50% of the variation in the dependent variable is explained by?",Predictor variables
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"When there are multiple regressors, then R2 is the square of the coefficient of multiple correlations (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables""). This metric will provide an indication of how well new data will be predicted by the model.","When there are multiple regressors, what is the square of the coefficient of multiple correlations?",R2
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"When there are multiple regressors, then R2 is the square of the coefficient of multiple correlations (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables""). This metric will provide an indication of how well new data will be predicted by the model.",What is the metric that provides an indication of how well new data will be predicted?,R2
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Adjusted R2 is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of the target field). The result is then subtracted from 1. It identifies the percentage of variance in the target field that is explained by the input or inputs. Adjusted R2 is always less than or equal to R2. A value of 1 indicates a model that perfectly predicts values in the target field. A value that is less than or equal to 0 indicates a model that has no predictive value. In the real world, adjusted R2 lies between these values.",What is adjusted R2 calculated by dividing the residual mean square error by?,Total mean square error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Adjusted R2 is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of the target field). The result is then subtracted from 1. It identifies the percentage of variance in the target field that is explained by the input or inputs. Adjusted R2 is always less than or equal to R2. A value of 1 indicates a model that perfectly predicts values in the target field. A value that is less than or equal to 0 indicates a model that has no predictive value. In the real world, adjusted R2 lies between these values.",What is the sample variance of the target field?,Total mean square error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Adjusted R2 is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of the target field). The result is then subtracted from 1. It identifies the percentage of variance in the target field that is explained by the input or inputs. Adjusted R2 is always less than or equal to R2. A value of 1 indicates a model that perfectly predicts values in the target field. A value that is less than or equal to 0 indicates a model that has no predictive value. In the real world, adjusted R2 lies between these values.",How is adjusted compared to R2?,
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.",What is a weakness of R2 because it cannot determine whether the coefficient estimates and predictions might be biased?,
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.",What happens when a predictor is added to a model?,The model will likely overfit and result in a high R2
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.",Adjusted R2 attempts to correct this overestimation?,
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.","When a new predictor or predictor improves the model less than expected, what will increase?",Adjusted R2
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.,What is a measure of the quality of an estimator or a predictor?,Mean Squared Error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.,A value closer to zero is always best?,Mean Squared Error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.,What is the residual sum of squares divided by?,Degrees of freedom
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (the absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.,What is the measure of errors between paired observations and computed as the average of all absolute errors?,Mean Absolute Error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (the absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.,What metric is used to measure accuracy?,Mean Absolute Error
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (the absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.,How does the MAPE compare predictions?,
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y.",What gives weight to large errors?,Root Mean Squared Error (RMSE)
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y.",What is the RMSE computed by first determining the residuals?,Difference between the actual and predicted y values.
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,"Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y.",The square root of the averaged squares will result in what?,RMSE
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,,Resource: Regression Metrics-scikit-learn,What is Regression Metrics-scikit-learn?,Resource
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.",What are models trained on?,the entire dataset
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.",What is the goal of acquiring the most reasonable model that is sufficiently simple?,Interpretable
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as",What is the AIC?,Akaike Information Criterion
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as",The AIC score of a model M is computed as what?,As follows
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,\\[ AIC(M)=(2K_{h}-2LL(M))/N \\],,nan
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.",What is the maximum log-likelihood of M on the dataset?,LL(M)
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.",What is LL(M) for regression?,Mean squared error
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.",How is a model with a smaller AIC value considered better?,for inference
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as",What is a BIC?,Bayesian Information Criterion
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as",What is the BIC based on?,Bayesian statistics
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,\\[ BIC(M)=K_{M}\\times logN-2LL(M) \\],,nan
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.","What are the variables KM, N, and LL(M) defined as in AIC?",AIC
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.",A model with a smaller BIC value is considered what?,Better for inference
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.","What can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily?",
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as",What is the minimum Description Length of a Model M?,MDL
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as",What is derived from information theory?,Minimum Description Length (MDL)
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,\\[ MDL =L(M)+L(D|H) \\],,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",Where is L(M) the number of bits required to represent the model h?,
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,,"Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",What is considered better for inference?,A model with a smaller MDL value
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.",What is an example of a doctor using a model to predict whether a patient has the flu?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.","What is a middle ""man"" between the prediction and the explanation of the prediction?",
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was madeunderstanding the why will make the problem clearer and affect problem-solving for future challenges.",What is important to data science and machine learning?,Interpretability
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was madeunderstanding the why will make the problem clearer and affect problem-solving for future challenges.",What is not enough to trust the predictions of a model based on prescribed metrics?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.",What is one reason why interpretability is important?,Bias identification
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.",What does a model grant loans to one person and not to another with similar credit scores and income?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.",Who can debug and audit models when they are easily interpretable?,data scientist and machine learning engineers
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).,What is not needed if a model does not have an impact of much significance?,Interpretability
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).,What does not help with detecting bias?,The context in which it is applied has been extensively investigated
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.,What is the next module?,An overview of the assessments or metrics that typically concern you as the data scientist.
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.,What are the metrics that typically concern you as a data scientist?,Assessments or metrics
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Reading: Should you trust that model?,What model should you trust?,That model?
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Reading: Should you trust that model?,What model does reading have?,?
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Reading: Should you trust that model?,How do you trust that model?,Should you trust that model?
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.",The authors of the above article proposed a technique to explain the predictions and usefulness of what?,Machine learning model
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.",What type of classifiers have the authors tested the technique?,A number of
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Local Surrogate Models""explain individual predictions of black box models.""","Local Surrogate Models ""explain individual predictions of what?",Black box models
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.,Shapley Value is concerned with explaining a prediction by assessing the importance of what?,Features
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Additional Resource: Sara Hooker: The Myth of the Perfect Model,What is Sara Hooker's book?,The Myth of the Perfect Model
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.",What did you learn about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.",What will be helpful for fixing issues with the model and explaining why a model produced its results?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.",What is a very important research area in data science and machine learning?,Interpretability
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.",What does explainability mean?,Changes in a model
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.",How can a data scientist measure the effects of trade-offs within a model?,Interpretability
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).",What is the difference between a useful solution and a solution that will lead to loss of money?,Errors
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).",What can be defined as the measurement used to determine the best model for a task?,Accuracy
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""",What are some sectors that are restricted by laws and standards in their use in banking and education industry?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""",What do some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process?,Banking and education industry
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,"There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""","Accuracy is very important in these sectors and as we have learned, what will typically lead to less interpretability?",accuracy
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Hall (2016) has recommended the following steps:,How many steps did Hall (2016) recommend?,six
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Hall (2016) has recommended the following steps:,What was Hall's recommendation?,The following steps
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Train black-box models and use them as benchmarks.,What are black-box models used as?,Benchmarks
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Train black-box models and use them as benchmarks.,What type of models are used as benchmarks?,Train black-box models
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Use different regression techniques.,What type of techniques do you use?,Regression
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Use different regression techniques.,What types of regression techniques are used?,Different regression techniques are used.
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Use black-box models in the deployment process.,What type of model is used in the deployment process?,Black-box model
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Use black-box models in the deployment process.,What is the purpose of blackbox models?,deployment process
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Train small interpretable ensemble models.,How do you train small interpretable ensemble models?,
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Create nonlinear predictors using black-box techniques.,What techniques are used to create nonlinear predictors?,Black-box techniques.
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Explain black box models better using variable importance measures.,What is a better way to explain black box models?,variable importance measures
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Explain black box models better using variable importance measures.,What measures do black boxes use?,Variable importance measures
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,,Figure 1. Accuracy versus Interpretability (Source: Rane-20181),What is the difference between Accuracy and Interpretability?,Figure 1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.",What is the purpose of the exploratory data analysis?,To summarize data and to make it easier to assimilate the information.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.",What are non-graphical techniques used to describe the data?,Summary statistics
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.",What is a dependent on the types of data present in your dataset?,Summarizing data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.",What are some techniques used to describe a large data set in its raw form?,- Describe it in terms of Summarizing Data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.","How can one use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties?",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.",What is important when using statistical inference to draw conclusions on an unknown population parameter?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.",What is the term for the sum total of all observations divided by the number of observations in the data?,Mean
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.",What does the arithmetic average mean mean mean?,Sum total of all observations divided by the number of observations in the data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.",What is the mid-value of a dataset?,The Median
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.",How does one first sort the data in ascending order to compute a median value?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.","For the (sorted) set 1, 3, 5, 7, 9, the median will be what?",5
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.",The median is defined to be how many middle values?,two
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.",What is the value that occurs most frequently in the dataset?,Mode
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.",What is a variable that has just one mode?,A uni-modal variable
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.",How many modes does a bimodal variable have if your data has more than two modes?,two
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.",What does the percentile tell you the position of a value in the dataset?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.",What is the median of the 50th percentile?,100cm
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","The are several measures to describe the spread, variability, or dispersion of a dataset","What are several measures to describe the spread, variability, or dispersion of a dataset?",The
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.,What can be calculated by subtracting the minimum value in a dataset from the maximum value?,Range of values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.,What does the range of a set of values only consider?,Two values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is",What is the average distance between each value and the mean of a dataset?,Mean Absolute Deviation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",\\[\\sum_i\\frac{\\mid x_i - \\mu\\mid}{N}\\],,nan
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.,Where is (N) the number of values?,in the data set
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.,What is the value in the data set?,i_th
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is",What measure of dispersion can tell you how values are spread out in a dataset and determine if the mean is a useful indicator of the values within the data?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is",What is the mean absolute deviation typically denoted by when working with time series forecasting methods?,(sigma2)
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",\\[\\sigma^2 = \\sum_i\\frac{(x_i - \\mu)^2}{N}\\],[sigma2 = sum_ifrac(x_i - mu2N]?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.",What is the name of the square root of the variance?,Standard deviation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.",What type of measure is the most commonly used?,Standard deviation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.",A low standard deviation tells you that the values are close to what?,Mean
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.",A high standard deviation means that there is a spread?,A high standard deviation means there is a spread.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.",What is used to cmeasure the confidence in statistical conclusionsd?,Standard deviation
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.",What does Interquartile Range not consider when looking at the spread of values in a dataset?,All observations
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.",What does IQR describe when arranged in ascending order?,50% of values
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.",The difference between the values in Quartile 3 and what is the difference between?,IQR
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.",How can you identify a value that is an outlier?,IQR
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:",How can we assess the distribution of our data?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:",What is a useful tool when making statistical inferences?,Normally distributed data
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.",What measures the degree to which the distribution of data lacks symmetry?,Skewness
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.",What is a dataset with 0 skewness considered normally distributed?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.",How does a skewerness of 0 vary?,It depends on the distribution of data.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.","If your data is between -1 and -0.5 or 0.5 and 1, what is your data?",Moderately skewed
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC),What is the Symmetrical Dataset with Skewness = 0?,Figure 1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.,Kurtosis looks at the outliers within the distribution?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.,What will describe the distribution of data by showing whether the tails of the distribution are more or less extreme?,This measure of shape
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.",What describes the linear relationship between two variables in your sample or population data?,Covariance
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.","Covariance can be negative, meaning your variables have a negative linear relationship, or zero (0), meaning the variables have no linear relationship or what?",Positive
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \\right |\\leq 1\\)",What does Correlation mean?,"A measure of how closely the relationship between two variables, x, and y"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \\right |\\leq 1\\)",What is a different measure than covariance?,Correlation coefficient
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \\right |\\leq 1\\)",When is the correlation value between -1 and 1?,When x and y are in the correlation coefficient.
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).,What does the correlation equal for all (i)?,1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).,What is the correlation for x_i=y_i?,1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.",What does the scatterplot of x and y mean?,straight line
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.",What is the correlation of 1 or -1?,"If the scatterplot is a straight line, then the correlation is 1 or 1."
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.","When the line slopes upward, there is a positive relationship between x & y and the correlation?",1
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.",A high correlation coefficient does not necessarily mean that the line has a steep slope; it means that the points in the scatterplot fall very close to what?,a straight line
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",Figure 2. Scatterplots for Four Hypothetical Datasets.,Figure 2. Scatterplots for Four Hypothetical Datasets?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.",What is the correlation coefficient?,A measure of linear association
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!","Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.",What is a measure of linear association?,The correlation coefficient
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.,What is one important note on correlation?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!",One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.,What does not mean there is a causal relationship between variables?,Having an association
Model Evaluation,Metrics and Interpretation,Module 19 Summary,,This is a new page with empty contents.,What is a new page with empty contents?,This
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"At the beginning of a  data science process on any task, it is important not just to understand the dataset and the tools available but also the hardware resources available to you for the task at hand. Most data science tasks rely on using statistical or numerical techniques to process the data and/or optimize some discrete/continuous optimization task. In doing so, youll need to assess if a task is feasible with the hardware you have and what hardware to use to make the most of the computational budget youre given to solve the analytic objective.",What is important to understand at the beginning of a data science process on any task?,The dataset and the tools available but also the hardware resources available to you for the task at hand
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"At the beginning of a  data science process on any task, it is important not just to understand the dataset and the tools available but also the hardware resources available to you for the task at hand. Most data science tasks rely on using statistical or numerical techniques to process the data and/or optimize some discrete/continuous optimization task. In doing so, youll need to assess if a task is feasible with the hardware you have and what hardware to use to make the most of the computational budget youre given to solve the analytic objective.",What do most data science tasks rely on to process the data and/or optimize?,Statistical or numerical techniques
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:",What is the goal of this chapter?,To give a brief tour of hardware resources one should look at when deciding which hardware to
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:",What is one of the two things that we want to give a brief tour of when choosing what?,Hardware resources
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"First, you will want to spend time to make sure that the tools you use actively use the computational resources you are given. Taking the time to understand if and when you can scale down any cloud computing resources you are using can save you time and money in performing any computational task, which then can be a boon as you assess different statistical techniques and their performance in your analytic objective.",What is the first thing that you want to do to make sure that the tools you use actively use the computational resources you are given?,Spend time
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"First, you will want to spend time to make sure that the tools you use actively use the computational resources you are given. Taking the time to understand if and when you can scale down any cloud computing resources you are using can save you time and money in performing any computational task, which then can be a boon as you assess different statistical techniques and their performance in your analytic objective.",What can save you time and money in performing a computational task?,Scaling down any cloud computing resources you are using.
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"Second, you will want to assess how you are using the various libraries common to data science. Compared to more traditional single-threaded and multi-threaded applications, data science libraries tend to require users to focus on operations, unlike those more traditionally used, such as conditional indexing and matrix multiplication. In this chapter, youll find motivation for learning these libraries to the depth necessary to perform these tasks, as they form the basis for highly performant and highly readable data science code.",What type of libraries do users need to focus on?,Data science libraries
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,,"Second, you will want to assess how you are using the various libraries common to data science. Compared to more traditional single-threaded and multi-threaded applications, data science libraries tend to require users to focus on operations, unlike those more traditionally used, such as conditional indexing and matrix multiplication. In this chapter, youll find motivation for learning these libraries to the depth necessary to perform these tasks, as they form the basis for highly performant and highly readable data science code.",What is the main purpose of data science libraries?,To require users to focus on operations
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"Once a set of business goals has been identified, one can proceed to formulate analytic objectives that state how the application of analytical methods to data can facilitate reaching the business goal. An analytic objective can typically be phrased along with the following template:",What is an analytic objective that can be formulated once a set of business goals has been identified?,How the application of analytical methods to data can facilitate reaching the business goal
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"Once a set of business goals has been identified, one can proceed to formulate analytic objectives that state how the application of analytical methods to data can facilitate reaching the business goal. An analytic objective can typically be phrased along with the following template:",What can be used to describe a business goal?,Analytic methods
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,As an incremental step towards business objective O,What is an incremental step towards business objective O?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,We work towards solving problem P,What is the goal of solving problem P?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,by focusing on specific tasks T,What is the focus of a specific task?,Teasing
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,and applying analytic methods M in conjunction with data D,Using analytic methods M in conjunction with data D?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,to create valuable functionality F and/or produce insight I,What does F and/or produce insight I create?,valuable functionality
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"This formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to treatment versus control condition). In fact, in academic settings, the effectiveness of data science methods will regularly be scrutinized using statistical tests, and it is good practice to principally strive for similar methodological rigor in industry applications. While data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ, this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts. It is the connecting element between the problem, the technique to be applied, the associated requirements and evaluation, as well as the business objective. Also, one should keep in mind that projects often comprise multiple analytic objectives that form a larger plan.",What can be considered a research hypothesis?,An analytic objective
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"This formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to treatment versus control condition). In fact, in academic settings, the effectiveness of data science methods will regularly be scrutinized using statistical tests, and it is good practice to principally strive for similar methodological rigor in industry applications. While data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ, this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts. It is the connecting element between the problem, the technique to be applied, the associated requirements and evaluation, as well as the business objective. Also, one should keep in mind that projects often comprise multiple analytic objectives that form a larger plan.","In academic settings, how will data science methods be scrutinized?",With statistical tests
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"This formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to treatment versus control condition). In fact, in academic settings, the effectiveness of data science methods will regularly be scrutinized using statistical tests, and it is good practice to principally strive for similar methodological rigor in industry applications. While data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ, this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts. It is the connecting element between the problem, the technique to be applied, the associated requirements and evaluation, as well as the business objective. Also, one should keep in mind that projects often comprise multiple analytic objectives that form a larger plan.",What is a good point of departure for framing data science work?,Analytic objectives
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,,"In order for you to become proficient in working with this definition, we will now move to examine its components.","In order for you to become proficient in working with this definition, what will we do to examine the components of this definition?",We will now move to examine its components.
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,Reading: Avoiding Bias from Feature Selection.,Reading: Avoiding Bias from Feature Selection?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,"Feature engineering can be performed before the model building process, i.e., during the data wrangling and exploratory data analysis phase, or it can be performed during model building. Later in this course, we will discuss cross-validation, but we must note here that feature engineering can be done during the cross-validation process. Cross-Validation is a model validation technique used to assess how a model will generalize to a new data set. At this time, feature engineering is done during the cross-validation loop.",What can feature engineering be performed before the model building process?,
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,"Feature engineering can be performed before the model building process, i.e., during the data wrangling and exploratory data analysis phase, or it can be performed during model building. Later in this course, we will discuss cross-validation, but we must note here that feature engineering can be done during the cross-validation process. Cross-Validation is a model validation technique used to assess how a model will generalize to a new data set. At this time, feature engineering is done during the cross-validation loop.",What is cross-validation used to assess how a model will generalize to a new data set?,model validation technique
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,"Feature engineering at any stage can introduce bias to the data. While you manipulate the data, you can unintentionally create a relationship between features that do not otherwise exist. The features that are selected or created during the feature engineering process can shape the insights that are gotten from the model.",What can introduce bias to the data?,feature engineering
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,Feature Engineering and Bias,"Feature engineering at any stage can introduce bias to the data. While you manipulate the data, you can unintentionally create a relationship between features that do not otherwise exist. The features that are selected or created during the feature engineering process can shape the insights that are gotten from the model.",What can create a relationship between features that do not otherwise exist?,feature engineering
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points cfartherd apart. Lets consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:",What is a dimensionality reduction technique that provides a low-dimensional representation of data through a linear transformation?,Principal Component Analysis (PCA)
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points cfartherd apart. Lets consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:",What does PCA require us to preserve all pairs of distances between different data points?,Principal Component Analysis
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points cfartherd apart. Lets consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:",How can we apply PCA to the dataset to get the following visualization?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,Source: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/,What does ryanwingate.com have?,a comprehensive guide to machine learning
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,Source: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/,What is the name of the source of the site?,RYANWILLET
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"As we can see, while some groups are visible, most of the digits are clumped together, making them really hard to distinguish. Given that MNIST has 10 distinct classes, what we would like is an algorithm that lets us differentiate between local distances and global distances a little better to make classes more visible.",How many classes does MNIST have?,10
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"As we can see, while some groups are visible, most of the digits are clumped together, making them really hard to distinguish. Given that MNIST has 10 distinct classes, what we would like is an algorithm that lets us differentiate between local distances and global distances a little better to make classes more visible.",What is the name of the algorithm that lets us differentiate between local distances and global distances?,MNIST
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Enter t-distributed stochastic neighbor embedding or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.",What is the term for t-SNE?,Distributed stochastic neighbor embedding
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Enter t-distributed stochastic neighbor embedding or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.",What is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions?,t-SNE
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Enter t-distributed stochastic neighbor embedding or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.",In what year was it introduced as a stochastic neighbor embedding method?,2002
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"How t-SNE works is that it tries to model the distribution of points in the higher-dimensional space as a set of Gaussian distributions, where we define the probability of some data point i picking another point j to be neighbors through the following equation:",What does t-SNE try to model?,Distribution of points in the higher-dimensional space as a set of Gaussian distributions
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"How t-SNE works is that it tries to model the distribution of points in the higher-dimensional space as a set of Gaussian distributions, where we define the probability of some data point i picking another point j to be neighbors through the following equation:",What is the probability of some data point i picking another point j to be neighbors?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,\n\\[ p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_j\\right\\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_k\\right\\|^2 / 2 \\sigma_i^2\\right)} \\],n[ p_j mid i=fracexp left(-left|mathbfx_i-mahbbffX_jright||2 / 2 sigma_i2right)sum_k?,neq
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"(If youre familiar with deep learning, you might notice the similarities between this method and the softmax function. Its also important to note that the distance function here is more of a suggestion, and you can use any custom distance function you want with most t-SNE implementations).",What does softmax function do?,Empty distance function
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"(If youre familiar with deep learning, you might notice the similarities between this method and the softmax function. Its also important to note that the distance function here is more of a suggestion, and you can use any custom distance function you want with most t-SNE implementations).",What is the distance function more of a suggestion?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"After creating this high-dimensional model, we then optimize a low-dimensional embedding, with the lower-dimensional embedding using a t-distribution instead with the following similarity function:",What do we do after creating a high-dimensional model?,Optimize a low-dimensional embedding
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"After creating this high-dimensional model, we then optimize a low-dimensional embedding, with the lower-dimensional embedding using a t-distribution instead with the following similarity function:",What does the lower-dimensional embedding use instead of t-distribution?,similarity function
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,\\[ q_{i j}=\\frac{\\left(1+\\left\\|\\mathbf{y}_i-\\mathbf{y}_j\\right\\|^2\\right)^{-1}}{\\sum_k \\sum_{l \\neq k}\\left(1+\\left\\|\\mathbf{y}_k-\\mathbf{y}_l\\right\\|^2\\right)^{-1}} \\],,nan
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a cgood enoughd representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.",How can we get a good lower-dimensional embedding for our high-dimensional data?,minimizing the distance between these distributions
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a cgood enoughd representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.",What does KL-divergence mean?,It is relatively easy to optimize and relatively easy to intuit.
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a cgood enoughd representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.",How are two distributions different if they associate largely different probabilities?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"If we go to our MNIST example, using t-SNE gets us the following results, which make the clusters a lot easier to see:",What is the MNIST example?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"If we go to our MNIST example, using t-SNE gets us the following results, which make the clusters a lot easier to see:",What does t-SNE mean?,Clusters
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Overall, t-SNE can be a far better visualization tool than PCA. However, it is very important to know the limitations of any new tool you consider for data science, especially data visualization. Given that visualizations can be misleading, it is important to treat a potentially better tool with skepticism.",What can be a better visualization tool than PCA?,t-SNE
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Overall, t-SNE can be a far better visualization tool than PCA. However, it is very important to know the limitations of any new tool you consider for data science, especially data visualization. Given that visualizations can be misleading, it is important to treat a potentially better tool with skepticism.",What is important to know about the limitations of any new tool you consider for data science?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the perplexity, and this roughly is the number of neighbors you want to consider as cclosed to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:",What is the problem with t-SNE?,The optimization process.
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the perplexity, and this roughly is the number of neighbors you want to consider as cclosed to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:",What are the parameters to the algorithm that need to be carefully considered in order to get good results out of the algorithm?,Perplexity and Perplexity
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the perplexity, and this roughly is the number of neighbors you want to consider as cclosed to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:",The most important parameter is known as what?,Perplexity
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the perplexity, and this roughly is the number of neighbors you want to consider as cclosed to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:",Larger datasets require what type of perplexity?,Larger
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Secondly, note that it is important to not really think too much about the distances between clusters and the relative size of any clusters reported. Given that t-SNE is stochastic, the distances between points tend to mean very little, if anything, and instead, it is important to look at the data in its entirety.",What is important to not think too much about?,The distances between clusters and the relative size of any clusters reported.
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Secondly, note that it is important to not really think too much about the distances between clusters and the relative size of any clusters reported. Given that t-SNE is stochastic, the distances between points tend to mean very little, if anything, and instead, it is important to look at the data in its entirety.",What is t-SNE stochastic?,
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Thirdly, it is important to note that t-SNE is sensitive to data scaling. Applying a standard scalar or another scaling system is important to get interpretable results, as otherwise, certain features will be scaled in a very different manner than any other.",What is important to note that t-SNE is sensitive to data scaling?,Thirdly
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Thirdly, it is important to note that t-SNE is sensitive to data scaling. Applying a standard scalar or another scaling system is important to get interpretable results, as otherwise, certain features will be scaled in a very different manner than any other.",What does applying a standard scalar or another scaling system need to get?,interpretable results
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like UMAP instead.",What is t-SNE?,Random
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like UMAP instead.",What is the loss function?,Non-convex
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like UMAP instead.",How can different initializations lead to different visualizations?,loss function is non-convex
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Hinton, G. E., & Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf","What is the name of the book published by S. Becker, S. Thrun, & K. Obermayer?",Advances in Neural Information Processing Systems
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Hinton, G. E., & Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf",What is a reference to the Advances in Neural Information Processing Systems?,The Stochastic Neighbor Embedding
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Maaten, L.van der and Hinton, G. (2008) Visualizing data using T-Sne, Journal of Machine Learning Research. Available at: https://jmlr.org/papers/v9/vandermaaten08a.html","Maaten, L.van der and Hinton, G. (2008) Visualizing data using what?",T-Sne
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"Maaten, L.van der and Hinton, G. (2008) Visualizing data using T-Sne, Journal of Machine Learning Research. Available at: https://jmlr.org/papers/v9/vandermaaten08a.html",http://jmlr.org/papers/v9/vandermaaten08a.html?,http://jmlr.org/papers/v9/vandermaaten
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15(93), 3221. Retrieved from http://jmlr.org/papers/v15/vandermaaten14a.html","What did van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms?",
Exploratory Data Analysis,Feature Engineering,t-SNE,References:,"van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15(93), 3221. Retrieved from http://jmlr.org/papers/v15/vandermaaten14a.html",What was published in the Journal of Machine Learning Research?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.",What can be used to adopt an efficient software development life cycle for your data science project?,Many different frameworks
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.",What has been tested and established pros and cons so that teams don't have to spend too much time choosing among them?,Different frameworks
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.",How is Agile Scrum popular in major technology companies?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Lets dive into how Agile is usually implemented in the industry and how to apply it to a data science project.,How is Agile usually implemented in the industry?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Lets dive into how Agile is usually implemented in the industry and how to apply it to a data science project.,What is the purpose of Agile?,To help a data science project
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Lets first understand the roles in the Agile Scrum framework:,What is the first step to understand the role of Agile Scrum?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.",What does the Scrum Master ensure that each sprint stays on track?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.",What is the most senior person in a project team?,Scrum Master
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.",How does a ScrumMaster help to remove or resolve issues?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.",What is the role of the product owner?,"To define the goals of each sprint, manage and prioritize the team backlog, and be the"
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.",What is a product owner's role?,"To define the goals of each sprint, manage and prioritize the team backlog."
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.",Who is the voice of the customer?,Product owner
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Team members. The people on this team are the ones who execute the work in each sprint. These teams, usually of three to seven people, can be composed of different specialties and strengths, or they can be teams of people with the same job roles.",What are the people on this team who execute the work in each sprint?,Team members
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Team members. The people on this team are the ones who execute the work in each sprint. These teams, usually of three to seven people, can be composed of different specialties and strengths, or they can be teams of people with the same job roles.",How many people can teams be composed of different specialties and strengths?,3 to 7
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Team members. The people on this team are the ones who execute the work in each sprint. These teams, usually of three to seven people, can be composed of different specialties and strengths, or they can be teams of people with the same job roles.",What are teams of people with different job roles?,- teams of people with the same job roles -
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Stakeholders. This is an informational role only. The stakeholders should be kept up-to-date on the product and sprint goals, have the opportunity to review and approve work during a sprint and provide feedback during the sprint retrospective.",What is an informational role for Stakeholders?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Stakeholders. This is an informational role only. The stakeholders should be kept up-to-date on the product and sprint goals, have the opportunity to review and approve work during a sprint and provide feedback during the sprint retrospective.",What should stakeholders be kept up-to-date on the product and sprint goals?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"User stories: A user story is simply a high-level definition of a work request. It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request. This short, simple description is written from the users perspective and focuses on outlining what the stakeholder wants (their goals) and why.",What is a high-level definition of a work request?,A user story
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"User stories: A user story is simply a high-level definition of a work request. It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request. This short, simple description is written from the users perspective and focuses on outlining what the stakeholder wants (their goals) and why.",What does a user story contain so the team can produce a reasonable estimate of the effort required to accomplish?,information
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"For example, if you are building an application that guides a user with directions, here is an example of a user story:",What is an example of an application that guides a user with directions?,A user story
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to navigate my way using the directions so that I can reach my destination.",What do I want to be able to do as a user?,Navigate my way
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to navigate my way using the directions so that I can reach my destination.",How do I navigate my way?,With the directions
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to change the destination so that I can reach a new destination.",What do I want to be able to do as a user?,Change the destination so that I can reach a new destination.
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"As a user, I want to be able to change the destination so that I can reach a new destination.",What is the purpose of changing the destination?,To reach a new destination.
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Sprints:  Sprints are a short span of work, usually taking between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to repeat these sprints until your product is feature ready continuously. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service. This feature helps you build products that generate value for the stakeholder.",What is a short span of work?,Sprints
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Sprints:  Sprints are a short span of work, usually taking between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to repeat these sprints until your product is feature ready continuously. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service. This feature helps you build products that generate value for the stakeholder.",How long does a sprint take to complete?,One to Three weeks
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Sprints:  Sprints are a short span of work, usually taking between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to repeat these sprints until your product is feature ready continuously. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service. This feature helps you build products that generate value for the stakeholder.",What is the goal of a Sprint?,To get your product feature ready continuously.
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Stand-up meetings: Daily stand-up meetings (typically under 10 minutes), also known as cdaily Scrum meetings,d are a great way to ensure everyone is on track and informed. These daily interactions are known as cstand upd because the participants are required to stay standing, helping to keep the meetings short and to the point. Regular sync-up helps in mitigating any new challenges early on.",What is a great way to ensure everyone is on track and informed?,Daily stand-up meetings
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Stand-up meetings: Daily stand-up meetings (typically under 10 minutes), also known as cdaily Scrum meetings,d are a great way to ensure everyone is on track and informed. These daily interactions are known as cstand upd because the participants are required to stay standing, helping to keep the meetings short and to the point. Regular sync-up helps in mitigating any new challenges early on.",What are cdaily Scrum meetings also known as?,Daily Scrum meetings
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Agile board: An Agile board helps your team track the progress of your project. This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software (like JIRA). If you love sticky notes, you can, for instance, consider MURAL to build your board virtually, and all pending activities can go onto this board.",What is an Agile board that helps your team track the progress of your project?,
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Agile board: An Agile board helps your team track the progress of your project. This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software (like JIRA). If you love sticky notes, you can, for instance, consider MURAL to build your board virtually, and all pending activities can go onto this board.",What is a simple Kanban board?,A whiteboard with sticky notes
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Agile board: An Agile board helps your team track the progress of your project. This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software (like JIRA). If you love sticky notes, you can, for instance, consider MURAL to build your board virtually, and all pending activities can go onto this board.",How can MURAL be built?,Virtually
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Backlog: As project requests are added through your intake system, they become outstanding stories in the backlog. During Agile planning sessions, your team will estimate story points for each task. During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration. Managing your backlog is a vital role for project managers in an Agile environment. In a project team, this will be the teams collective effort.",What is a vital role for project managers in an Agile environment?,Managing your backlog
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Backlog: As project requests are added through your intake system, they become outstanding stories in the backlog. During Agile planning sessions, your team will estimate story points for each task. During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration. Managing your backlog is a vital role for project managers in an Agile environment. In a project team, this will be the teams collective effort.",What is an example of a project team's collective effort?,Stories in backlog
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,"Backlog: As project requests are added through your intake system, they become outstanding stories in the backlog. During Agile planning sessions, your team will estimate story points for each task. During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration. Managing your backlog is a vital role for project managers in an Agile environment. In a project team, this will be the teams collective effort.",How do you estimate story points for each task?,During Agile planning sessions
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Sprint retrospective:  This is a meeting where the team comes together to recognize what has worked and what has not worked. The team makes efforts to make sure the concerns are dealt with to ensure the smooth completion of the project.,What is a meeting where the team gathers to recognize what?,What has worked and what has not worked.
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Sprint retrospective:  This is a meeting where the team comes together to recognize what has worked and what has not worked. The team makes efforts to make sure the concerns are dealt with to ensure the smooth completion of the project.,What is the goal of the team to ensure that the concerns are dealt with?,Smooth completion of the project
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Demonstration: This is when the team demonstrates a working product to the stakeholders in order to show them the value generated from the project and to keep them happy.,What does the team demonstrate to the stakeholders?,A working product
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,,Demonstration: This is when the team demonstrates a working product to the stakeholders in order to show them the value generated from the project and to keep them happy.,What is the goal of the team to show the value of the project?,Demonstration
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Suppose a utility company wants to introduce personalized service management to its customers, including a secure electronic payment facility and the ability to view usage statistics. This business objective will lead to the development of a mobile application for customer service management. Business requirements for the above scenario will include describing the context, scope, and background of the business need, including the reasons for the proposed change. Business requirements are collected and decomposed to define other types of requirements.",What does a utility company want to introduce to its customers?,personalized service management
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Suppose a utility company wants to introduce personalized service management to its customers, including a secure electronic payment facility and the ability to view usage statistics. This business objective will lead to the development of a mobile application for customer service management. Business requirements for the above scenario will include describing the context, scope, and background of the business need, including the reasons for the proposed change. Business requirements are collected and decomposed to define other types of requirements.",What is the purpose of a mobile application for customer service management?,To view usage statistics.
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Suppose a utility company wants to introduce personalized service management to its customers, including a secure electronic payment facility and the ability to view usage statistics. This business objective will lead to the development of a mobile application for customer service management. Business requirements for the above scenario will include describing the context, scope, and background of the business need, including the reasons for the proposed change. Business requirements are collected and decomposed to define other types of requirements.",How are business requirements collected?,Decomposed
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Once business requirements are defined, stakeholders and systems that support the business requirement(s) are identified. System requirements are a detailed description of the system and its operational and development constraints. They include the system software that will support the solution, processing and memory requirements, and other application software considerations. User requirements describe functions or tasks that a user must perform within the system. These tasks will support the business objectives that are defined prior to the requirements gathering process.",What is a detailed description of the system and its operational and development constraints?,System requirements
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Once business requirements are defined, stakeholders and systems that support the business requirement(s) are identified. System requirements are a detailed description of the system and its operational and development constraints. They include the system software that will support the solution, processing and memory requirements, and other application software considerations. User requirements describe functions or tasks that a user must perform within the system. These tasks will support the business objectives that are defined prior to the requirements gathering process.",What does a user need to perform within the system?,Functions or tasks
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements",Use cases and user stories represent user requirements and provide a big picture of what the user will be able to do within a system. An example of a use case is cMake Paymentd on a mobile application. We will describe user and system requirements later in this unit.,What is an example of a use case?,cMake Payments on a mobile application
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements",Use cases and user stories represent user requirements and provide a big picture of what the user will be able to do within a system. An example of a use case is cMake Paymentd on a mobile application. We will describe user and system requirements later in this unit.,What does cMake Paymentd provide?,A big picture of what the user will be able to do within a system.
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Solution requirements are grounded in software engineering. In this course, we will be tailoring solution requirements to the data science process. The solution requirements for a data science project are classified into functional and non-functional requirements, as well as requirements that consider parts of an analytic solution that are different from the traditional IT systems. The typical analytic solution will consider data and models (e.g., predictive models), and a business intelligence solution will include requirements for reports and dashboards.",What are the core principles of software engineering?,Solution requirements
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Business Requirements,System and User Requirements,Solution Requirements","Solution requirements are grounded in software engineering. In this course, we will be tailoring solution requirements to the data science process. The solution requirements for a data science project are classified into functional and non-functional requirements, as well as requirements that consider parts of an analytic solution that are different from the traditional IT systems. The typical analytic solution will consider data and models (e.g., predictive models), and a business intelligence solution will include requirements for reports and dashboards.",What is a typical analytic solution that will include requirements for reports and dashboards?,Business intelligence solution
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Quiz 5,,,What does nan do?,He is a nurse
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Quiz 5,,,What is the name of the nnan?,The nan name is Peter Durning.
Data Science Project Planning,Developing a Vision,The Vision Document,,The important components of a vision document are described as follows:,What are the important components of a vision document described as?,Voici
Data Science Project Planning,Developing a Vision,The Vision Document,,"Problem Description. First, you need to determine the real-world problem that you are trying to address. You should also look into the literature to identify existing solutions and their limitations. Your goal is to propose an improvement to these solutions in some ways so as to yield tangible social or business impacts.",What do you need to determine first?,The real-world problem that you are trying to address.
Data Science Project Planning,Developing a Vision,The Vision Document,,"Problem Description. First, you need to determine the real-world problem that you are trying to address. You should also look into the literature to identify existing solutions and their limitations. Your goal is to propose an improvement to these solutions in some ways so as to yield tangible social or business impacts.",What is the goal of a problem description?,To propose an improvement to these solutions in some ways so as to yield tangible social or business impacts
Data Science Project Planning,Developing a Vision,The Vision Document,,"Proposed Solution. Next, come up with an overarching framework for your solution. An example solution formulation could be cA framework that supports a declarative description of a configuration space and automatically evaluates all the options, finding the best given some measure and labeled dataset.d Compare your solution with the existing solutions identified previously. What makes your solution better?",What is a framework that supports a declarative description of a configuration space and automatically evaluates all the options?,cA framework
Data Science Project Planning,Developing a Vision,The Vision Document,,"Proposed Solution. Next, come up with an overarching framework for your solution. An example solution formulation could be cA framework that supports a declarative description of a configuration space and automatically evaluates all the options, finding the best given some measure and labeled dataset.d Compare your solution with the existing solutions identified previously. What makes your solution better?",What does cA framework support?,A declarative description of a configuration space
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scientific Hypotheses. Now you need to formalize the scientific hypotheses underlying your proposed solution. Keep in mind that hypotheses need to be testable assumptions. For example, if you make the assumption that the majority of your users are teenagers, but your platform doesnt collect users ages, then your assumption is not testable. Then, develop a plan to validate your hypothesis as you develop your solution. With respect to a technical data science solution, you can formulate your hypothesis along with one of the following themes:",What do you need to formalize?,Scientific Hypotheses
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scientific Hypotheses. Now you need to formalize the scientific hypotheses underlying your proposed solution. Keep in mind that hypotheses need to be testable assumptions. For example, if you make the assumption that the majority of your users are teenagers, but your platform doesnt collect users ages, then your assumption is not testable. Then, develop a plan to validate your hypothesis as you develop your solution. With respect to a technical data science solution, you can formulate your hypothesis along with one of the following themes:",What does the assumption that the majority of your users are teenagers?,Because your platform doesn't collect users ages
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scientific Hypotheses. Now you need to formalize the scientific hypotheses underlying your proposed solution. Keep in mind that hypotheses need to be testable assumptions. For example, if you make the assumption that the majority of your users are teenagers, but your platform doesnt collect users ages, then your assumption is not testable. Then, develop a plan to validate your hypothesis as you develop your solution. With respect to a technical data science solution, you can formulate your hypothesis along with one of the following themes:",How can you formulate a plan to validate your hypothesis?,As you develop your solution.
Data Science Project Planning,Developing a Vision,The Vision Document,,cConstructived: It is possible to build such a framework.,cConstructived: It is possible to build a framework?,
Data Science Project Planning,Developing a Vision,The Vision Document,,cFormatived: The proposed solution is cfast /good enoughd,What is the proposed solution to cFormatived?,cfast/good enoughd
Data Science Project Planning,Developing a Vision,The Vision Document,,cFormatived: The proposed solution is cfast /good enoughd,What is cfast / good enoughd?,The proposed solution
Data Science Project Planning,Developing a Vision,The Vision Document,,cEmpiricald: The proposed solution will significantly outperform the state-of-the-art baseline measured by a certain metric.,What does cEmpiricald mean?,cEmpiricald means Comprehensive solution
Data Science Project Planning,Developing a Vision,The Vision Document,,cEmpiricald: The proposed solution will significantly outperform the state-of-the-art baseline measured by a certain metric.,What will the proposed solution surpass?,Benchmark
Data Science Project Planning,Developing a Vision,The Vision Document,,"Major Features. For the next step, you should describe in more specific terms how the system features you plan to implement form the proposed solution to the problem (e.g., a list of major features or components of your solution). This is really important from a ""traceability"" perspective as it's easy to miss major features as you dive deeper into the project. Here are the important requirements for the proposed features:",What are the major features of the system you plan to implement?,A list of major features or components of your solution
Data Science Project Planning,Developing a Vision,The Vision Document,,"Major Features. For the next step, you should describe in more specific terms how the system features you plan to implement form the proposed solution to the problem (e.g., a list of major features or components of your solution). This is really important from a ""traceability"" perspective as it's easy to miss major features as you dive deeper into the project. Here are the important requirements for the proposed features:","What is important from a ""traceability"" perspective as it's easy to miss as you dive deeper into the project?",Major Features
Data Science Project Planning,Developing a Vision,The Vision Document,,"Features should not be technical constraints. For example, cthe system will use an Amazon AWS load balancer to manage trafficd is a bad example, while cthe system will use a cloud-based load balancer to manage trafficd is a good example.",What is a bad example of a cloud-based load balancer?,cthe system will use an Amazon AWS load balancer to manage trafficd
Data Science Project Planning,Developing a Vision,The Vision Document,,Features should be distinct in the sense that no two features should overlap.,What should be distinct in the sense that no two features should overlap?,Features
Data Science Project Planning,Developing a Vision,The Vision Document,,"Features should be traceable, with a unique name or identifier, so that they can be traced across different documents and project stages.","What should be traceable, with a unique name or identifier, so that they can be traced across different documents and projects?",Features
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scope. At this point, you will want to outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline. If there is something your project will specifically not do, you should note it here. For example, be clear about how much (or how little) data will be covered, whether your solution will meet certain run-time requirements in terms of responsiveness, whether your solution will be deployed as a web service or application or exist only as a code repository/Jupyter notebook, etc.",What will you want to outline at this point?,The boundaries of your project.
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scope. At this point, you will want to outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline. If there is something your project will specifically not do, you should note it here. For example, be clear about how much (or how little) data will be covered, whether your solution will meet certain run-time requirements in terms of responsiveness, whether your solution will be deployed as a web service or application or exist only as a code repository/Jupyter notebook, etc.",What will be delivered at the end of the project timeline?,What
Data Science Project Planning,Developing a Vision,The Vision Document,,"Scope. At this point, you will want to outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline. If there is something your project will specifically not do, you should note it here. For example, be clear about how much (or how little) data will be covered, whether your solution will meet certain run-time requirements in terms of responsiveness, whether your solution will be deployed as a web service or application or exist only as a code repository/Jupyter notebook, etc.",How much data will be covered?,How much
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.",What describes the probability of an event based on prior knowledge of conditions related to that event?,The Bayes Theorem
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.",What does the Bayes Theorem support?,Accurately assessing the risk of macular degeneration based on a certain
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,Bayes Rule (Source: https://www.psychologyinaction.org),What is the source of Bayes Rule?,https://www.psychologyinaction.org
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,Additional Reading: Bayes Theorem,What is the title of Bayes Theorem?,Additional Reading
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,Additional Reading: Overview of Bayesian Statistics,What is an overview of Bayesian Statistics?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.",What is Bayesian Inference applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.",What is a result of a likelihood function and a prior probability?,The posterior probability is a result of a likelihood function and a prior probability.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.",What is naive Bayes based on?,Bayes theorem
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.",What is the name of the Reverend?,Thomas Bayes
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.",How can NB be used for high-dimensional data?,Model complexity is not important
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.",What algorithm is not considered the go-to algorithm for estimating the probability of an observation's class?,NB
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.",What is NB useful for ranking and classification tasks?,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.",How does NB compute a zero probability to that record?,It is computed using a regression model.
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,Naive Bayes (NB) Method,"There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.",What are some Bayesian Methods that can be used in Data Science?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.",What does an objective function quantify?,How well or badly a model is performing
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.",What are objective functions in deep learning defined so that a lower value is better?,Loss functions
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.",Why are loss functions often called?,Because of their defined nature.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Following are some of the commonly used loss functions based on the problem at hand:,What are some of the commonly used loss functions based on?,The problem at hand
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.",What is a problem where the model predicts a real value?,Regression Problem
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.",The last layer in the model consists of a single node with a linear activation function?,A regression problem where the model predicts a real value
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.",What is the Mean Squared Error?,A loss function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.",What is an example of a binary classification problem?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.",What is the final layer in the model?,A single neuron with a sigmoid activation
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.",How can a Binary Cross Entropy function be used?,As a loss function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.",What is a Multi-Class Classification Problem?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.",What is the final layer in the model?,Neurons
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.",The Cross-Entropy function is used as what?,A loss function
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.","When a loss function has been defined, an optimization algorithm is used to update the network parameters based on what?",the obtained loss value
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.",What is usually the sum of the loss values obtained for each example in the training dataset?,The loss
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.",An optimization algorithm iteratively calculates the next point using the gradient at the current position and then scales it by a learning rate and subtracts the obtained value from what position?,current position
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Following are some common optimization algorithms along with their advantages and disadvantages:,What are some common optimization algorithms along with their advantages and disadvantages?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.",What algorithm is used heavily in linear regression and classification algorithms?,Gradient Descent
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.",What is a first-order optimization algorithm that is dependent on the first order derivative of a loss function?,Gradient descent
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.",How is the loss transferred from one layer to another?,Backpropagation
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ w:=w-\\eta \\nabla Q_{i}(w) \\],[ w:=w-eta nabla Q_i(w)?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Easy to compute,What is easy to compute?,simple to compute.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Easy to implement,What is easy to implement?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting stuck in a local minima,What is the name of a local minima?,escapable minima
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting stuck in a local minima,What type of minima is there?,Local
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Convergence is slow as updates are calculated after calculating the gradient for the entire dataset,What is slow as updates are calculated after calculating the gradient for the entire dataset?,Convergence
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computation for entire dataset requires a large memory,Computation for entire dataset requires a large memory?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.",What is a variant of Stochastic Gradient Descent?,SGD
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.",What is the SGD?,Stochastic Gradient Descent
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.",How are model parameters updated?,More frequently
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ w:=w-\\eta \\nabla Q_{i}(w) \\],[ w:=w-eta nabla Q_i(w)?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Converges in lesser time because of frequent updates,What happens in less time due to frequent updates?,Converges
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Lesser memory requirements for calculating updates,What are less memory requirements for calculating updates?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Lesser memory requirements for calculating updates,What is less memory requirement for calculation?,Updates
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Less likely than Gradient Descent to get stuck in a local minima,What type of Descent is less likely to get stuck in a local minima?,Gradient
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",High variance in parameter updates due to high frequency in updates,What is a high variance in parameter updates due to high frequency in updates?,Parameter updates
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What needs to be correctly chosen and adjusted for effective training?,Learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What is a requirement for a correct learning rate?,Having a correct learning rate chosen and adjusted.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given batch. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.",How does Mini-Batch Gradient Descent perform loss calculation and parameter updates for a given batch?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given batch. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.",What is a fixed-sized subset randomly sampled from the training dataset?,A batch
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Frequent updates and lesser variance as compared to SGD,"Compared to SGD, what is the difference between updates and variance?",lesser variance
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Moderate amount of memory requirements,How many memory requirements are there?,Moderate amount
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Moderate amount of memory requirements,How much memory is required?,Moderate amount
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What needs to be correctly chosen and adjusted for effective training?,Learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What is a requirement for a correct learning rate?,Having a correct learning rate chosen and adjusted.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate is constant for all parameters which might not be desirable,What is constant for all parameters which might not be desirable?,Learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate is constant for all parameters which might not be desirable,What is the learning rate constant for?,All parameters
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting trapped in a local minima,What is an example of a local minima?,A broken window
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Susceptible to getting trapped in a local minima,What type of minima is a person who is unable to get trapped in?,local minima
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.",What solves the problem of high variance in parameter updates due to frequent updating?,Addition of momentum to SGD
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.",What is multiplied by a momentum term and added to the current calculated update?,Historical parameter updates
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.",How does SGD oscillate between either direction of the gradient and updates the weights accordingly?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Reduces the high variance in model updates by SGD,What reduces the high variance in model updates by SGD?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Faster convergence than Gradient Descent,What is faster convergence than Gradient Descent?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The momentum hyperparameter needs to be additionally tuned,What does the momentum hyperparameter need to be tuned to?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The momentum hyperparameter needs to be additionally tuned,What is the need for a more tuned momentum hypermeter?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The momentum hyperparameter needs to be additionally tuned,How is the momentum superparameter tuned?,additionally
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What needs to be correctly chosen and adjusted for effective training?,Learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Learning rate needs to be correctly chosen and adjusted for effective training,What is a requirement for a correct learning rate?,Having a correct learning rate chosen and adjusted.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.",What algorithm uses different learning rates for each iteration?,The adaptive gradient descent algorithm
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.",What depends on the difference in the parameters during training?,The change in learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.","The more parameters change, the more minor the learning rate changes?","The more parameters change, the more minor the learning rate changes."
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\eta_{\\mathrm{t}}^{\\prime} \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}(\\mathrm{t}-1)} \\],What is the name of the mathrmw?,mathrmt1-eta_mat
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\eta_{\\mathrm{t}}^{\\prime} \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}(\\mathrm{t}-1)} \\],What is fracpartial?,mathrmLpartial mathrmL
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\eta_{\\mathrm{t}}^{\\prime}=\\frac{\\eta}{\\operatorname{sqrt}\\left(\\alpha_{\\mathrm{t}}+\\epsilon\\right)} \\],What does [ eta_mathrmtprime=fracEtaoperatornamesqrtleft?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Reduces the need to manually modify learning rate,What reduces the need to manually modify the learning rate?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Tends to have faster convergence than Gradient Descent and SGD,What are Tends to have faster convergence than Gradient Descent and SGD?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate",What might be decreased aggressively and monotonically?,The learning rate
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate",What might result in a very small learning rate?,Reduced aggressively and monotonically
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.",What does RMSProp address the issue of?,varying gradient values
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.",What is the problem with varying gradient values?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.",How does the algorithm focus on accelerating the optimization process?,By decreasing the number of function evaluations.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","\\[ v(w, t):=\\gamma v(w, t-1)+(1-\\gamma)\\left(\\nabla Q_{i}(w)\\right)^{2} \\]","What is the name of the v(w, t):=gamma?","v(w, t-1)"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","\\[ v(w, t):=\\gamma v(w, t-1)+(1-\\gamma)\\left(\\nabla Q_{i}(w)\\right)^{2} \\]",left(nabla Q_i(w)right?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","\\[ w:=w-\\frac{\\eta}{\\sqrt{v(w, t)}} \\nabla Q_{i}(w) \\]","nabla Q_i(w, t)?",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","\\[ w:=w-\\frac{\\eta}{\\sqrt{v(w, t)}} \\nabla Q_{i}(w) \\]",What does w:=w-fracetasqrtv?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Requires lesser tuning than other optimization algorithms,What algorithm requires less tuning than other optimization algorithms?,"For example, if it requires less tuning than other optimization algorithms."
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Faster convergence,What is a faster convergence?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks,What does the initial learning rate need to be set manually?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks,What should be carefully chosen to determine the learning rate?,Manually.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks,The suggested value does not work for what?,all tasks
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.",What does AdaGrad tend to remove?,The decaying learning rate problem
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.",What is used to store the leaky average of the second moment gradient?,Two state variables
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.",How many state variables do AdaDelta use?,two
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\mathbf{s}_{t}=\\rho \\mathbf{s}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{2} \\],What does [ mathbfs_t=rho?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ {x}_{t} = {x}_{t-1} - {g}_{t}^{\\prime} \\],What is the name of the x_t?,The name of the x_t is General_T.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} \\],[ mathbfg_tprime=fracsqrtDelta methbbfx_t-1+epsilon odot?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ \\Delta \\mathbf{x}_{t}=\\rho \\Delta \\mathbf{x}_{t-1}+(1-\\rho) \\mathbf{g}_{t}^{\\prime 2} \\],[ Delta mathbfx_t=rho?,delta mathbfx_t=rho
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Elevates the learning rate decay problem in AdaGrad,What is the learning rate decay problem in AdaGrad?,Elevates
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computationally expensive,What is amputationally expensive?,Amputationally expensive surgery.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computationally expensive,What is the cost of computation?,"Approximately 4,000 dollars (USD)"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.",What does Adam optimizer use to update the learning rate?,Moments of first and second order
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages","Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.",What does RMSProp use instead of maintaining a single learning rate through training?,momentum of first order
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",\\[ m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right)\\left[\\frac{\\delta L}{\\delta w_{t}}\\right] v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right)\\left[\\frac{\\delta L}{\\delta w_{t}}\\right]^{2} \\],[ m_t=beta_1?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Rapid convergence,What is rapid convergence?,A rapid convergence is a term used to describe rapid convergence.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Rectifies vanishing learning rate and high variance,What is the vanishing learning rate and high variance?,Rectifies
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computationally expensive,What is amputationally expensive?,Amputationally expensive surgery.
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Computationally expensive,What is the cost of computation?,"Approximately 4,000 dollars (USD)"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",Equations Reference: Link,What is the Equations Reference?,Link
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization),What does awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization do?,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages",(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization),What does awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization do?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Statistical Inference is the process of drawing inferences from your data using probability theory. Statistical inference is used to draw scientific conclusions and test hypotheses.,What is the process of drawing inferences from your data using?,Probability theory
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Statistical Inference is the process of drawing inferences from your data using probability theory. Statistical inference is used to draw scientific conclusions and test hypotheses.,What is used to draw scientific conclusions and test hypotheses?,Statistical inference
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,"If n samples are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed.",What is the name of n samples that are drawn from the same distribution?,They are said to be independently and identically distributed.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,"If n samples are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed.",What is a n sample?,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,"As the sample size n increases, the standard error approaches the true standard deviation for large n. This is because the standard error is an estimate of the true value of the standard deviation.",What is an estimate of the true value of the standard deviation?,The standard error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,"As the sample size n increases, the standard error approaches the true standard deviation for large n. This is because the standard error is an estimate of the true value of the standard deviation.",What does the standard error approach?,The true standard deviation
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A hypothesis test is a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset.,What is an example of a hypothesis test?,A statistical procedure
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A hypothesis test is a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset.,What is a statistical procedure that allows you to test some assumption about some fact about the true distribution of your dataset?,A hypothesis test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A hypothesis test is used to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.,What is used to demonstrate that certain hypotheses are unlikely given the assumptions and evidence?,A hypothesis test
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A hypothesis test is used to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.,What is NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold?,Hypotheses
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,The two error types in hypothesis testing:,What are the two types of error types in hypothesis testing?,:
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Type I error occurs when you reject the null hypothesis when it should be accepted.,What type of error occurs when you reject the null hypothesis when it should be accepted?,Type I error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.,What happens when you accept the null hypothesis?,It should be rejected.
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.,What error occurs when you fail to reject the hypothesis when it should be rejected?,Type II error
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.,What does a smaller p-value mean?,Strong evidence in favor of the alternative hypothesis
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,,A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.,What is stronger evidence in favor of the alternative hypothesis?,A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Just like other business processes, a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influences the stakeholders, existing system environment, and support structure. The data whose analysis supports the proposed solution may have to be gathered both from inside and outside the organization. Overall, this process of initiating and executing a data science project can be challenging, and it requires technical expertise as well as domain guidance. A Gartner study showed that 85% of data science projects fall short of expectations. Project expectations are defined by your client, and the data science team creates a solution vision that will meet their expectations. The path to defining those project expectations must be tread carefully to avoid project failure.",What does a Gartner study show about 85% of data science projects fall short of?,Expectations
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Just like other business processes, a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influences the stakeholders, existing system environment, and support structure. The data whose analysis supports the proposed solution may have to be gathered both from inside and outside the organization. Overall, this process of initiating and executing a data science project can be challenging, and it requires technical expertise as well as domain guidance. A Gartner study showed that 85% of data science projects fall short of expectations. Project expectations are defined by your client, and the data science team creates a solution vision that will meet their expectations. The path to defining those project expectations must be tread carefully to avoid project failure.",What is a key component of a data science project?,Technical expertise as well as domain guidance
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"According to the Chaos Report, a group that tracks IT project failure, reports that data science projects might fail due to multiple reasons, including those that occur in general IT projects. However, there are some unique issues that data science teams must consider to ensure the success of their projects. Some of the reasons for data science project failure include:",What group reports that data science projects might fail due to multiple reasons?,Chaos Report
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"According to the Chaos Report, a group that tracks IT project failure, reports that data science projects might fail due to multiple reasons, including those that occur in general IT projects. However, there are some unique issues that data science teams must consider to ensure the success of their projects. Some of the reasons for data science project failure include:",What are some of the reasons for data science project failure?,
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Insufficient or inappropriate data,","Insufficient or inappropriate data, what type of data does the lack of or inappropriate data cause?",Causes confusion
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Lack of technical data science skills,","Lack of technical data science skills, lack of what?",The right combination of skills.
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Lack of technical data science skills,",Lack of what kind of skills does lack?,technical data science skills
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Lack of technical data science skills,",What type of knowledge does lack lack of?,Technical data science
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Issues with project management,",What is a problem with project management?,
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Issues with project management,",What does a project management issue?,
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Inaccurate interpretation of results, and",Inaccurate interpretation of results and inaccuracy interpretation of what?,results
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"Inaccurate interpretation of results, and",What is the result of a lack of accuracy?,Interpretation of results
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,Mismanaged client expectations.,What is a mismanagement of client expectations?,
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,Mismanaged client expectations.,What is the result of a poorly managed client expectation?,The result of a poorly managed client expectation.
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"The above-mentioned issues are related to a misunderstanding regarding the clients business needs and to inadequate communication between the data science project team and the business stakeholders. It is important to identify and understand your clients business needs, environment, and current solution. This will increase the chances of meeting the business objectives and providing the right analytical solution.",What are the above mentioned issues related to?,A misunderstanding regarding the clients business needs
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"The above-mentioned issues are related to a misunderstanding regarding the clients business needs and to inadequate communication between the data science project team and the business stakeholders. It is important to identify and understand your clients business needs, environment, and current solution. This will increase the chances of meeting the business objectives and providing the right analytical solution.","What is important to identify and understand your clients business needs, environment, and current solution?",
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"In this unit, we will discuss how you derive analytic objectives from a clients or organizations business needs. In doing so, you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset. While these aspects are certainly important parts of many data science projects, being a successful data scientist involves being mindful of the big picture to envision, design, implement and ultimately deploy creative solutions for real-world problems. You will find that this course introduces a data science approach that is grounded in scientific research, software engineering principles, and experimentation.",What do you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset?,
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"In this unit, we will discuss how you derive analytic objectives from a clients or organizations business needs. In doing so, you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset. While these aspects are certainly important parts of many data science projects, being a successful data scientist involves being mindful of the big picture to envision, design, implement and ultimately deploy creative solutions for real-world problems. You will find that this course introduces a data science approach that is grounded in scientific research, software engineering principles, and experimentation.",What is a data science approach that is grounded in?,"Scientific research, software engineering principles, and experimentation."
Problem Identification and Solution Vision,Problem Identification,Problem Identification,,"The remainder of this unit focuses on the ability to identify problems and envision a solution, which is among the most important skills a data scientist must possess.",What is one of the most important skills a data scientist must possess?,Identifying problems and envisioning a solution
Collecting and Understanding Data,Sparse Matrix,Quiz 2,,,What does nan do?,He is a nurse
Collecting and Understanding Data,Sparse Matrix,Quiz 2,,,What is the name of the nnan?,The nan name is Peter Durning.
Collecting and Understanding Data,Data Collection,Study Design,,"To connect the studys objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.",What can a study fall into multiple categories of?,Study designs
Collecting and Understanding Data,Data Collection,Study Design,,"To connect the studys objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.",What is a good study design?,
Collecting and Understanding Data,Data Collection,Study Design,,Exploratory,What is the name of the subject of the study?,Exploratory Psychology
Collecting and Understanding Data,Data Collection,Study Design,,Exploratory,What type of research does the study of a person's life?,exploratory
Collecting and Understanding Data,Data Collection,Study Design,,Confirmatory,What is a confirmation of confirmation?,E-mail
Collecting and Understanding Data,Data Collection,Study Design,,Confirmatory,What is the confirmation of acceptance?,A digitized confirmation code.
Collecting and Understanding Data,Data Collection,Study Design,,Bottom-up (without a pre-specified question),What does Bottom-up mean?,
Collecting and Understanding Data,Data Collection,Study Design,,Bottom-up (without a pre-specified question),What is the bottom-up?,Without a pre-specified question
Collecting and Understanding Data,Data Collection,Study Design,,Can lead to knowledge discovery or new theory,What can lead to knowledge discovery or new theory?,
Collecting and Understanding Data,Data Collection,Study Design,,Uses inductive logic and the logic of discovery,Inductive logic and the logic of discovery are examples of what?,Uses inductive logic and the logic of discovery are examples of what is called proof.
Collecting and Understanding Data,Data Collection,Study Design,,Top-down (with a specified falsifiable hypothesis),What does top-down contain?,falsifiable hypothesis
Collecting and Understanding Data,Data Collection,Study Design,,Top-down (with a specified falsifiable hypothesis),What does the falsifiable hypothesis have?,A specified falsifiable hypothesis has a specified drop-down effect.
Collecting and Understanding Data,Data Collection,Study Design,,Tests an existing theory,Tests an existing theory?,
Collecting and Understanding Data,Data Collection,Study Design,,Tests an existing theory,What is an example of a theory that is tested?,An example of a theory that is tested is shown is a theory that is tested.
Collecting and Understanding Data,Data Collection,Study Design,,"Use deductive logic, the logic of justification, and reconstructed logic",What is the logic of justification?,Deductive logic
Collecting and Understanding Data,Data Collection,Study Design,,"Use deductive logic, the logic of justification, and reconstructed logic",What is reconstructed logic?,
Collecting and Understanding Data,Data Collection,Study Design,,Comparative,What is the difference between Comparative and Comparative?,There is a difference between Comparative and Comparative.
Collecting and Understanding Data,Data Collection,Study Design,,Non-Comparative,What is a non-comparative component?,slicing open diff
Collecting and Understanding Data,Data Collection,Study Design,,Contrasts one subject with another based on certain measures.,What is one subject based on?,Certain measures
Collecting and Understanding Data,Data Collection,Study Design,,Contrasts one subject with another based on certain measures.,What is another subject?,Contrasts one subject with another subject based on certain measures.
Collecting and Understanding Data,Data Collection,Study Design,,Estimates or predicts absolute outcomes of the certain subject matter without explicitly making a comparison with its counterpart.,Who estimates or predicts absolute outcomes of the certain subject matter without explicitly making a comparison with its counterpart?,
Collecting and Understanding Data,Data Collection,Study Design,,Experimental,What is the term for a test?,Experimental
Collecting and Understanding Data,Data Collection,Study Design,,Experimental,What type of test does the experimental test do?,An experimental test does not exist.
Collecting and Understanding Data,Data Collection,Study Design,,Observational,What is Observational?,What is Observational?
Collecting and Understanding Data,Data Collection,Study Design,,Observational,What is the purpose of observational observational observations?,To gather information about the purpose of observational observational observations
Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.",What is the purpose of experiments?,"To compare responses of subjects to certain outcome measures, under different conditions."
Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.",What is a variable that can influence the outcomes?,Levels
Collecting and Understanding Data,Data Collection,Study Design,,"To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.",What is often a plan of manipulation or assignment of subjects to treatment?,To conduct experiments
Collecting and Understanding Data,Data Collection,Study Design,,"To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.",What are experimental designs called?,Experimental designs
Collecting and Understanding Data,Data Collection,Study Design,,Good experimental designs use randomization to determine which treatment a subject receives.,What type of design uses randomization to determine which treatment a subject receives?,Good experimental designs
Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of observational studies is to draw inferences about the effect of an cexposured or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.",The purpose of observational studies is to draw inferences about the effect of what on subjects?,An cexposured or intervention.
Collecting and Understanding Data,Data Collection,Study Design,,"The purpose of observational studies is to draw inferences about the effect of an cexposured or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.",The assignment of subjects to groups is observed rather than manipulated by what?,The data scientist.
Collecting and Understanding Data,Data Collection,Study Design,,"Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.",Observational research involves the direct observation of what?,Individuals in their natural settings.
Collecting and Understanding Data,Data Collection,Study Design,,"Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.","What is determined by individual preferences, practices, or policy decisions?",Who does or does not receive intervention
Collecting and Understanding Data,Data Collection,Study Design,,It is therefore important for readers of observational research to consider if alternative explanations for study results exist.,What is important for readers of observational research to consider if alternative explanations exist for study results?,
Collecting and Understanding Data,Data Collection,Study Design,,"Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.",What is central to science?,Establishing causal inference
Collecting and Understanding Data,Data Collection,Study Design,,"Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.",What is not possible to establish causal inference definitively with a nonexperimental study?,cause and effect relationships
Collecting and Understanding Data,Data Collection,Study Design,,"Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.","When a randomized experiment randomly assigns subjects to treatments, what should be responsible for patterns observed in the data?",Unmeasured variable
Collecting and Understanding Data,Data Collection,Study Design,,"In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.",What is better because more data yields more information?,more data
Collecting and Understanding Data,Data Collection,Study Design,,"In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.",What is arguably more important than the availability of the data itself?,Modality in which data is collected
Collecting and Understanding Data,Data Collection,Study Design,,"In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.","If the data that is being collected has too little information to inform about the questions of interest, then the conclusions may not be what?",Very informative
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in todays data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive to do so. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do from an IT level.",What type of tools will you use in interpreting environments?,Variety of tools
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in todays data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive to do so. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do from an IT level.",What is the cost of managing the underlying hardware in todays data science systems?,Typically prohibitively expensive.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in todays data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive to do so. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do from an IT level.","How can managing the security, fault tolerance, and data governance of the systems you are working on?",incredibly difficult
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In situations like these, especially when you might not know the scale of the system you want to build, cloud computing can be a great boon to your efforts. By purchasing either Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS), you can choose to rely on another company to manage some aspect of the underlying hardware/software issues and focus on data science. The main benefits of cloud computing are the following:",What can be a great boon to your efforts in situations like these?,cloud computing
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"In situations like these, especially when you might not know the scale of the system you want to build, cloud computing can be a great boon to your efforts. By purchasing either Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS), you can choose to rely on another company to manage some aspect of the underlying hardware/software issues and focus on data science. The main benefits of cloud computing are the following:",What is the main benefit of cloud computing?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You are able to trade the cost of buying all of the hardware for your projects and managing them, for whatever cost the cloud provider asks you to pay. In practice, these costs will be based on your usage rather than the hardware itself, which can allow for savings in the long run.",What is the cost of buying all of the hardware for your projects?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You are able to trade the cost of buying all of the hardware for your projects and managing them, for whatever cost the cloud provider asks you to pay. In practice, these costs will be based on your usage rather than the hardware itself, which can allow for savings in the long run.",What does the cloud provider ask you to pay?,whatever cost
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You are able to trade the cost of buying all of the hardware for your projects and managing them, for whatever cost the cloud provider asks you to pay. In practice, these costs will be based on your usage rather than the hardware itself, which can allow for savings in the long run.","In practice, what will the costs be based on?",Your usage
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You do not need to worry about capacities as much. As the main cloud providers have large data systems in place, you do not need to worry about infrastructure capacity or about ensuring that systems cache as intended. Instead, you can make use of the servers that are present to accomplish your goals.",What are the main cloud providers that have large data systems in place?,
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You do not need to worry about capacities as much. As the main cloud providers have large data systems in place, you do not need to worry about infrastructure capacity or about ensuring that systems cache as intended. Instead, you can make use of the servers that are present to accomplish your goals.",What can you make use of to accomplish your goals?,Servers
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You can purchase these services for potentially less than what you might have thought possible. As you share the cloud with other users, you are able to pay less for more power which, along with specialized chips only available in some cloud instances, can make the cloud much cheaper for the processing power given than other systems.",What can you buy for less than what you thought?,Cloud services
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"You can purchase these services for potentially less than what you might have thought possible. As you share the cloud with other users, you are able to pay less for more power which, along with specialized chips only available in some cloud instances, can make the cloud much cheaper for the processing power given than other systems.",What can make the cloud much cheaper for the processing power given than other systems?,specialized chips
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For all of these reasons, the cloud is becoming the place to do data science in, especially for large and unstructured data. As a result, however, you will need to understand the cloud and use it accordingly to manage your data. In particular, you will need to pay attention to the types of storage and processing you are purchasing from the cloud. In general, there are many ways to use the cloud, but the most important factor to consider when using the cloud to host data is the structure/usage patterns of the underlying data.",What is becoming the place to do data science in?,The cloud
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For all of these reasons, the cloud is becoming the place to do data science in, especially for large and unstructured data. As a result, however, you will need to understand the cloud and use it accordingly to manage your data. In particular, you will need to pay attention to the types of storage and processing you are purchasing from the cloud. In general, there are many ways to use the cloud, but the most important factor to consider when using the cloud to host data is the structure/usage patterns of the underlying data.",What is the most important factor to consider when using the cloud to host data?,The structure/usage patterns of the underlying data.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For unstructured data that has high-data temperature, i.e., data that you need to read and write a lot, you should try to use a block-based storage system. Systems like these work like network hard drives, which can make them incredibly useful when you wish to perform large-scale processing tasks and data cleaning tasks.",What type of storage system does unstructured data have?,block-based
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For unstructured data that has high-data temperature, i.e., data that you need to read and write a lot, you should try to use a block-based storage system. Systems like these work like network hard drives, which can make them incredibly useful when you wish to perform large-scale processing tasks and data cleaning tasks.",What is a block-based storage system that can be used for?,Large-scale processing tasks and data cleaning tasks
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For large files that are low-write but high-read, an object-based storage system might be more beneficial. Object-based systems store files as key-value pairs, allowing people with the write key and access permissions to download the files accordingly. They are best for Binary Large OBject files (BLOBs) or when you might want to distribute a data file to many people to allow them to process it on their own systems.",What is an object-based storage system that stores files as key-value pairs?,A key-value pairs
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For large files that are low-write but high-read, an object-based storage system might be more beneficial. Object-based systems store files as key-value pairs, allowing people with the write key and access permissions to download the files accordingly. They are best for Binary Large OBject files (BLOBs) or when you might want to distribute a data file to many people to allow them to process it on their own systems.",What are the best for binary large OBject files?,BLOBs
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For structured data that you might need to write to constantly, you might want to consider a relational database system. These systems enforce a lot of constraints on the underlying data but, in doing so, ensure that each operation has Atomicity, Consistency, Isolation, and Durability properties. These systems use the SQL, or Structured Query Language, as a basis for every operation available and are especially important when you want consistency in your data.",What does a relational database system enforce?,Constraints on the underlying data
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"For structured data that you might need to write to constantly, you might want to consider a relational database system. These systems enforce a lot of constraints on the underlying data but, in doing so, ensure that each operation has Atomicity, Consistency, Isolation, and Durability properties. These systems use the SQL, or Structured Query Language, as a basis for every operation available and are especially important when you want consistency in your data.",What does the Structured Query Language serve as?,A basis for every operation available.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Alternatively, if your data is not structured enough to fit in the tight schema requirements of a relational database, one can try a non-relational database system. These systems dont need to adhere to the requirements of an  SQL schema structure and thus to the consistency provided but gain added flexibility and scale. Such databases allow you to store a wider variety of data and scale it up to more computers much easier than a traditional database system.",What does a non-relational database system need to adhere to?,An SQL schema structure
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Alternatively, if your data is not structured enough to fit in the tight schema requirements of a relational database, one can try a non-relational database system. These systems dont need to adhere to the requirements of an  SQL schema structure and thus to the consistency provided but gain added flexibility and scale. Such databases allow you to store a wider variety of data and scale it up to more computers much easier than a traditional database system.",What is a better way to store a wider variety of data?,A non-relational database system.
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Each of these systems will be useful for different use cases, and you will need to carefully consider your use case before deciding on what to use. Besides these hosting concerns, you will also need to consider the following carefully:",What will each of these systems be useful for?,Different use cases
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Each of these systems will be useful for different use cases, and you will need to carefully consider your use case before deciding on what to use. Besides these hosting concerns, you will also need to consider the following carefully:",What should you consider before you decide on what to use?,Your use case
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Budget: As cloud resources are based on what you use, it pays to monitor your resources carefully and constantly check your usage limits. Most cloud providers have methods to monitor your usage, which can help you both understand and automatically stop extraneous usage accordingly.",What is your budget based on?,What you use
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Budget: As cloud resources are based on what you use, it pays to monitor your resources carefully and constantly check your usage limits. Most cloud providers have methods to monitor your usage, which can help you both understand and automatically stop extraneous usage accordingly.",What do most cloud providers have to do to monitor your usage?,Modify
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Budget: As cloud resources are based on what you use, it pays to monitor your resources carefully and constantly check your usage limits. Most cloud providers have methods to monitor your usage, which can help you both understand and automatically stop extraneous usage accordingly.",How can you stop extraneous use?,Automatically
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Security: While cloud providers maintain the security of the infrastructure for you, you still need to keep in mind good practices for the security of the software you are running. Checking if your systems are vulnerable to the latest Log4J or SQL injection attack is not your main priority as a data scientist, but knowing best practices and discussing the architecture with experts is a huge priority, especially if the data in question is confidential user data.",What do cloud providers maintain for you?,The security of the infrastructure
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Security: While cloud providers maintain the security of the infrastructure for you, you still need to keep in mind good practices for the security of the software you are running. Checking if your systems are vulnerable to the latest Log4J or SQL injection attack is not your main priority as a data scientist, but knowing best practices and discussing the architecture with experts is a huge priority, especially if the data in question is confidential user data.",What is not your main priority as a data scientist?,Log4J or SQL injection attack
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,,"Your Security: While cloud providers maintain the security of the infrastructure for you, you still need to keep in mind good practices for the security of the software you are running. Checking if your systems are vulnerable to the latest Log4J or SQL injection attack is not your main priority as a data scientist, but knowing best practices and discussing the architecture with experts is a huge priority, especially if the data in question is confidential user data.",How is the data in question confidential?,User data
Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,,"We use a framework to formulate questions that carefully define business needs and extract business objectives that can be met using data science techniques. This framework can help a data science team meet the expectations of the client and deliver a solution that meets the business needs. The Evidence Value Proposition (EVP) framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology. This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.\r",What is a framework that can help a data science team meet the expectations of the client and deliver a solution that meets the business needs?,EVP
Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,,"We use a framework to formulate questions that carefully define business needs and extract business objectives that can be met using data science techniques. This framework can help a data science team meet the expectations of the client and deliver a solution that meets the business needs. The Evidence Value Proposition (EVP) framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology. This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.\r",What framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology?,The Evidence Value Proposition (EVP) framework
Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,,Figure 1. Evidence Value Proposition Framework,What is the evidence value proposition framework?,
Deep Learning and Model Deployment,CPU vs. GPU,Quiz 8,,,What does nan do?,He is a nurse
Deep Learning and Model Deployment,CPU vs. GPU,Quiz 8,,,What is the name of the nnan?,The nan name is Peter Durning.
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle structures the activities of the data science team. It should not be considered to be linear as there must be iterations of questioning and research involved in each phase. The framework consists of several major stages:  The framework involves input from various members of the data science team, as well as the client, as stated below:",What structure does the data science lifecycle structure?,The data science lifecycle structure is the activities of the data science team.
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle structures the activities of the data science team. It should not be considered to be linear as there must be iterations of questioning and research involved in each phase. The framework consists of several major stages:  The framework involves input from various members of the data science team, as well as the client, as stated below:",What does the framework consist of?,several major stages
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle structures the activities of the data science team. It should not be considered to be linear as there must be iterations of questioning and research involved in each phase. The framework consists of several major stages:  The framework involves input from various members of the data science team, as well as the client, as stated below:",The framework involves input from what?,"The data science team, as well as the client"
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Business Understanding involves framing the objectives and assessing data science readiness. The client and data science team are involved in this stage to ensure that the analytic solutions meet the business objectives.,What involves framing the objectives and assessing data science readiness?,Business Understanding
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Business Understanding involves framing the objectives and assessing data science readiness. The client and data science team are involved in this stage to ensure that the analytic solutions meet the business objectives.,Who is involved in this stage to ensure that the analytic solutions meet the business objectives?,The client and data science team
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Data Acquisition involves gathering data from various appropriate sources. Data preparation techniques are employed to ensure the data is useful for analysis.,What involves collecting data from various appropriate sources?,Data Acquisition
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Data Acquisition involves gathering data from various appropriate sources. Data preparation techniques are employed to ensure the data is useful for analysis.,Data preparation techniques are employed to ensure the data is useful for what?,Analysis
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.",What is the first step in modeling?,Business understanding
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.",What is often mistaken as the first stage of the process?,Modeling
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.",Modeling typically involves what?,"feature engineering, algorithm selection, model training, and evaluation"
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Deployment involves the implementation of the solution developed in the operating environment of the business. One should always remember that a business needs to measure the impact of the deployed solution to ensure the success of the solution.,What involves the implementation of the solution developed in the operating environment of the business?,deployment
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,Deployment involves the implementation of the solution developed in the operating environment of the business. One should always remember that a business needs to measure the impact of the deployed solution to ensure the success of the solution.,What is the purpose of a business to measure the impact of the deployed solution?,To ensure the success of the solution.
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle can seem daunting at first, but a data scientist will not complete all the tasks alone. A productive team will consist of individuals with complementary skills filling various roles that can ensure the project is successfully executed.",What can seem daunting at first?,The data science lifecycle.
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,,"The data science lifecycle can seem daunting at first, but a data scientist will not complete all the tasks alone. A productive team will consist of individuals with complementary skills filling various roles that can ensure the project is successfully executed.",What will a team of people do to ensure the project is successful?,Filling various roles
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","[Required Reading] Paper: Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-52). (Requires CMU credentials to access)",What paper did the CHI Conference on Human Factors in Computing Systems hold?,Manipulating and measuring model interpretability.
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","[Required Reading] Paper: Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-52). (Requires CMU credentials to access)",What is the requirement for CMU credentials to access?,Yes
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.",Who is the first author to study AI and ethics in research and engineering?,Senior Program Manager
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.",What research group at Microsoft studies how to help laypeople make sense of numerical data?,Computational Social Science
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.,What is the purpose of the paper?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.,What are ML researchers and practitioners who build?,machine learning systems
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence peoples everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.",What are machines increasingly used to aid decision-making in high-stakes domains?,Machine learning models
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence peoples everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.",People are reluctant to use these models due to concerns about their underlying mechanisms and fairness?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence peoples everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.",What is one such research work?,machine learning interpretability
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on peoples behavior.",What is interpretability a latent property that cannot be directly measured?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on peoples behavior.",What does interpretability have a measurable influence on?,Peoples behavior
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.",What does the paper report that clear models with less features are not better than complex or black-box models in their ability to help people make beneficial decisions?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects:",What is the overall experimental procedure to?,Variable factors that may influence a models interpretability and measure their effect on peoples behaviors
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects:",What factors may influence a model interpretability and measure their effect on peoples behaviors?,Variable factors
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ1: How well can people simulate a models prediction?,How well can people simulate a model prediction?,QA1
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ2: To what extent do people follow a models prediction when its beneficial for them to do so?,What do people follow when it is beneficial for them to do?,To what extent
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",RQ3: How well can people detect when a model has made a mistake and correct it?,How can people detect when a model has made a mistake and correct it?,RQ3
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).",What is the primary task in each experiment?,To predict the price of apartments in New York City.
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).",How many features can the participants see in the dataset?,8
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).",What is a 2 x 2 experiment setting?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartments selling price. They were then shown the models prediction and asked for their own prediction of the apartments selling price. For RQ1, the authors measured the difference between peoples guesses of the models prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the models prediction in their guess for the apartments ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.",How many apartment data points were used in the study?,12
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartments selling price. They were then shown the models prediction and asked for their own prediction of the apartments selling price. For RQ1, the authors measured the difference between peoples guesses of the models prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the models prediction in their guess for the apartments ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.",What were the internals of the model?,Clear or black box
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartments selling price. They were then shown the models prediction and asked for their own prediction of the apartments selling price. For RQ1, the authors measured the difference between peoples guesses of the models prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the models prediction in their guess for the apartments ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.",Who measured the difference between peoples guesses of models prediction and the actual prediction result for the apartments selling price?,The authors
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",Based on the papers findings:,"Based on the papers findings, what did the papers report?",The papers report did not mention any particular issue.
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",A clear model with a smaller number of features was easiest for participants to simulate.,What was the easiest model for participants to simulate?,A clear model with a smaller number of features
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There were no significant differences in the participants trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.,How many features did participants not trust in the model?,2
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There were no significant differences in the participants trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.,What was the black-box model with 8 features?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.",What type of model is less likely to correct inaccurate predictions?,black-box model
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.",What can be harmful to the transparency of the data?,Cognitive overload
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There are two important takeaways from the paper:,What are two important takeaways from the paper?,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.",What is not purely a computational problem?,Machine learning interpretability
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.",What is the key to machine learning interpretability?,A human-centered focus
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.",Intuition alone is not sufficient to interpret what?,models
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.","More empirical studies that cover a wider range of domain models, factors, and outcomes are needed?",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,The ability to study and understand a projects business objective in sufficient depth in order to maximize the benefit from leveraging data is a critical skill for data scientists. This skill can be thought of as having three aspects:,What is a critical skill for data scientists?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,The ability to study and understand a projects business objective in sufficient depth in order to maximize the benefit from leveraging data is a critical skill for data scientists. This skill can be thought of as having three aspects:,What are three aspects of a data science skill?,- To study and understand a project business objective in sufficient depth
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,Engagement with the client and research about the specific circumstances of the clients business.,What is the purpose of a client's engagement with the client?,Research about the specific circumstances of the clients business.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).","What does one engage with the client by learning about their business or organization, its customers/clients, the market and its competitors?",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).",What may involve a workshop-like event between the business unit and the data science team?,This may involve a workshop-like event between the business unit and the data science team.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,Engagement with the client and research around the general domain in which the clients business operates .,What type of research does the client work on?,The general domain in which the clients business operates
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,Engagement with the client and research around the general domain in which the clients business operates .,What is the scope of the client business?,General Domain
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Second, preparation will often require additional research around the clients general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the clients experts in a serious conversation about the topic without frequently stumbling over misunderstandings.",What type of research does a data scientist need to do?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Second, preparation will often require additional research around the clients general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the clients experts in a serious conversation about the topic without frequently stumbling over misunderstandings.",What is the purpose of the data scientist's research?,To be convinced that the domain is sufficiently understood and that the business wants to reach the objective.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Second, preparation will often require additional research around the clients general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the clients experts in a serious conversation about the topic without frequently stumbling over misunderstandings.",How can a business communicate with clients experts?,In a serious conversation about the topic without stumbling over misunderstandings.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,Spending continuous efforts to maintain alignment of the ongoing project with changes or new information in both the general domain and specific circumstances.,What is the purpose of spending continuous efforts to maintain alignment of the ongoing project with changes or new information?,The general domain and specific circumstances.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science projects success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.",What is a common pitfall for a data science project?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science projects success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.",What is an example of an interesting machine learning aspect of a project that is discovered?,A dataset suitable for deep neural networks
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science projects success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.","In extreme cases, what can lead to the application of overly complex methods to unsuitable datasets?",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",What is one of the most rewarding aspects of the data science profession?,Lifelong learning process
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",What is not uncommon for data scientists to have some formal education or professional experience in specific disciplines?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,,"Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",Who may decide to specialize in a certain field?,Trained data scientists
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,The problem underlying an analytic objective should satisfy the following criteria:,What is the problem at the base of an analytic objective?,satisthe following criteria
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,Solving the problem must be part of a possible solution vision toward the business objective.,What must be part of a possible solution vision toward the business objective?,Solving the problem
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.",Who should be confident that data exists that can facilitate that solution?,Domain experts
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.",What must be available to retrieve data?,Sources
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,The problem must be specific and realistic so that a corresponding data science project can succeed in principle.,What must be specific and realistic so that a data science project can succeed in principle?,The problem
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.",What is the purpose of a solution vision?,To describe how the business can reach the objective.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.",How can the solution vision be decomposed into sub-problems?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.",What team will be responsible for leveraging data to help solve sub-problems and/or produce data-derived insights?,data science team
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"As data-driven methods are still in the phase of being adopted across many fields, the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement. In such a case, it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond cwe want to leverage data somehowd and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.",What may be the main business objective that starts the whole engagement?,Using data to support business objectives
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"As data-driven methods are still in the phase of being adopted across many fields, the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement. In such a case, it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond cwe want to leverage data somehowd and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.",What should be fulfilled in order to progress beyond cwe want to leverage data somehowd?,Higher-level business needs
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.",What does an online company ask for for a data science project?,The use of social media data.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.",What do the boards want to keep up with?,The competition
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.",How are social media streams typically used in retail businesses?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.",Why does the client ask for an analysis of social media data?,To inform their decision.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Second, there must be a sound presumption that the problems solution must benefit from the use of data. A collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution, including data that would need to be collected as part of the project. In order to be suitable, the data should exhibit informative patterns relating to the problem. It takes a combination of substantive expertise and data science skills to assess this criterion.",What should a collaborating team of domain experts and data scientists discuss?,The available data sources and their suitability for the solution
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Second, there must be a sound presumption that the problems solution must benefit from the use of data. A collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution, including data that would need to be collected as part of the project. In order to be suitable, the data should exhibit informative patterns relating to the problem. It takes a combination of substantive expertise and data science skills to assess this criterion.",What should the data exhibit in order to be suitable for the problem?,Informative patterns
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,Domain-specific problems around the availability of data include:,What are some domain-specific problems around the availability of data?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Lack of readiness in the organization (e.g., the organization's data is not readily processable), unsuitability of the data for the objective (e.g., data is old/stale, out of domain, incomplete, or suffers from an obvious bias), or difficulty in collecting data because the expert labor involved is too expensive.",What is the reason the organization's data is not readily processable?,Lack of readiness
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Lack of readiness in the organization (e.g., the organization's data is not readily processable), unsuitability of the data for the objective (e.g., data is old/stale, out of domain, incomplete, or suffers from an obvious bias), or difficulty in collecting data because the expert labor involved is too expensive.",What is a reason for the lack of readiness in the organization?,"The organization's data is not readily processable,"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Various technical objections, such as too little data for the required methods, fragmentation of data across multiple units with no suitable way of joining, or overwhelming imprecision/noise in the data. In cases where data is available, but the existence of a csignald for the problem is uncertain, exploring whether the signal exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself.",What type of objections can arise from data fragmentation?,Technical
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Various technical objections, such as too little data for the required methods, fragmentation of data across multiple units with no suitable way of joining, or overwhelming imprecision/noise in the data. In cases where data is available, but the existence of a csignald for the problem is uncertain, exploring whether the signal exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself.",What can be considered an exploratory analytic objective?,A signal
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Typically, data science projects/consultations involve a preliminary data survey that informs or even precedes a longer substantive discussion. It is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client.",What type of project involves a preliminary data survey that informs or even precedes a longer substantive discussion?,Data science
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Typically, data science projects/consultations involve a preliminary data survey that informs or even precedes a longer substantive discussion. It is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client.",What is strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations?,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.",What is the main purpose of a data science project?,To assess its feasibility based on available data and methods.
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.",What should the technicians be careful not to exaggerate?,analytical capacities
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,,"Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.",How should a particular neural network model perform a classification task?,Very well
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"The requirements gathering process is not linear. It is important to consider an evaluation at each step of the process. Doing this will ensure that errors are identified and fixed early, as it can be costly and time-consuming for errors to go undetected. However, it is a best practice to conduct validation and verification exercises for your requirements after they have been defined. It is important to confirm that requirements meet the needs of the business and users, and the requirements can be traced back to the defined business and analytic objectives.",What process is not linear?,The requirements gathering process
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"The requirements gathering process is not linear. It is important to consider an evaluation at each step of the process. Doing this will ensure that errors are identified and fixed early, as it can be costly and time-consuming for errors to go undetected. However, it is a best practice to conduct validation and verification exercises for your requirements after they have been defined. It is important to confirm that requirements meet the needs of the business and users, and the requirements can be traced back to the defined business and analytic objectives.",What is important to consider at each step of the process?,An evaluation
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"The requirements gathering process is not linear. It is important to consider an evaluation at each step of the process. Doing this will ensure that errors are identified and fixed early, as it can be costly and time-consuming for errors to go undetected. However, it is a best practice to conduct validation and verification exercises for your requirements after they have been defined. It is important to confirm that requirements meet the needs of the business and users, and the requirements can be traced back to the defined business and analytic objectives.",How can errors go undetected?,costly and time consuming
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Requirements should be validated to ensure that they are complete, correct, traceable, verifiable, and testable. It seems logical to perform validation when the solution has been developed but before it is delivered to the client. This is because you can test the actual solution. But what happens when you find errors that require the project team to start from the beginning of the development process? Time and money have been wasted! This grave mistake can be avoided if previously defined requirements are validated.","What should be validated to ensure they are complete, correct, traceable, verifiable, and testable?",Requirements
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Requirements should be validated to ensure that they are complete, correct, traceable, verifiable, and testable. It seems logical to perform validation when the solution has been developed but before it is delivered to the client. This is because you can test the actual solution. But what happens when you find errors that require the project team to start from the beginning of the development process? Time and money have been wasted! This grave mistake can be avoided if previously defined requirements are validated.",What happens when you find errors that require the project team to start from the beginning of the development process?,Time and money have been wasted
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Validating requirements will most likely involve testing without implementation, but one should remember that the project team will also validate and verify the requirements throughout the solution development process. Validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost.",What will most likely involve testing without implementation?,Validating requirements
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Validating requirements will most likely involve testing without implementation, but one should remember that the project team will also validate and verify the requirements throughout the solution development process. Validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost.",What will validate and verify the requirements throughout the development process?,The project team.
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Validating requirements will most likely involve testing without implementation, but one should remember that the project team will also validate and verify the requirements throughout the solution development process. Validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost.",Validating requirements at this stage will ensure that solution developers will produce what?,the right deliverables
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,Requirements can be validated by following the steps below:,What can be validated by following the steps below?,Requirements
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,Review requirements. The requirements management plan will be peer-reviewed to identify that each documented requirement is verifiable and unambiguous. The peer-review process should be well-defined so that reviewers can discuss their interpretations of requirements. This will reveal any ambiguity. The reviewers will produce a summary of the defects document. This document will be used by the business analyst and project team to revise requirements.,What will be reviewed to determine that each documented requirement is verifiable and unambiguous?,The requirements management plan
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,Review requirements. The requirements management plan will be peer-reviewed to identify that each documented requirement is verifiable and unambiguous. The peer-review process should be well-defined so that reviewers can discuss their interpretations of requirements. This will reveal any ambiguity. The reviewers will produce a summary of the defects document. This document will be used by the business analyst and project team to revise requirements.,The peer-review process should be well-defined so that reviewers can discuss their interpretations of what?,Requirements
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,Review requirements. The requirements management plan will be peer-reviewed to identify that each documented requirement is verifiable and unambiguous. The peer-review process should be well-defined so that reviewers can discuss their interpretations of requirements. This will reveal any ambiguity. The reviewers will produce a summary of the defects document. This document will be used by the business analyst and project team to revise requirements.,What document will be used by the business analyst and project team?,A document that will be used by the business analyst and project team to revise requirements
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Prototype requirements. Prototyping can help the project and client team determine if the requirements can be considered complete. Simulations are a popular prototyping technique, but at this stage of solution development, creating simulations will be time-consuming. There are less time-consuming prototyping techniques that can help validate requirements. Proof-of-concept (POC) prototypes and paper mockups can be used to demonstrate the feasibility and completeness of requirements.",What is a popular prototyping technique?,Simulations
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Prototype requirements. Prototyping can help the project and client team determine if the requirements can be considered complete. Simulations are a popular prototyping technique, but at this stage of solution development, creating simulations will be time-consuming. There are less time-consuming prototyping techniques that can help validate requirements. Proof-of-concept (POC) prototypes and paper mockups can be used to demonstrate the feasibility and completeness of requirements.",What can be used to demonstrate the feasibility and completeness of requirements?,Proof-of-concept (POC) prototypes and paper mockups
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Test requirements. Requirements can be validated by users. Acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria. Acceptance criteria are set by end-users of the solution. A project team might worry about the users' inability to define the acceptance criteria. The project team can guide users in thinking about how the solution meets their needs. Acceptance criteria can be defined using the S.M.A.R.T goals. Once acceptance criteria are set, users can conduct acceptance tests.",What is done to assess whether a solution will meet predefined criteria called acceptance criteria?,Acceptance tests
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Test requirements. Requirements can be validated by users. Acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria. Acceptance criteria are set by end-users of the solution. A project team might worry about the users' inability to define the acceptance criteria. The project team can guide users in thinking about how the solution meets their needs. Acceptance criteria can be defined using the S.M.A.R.T goals. Once acceptance criteria are set, users can conduct acceptance tests.",What is set by end-users of the solution?,Acceptance criteria
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"Test requirements. Requirements can be validated by users. Acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria. Acceptance criteria are set by end-users of the solution. A project team might worry about the users' inability to define the acceptance criteria. The project team can guide users in thinking about how the solution meets their needs. Acceptance criteria can be defined using the S.M.A.R.T goals. Once acceptance criteria are set, users can conduct acceptance tests.",Who can guide users in thinking about how the solution meets their needs?,The project team
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"A successful requirements gathering exercise is not yet deemed successful when the activities in the requirements management plan have been completed. After the results of the requirements gathering process have been documented and validated, a formal sign-off will confirm that all parties approve and accept the defined requirements. This is a signal that the project team can officially begin developing a solution/solutions.",When is a successful requirements gathering exercise deemed successful?,When the activities in the requirements management plan have been completed
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,Validating Requirements,"A successful requirements gathering exercise is not yet deemed successful when the activities in the requirements management plan have been completed. After the results of the requirements gathering process have been documented and validated, a formal sign-off will confirm that all parties approve and accept the defined requirements. This is a signal that the project team can officially begin developing a solution/solutions.",What is the sign-off that confirms that all parties approve and accept the defined requirements?,A formal sign-off
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.",What does R. Bellman define the curse of dimensionality?,
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.",What is the term used to describe a dataset with a large number of variables?,Dimensionality
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.",What does feature engineering inform the models that you will build?,
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.",What is a model-based feature engineering technique?,
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.",What is used to reduce the dimensionality of a dataset?,Principal Component Analysis (PCA)
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.",What does PCA mean?,Principal Component Analysis
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.",How can you reduce the dimension of your feature space?,By eliminating redundancies or irrelevant features.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.",What is a useful technique to use when there are variables that need to be dropped?,PCA
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.",What are some other techniques for dimension reduction?,Linear Discriminant Analysis (LDA)
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a csmalld number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.,What is a linear transformation technique?,PCA
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a csmalld number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.,What does PCA involve?,performing the eigendecomposition on the covariance matrix
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a csmalld number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.,How many dimensions is PCA considered?,csmalld number
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",The following steps are used when performing PCA:,What steps are used when performing PCA?,The following steps are used when performing PCA.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Standardize the data.,What is the standardization of the data?,It is called standardization of the data.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Standardize the data.,What does standardization mean?,The data
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Compute the Covariance matrix of dimensions in the data.,Compute the Covariance matrix of dimensions in the data?,
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.",What is a non zero vector that changes by a scalar factor when a linear transformation is applied to it?,Eigenvector
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.",What is an eigenvalue called?,A characteristic value1 or a set of scalars.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.,What are the top eigenvalues that correspond to the k largest?,k Eigenvectors
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Construct the projection matrix W from the selected k Eigenvectors.,What is the projection matrix W?,Construct
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Construct the projection matrix W from the selected k Eigenvectors.,What is a projection matrix?,W
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.,Transform the original data set X via W to obtain what?,k-dimensional feature subspace Y.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.,What is the new feature subspace Y?,k-dimensional
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA in Python Example: Principal Component Analysis in three (3) steps.,What is the PCA in Python example?,Principal Component Analysis in three (3) steps.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA in Python Example: Principal Component Analysis in three (3) steps.,How many steps does PCA consist of?,Three (3) steps
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction",PCA in Python Example: Principal Component Analysis in three (3) steps.,What is a PCA example in Python?,Principal Component Analysis in three (3) steps.
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"Dimensionality,Feature Extraction","Reading: A Brief Article-Principal Component Analysis (Lever, Krzywinski, and Altman, 2017)",What is reading: A Brief Article-Principal Component Analysis?,
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Once it is determined that an organization is Data Science ready, the project team will gather requirements that will enable the development of an analytics solution to meet the defined business need(s).",When is Data Science ready?,"Once it is determined an organization is Data Science ready, the project team will gather requirements that will"
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Once it is determined that an organization is Data Science ready, the project team will gather requirements that will enable the development of an analytics solution to meet the defined business need(s).",What will the project team gather to help develop an analytics solution?,Requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",Let us explore a scenario with a two-year college that is seeking to improve the recruitment of new students and increase the retention of existing students by 10% over the next five (5) years. The college would like to use predictive modeling to achieve this objective. The predictive model should provide insights that can inform business processes related to recruitment and retention.,What is the goal of a two-year college?,To improve the recruitment of new students and increase the retention of existing students by 10% over the next
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",Let us explore a scenario with a two-year college that is seeking to improve the recruitment of new students and increase the retention of existing students by 10% over the next five (5) years. The college would like to use predictive modeling to achieve this objective. The predictive model should provide insights that can inform business processes related to recruitment and retention.,How long does the college want to increase the retention of existing students?,5 (5) years
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",Let us explore a scenario with a two-year college that is seeking to improve the recruitment of new students and increase the retention of existing students by 10% over the next five (5) years. The college would like to use predictive modeling to achieve this objective. The predictive model should provide insights that can inform business processes related to recruitment and retention.,What does a predictive model provide that can inform business processes?,Insights
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Upon meeting with project sponsors, assessing the client for data science readiness, and defining business and analytic objectives, the project team embarks on gathering requirements for this project. The business analyst (BA) identifies all necessary stakeholders and systems related to recruitment and retention in the college. The BA takes the lead in eliciting information about user needs and system constraints.",What is the name of the business analyst that identifies all necessary stakeholders and systems related to recruitment and retention in college?,BA
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Upon meeting with project sponsors, assessing the client for data science readiness, and defining business and analytic objectives, the project team embarks on gathering requirements for this project. The business analyst (BA) identifies all necessary stakeholders and systems related to recruitment and retention in the college. The BA takes the lead in eliciting information about user needs and system constraints.",What does the BA take the lead in?,Eliciting information about user needs and system constraints.
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","In this scenario, requirements elicitation involves interviewing stakeholders and document analysis. Once user and system requirements are defined, the project team can now define the requirements for the analytic solution.",What involves interviewing stakeholders and document analysis?,Requirement
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","In this scenario, requirements elicitation involves interviewing stakeholders and document analysis. Once user and system requirements are defined, the project team can now define the requirements for the analytic solution.",What are the requirements for the analytic solution?,User and system requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","While distilling analytic objectives for this project, the project team determined that an exploratory analysis of the colleges prospective student data is needed to perform segmentation for targeted advertising and recruitment efforts. Prior to performing a segmentation task or cluster analysis, the team defines the requirements for the data that will be explored.",What is needed to perform segmentation for targeted advertising and recruitment efforts?,An exploratory analysis of the colleges prospective student data
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","While distilling analytic objectives for this project, the project team determined that an exploratory analysis of the colleges prospective student data is needed to perform segmentation for targeted advertising and recruitment efforts. Prior to performing a segmentation task or cluster analysis, the team defines the requirements for the data that will be explored.",What does the team define before performing a segmentation task?,The requirements for the data that will be explored
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Defining data requirements involves understanding the data needed to reach the business and analytic objectives. Defining the data requirements will also involve considerations for data sources, data management, and data extraction. The following questions must be answered to determine the data requirements:",What does Defining Data requirements involve understanding?,The data needed to reach the business and analytic objectives.
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Defining data requirements involves understanding the data needed to reach the business and analytic objectives. Defining the data requirements will also involve considerations for data sources, data management, and data extraction. The following questions must be answered to determine the data requirements:","What will be considered for data sources, data management and data extraction?",Data requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",How can data be used to meet the business and analytic objectives?,How can data be used to meet the business and analytic objectives?,
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",What data is needed for developing the analytic solution? Can the data be collected from credible sources?,What is needed to develop the analytic solution?,Dates
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",What data is needed for developing the analytic solution? Can the data be collected from credible sources?,What can the data be collected from credible sources?,
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",How can the data be prepared to visualize and answer relevant questions?,How can the data be prepared to visualize and answer relevant questions?,
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Although the talent for developing models resides within the project team, the model requirements are defined collaboratively. Model explainability is important to ensure that a client understands the insights that can be gleaned from the results/outputs from the model(s).",What is important to ensure that a client understands the insights that can be gleaned from the results/outputs from the model?,Model explainability
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Lets consider the scenario above. The client will benefit from models that predict the probability of a prospective applicant enrolling in the college. Without understanding the output of a predictive model, the client will not be able to decipher the observations that could have a positive yield from their engagement efforts. This can result in the college wasting funds in targeting prospective students, therefore leading to a failed solution.",What type of model will the client benefit from?,Predictive
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Lets consider the scenario above. The client will benefit from models that predict the probability of a prospective applicant enrolling in the college. Without understanding the output of a predictive model, the client will not be able to decipher the observations that could have a positive yield from their engagement efforts. This can result in the college wasting funds in targeting prospective students, therefore leading to a failed solution.",What can result in the college wasting funds in targeting prospective students?,A negative yield from their engagement efforts
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Model requirements describe the behavior and attributes of a model. The figure below highlights questions that should be answered, defining requirements for a model/models:",What describe the behavior and attributes of a model?,Model requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Model requirements describe the behavior and attributes of a model. The figure below highlights questions that should be answered, defining requirements for a model/models:",What is the figure below that highlights questions that should be answered?,Model requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",Figure 1. Defining Model Requirements,What is the definition of Model Requirements?,Figure 1
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Reports and Dashboards: ""Reports are often considered as a solution for business intelligence projects. If the college needs to generate reports, the project team will work with the college to define the requirements for reports by inquiring about (1) the current reports used in the college and the changes that need to be made to the reports, (2) data sources for the reports, (3) considerations for users generating reports i.e. how can an authorized user customize a report to display the data they need?"".",What is often considered as a solution for business intelligence projects?,Reports
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Reports and Dashboards: ""Reports are often considered as a solution for business intelligence projects. If the college needs to generate reports, the project team will work with the college to define the requirements for reports by inquiring about (1) the current reports used in the college and the changes that need to be made to the reports, (2) data sources for the reports, (3) considerations for users generating reports i.e. how can an authorized user customize a report to display the data they need?"".",What does the project team work with the college to define?,The requirements for reports
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Reports and Dashboards: ""Reports are often considered as a solution for business intelligence projects. If the college needs to generate reports, the project team will work with the college to define the requirements for reports by inquiring about (1) the current reports used in the college and the changes that need to be made to the reports, (2) data sources for the reports, (3) considerations for users generating reports i.e. how can an authorized user customize a report to display the data they need?"".",How can an authorized user customize a report to display data they need?,
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements","Similar to traditional IT projects, requirements should also be gathered for analytic solutions that are packaged. i.e., not developed in-house or outsourced.",What should be gathered for analytic solutions that are not developed in-house or outsourced?,Requirements
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Data Requirements,Model Requirements",Reading: Requirements for an IIoT (Industrial Internet of Things) Predictive Maintenance Solution.,Reading: Requirements for an IIoT Predictive Maintenance Solution?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Data Visualization,,Please see the Data Visualization primer on Sail().,What is the Data Visualization primer on Sail()?,
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Data Visualization,,Please see the Data Visualization primer on Sail().,What does the data visualization primer appear on?,Sail
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?",What does data science often include?,descriptive analysis
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?",What is the prevalence of prescriptive analytics?,continues to grow
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?",How is data science now something that drives decision-making?,Sometimes without any input from a human.
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.","When discussing accountability, we are not specifically looking for someone to blame when something goes wrong?",
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.",What do we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically according to values or principles we wish them to adhere to?,Respectively and ethically according to values or principles we wish them to adhere to.
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.","When thinking about accountability mechanisms, we need to think about who specifically carry that responsibility?",who
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.",When did Madeleine Elish write a very fascinating article?,2016
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.",What is the concept of the moral crumple zone?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.",How are systems designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system?,moral crumple zone
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.",When are automotive crumple zones designed to be destroyed?,In an accident
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesnt happen again. We care about both.",What is the key takeaway from accountability?,It is not just about finding who is to blame; blame can be engineered and planned in
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesnt happen again. We care about both.",What is accountability about?,Little decisions made by a group of people created a system.
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesnt happen again. We care about both.",How can blame be engineered and planned in advance?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.",In what section did we talk about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data?,In the previous section
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.",What is attained in the decision-making process by designing and implementing data systems that are transparent and auditable?,Accountability
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place.","When an ethical concern arises in a data science solution, what means transparency?",Disclosure of the involvement and actions of human actors
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place.","What means disclosing the involvement and actions of human actors, the data being used and its source?",Transparency
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as cWhy am I seeing this ad?d (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.",What have data scientists used in the past to justify the outcome of a data science solution?,Human involvement or lack of human involvement
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as cWhy am I seeing this ad?d (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.",What is cWhy am I seeing this ad?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",Figure 1. Why am I seeing this ad? (Source: LinkedIn),What is the name of the ad?,What Is The Name of the Ad?
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",Figure 2. High-level Design of cWhy am I seeing this ad?d Initiative (Source: LinkedIn),What is a high-level design of cWhy am I seeing this ad?,Initiative
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Figure 2 shows the flow of control of cWhy am I seeing this ad?d on LinkedIn, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIns effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.",What is the flow of control of cWhy am I seeing this ad?,Figure 2
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Figure 2 shows the flow of control of cWhy am I seeing this ad?d on LinkedIn, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIns effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.",How are the matching and standardization modules displayed to the users?,In the backend
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Figure 2 shows the flow of control of cWhy am I seeing this ad?d on LinkedIn, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIns effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.",What is it likely that all users of LinkedIn would go through the details of the algorithm that decides what?,Which ads to show
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:",The title of this section is taken from what article?,2016
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:",The authors challenge the ideal of what?,transparency
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:",What is the title of the article in 2016?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",Transparency can privilege seeing over understanding,Transparency can privilege seeing over understanding what?,the difference between the two types of frauds
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations",Transparency can privilege seeing over understanding,What can privilege viewing over understanding over understanding?,Transparency
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Besides these limitations, other pitfalls of creverse-engineeredd as a strategy for transparency:",What are the other pitfalls of creverse-engineered as a strategy for transparency?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Set reasonable expectations to disclose what is known. While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really dont know exactly how it works. We dont know what we dont know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.",What does a customer wonder why their favorite product is being discontinued?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Set reasonable expectations to disclose what is known. While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really dont know exactly how it works. We dont know what we dont know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.",What could have been acting logically but not illogically?,The decision-maker
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.","Who wrote ""Complexity distributes responsibility""?",Joseph Weizenbaum
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.",What is the subject of a computer program that gets to a certain level of complexity?,Ex-ante versus post hoc accountability
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.",How is it difficult or impossible to identify who is responsible for which component of the system?,complex
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.","When data science projects become increasingly complex, there are limitations on what?",deciding who should be responsible when something goes wrong
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.",What is a one-way street without a critical audience?,transparency
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.",What can lead to even greater ethical failures?,Transparency without a proper audience
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.",Transparency requires disclosures from what?,the data scientist as well as critical audiences
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",What is another component of accountability in data science?,Auditing
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",What is auditing used in social science research as an experimental test to discover if a system is doing what it was intended to do and if it results in desirable or undesirable consequences?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",In what decade was auditing developed by economists at the Department of Housing and Urban Development?,1970s
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.",How many people were sent to apply for the same apartment at the same time?,Two
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.",What has been used in the US to diagnose employment and racial discrimination?,Auditing
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.",What is another example of how audit studies were conducted?,A famous field experiment on labor market discrimination
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.",How did the experimenter use made-up names that were likely to be associated with a particular race or gender?,
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In data science applications, when transparency might not be feasible due to the protection of trade secrets or prevention of the system being gamed by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.","In data science applications, when transparency might not be feasible due to protection of trade secrets or prevention of the system being gamed by bad actors, what is auditing a counterpart to?",transparency for accountability
Collecting and Understanding Data,Ethics of Data Science,Accountability,"Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations","In data science applications, when transparency might not be feasible due to the protection of trade secrets or prevention of the system being gamed by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.",What can auditing be performed by an internal team whose job is to think through security vulnerabilities within their organization?,Yes.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.","What could refer to data from multiple sources like speech, image, video, and text data?",Language data
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.",Language processing tasks applied to preprocessing this depend largely on what?,form of data and its source
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.",The rest of the module focuses on what type of data as input and output?,Text
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.","What is the process of splitting an input sentence, paragraph, or entire document into a list of tokens?",tokenization
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.","What does the string sentence ""Today is a good day"" yield when tokenized?","Day', 'is', 'is', 'good', and"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.","In English, tokenizers choose to split what?",Contracted words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.",What is a process that can be used for tokenization in other languages?,word segmentation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.",What is an example of a word segmentation that is used to divide characters into individual words?,Word segmentation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.",What are words that belong to a closed class of words in the vocabulary of a language that do not carry meaning but function in a sentence to get the syntactic relations right?,Stop words
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.",What are Typical stop word lists in English words?,"Ca,d cthe,d con,d etc"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.","When tokenization is done, one may need to remove all such words in what process?",stop word remove
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.",What is a common language processing task?,Spelling checking and correction
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.",What are some languages with richer morphology called?,"German, Hungarian, Finnish, Turkish, and Arabic"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d",What type of analysis typically segments words into their constituent morphemes?,Morphological
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d",What would a morphological analyzer need to know about the root words cstopd and the suffix c-ingd?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d","When the following suffix starts with a vowel, what would represent a word like ceasy+estd with?",stop+Verb+Past
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.",What type of languages have many other orthographical processes usually rooted in phonology?,Morphologically complex languages
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.",What is a common term for a phonological language?,grammatical complex
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.,What are the state-of-the-art tools for morphological analysis relying on?,The well-established computational formalism of finite state transducers.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.,What are finite state transducers that take in a description of the root and affix lexicon?,Words in a language.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.",What is a clighterd operation called?,stemming
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.",What is the term for heuristically stripping off known word endings to get to a base word that can be used as a proxy for the word?,Stemming
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.","In what language does Stemming map ""change,"" ""change"" and ""changes"" to ""chang?""",English
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.",What algorithm is used for stemming for English?,Porter Stemmer
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.",What is a slightly more accurate version of stemming called?,Lemmatization
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.",What does lemmatization do?,A more informed version of stemming
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.",What would a full morphological analyzer do?,Lemmatization
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.",What would be a tool of choice for languages with complex morphology?,A full morphological analyzer
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.",What is a morphological interpretation of a word?,A lemma
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.",What is an additional syntactically relevant information?,Any word morphological interpretation
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.",What type of information has been coded as part of speech tags for English?,Morphological
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.",What is the de facto part of the coding scheme for English called?,Penn Treebank part-of-speech tag set
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.",The universal dependencies project has established a smaller common tag set for what language?,English
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:",What does the Penn Treebank convention assign short symbols to words as a part-of-speech category?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:",What would cbooksd get?,VBN
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:",How many possible parts of cwordd have?,6
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Adverb (RB) c...up and down Floridad,What is the name of the adverb?,RB
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Particle (RP) c ..keep the ball downd,What c..keep the ball downd?,Particle (RP)
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Particle (RP) c ..keep the ball downd,What is the name of the ball that is a part of the RP?,Keep the ball downdoo.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Preposition (IN) c..down the centerd,Preposition (IN) c..down what is the centerd?,centerd
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Preposition (IN) c..down the centerd,What is the preposition for the center of the center?,IN
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Adjective (JJ) c..down payment..d,Adjective (JJ) c..down payment..d d. What is the term for adjective payment?,Down payment
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Verb (VBP) cwe down five glasses of beer every nightd,How many glasses of beer does VBP cwe every night?,Five
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Verb (VBP) cwe down five glasses of beer every nightd,How many glass of beer do VBP drink every night?,Five
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Verb (VBP) cwe down five glasses of beer every nightd,What is the name of the verb that clives five glasses of a beer every nightd?,VBP
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Noun (NN) cthey fill the comforter with downd,Who fills the comforter with downd?,Noun (NN)
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Noun (NN) cthey fill the comforter with downd,What do nouns fill?,Comforter
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,","In general, almost every word in an English sentence will have multiple POS tags?",Yes.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,","In a rather artificial sentence, what type of tag does a sentence have?",POS tag
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).",What is a modal verb?,MC
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).",What is the second ccand?,a tenseless verb (VB)
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).",How is the third kand a singular noun?,NN
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).,What is the task of determining the contextually correct POS tag for a word in a sentence called?,part-of-speech-tagging (POS tagging)
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).,What is part-of-speech-tagging?,A word in a sentence
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.",What is typically done with a sequence-to-sequence mapping approach?,POS tagging for English
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.",The sequence of the words and the sequence of POS tags coming out is usually done with what?,a sequence-to-sequence mapping approach
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.",How are POS tagging methods trained?,Manually tagged data from the Penn treebank.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.",What were the early approaches for POS tagging for English based on?,Hidden Markov Models
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.",What did Hidden Markov Models use to handle previously unseen words?,Standard training data.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.",How did the Viterbi algorithm select the most likely sequence of tags for words in an input sentence?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.",What was used for Conditional Random Fields?,More sophisticated but computationally more expensive approaches.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.",What is the name of the machine learning approach that can be used for sequence-to-sequence transformation?,Conditional Random Fields
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.",How can recurrent neural networks be used?,any machine learning approach
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.",What does NER seek to identify?,The span and the types of these named entities
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.",What is the name of the entity that is not necessarily named?,NER
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.",How can NER be considered?,Under the same umbrella.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Figure 1. Named Entity Recognition with spacy.,Figure 1 Named Entity Recognition with spacy?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.",What is another example of sequence-to-sequence transformation?,NER
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.",How many symbols do we have for each named entity category?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.",What is a token that begins a person-named entity?,B-PER
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.",What is the name of a token that is not colored?,O label
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.",What would cJusticed and cDepartmentd get?,I-ORG label
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.",What does the sequence-to-sequence transformation take in?,The sequence of tokens
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.",What do sophisticated named-entity recognizers use to assign one of the labels to each of the tokens?,A classifier
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d",What is the name of the classifier from cJurafsky and Martin?,NER
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.,What is used to train a classifier?,A training set of labeled named entities
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.,What does NER perform on new sentences?,Performs NER on new sentences.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.",How many metrics are used to evaluate NER systems?,Three
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.",What is shown in the figure below?,"precision, recall, and (F_1)"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.",What is the process of assigning syntactic structures to a sentence?,Parsing
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.",What is a constituency parser based on?,typically context-free grammar
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.",How many tokens are encoded in the input sentence in the CYK algorithm?,n
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:,How many parse trees would the parser generate?,Two
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:,What is the output sentence for a boy with a flower?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",but it would not be able to tell which of these parses is the ccorrectd one.,What type of parse is the ccorrected?,parse
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.",What can be used to build statistical parsers?,Treebanks
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.",How can the CYK algorithm be augmented to produce the most likely parse of an input sentence?,In the same amount of time
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.",What is the most commonly used method for parsing?,Transition-based dependency parsing
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.",What makes one pass over the input words and decides in (O(n) on the dependency relations between lexical items?,A stack
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.",What is a transition-based parse?,A classifier
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing","Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.",What can be trained by using training data obtained by transforming a dependency treebank into?,A sequence of parser actions
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.,What type of algorithms do other dependency parsing algorithms employ?,More sophisticated algorithms
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing",Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.,What do Graph-based methods label a graphical representation of the sentence and assign weights to each arc?,Word-to-word relations
Deep Learning and Model Deployment,Model Deployment,Quiz 10,,,What does nan do?,He is a nurse
Deep Learning and Model Deployment,Model Deployment,Quiz 10,,,What is the name of the nnan?,The nan name is Peter Durning.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product",What is a Process?,AI Philosophy
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product",What is an AI philosophy?,A process
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product",What is a Process?,AI Philosophy
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Philosophy: A Process, not a Product",What is an AI philosophy?,A process
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",What requires understanding the organization needs and its readiness to incorporate and support any analytical solution?,The provision of analytical solutions to an organization.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",A good solution will fail if the organization and its stakeholders are not equipped to support what?,the solution
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",What is important when engaging with potential clients seeking analytical solutions?,To assess the organizations readiness.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point, the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided adequate information about the companys data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.",What is the name of the organization that has identified all the current sources of data to be considered?,Data Science Ready
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point, the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided adequate information about the companys data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.",What has been normalized and integrated into a form that supports data science?,The data
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point, the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided adequate information about the companys data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.",The Case in point scenario showed that Monro Inc. was data science ready?,
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"Data Science Enabled. A preliminary analysis indicates that the data supports the desired analytic objectives (useful correlations are identified, predictive models prove to be accurate enough, etc.). At this point, the organization can claim that it is ready to use data science to influence the capabilities of its workflows, products, and services.",What is Data Science Enabled?,
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"Data Science Enabled. A preliminary analysis indicates that the data supports the desired analytic objectives (useful correlations are identified, predictive models prove to be accurate enough, etc.). At this point, the organization can claim that it is ready to use data science to influence the capabilities of its workflows, products, and services.",What is a preliminary analysis of data that indicates it supports the desired analytic objectives?,
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Ready. The organization has determined how to leverage the insights from data science (e.g., predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow, product, or service. At this point, the organization can claim that it understands how to incorporate AI into its workflows, products, and services to provide enhanced capabilities for end-users.",What is the purpose of AI Ready?,To integrate insights from data science.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Ready. The organization has determined how to leverage the insights from data science (e.g., predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow, product, or service. At this point, the organization can claim that it understands how to incorporate AI into its workflows, products, and services to provide enhanced capabilities for end-users.",What does AI Ready mean?,Integrated intelligence ready means a continuous improvement effort.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Ready. The organization has determined how to leverage the insights from data science (e.g., predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow, product, or service. At this point, the organization can claim that it understands how to incorporate AI into its workflows, products, and services to provide enhanced capabilities for end-users.",How can AI help end-users?,enhanced capabilities
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Enabled. The organization has deployed the new software in a relevant context and is able to directly measure the impact (e.g., increased sales). At this point, the organization can claim that it has implemented an AI solution and is gathering feedback to show that it really works with real end-users. An organization can introduce the data science decision into a real-world setting and measure if this implementation works.",What is an organization able to measure the impact of AI Enabled?,Increased sales
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Enabled. The organization has deployed the new software in a relevant context and is able to directly measure the impact (e.g., increased sales). At this point, the organization can claim that it has implemented an AI solution and is gathering feedback to show that it really works with real end-users. An organization can introduce the data science decision into a real-world setting and measure if this implementation works.",What can an organization claim that it has implemented an AI solution?,Yes.
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product",,"AI Enabled. The organization has deployed the new software in a relevant context and is able to directly measure the impact (e.g., increased sales). At this point, the organization can claim that it has implemented an AI solution and is gathering feedback to show that it really works with real end-users. An organization can introduce the data science decision into a real-world setting and measure if this implementation works.",Who can measure if the data science decision works?,An organization
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.",What are the general necessities for a company to remain competitive or expand in the long term?,Business needs
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.",What can be stated in broader terms?,Business needs
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.",When will the data science team work with a client?,At the start of a data science project.
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"To meet a particular need, the company will identify one or more specific business objectives it can work towards. These objectives are stated fairly concretely and often have time periods associated with them, after which, somewhat simplified, they will be considered reached (in case of success) or not reached (in case of failure). For example, a business may want to increase sales of a particular product by a certain margin through online advertising, be smarter about scaling production to market fluctuations, reduce travel costs for its logistics, or get a better sense of current trends in its customer base for better product development.",What is the purpose of a specific business objectives?,To achieve a specific goal.
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"To meet a particular need, the company will identify one or more specific business objectives it can work towards. These objectives are stated fairly concretely and often have time periods associated with them, after which, somewhat simplified, they will be considered reached (in case of success) or not reached (in case of failure). For example, a business may want to increase sales of a particular product by a certain margin through online advertising, be smarter about scaling production to market fluctuations, reduce travel costs for its logistics, or get a better sense of current trends in its customer base for better product development.",What are some of the objectives that a company can work towards?,
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"To meet a particular need, the company will identify one or more specific business objectives it can work towards. These objectives are stated fairly concretely and often have time periods associated with them, after which, somewhat simplified, they will be considered reached (in case of success) or not reached (in case of failure). For example, a business may want to increase sales of a particular product by a certain margin through online advertising, be smarter about scaling production to market fluctuations, reduce travel costs for its logistics, or get a better sense of current trends in its customer base for better product development.",How is a business able to increase sales by a certain margin?,Online advertising
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them. However, the objectives in question will typically be framed in terms of business terminology and assumptions. A data science team will first engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem, as well as the available data and resources, it will work with the client to develop a solution vision. This vision is effectively a walkthrough of what a full data-driven solution to the problem could look like. Based on this, one can identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.",What can data science methods be used to facilitate the company's efforts to meet?,Business needs and objectives
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them. However, the objectives in question will typically be framed in terms of business terminology and assumptions. A data science team will first engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem, as well as the available data and resources, it will work with the client to develop a solution vision. This vision is effectively a walkthrough of what a full data-driven solution to the problem could look like. Based on this, one can identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.",What will the objectives in question typically be framed in terms of?,Business terminology and assumptions
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them. However, the objectives in question will typically be framed in terms of business terminology and assumptions. A data science team will first engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem, as well as the available data and resources, it will work with the client to develop a solution vision. This vision is effectively a walkthrough of what a full data-driven solution to the problem could look like. Based on this, one can identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.",Who will first engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts?,A data science team
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"In this context, an analytic objective states, in specific terms, what the data science project should produce (insight, resource, model, etc.) and serves as the main success criterion for the team. On the technical side, it drives the derivation of requirements for the project. On the business side, it is critical that analytical objectives stay firmly connected to the business objectives they facilitate.",What is the main success criterion for a data science project?,An analytic objective
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"In this context, an analytic objective states, in specific terms, what the data science project should produce (insight, resource, model, etc.) and serves as the main success criterion for the team. On the technical side, it drives the derivation of requirements for the project. On the business side, it is critical that analytical objectives stay firmly connected to the business objectives they facilitate.",What does an analytic objective do on the technical side?,Drives the derivation of requirements for the project.
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,,"In this context, an analytic objective states, in specific terms, what the data science project should produce (insight, resource, model, etc.) and serves as the main success criterion for the team. On the technical side, it drives the derivation of requirements for the project. On the business side, it is critical that analytical objectives stay firmly connected to the business objectives they facilitate.",How is an analytical objective firmly connected to business objectives?,It is critical that analytical objectives stay firmly connected to business objectives they facilitate.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they dont take the context of relative positivity between words into consideration. For example, cgood food and terrible serviced and cterrible food and good serviced mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.",What are methods like Bag-of-words and term frequency?,Simple yet highly effective techniques
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they dont take the context of relative positivity between words into consideration. For example, cgood food and terrible serviced and cterrible food and good serviced mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.",What does cgood food mean?,different things
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).",What is a category of language models that are constructed by calculating?,n-gram probabilities
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).",What is the conditional probability that the n-grams last word follows the particular n-1 gram?,An n-grams probability
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).",The Bi-gram model for the phrase cgood food and terrible serviced would require what?,modeling conditional probabilities of every two consecutive words.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"\\[ P(W_{n}|W_{n-1})=\\frac{P(W_{n-1},W_{n})}{P(W_{n-1})} \\]","[ P(W_n|W_n-1)=fracP(W-n1,W_-n)?",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Where the probability \\(P()\\) of a token \\(W_{n}\\) given the preceding token \\(W_{n-1}\\) is equal to the probability of their bigram \\(P(W_{n-1},W_{n})\\), divided by the probability of the preceding token.",What is the probability of a token (W_n) given the preceding token?,(P())
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences cBig blue skies look appealing.d and cColorless green ideas sleep furiously.d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one  a sentence she can say someone could use.",What can be interpreted as a useful measure of the plausibility of a sentence in a language?,Overall probability
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences cBig blue skies look appealing.d and cColorless green ideas sleep furiously.d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one  a sentence she can say someone could use.",What are the sentences cBig blue skies and cColorless green ideas look appealing?,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.",What can be estimated from large-scale text corpora?,Probabilities
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.",What is used to estimate probabilities for n-grams that have not been observed in the training data?,Various smoothing methods.
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,These days people build (classical) language models using well-established toolkits:,What is the name of the language model that people build today?,Classical
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,These days people build (classical) language models using well-established toolkits:,What are the tools that people use to build language models?,Well-established toolkits
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,SRILM Toolkit: https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit,What is the SRILM Toolkit?,It is a self-destruct toolkit for sri language-modeling
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html,CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/dorcas/toolkit_documentation.html?,http://www.cs.cmu.edu/dorcas/
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/,What is the KenLM Language Model Toolkit?,https://kheafield.com/code/kenlm/
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/,http://kheafield.com/code/kenlm/?,KenLM Language Model Toolkit
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,,"Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.","Which toolkit provides executables and/or API and options to build, smooth, evaluate and use language models?",Each
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Functionality is the most vital characteristic of an abstract data type that every user must understand. It is often communicated via an API that specifies the operations and input-output data for each API call, as well as a description of the function that the call delivers.",What is the most vital characteristic of an abstract data type that every user must understand?,Functionality
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Functionality is the most vital characteristic of an abstract data type that every user must understand. It is often communicated via an API that specifies the operations and input-output data for each API call, as well as a description of the function that the call delivers.",What is often communicated via an API that specifies the operations and input-output data for each API call?,Functionality
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"A specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. Operations such as vector addition or the dot product of two vectors can be then implemented using this representation.",What can be used to implement multiple abstract data sets?,A specific concrete data structure.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"A specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. Operations such as vector addition or the dot product of two vectors can be then implemented using this representation.",How can a pair of real numbers implement two-dimensional vectors in a cartesian vector space?,A concrete structure.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"A specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. Operations such as vector addition or the dot product of two vectors can be then implemented using this representation.",What type of operations can be implemented using this representation?,Vector addition or the dot product of two vectors.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Concrete data structures are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.",Concrete data structures are used to implement what?,Abstract data types correctly
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Concrete data structures are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.",What determines the cost of the abstract data?,This implementation.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.",What is the study of how an algorithm grows as a function of the size of the input data to the algorithm?,Asymptotic analysis
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.",What is a basic idea to model how the growth rate of two functions compares to large input?,
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.","When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on what?",Size of the input
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Big-O Notation: We say a function f(n) is O(g(n) if there are positive constants c and n0 such that for n  n0 f(n)  cg(n). That is, beyond n0, f(n) grows at most as fast as c  g(n). That is, c  g(n) always dominates f(n) in growth.",What does a function f(n) mean if there are positive constants c and n0?,O(g)
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"Big-O Notation: We say a function f(n) is O(g(n) if there are positive constants c and n0 such that for n  n0 f(n)  cg(n). That is, beyond n0, f(n) grows at most as fast as c  g(n). That is, c  g(n) always dominates f(n) in growth.",What is c g (n) always dominates in growth?,f(n)
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"It is important to understand how the time and memory cost grow with the size of the input, i.e., the time and space complexity for an algorithm. There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime).",What is important to understand how the time and memory cost grow with the size of the input?,It is important to understand how the time and memory cost grow with the size of the input.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"It is important to understand how the time and memory cost grow with the size of the input, i.e., the time and space complexity for an algorithm. There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime).",What is often a trade-off between runtime and space complexity?,There is often a trade-off between runtime and space complexity.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,Learning to use the right data structure for different use cases is an essential skill for writing efficient programs and building complex applications.,What is an essential skill for writing efficient programs and building complex applications?,Knowing to use the right data structure for different use cases.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,Learning to use the right data structure for different use cases is an essential skill for writing efficient programs and building complex applications.,What does learning to use the right data structure for different use cases mean?,An essential skill for writing efficient programs and building complex applications.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"An algorithm represents data and relations between data items using a variety of abstract data types. Some of the most commonly used abstract data types include Sequences, Sets, Tables, Graphs, Trees, and Priority Queues.",What is an algorithm that represents data and relations between data items?,An algorithm that represents data and relations between data items.
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,,"An algorithm represents data and relations between data items using a variety of abstract data types. Some of the most commonly used abstract data types include Sequences, Sets, Tables, Graphs, Trees, and Priority Queues.",What are some of the most commonly used abstract data types?,"Sequences, Sets, Tables, Graphs, Trees, and Prior"
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]",What is all you need to know about neural information processing systems?,Attention
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]",What is the name of the book that explains the importance of attention?,Advances in neural information processing systems
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.",What module was introduced earlier in the course?,Transformers
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.",What is the main focus of the research paper module?,Understanding how the paper was presented to the research community and which areas it contributes to.
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Googles services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.","What is the name of the group of researchers and engineers from Google Brain, Google Research, and the University of Toronto?",The authors
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Googles services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.",When did the authors leave their affiliations to find their own NLP startups?,At the time of this paper.
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.,What is the focus of the paper?,Language translation
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.,What does the paper pivot from the RNN/LSTM paradigm?,A new transformer architecture
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.",In what year did deep learning resurgence occur?,2012
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.",What is LSTM designed to handle?,Sequential data
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.",How many times more matrix computations are required for RNNs?,three times more
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There are two primary innovations from the paper.,How many innovations are there from the paper?,Two
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",There are two primary innovations from the paper.,What are the two primary innovations?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.",What is a novel way of representing word order in a sentence?,Positional Encoding
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.",What does an RNN know that cliked comes after?,cId
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.",How do Transformers construct inputs that consist of both the original tokens and their index locations?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.",What is a mechanism that relates different positions of a single sequence to compute a representation of this sequence?,Self-attention
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.",What does self-attention allow a neural network to understand a word in the context of other words around it?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.",Self-attention has been used in prior works in conjunction with what?,recurrent or convolutional neural networks
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):",What model architecture is proposed for language translation?,Transformer
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):",What is the output sequence of tokens?,A French sentence
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., \\(PE_{(pos, 2i)}\\) or \\(PE_{(pos, 2i+1)}\\) to yield the word vector with positional information for each token.",Step 1: Convert each input sequence token to what?,its vector embedding
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., \\(PE_{(pos, 2i)}\\) or \\(PE_{(pos, 2i+1)}\\) to yield the word vector with positional information for each token.",What is the word vector with positional information for each token?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.",Step 2: Pass the input sequence word vectors into the encoder block?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.",How many times is the attention unit generated for every token in the output sequence?,8
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.",What is the name multi-headed?,Attention unit
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",Step 3 is similar to Step 1 but carried out on the output sequence tokens.,Step 3 is similar to Step 1 but carried out on what?,output sequence tokens
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).",Step 4: Pass the output sequence word vectors into the decoder block?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).",What is the first attention unit that generates an attention vector for every token?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).",How much is the token related to other tokens before and included in the same sentence?,How much
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).",Step 5: The output from step 4 is fed to what?,a standard linear classifier
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).",The layer outputs the probability that each word in French is the next output?,
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).",What is the word with the highest probability value predicted to be?,The next output token
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.",How is batch normalization applied after every unit?,To smoothen the data and make it easier to learn with larger learning rates.
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.",What is a way to smooth the data?,Batch normalization
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.",What happens to the output sequence during inference?,It is replaced by an empty sequence with only a start-of-sequence
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.",What is the result of an empty sequence with only a start-of-sequence token?,prediction goal
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.",The transformer will predict the next token one by one and add each predicted token to what?,The output sequence
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.,How many tasks was the above architecture evaluated?,Two
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?",The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.,What is a description of the training process and hyperparameters used in the study?,Section 5
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.",What is the name of the novel Transformer architecture?,The name is RNN/LSTM
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.",How many times has the paper been cited in Google Scholar?,"47,000"
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?","The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.","What type of model is BERT, GPT-3, and T5 used?",Pre-trained Transformer
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.",What is critical when it comes to human subjects research?,Informed consent
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.",What is the right of the individual to know that they are being studied?,Informed consent
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.","In this module, we will explore what concept of informed consent?",The concept of informed consent is a concept that is relevant to human subjects research.
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.",When did the study begin?,1932
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.",How many African American men were recruited and matched against 201 uninfected subjects?,399
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.",What was the name of the study conducted at Tuskegee University?,United States Public Health Service (USPHS)
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,Figure 1. Scenes from the Tuskegee Syphilis Study. (Source: https://www.rmpbs.org/),Figure 1. Scenes from the Tuskegee Syphilis Study?,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.",The subjects were instructed to make regular visits to what clinic?,The clinic
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.",What was the purpose of a cspecial free treatment?,To study the neurological effects of syphilis.
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",What drug was a safe and effective treatment for syphilis?,Penicillin
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.","When did the Department of Health, Education, and Welfare terminate the study?",1972
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",How many of the test subjects were still alive at the time?,74
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.",What is the process called when a study is being done on a human subject?,informed consent
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.",What was the Tuskegee Study of Untreated Syphilis in the African American Male?,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.",What did the subjects not know about the study of neurological effects of syphilis?,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.",The subjects were misinformed about possible treatments for what?,syphilis
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.",What did the researcher evaluate the benefit to society or science versus the harm to the participants?,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.","What is a fundamental principle of informed consent that the party facing potential harm has the right to decide on their own the balance between the benefits to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face?",
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.",What is informed consent based on?,Fair Information Practice Principles (FIPPs)
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.",What was the Privacy Act created in 1974?,Fair Information Practice Principles (FIPPs)
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.",When was informed consent developed?,1974
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.",Who is required to say cI accept before the process begins?,users
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.",What does repurposing of data mean?,Unethical
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.",What is a caveat about a patient being studied to help future patients?,Medical data
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.",How is retrospective data analysis called?,"Or, retrospective data analysis is called retrospective data analysis."
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.",What is the problem with informed consent?,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,,"In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.",What is a crucial question for conducting meaningful and ethical data science research?,
