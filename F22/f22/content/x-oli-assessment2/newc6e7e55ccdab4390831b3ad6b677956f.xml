b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE assessment PUBLIC "-//Carnegie Mellon University//DTD Assessment MathML 2.4//EN" "http://oli.web.cmu.edu/dtd/oli_assessment_mathml_2_4.dtd"><assessment xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" id="newc6e7e55ccdab4390831b3ad6b677956f" recommended_attempts="1" max_attempts="1"><title>Quiz 7</title><page id="b95415be93634eb58c8e0f621a7629f6"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="eecefeebdbed485b9e5b506d9b552511"><title>Question 1</title><content available="always"><p id="e2e8620ee6ed40cbbe5d0e306dc39643" /></content><multiple_choice id="da16d7be1f234c99b0af6475f8edf24f" grading="automatic" select="single"><body><p id="a2af9df53e374595966a151c39163562">Which of the following is NOT among the reasons for people&apos;s resistance to using ML models to aid decision-making, based on the Introduction in the research paper by Poursabzi-Sangdeh et al. (2021)?</p></body><input shuffle="true" id="b479de15963b4ddd8dae9ebfead3efaf" labels="false"><choice value="aa44d5ab44464fbdba8a8a9ce8dcfe53">ML models are not accurate enough to aid people in decision-making.</choice><choice value="f73397ccb0814149b078ec15ceab16c5">People find it hard to understand models, including the data they rely on.</choice><choice value="e38967cfdcce43b2844e643f56e875e1">Models do not rely on the right information.</choice><choice value="beb0f661e94a4ed692e1db73f19bacdb">Models might be unfair.</choice><choice value="bc4cfa08c2034fb18995ff3ae33989ac">All of the above reasons are mentioned in the paper to justify why people are resistant to using ML models to aid decision-making.</choice></input><part id="c31eb97d15dc43fe9035e8babdab4030"><response match="aa44d5ab44464fbdba8a8a9ce8dcfe53" score="10"><feedback><p id="ecd864893cab4c1ab9a655cdbcae4aa7">Correct. This is not one of the reasons mentioned in the paper. In fact, the paper claims the opposite, that ML models are even more accurate than people in many tasks.</p></feedback></response><response match="f73397ccb0814149b078ec15ceab16c5" score="0"><feedback><p id="bff618a32508493697ea37b242e34e20">Incorrect. Paragraph 3 of the introduction mentions that not understanding models contributes to people\xe2\x80\x99s resistance to using models.</p></feedback></response><response match="e38967cfdcce43b2844e643f56e875e1" score="0"><feedback><p id="e92ad800dc4a4f8bbb37d696c813b7e1">Incorrect. Paragraph 3 of the introduction mentions that concerns over models not using the right information contribute to people\xe2\x80\x99s resistance to using models.</p></feedback></response><response match="beb0f661e94a4ed692e1db73f19bacdb" score="0"><feedback><p id="a8bcb64ae0714836af4cb6fdf2a9b6ab">Incorrect. Paragraph 3 of the introduction mentions that concerns over models behaving in ways that are unfair contribute to people\xe2\x80\x99s resistance to using models.</p></feedback></response><response match="bc4cfa08c2034fb18995ff3ae33989ac" score="0"><feedback><p id="cf8b77ac17e94670bc4a87f3f98a9575">Incorrect. \xe2\x80\x9cML models are not accurate enough to aid people in decision making\xe2\x80\x9d is not mentioned in the paper. In fact, the paper claims the opposite, that ML models are even more accurate than people in many tasks.</p></feedback></response></part></multiple_choice><multiple_choice id="d252ffc3cbbb406890c756a849e88678" grading="automatic" select="single"><body><p id="b36e3f71e0594cb1bcfe5d9b09292225">In the research paper by Poursabzi-Sangdeh et al. (2021), which of the following findings in experiments 1 and 2 were not replicated in experiment 3?</p><ol id="c4366e2d930f49478ede74cfa7d967de"><li><p id="d5b33e34978745ffa6545516902b95e4">(Simulation) The finding was that people who saw the clear, two-feature model could more closely simulate the prediction of the model shown to them than those in other conditions.</p></li><li><p id="f6fdcb08e2684e3db2eb64ce3a9d36c9">(Deviation) The finding was that people who saw the clear, two-feature model did not follow the model prediction more than those who saw the black-box, 8-feature model when it was beneficial for them to do so.</p></li><li><p id="d83cd318d6de4d6188ad0e94d72374be">(Detection of mistakes) The finding was that participants who saw the clear models were not able to identify the mistakes on unusual data points as well as those who saw the black-box models.</p></li></ol></body><input shuffle="true" id="e5400fc551d848049d0460c1c6ef90d0" labels="false"><choice value="f3b3e334cd644ab7a9e276da33d00438">Finding 1</choice><choice value="e390533391984bd2a678a9a6c1960def">Finding 2</choice><choice value="d0d59c76a9584b64891eaf880f99a271">Finding 3</choice><choice value="d1a0de7ee3c24416bfd21bdfb15293c3">Finding 1 and 2</choice><choice value="a17696f33f744777a3955f37608fe5ef">Finding 1, 2, and 3</choice></input><part id="c59ca72a68e34540b82e91a232192aa5"><response match="f3b3e334cd644ab7a9e276da33d00438" score="0"><feedback><p id="c9cdb5a3247c492e84fe126ada1a5463">Incorrect. Experiment 3 did not report any finding for the simulation hypothesis.</p></feedback></response><response match="e390533391984bd2a678a9a6c1960def" score="0"><feedback><p id="db875bbf674f4107a9e87e6bd8ee9117">Incorrect. According to the first paragraph in section 5.2, this finding was replicated in experiment 3.</p></feedback></response><response match="d0d59c76a9584b64891eaf880f99a271" score="10"><feedback><p id="a0a41c11dbe040ca986a33029908082f">Correct. According to the third paragraph in section 5.2, this finding is different from what was shown in experiments 1 and 2.</p></feedback></response><response match="d1a0de7ee3c24416bfd21bdfb15293c3" score="0"><feedback><p id="d57d0db8039343088ea27fe0684ab53f">Incorrect. Experiment 3 did not report any finding for the simulation hypothesis.</p></feedback></response><response match="a17696f33f744777a3955f37608fe5ef" score="0"><feedback><p id="f37280e962384402b72189c637e599c4">Incorrect. Experiment 3 did not report any finding for the simulation hypothesis.</p></feedback></response></part></multiple_choice><multiple_choice id="a957ffa4d50d42caa42f7ef6b6e74eeb" grading="automatic" select="single"><body><p id="a786f259777a4c47b1d2ca951c0c9eb4">In experiment 4 of Poursabzi-Sangdeh et al. (2021), what was the authors&apos; explanation for why the focus message helped people who saw the clear model detect and correct the model&apos;s sizable mistake?</p></body><input shuffle="true" id="df7d58fe21404205b991b9c975208bbe" labels="false"><choice value="c2f04cb418454c1cada7b01a6d85b61b">The focus message alerted people to the unusual apartment features that they may have missed otherwise.</choice><choice value="a8b55864420542d3bc6ad9f03e83db75">The focus message explicitly mentioned that the model made a mistake that needs further review.</choice><choice value="c2d2661bba5e4892bccf572d0bb9cfd8">The focus message provided tips for the user to calculate the housing prices on their own and compare the results with the model\xe2\x80\x99s predictions.</choice><choice value="d4998b4d14ef4cfebeadf3d42eef9d10">The focus message reminded people that they have access to more features than the model does.</choice><choice value="ed38892e8d0b45e1be2be914567e3a4f">The author provided no explanation for the effect of the focus message.</choice></input><part id="bd5f49253b7c4ce28d6a55e34d84ec28"><response match="c2f04cb418454c1cada7b01a6d85b61b" score="10"><feedback><p id="d87cf304f7ae467d842c3c6a39fdee8d">Correct. People who saw the clear model were overwhelmed with information and missed the unusual apartment features, which the focus message reminded them of.</p></feedback></response><response match="a8b55864420542d3bc6ad9f03e83db75" score="0"><feedback><p id="edbade45103f4fb9a17fe25666537bf7">Incorrect. This was not the function of the focus message in experiment 4.</p></feedback></response><response match="c2d2661bba5e4892bccf572d0bb9cfd8" score="0"><feedback><p id="b930522855fc4bf4a88ce53fd28b499a">Incorrect. This was not the function of the focus message in experiment 4.</p></feedback></response><response match="d4998b4d14ef4cfebeadf3d42eef9d10" score="0"><feedback><p id="c57e1f08ce324825b3c4cd88ad98a2bf">Incorrect. This was not the function of the focus message in experiment 4.</p></feedback></response><response match="ed38892e8d0b45e1be2be914567e3a4f" score="0"><feedback><p id="d8c4a054bd944ec0859ab8cece9da2ff">Incorrect. People who saw the clear model were overwhelmed with information and missed the unusual apartment features, which the focus message reminded them of.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="fdc6a0952a26452da28b0fa0d1c775c2"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="d9f1b98d62484cda8c39a46c84aadb8d"><title>Question 2</title><multiple_choice id="b58c0889dcfe473fb5889790c396f1c1" grading="automatic" select="single"><body><p id="ae2bc614b0af40bbb45c597806824454">Doshi-Velez and Kim (2017) proposed levels for evaluating interpretability, including:</p></body><input shuffle="true" id="f5d1233b78374c6db2ef9580492b40c4" labels="false"><choice value="ed22e01fc17343f69b0278f8eae9bc25">Human, Function, and Application.</choice><choice value="e44a9bc8c81e4a18911f08ae7dd6024b">Application, Data, and Human.</choice><choice value="d297f93b032d4f599a6c83dfb873ef64">Application, System, and Human.</choice><choice value="cddec257f0c243d19efa4efa5614af25">System, Model, and Feature.</choice><choice value="fb947986960145ea8bcdc797dcb5eb7a">System, Model, and Human.</choice></input><part id="f7623e62c1ed431590f4d9e62bbecb9c"><response match="ed22e01fc17343f69b0278f8eae9bc25" score="10"><feedback><p id="d3c651e8c2fe4b62acb4d0fd591e42e6">Correct. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="e44a9bc8c81e4a18911f08ae7dd6024b" score="0"><feedback><p id="d0099a1a479c4617b5899c0c396fa35c">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="d297f93b032d4f599a6c83dfb873ef64" score="0"><feedback><p id="f7de988967114e0794258b32864c0f7e">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="cddec257f0c243d19efa4efa5614af25" score="0"><feedback><p id="b9c346a4dc08459fb70ff4270e6a01a9">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="fb947986960145ea8bcdc797dcb5eb7a" score="0"><feedback><p id="b6fdddd31ecc4c86b5ed59a25ee7fbf6">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response></part></multiple_choice><multiple_choice id="c5667b43b55e46e89c60b563fc4a5b1b" grading="automatic" select="single"><body><p id="c01c5e00334d40718a3ed6b4f6ede619">What is the Taxonomy of evaluation approaches for interpretability proposed by Doshi-Velez and Kim (2017)?</p></body><input shuffle="true" id="fa11b1768a4847229ff2b81b6703ad2e" labels="false"><choice value="db0d9c6a742d466da29964beac22cfca">Application-Grounded, Data-Grounded, and Human-Grounded.</choice><choice value="ce8db33038a64c9f92723a8eacfc44e0">Application-Grounded, System-Grounded, and Human-Grounded.</choice><choice value="c42eec7698134aa9b5c7f51e25c59ee1">System-Grounded, Model-Grounded, and Feature-Grounded.</choice><choice value="fff03ca2227f41acaf5cbb25c6df9b42">Human-Grounded, Functionally-grounded, and Application-Grounded.</choice><choice value="bffe913dfab141a684985e9ea4ea4187">System-Grounded, Model-Grounded, and Functionally-grounded.</choice></input><part id="bf932b75c3634cefbe59f2710d387bcb"><response match="db0d9c6a742d466da29964beac22cfca" score="0"><feedback><p id="ad87f2c58ebd41d4a419d8b4b18af082">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="ce8db33038a64c9f92723a8eacfc44e0" score="0"><feedback><p id="b51265da185f4e2badd898459a4efa9c">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="c42eec7698134aa9b5c7f51e25c59ee1" score="0"><feedback><p id="d87c035645cb4c1c886275c8a58c729b">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="fff03ca2227f41acaf5cbb25c6df9b42" score="10"><feedback><p id="d4d0bfc61d664fefb46ab73cacb8e210">Correct. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response><response match="bffe913dfab141a684985e9ea4ea4187" score="0"><feedback><p id="afd7816009c84ed9ba4ae0eb384ad313">Incorrect. Doshi-Velez and Kim (2017) propose a taxonomy for the evaluation of interpretability: application-grounded, human-grounded, and functionally-grounded.</p></feedback></response></part></multiple_choice><multiple_choice id="ff1c3059855e477f97a116642410da53" grading="automatic" select="single"><body><p id="e954f0c1986b4a92a1e0bf15ac6624e3">Which of the following methods were not recommended by Hall (2016) to retain interpretability while improving accuracy?</p></body><input shuffle="true" id="eaf258eca4484dbea5a5561893b441c2" labels="false"><choice value="d9ca5ac0967b4146a24045af04f18efe">Train and utilize black-box models as benchmarks.</choice><choice value="a8b841c5939847c59fb1f610deb7a956">In the deployment process, use black-box models.</choice><choice value="a4c22c6825b44ea1b6ebb1831d1e53c7">Using black-box approaches, create linear predictors.</choice><choice value="e50f203d2ba649a3a0d4572f8bb5d53e">Using variable importance measures, you can better explain black box models.</choice><choice value="d71912357d2a418baa967df11dfd270e">Train small interpretable ensemble models.</choice></input><part id="d9d50544880c4e6985fbfb4a98174ae2"><response match="d9ca5ac0967b4146a24045af04f18efe" score="0"><feedback><p id="a95fd36d41dd46bca1d21f8050c0978e">Incorrect. Hall (2016) recommended to Train black-box models and use them as benchmarks; Use different regression techniques; Use black-box models in the deployment process; Train small interpretable ensemble models; Create nonlinear predictors using black-box techniques; Explain black box models better using variable importance measures.</p></feedback></response><response match="a8b841c5939847c59fb1f610deb7a956" score="0"><feedback><p id="fa75e16dfacf494fbeb22c71139a4759">Incorrect. Hall (2016) recommended to Train black-box models and use them as benchmarks; Use different regression techniques; Use black-box models in the deployment process; Train small interpretable ensemble models; Create nonlinear predictors using black-box techniques; Explain black box models better using variable importance measures.</p></feedback></response><response match="a4c22c6825b44ea1b6ebb1831d1e53c7" score="10"><feedback><p id="a849998a9a724d849f8eea746dc72353">Correct. Hall (2016) recommended to Train black-box models and use them as benchmarks; Use different regression techniques; Use black-box models in the deployment process; Train small interpretable ensemble models; Create nonlinear predictors using black-box techniques; Explain black box models better using variable importance measures.</p></feedback></response><response match="e50f203d2ba649a3a0d4572f8bb5d53e" score="0"><feedback><p id="e0fa6dd7dc3746f983c116e10ec03651">Incorrect. Hall (2016) recommended to Train black-box models and use them as benchmarks; Use different regression techniques; Use black-box models in the deployment process; Train small interpretable ensemble models; Create nonlinear predictors using black-box techniques; Explain black box models better using variable importance measures.</p></feedback></response><response match="d71912357d2a418baa967df11dfd270e" score="0"><feedback><p id="fa0c0a303ad94a0190da00af7e03cc55">Incorrect. Hall (2016) recommended to Train black-box models and use them as benchmarks; Use different regression techniques; Use black-box models in the deployment process; Train small interpretable ensemble models; Create nonlinear predictors using black-box techniques; Explain black box models better using variable importance measures.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="ae5cf35c97244cf38e7f9f5361c21398"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="bea1e479b61d4e71b52e810cfc6f8118"><title>Question 3</title><content available="always"><p id="f603dfce34164464b5ba109127046d9e" /></content><multiple_choice id="a1272bd180e24671a0b0ddbff84a8ce6" grading="automatic" select="single"><body><p id="c46960377b0e44b799ca2e9b5b79c661">You are building a computer vision model to predict whether a patient has glaucoma (positive class label) or not using images from ophthalmic imaging. What evaluation metric would you want to maximize to ensure that as many patients with glaucoma as possible are identified and treated?</p></body><input shuffle="true" id="f1b3516f700a41e3b0f6aa68d876cea9" labels="false"><choice value="f9eb75d584374bec8616f4dfc269685f">Accuracy</choice><choice value="c8dae49b422f443bb86e2229ce4f8751">Precision</choice><choice value="f50796ad2e194f29a76118089d3391df">Recall</choice><choice value="f3ee384e7fe4464a9eba576982bb2237">F1</choice><choice value="c2c361d9431a4e7b9e60ed0749ecef0b">AUC</choice></input><part id="dfb37b94fa694cc7aa732016bf915166"><response match="f9eb75d584374bec8616f4dfc269685f" score="0"><feedback><p id="cef651d3b3b34866bd5fa39d89ebb759">Incorrect. When detecting glaucoma in patients (and arguably any medical disease), you should aim for a high <em>recall</em> value as an evaluation metric. Predicting people with healthy eyes as having glaucoma (false positive) is much less detrimental than telling patients with glaucoma that they don\xe2\x80\x99t have the disease (false negative).</p></feedback></response><response match="c8dae49b422f443bb86e2229ce4f8751" score="0"><feedback><p id="b253486a1d2448f09bfa6dc51ee2a51c">Incorrect. When detecting glaucoma in patients (and arguably any medical disease), you should aim for a high <em>recall</em> value as an evaluation metric. Predicting people with healthy eyes as having glaucoma (false positive) is much less detrimental than telling patients with glaucoma that they don\xe2\x80\x99t have the disease (false negative).</p></feedback></response><response match="f50796ad2e194f29a76118089d3391df" score="10"><feedback><p id="e387958d264e41d1a99c23be2889f0c2">Correct. When detecting glaucoma in patients (and arguably any medical disease), you should aim for a high <em>recall</em> value as an evaluation metric. Predicting people with healthy eyes as having glaucoma (false positive) is much less detrimental than telling patients with glaucoma that they don\xe2\x80\x99t have the disease (false negative).</p></feedback></response><response match="f3ee384e7fe4464a9eba576982bb2237" score="0"><feedback><p id="f4f0668a62ad4e32a608c36f57f52752">Incorrect. When detecting glaucoma in patients (and arguably any medical disease), you should aim for a high <em>recall</em> value as an evaluation metric. Predicting people with healthy eyes as having glaucoma (false positive) is much less detrimental than telling patients with glaucoma that they don\xe2\x80\x99t have the disease (false negative).</p></feedback></response><response match="c2c361d9431a4e7b9e60ed0749ecef0b" score="0"><feedback><p id="cec79733dfa34b3cbafd18139a3e7458">Incorrect. When detecting glaucoma in patients (and arguably any medical disease), you should aim for a high <em>recall</em> value as an evaluation metric. Predicting people with healthy eyes as having glaucoma (false positive) is much less detrimental than telling patients with glaucoma that they don\xe2\x80\x99t have the disease (false negative).</p></feedback></response></part></multiple_choice><multiple_choice id="cf1e1a54465d44fbbd612dad98e3fcb6" grading="automatic" select="single"><body><p id="e4a69934935a4857858153e28af59bde">You are working with a dataset containing 400 mushroom pictures, and your goal is to detect via the pictures if the mushroom is poisonous (positive) or not poisonous (negative). You have trained a classifier on this dataset and have received the results in the table. Which of the following is the correct interpretation of the Accuracy rate?</p><table id="fca765e957394ad9ac1f38e8e85f4ae1" summary="" rowstyle="plain"><cite id="id7927ca666484aba8bd323b673d6ad69" /><caption><p id="b458f2bc4c5240a691c647c0303a67ab" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="d978b104fd664f3f8586d42ddf9d7792">Accuracy</p></td><td colspan="1" rowspan="1" align="left"><p id="eeb2aac1ecf14b029089a6c642fece08">0.76</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="cb3357a4383e48a89756171f13109597">Specificity</p></td><td colspan="1" rowspan="1" align="left"><p id="c96fda7be32d4bdb9b7b729f14757a90">0.64</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="da76ee5cd70542daafb0749529172fb6">Sensitivity</p></td><td colspan="1" rowspan="1" align="left"><p id="cf5312e9e2f54de3abf3f511bd4b17fe">0.86</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="c56fcdbe800149c2b66cfb73903a5302">Precision</p></td><td colspan="1" rowspan="1" align="left"><p id="dd8e7077e317465aa07cf890170699d2">0.75</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="ed4a6dabb4cd4bfaa058d4885cdc250a">AUC</p></td><td colspan="1" rowspan="1" align="left"><p id="ac84615590174ceb867f65910893af1d">0.80</p></td></tr></table></body><input shuffle="true" id="f239a144424b4d8db8cc1e13725bb1d4" labels="false"><choice value="d0afac76002c4049ab7a4824e0ac172d">The classifier was able to correctly classify 76% out of the 400 mushrooms.</choice><choice value="cf83f4b6333643beaa8ad6e7a7e46ffc">76% of the mushrooms the classifier classified as poisonous are actually poisonous.</choice><choice value="b13c30894d5d42d7a5160577f1abc9c3">The classifier was able to correctly classify 76% of all the mushrooms that are actually poisonous.</choice><choice value="cf64961e1a69471caefa0717dde2dcc2">The classifier was able to correctly classify 76% of all the mushrooms that are actually non-poisonous.</choice><choice value="f49a620b49bd4a7687b6709281def910">The classifier was able to correctly classify 24% out of the 400 mushrooms.</choice></input><part id="f15ed7a62c514968ae640449f7ad2539"><response match="d0afac76002c4049ab7a4824e0ac172d" score="10"><feedback><p id="da685bb2f2da4db39c787a8ab3d41025">Correct. Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points.</p></feedback></response><response match="cf83f4b6333643beaa8ad6e7a7e46ffc" score="0"><feedback><p id="f44d0258e6604099934fd76800fc53a3">Incorrect. This is the Precision measure.</p></feedback></response><response match="b13c30894d5d42d7a5160577f1abc9c3" score="0"><feedback><p id="cc8843b3c584497180b66b08adb132f8">Incorrect. This is the Sensitivity measure.</p></feedback></response><response match="cf64961e1a69471caefa0717dde2dcc2" score="0"><feedback><p id="b2ea2fcbd4064055a7a56c30fd528fcf">Incorrect. This is the Specificity measure.</p></feedback></response><response match="f49a620b49bd4a7687b6709281def910" score="0"><feedback><p id="c78bf2b78e104be2aef2d84beb900e6e">Incorrect. Accuracy is defined as the number or percentage of correctly predicted/classified data points out of ALL the data points. This figure gives the percentage that was not accurately classified.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="e7570ddb72fb41df81a83cf71f158624"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="a7829b46326b4897b19ef2bce6b28449"><title>Question 4</title><content available="always"><p id="d0bba7d5a57744a5933b8419111bd3c3" /></content><multiple_choice id="fdb084c1e73e49b1a274724e986c73d7" grading="automatic" select="multiple"><body><p id="b676e5fac2cd403089806f39f7f17c27">Which of the following is (are) NOT correct about the ROC curve?</p></body><input shuffle="true" id="c84190c8057740d395147c3604a0a9af" labels="false"><choice value="A">The ROC curve shows the tradeoff between the probability of the model making predictions that positive observations are truly positive versus the chance of it making predictions that non-positive observations are positive.</choice><choice value="B">An ideal classifier would classify apples as apples without misclassifying more non-apples as apples.</choice><choice value="C">The ROC curve plots the recall of a classifier on the y-axis against its specificity on the x-axis.</choice><choice value="D">An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess.</choice><choice value="E">The decision threshold of a classifier must be predetermined and unchanged regardless of the results of the classifier to maintain the model\xe2\x80\x99s integrity.</choice></input><part id="d66bc2ff503b4755877024ac304b025f"><response match="D,C,B,A,E" score="0"><feedback><p id="f4c8a4c134a8472394e1031621c4bb25">Incorrect. Not all are incorrect about the ROC curve.</p></feedback></response><response match="A" score="0"><feedback><p id="da28ec31e84d4d9bb1d8c18d38977269">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p></feedback></response><response match="B" score="0"><feedback><p id="efa0a3e34b9549d3bcf5cc8c8d8ae295">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p></feedback></response><response match="C" score="5"><feedback><p id="b5ea78a698e843188ecd2103705035b6">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="e4a201cc5ec74b27a9683cb95659d6ee">However, this is not the only correct answer. </p></feedback></response><response match="D" score="0"><feedback><p id="fca34724401f4f42a12d8b70e37bda5c">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="E" score="5"><feedback><p id="c33dcc215b9241f299c9b078ac2a8a20">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="d5573a830b05471eb4c7cc0d0fef780b">However, this is not the only correct answer.</p></feedback></response><response match="A,B" score="0"><feedback><p id="d2749f8934f349d29c5865c35f4639f5">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="bc4f565524014c479a6b26f600206b72">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p></feedback></response><response match="A,C" score="3"><feedback><p id="ccb48535f56e4342b82023e89a707eeb">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="d557d93029244b299159a8c955d8651e">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="f0d0bd2abe3c4e1e8a4e6134d6ae83ce">However, this is not the only correct answer. </p></feedback></response><response match="D,A" score="0"><feedback><p id="df228f61e8244ff0a8ec3a860029c93e">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="bf20a1579b4d432e8fb9c940a28d8e7b">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="A,E" score="3"><feedback><p id="ab25a15db2614c54aad46b9fa96e1a44">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="c4e35e7faecc4165b79086ede1669ebd">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="a07a8a277faa456a9c5094971c69560c">However, this is not the only correct answer.</p></feedback></response><response match="B,C" score="3"><feedback><p id="b8bf5e9ad8894561b4ce2ea848156dfa">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="baa49829d535498181907f405d17bf31">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="a2b049a5988645f3894459d820f61edf">However, this is not the only correct answer. </p></feedback></response><response match="D,B" score="0"><feedback><p id="b53056da15ac4f2b94308f1f6076b48d">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="eccea570c0e64389a8312117d73caee1">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="B,E" score="3"><feedback><p id="b60830174e9f49a5b39dd902492910d5">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="e18ce342ec384982a6c9d12c658397ae">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="b5e9051491b64e77acf59e28e3dd91b4">However, this is not the only correct answer.</p></feedback></response><response match="D,C" score="3"><feedback><p id="ed1822fa83244154bc927cc092063a9d">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="f388e4a620ed4a70a79455f5cb467e09">However, this is not the only correct answer. </p><p id="e7fcbce273ba4ec4b8ba336a67404370">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="C,E" score="10"><feedback><p id="df54b14de10141f9aeb0bae1f35e7d53">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="b094f01e50b24f6d995f8d352bb46f92">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,E" score="3"><feedback><p id="cc1d2d3550d348b19a319f003d37030a">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="ecbee82b40e045baa00b12668481016a">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="eea1a1f2230244cd937ef3085f472904">However, this is not the only correct answer.</p></feedback></response><response match="A,B,C" score="1"><feedback><p id="cc4ecefff14d4c41a2e79b916f5553a8">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="bba57cdf65c5453dbbab9a1d3365adfc">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="df8d14a341a74986ab66a7fff306abdb">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="fc23469ae547474f89628e1d038ab624">However, this is not the only correct answer. </p></feedback></response><response match="D,A,B" score="0"><feedback><p id="f1e753329539474fb3b66dc52891a92f">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="d8f98a988f994df09617f0bef06cfbeb">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="c4478337a77d43668b9a2ab494981de4">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="E,A,B" score="0"><feedback><p id="ffbe1c418ae64c799997619579f007ba">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="ea5fba83ac874e86b0fe8261b902fb37">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="ae4f8f6f8d744de989d844f8ba793542">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="efcb62bf89884beab449f99dfc053100">However, this is not the only correct answer.</p></feedback></response><response match="D,A,C" score="1"><feedback><p id="ad19cc0b69f8453b9655eced2f769126">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="a33a39b584284f3da9193aa2efab6a9e">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="ded42c51dd384aae86aecbc1e136a501">However, this is not the only correct answer. </p><p id="d8e48d377c0141979879fd5f72d901d8">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="E,A,C" score="8"><feedback><p id="ffecdfbe62dd42a8b41ff253f3e2be7e">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="a4bf25b3d5dd49ce8a3e8e37757f2551">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="e2fa642e0c25407e8b2c16b6b065498a">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,B,C" score="1"><feedback><p id="bbb54d82d89f4edbb7e9d06fd4654679">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="eb32ffb0cad544cd9a2ea0e0f4d89f85">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="de5260a59db14da3be620a816a79012e">However, this is not the only correct answer. </p><p id="e540be316ef64885a6c2c384c1f27a60">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="E,B,C" score="8"><feedback><p id="ba58e99e1b474b6ca6ba2b5f3924eebc">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="c6ae6876e9be4daf95b57bfce63805d5">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="a04b749959ab408093fc328d7ccedc9e">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,E,B" score="1"><feedback><p id="ee05c739ecd74026bbf7a78c52596147">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="d173946366424b9fa8892d497a4515f7">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="d79becaaa880489ba8d685eb96a4e5b8">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="afa7d237fc984b9f9c7c3875bbad0f18">However, this is not the only correct answer.</p></feedback></response><response match="D,E,C" score="8"><feedback><p id="d2590149e20948c28607a9c36bc24dbf">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis. </p><p id="de4da09149ed4c6eb93510df97a85674">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="efb8c111a557428a92c6b76e58105875">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,A,B,C" score="0"><feedback><p id="c3af009199674b808e1f3fb71e9cbc76">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="aa07fb3317614523b26aef35bfd0e63b">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="e6e5774a3a6442dfa27550a1378d731f">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="b85edfc027be4e34919220ef0787a06e">However, this is not the only correct answer. </p><p id="e71330c2ae954672a71443631fe995bf">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p></feedback></response><response match="E,A,B,C" score="6"><feedback><p id="ce6c818a18784aee981214946780a35a">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="bcba229ddbbc4d80a67035b8554b15e2">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="cb64315433ab4cee80561eb157d9ab3e">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="bec3f323a38a4636841aa7e31730024c">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,E,A,B" score="0"><feedback><p id="fc70b89e7864411caf475a1b1fee2a9d">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="ce8b97a650b945b5a9b236330c1e8754">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="b5d5b098dd9f4b86bb6f0e9c4034b3bf">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="af3e3ed0aa08442dabe3778b6e829945">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p><p id="e90efc7025ec482c9a439477fc13e364">However, this is not the only correct answer.</p></feedback></response><response match="D,E,A,C" score="6"><feedback><p id="c373f1dc46344db0b699c49456584ad6">Incorrect. The ROC curve shows the tradeoff between the true-positive rate and the false-positive rate.</p><p id="abdd6a4be15844a387702408a7af689c">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="ccf71dfa34d2454ca1368c10e71111b3">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="fa02dbe955d84da4b64631e62d11aede">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="D,E,B,C" score="6"><feedback><p id="ce19d9a924b14634a20bfd6d8b93baae">Incorrect. An ideal classifier would predict observations that are apples as apples and observations that are not apples as not apples without misclassifying non-apples as apples.</p><p id="cfa8a46c98be49ad8d9110ffbee4421d">Correct. The ROC curve plots the true-positive rate (recall) of a classifier on the y-axis against its false-positive rate (1 - specificity) on the x-axis.</p><p id="d51b4bbc28c54ee1b969d881b371837e">Incorrect. An AUC of 0.5 means that the model\xe2\x80\x99s ability to distinguish between classes is as good as a random guess\xe2\x80\x93that is, 0.5 in a binary classifier.</p><p id="be12d53d70884d2caa263e80d58de4fb">Correct. The ROC curve shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. The decision threshold can be adjusted depending on the cost of incorrect classification (cost of making errors of predicting false negative).</p></feedback></response><response match="A,B,C,D" name="AUTOGEN_{A,B,C,D}" score="0"><feedback><p id="cb52a1a3bb00431fb23224ef2203b1ab" /></feedback></response><response match="A,B,C,D,E" name="AUTOGEN_{A,B,C,D,E}" score="0"><feedback><p id="a76978d646a74c6d95efa5fda14657a9" /></feedback></response><response match="A,B,C,E" name="AUTOGEN_{A,B,C,E}" score="0"><feedback><p id="b09c4b2eab48499ab30158c7b45f1d8a" /></feedback></response><response match="A,B,D" name="AUTOGEN_{A,B,D}" score="0"><feedback><p id="c34ec074839740db99d77e7e49434141" /></feedback></response><response match="A,B,D,E" name="AUTOGEN_{A,B,D,E}" score="0"><feedback><p id="daae516ae29b41f1a07fe63031094372" /></feedback></response><response match="A,B,E" name="AUTOGEN_{A,B,E}" score="0"><feedback><p id="d392e5a0d1cb442b9004aae16529afec" /></feedback></response><response match="A,C,D" name="AUTOGEN_{A,C,D}" score="0"><feedback><p id="e372679a4412408cbc6436d5d5784a48" /></feedback></response><response match="A,C,D,E" name="AUTOGEN_{A,C,D,E}" score="0"><feedback><p id="fb14115d50e441c8bea878b3db04902c" /></feedback></response><response match="A,C,E" name="AUTOGEN_{A,C,E}" score="0"><feedback><p id="b90fc7d6e2e64c6eb9f04490015eafbd" /></feedback></response><response match="A,D" name="AUTOGEN_{A,D}" score="0"><feedback><p id="ae6c330f5f4649adb1c6c8d33ef8e6d7" /></feedback></response><response match="A,D,E" name="AUTOGEN_{A,D,E}" score="0"><feedback><p id="e271881b26384135b22f7bc70b47ea4b" /></feedback></response><response match="B,C,D" name="AUTOGEN_{B,C,D}" score="0"><feedback><p id="bb8dfe7a9b8746d49a01ccdd508733f0" /></feedback></response><response match="B,C,D,E" name="AUTOGEN_{B,C,D,E}" score="0"><feedback><p id="bf64d197dcd44612acdc9505b228e91e" /></feedback></response><response match="B,C,E" name="AUTOGEN_{B,C,E}" score="0"><feedback><p id="fe32bb50a99a45fcaf44cd456f8e999a" /></feedback></response><response match="B,D" name="AUTOGEN_{B,D}" score="0"><feedback><p id="da000837d8e64ee1afda9e9fc820f9cf" /></feedback></response><response match="B,D,E" name="AUTOGEN_{B,D,E}" score="0"><feedback><p id="c40c3ef019fc48f5be8b21b0a1d30803" /></feedback></response><response match="C,D" name="AUTOGEN_{C,D}" score="0"><feedback><p id="c37717ea70ab4c7a9b460521a31d2fe7" /></feedback></response><response match="C,D,E" name="AUTOGEN_{C,D,E}" score="0"><feedback><p id="a5fe53f985e3467d84b48c2d4b1fae06" /></feedback></response></part></multiple_choice><multiple_choice id="a670bcbed43748a9b5083509c76dacd3" grading="automatic" select="single"><body><p id="d97190f8d9cc46d0a2038696ff1fa8d1">According to the visualization below, what is the recall if we want the iACP model to have the (1- specificity) of at least 60%?</p><image id="e1655ccfb61c4fa68158a18ebd1e00c0" src="../webcontent/Comparison-of-binormal-receiver-operating-characteristics-ROC-curves-for-ACPs.png" alt="" style="inline" vertical-align="middle" height="480" width="500"><caption><p id="ba5d10b3feec4bc49da89bd5b116b706" /></caption><popout enable="false"></popout></image></body><input shuffle="true" id="d12ecba86bd84c3d8696e6e14bdc1127" labels="false"><choice value="fe07532340e2451283b8659afa4d8f58">0.747</choice><choice value="a5b70bc8e09841d2942fb7744974a671">0.85</choice><choice value="d6bd0fbd914d427ab0366809c9087751">0.6</choice><choice value="ec65a85e85ad4d6eacdcaba143cf5a13">0.4</choice><choice value="e379dfe7e82149fd8db1a338ad5b4a85">0.25</choice></input><part id="ab111fa8111e45129442c9c151f76419"><response match="fe07532340e2451283b8659afa4d8f58" score="0"><feedback><p id="df6aee24db8d4041aa41504e8e659f6e">Incorrect. This is the AUC of the model.</p></feedback></response><response match="a5b70bc8e09841d2942fb7744974a671" score="10"><feedback><p id="a6b9ec90eccc4a659f47f2ab98d581ea">Correct. Looking at the plot, where the false positive rate on the x-axis is 0.6, the true positive rate on the y-axis is about 0.85.</p></feedback></response><response match="d6bd0fbd914d427ab0366809c9087751" score="0"><feedback><p id="c0268026284c4052856eeaf05c0a0d59">Incorrect. This is the false-positive rate of the model.</p></feedback></response><response match="ec65a85e85ad4d6eacdcaba143cf5a13" score="0"><feedback><p id="baf046cc11aa48baa9065596eab1a251">Incorrect. This is the true-negative rate of the model.</p></feedback></response><response match="e379dfe7e82149fd8db1a338ad5b4a85" score="0"><feedback><p id="c81121a078fc4f6aacb5b8d13a01496e">Incorrect. This is the false-negative rate of the model.</p></feedback></response></part></multiple_choice><multiple_choice id="f8f1793f5c7540f4a48ec0320ce9ae9f" grading="automatic" select="single"><body><p id="d98a56a20a7e45cbbd17d2e344476f6d">According to the visualization below, what is the recall if we want the iACP model to have the (1- specificity) of at least 40%?</p><image id="a10057f106f647158c908aba840bd89c" src="../webcontent/Comparison-of-binormal-receiver-operating-characteristics-ROC-curves-for-ACPs.png" alt="" style="inline" vertical-align="middle" height="480" width="500"><caption><p id="a8c6e973fa54450aa1a8338eb0ef0152" /></caption><popout enable="false"></popout></image></body><input shuffle="true" id="cc766316b01d49c591c703efdb2a3b94" labels="false"><choice value="f5e31621ec3d4bc49812f3a792a432c3">0.747</choice><choice value="a4c866dccb254113ae87c5253e8e5782">0.75</choice><choice value="b38218c2b88d46e1871935766f80d0bb">0.39</choice><choice value="dce53d4a999d42028119b1f7fcb87b8b">0.4</choice><choice value="fa558a0381b7479f929770df0fe030d5">0.25</choice></input><part id="d4e0f47aba4140478f959caaede31af3"><response match="f5e31621ec3d4bc49812f3a792a432c3" score="0"><feedback><p id="e3e50ae95c524005ad0a8b7f272b8e4f">Incorrect. This is the AUC of the model.</p></feedback></response><response match="a4c866dccb254113ae87c5253e8e5782" score="10"><feedback><p id="e98e2bfab96d438a9131e794ac8035f6">Correct. Looking at the plot, where the false positive rate on the x-axis is 0.4, the true positive rate on the y-axis is about 0.75.</p></feedback></response><response match="b38218c2b88d46e1871935766f80d0bb" score="0"><feedback><p id="a838c87e141b423abf3981fc5a7a51d0">Incorrect. This is the false-positive rate of the model.</p></feedback></response><response match="dce53d4a999d42028119b1f7fcb87b8b" score="0"><feedback><p id="d6d0a78b02de4cfaab1591ce38783602">Incorrect. This is the true-negative rate of the model.</p></feedback></response><response match="fa558a0381b7479f929770df0fe030d5" score="0"><feedback><p id="ee15bc7148074701b22a7636df5c2143">Incorrect. This is the false-negative rate of the model.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="d9773ae68229459c811a5555cd4f9b37"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="a6bd81c2f7fb4e3c805b34caa942f72b"><title>Question 5</title><content available="always"><p id="bb64d4a5f1c84747bcfa0830aa538b84" /></content><multiple_choice id="c4c7181f8f4d4ed9b045874b9fd90f00" grading="automatic" select="multiple"><body><p id="e7204b5daba04d7b851640606ac39860">Which statement(s) best describe when interpretability is preferred over flexibility/accuracy?</p></body><input shuffle="true" id="fe8fd3ac10b748e7a9d0f6609430102e" labels="false"><choice value="A">Never. Methods must always have both capabilities.</choice><choice value="B">Accuracy/flexibility should never be compromised. Always go with methods that have accuracy/flexibility.</choice><choice value="C">When trying to make inferences, e.g., you want to understand the relationship between y and x when x changes.</choice><choice value="D">When you are trying to make predictions, e.g., when you want to predict stock prices.</choice><choice value="E">When trying to make causal inferences, e.g., you want to understand if x causes y or y causes x.</choice></input><part id="fe9b3404dafb433cb7ecbf0760ae1e66"><response match="D,C,B,A,E" score="0"><feedback><p id="c702134cf8f546c6befb07e2b210ba05">Incorrect. Not all options are correct.</p></feedback></response><response match="A" score="0"><feedback><p id="fd586b6427cd40b0a8972efd5367ef84">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p></feedback></response><response match="B" score="0"><feedback><p id="dd41efdafb1f42138f4c756ba585c96b">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p></feedback></response><response match="C" score="5"><feedback><p id="ad98f8fe9452435082bee3549771ef00">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="b202a35ac1d944049e80793484a3bf68">However, this is not the only correct answer. </p></feedback></response><response match="D" score="0"><feedback><p id="e7df834a160b4461847234496aea55d1">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="E" score="5"><feedback><p id="b916a36cedbc464a9d17debbf9a64a4f">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="d647b8f0f11547958cf788e9f859ef2d">However, this is not the only correct answer.</p></feedback></response><response match="A,B" score="0"><feedback><p id="f160503902374cbaacc768e3fed4f858">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="fc9682cfe8d74020b96bfa31e7ce4547">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p></feedback></response><response match="A,C" score="3"><feedback><p id="c4040f4a797c4b89a81b615b238ac5a9">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="f040fac245764ffd81cf0044268087e2">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="f91fba7ebd814121808b89217358ecf2">However, this is not the only correct answer. </p></feedback></response><response match="D,A" score="0"><feedback><p id="a65957e81a13449396c703b6d11fb15a">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="f14211c6962c41c9803a201e728c846a">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="A,E" score="3"><feedback><p id="ae5171a2271345feaec3dcd7586d8600">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="f18c4a6b0d7140ed9e2ea473f3d3a183">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="eaed9bc0c3c748dba6615c84ecc9ca31">However, this is not the only correct answer.</p></feedback></response><response match="B,C" score="3"><feedback><p id="a97bbb0c72b44c82a4db4d98cf20727a">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="b537e1487d7d42cbaf7521838e0b4042">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="ce920735e8ae4aa09d99bbab40bd79c9">However, this is not the only correct answer. </p></feedback></response><response match="D,B" score="0"><feedback><p id="ab12c6579dd9441c88a8b5ecc47d2cda">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="ace21fea0eb3404e8170ec5123f0313d">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="B,E" score="3"><feedback><p id="e79c451a910f4cbbb3ec4c81220543a0">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="e0c3a033d0bb433e8662d56712ea8b04">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="c6f2bfe74a314bbb92769c7af162f21d">However, this is not the only correct answer.</p></feedback></response><response match="D,C" score="3"><feedback><p id="d435963bab514e94b0a848e0cf8e3c2e">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="a98dfe112cde4e3d8eeb125dde16f5c6">However, this is not the only correct answer. </p><p id="b690b09f6eac4ab1a169ee29f108add9">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="C,E" score="10"><feedback><p id="d9977579aaea4c07977f479abc2721dd">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="df5fc035b9f24478836b5975f91d1e9c">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,E" score="3"><feedback><p id="fad6d434a23e4739b60bc6f36857492a">Incorrect. When making predictions, we want accuracy!</p><p id="ebbcf5610e104402b412155227fe3272">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="d21462c9e5d04f9eba98191b607269c8">However, this is not the only correct answer.</p></feedback></response><response match="A,B,C" score="1"><feedback><p id="b08f6e6509814c5395f0d3ec28a1474b">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="fefe850649a443bf9f23b2994bd24867">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="cfd019d3f5674b28859a726d514fb48b">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="a0ed74935917410aaaf23fae98dfa96c">However, this is not the only correct answer. </p></feedback></response><response match="D,A,B" score="0"><feedback><p id="fc31acee5ecb46a481d5c086f6bca70b">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="f741ff0d32504bdfb42453288cbeb370">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="d3d09f7624804abe91c56c0acc3151b3">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="E,A,B" score="0"><feedback><p id="a3d4848904624c2f8db9b7f1e2b5056c">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="eb00bfdd35384b45b6f6726eda72cde3">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="a583e8c9e42747cfaf4574747b1785f9">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="a382e8dd28ca476e8a9bd04612bc3cdf">However, this is not the only correct answer.</p></feedback></response><response match="D,A,C" score="1"><feedback><p id="f63a6878cf5a400d860d389dda208068">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="a73337f94aef4c228d089352ae511521">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="b4e85430efa943adbb6c2c7e5d36fcff">However, this is not the only correct answer. </p><p id="f5267ffdfdaf4fdf8b6e88e701e37ca9">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="E,A,C" score="8"><feedback><p id="cedae73f98d24f22ac6e0a58da127fcd">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="e5e093022fc34a3395c3b460e11d8d0a">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="d82f1856e20f4d36b507683d956017f1">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,B,C" score="1"><feedback><p id="b98547917e324d2cb148f224878685fe">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="e3843ca1c238488ca1d018bfd681a530">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="cc34597f391e4521b97c7419ead1b84a">However, this is not the only correct answer. </p><p id="f14c8e0e847c4230a211f9b66d6caa98">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="E,B,C" score="8"><feedback><p id="f123890ab9dd48b38fe36a8d7fbd5154">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="d63e19a982b94093b715d6531aed7755">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="fa9842d5ca9c44de8a7eee3238c3c986">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,E,B" score="1"><feedback><p id="c2800e859fb240918e4803decb8672c8">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="cce08ce24417430eb05a5d155d43b4f8">Incorrect. When making predictions, we want accuracy!</p><p id="a51bf256b1134197844a982b864bb7aa">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p><p id="d2fc402f751e464d91af91bf2b40806d">However, this is not the only correct answer.</p></feedback></response><response match="D,E,C" score="8"><feedback><p id="a3bb4b7d8ead412da44a21ef7ef12e5d">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="ff363394f1a4404dbb577817d90f498c">Incorrect. When making predictions, we want accuracy!</p><p id="d98e3212b0c1499d82c9a7c83904a4e0">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,A,B,C" score="0"><feedback><p id="c5482f4676a94a32afb3c38859bac76a">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="bcfec554b0154c68af44a9c149b4543c">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="bcd8e1c3213d426cb2fa679c9aaa0621">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="cf729c555bd945078e960360eaf29074">However, this is not the only correct answer. </p><p id="c6b4f5923c984e7bb630ae2693bbf3a7">Incorrect. When making predictions, we want accuracy!</p></feedback></response><response match="E,A,B,C" score="6"><feedback><p id="cad2f859d61b46258cf286d317ce69a8">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="faef3b207b93454ca9c314a6d1443604">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="e14133539c104bf0b5b3507277b45176">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="d986c24af69441c89cae7e1c8fb00a94">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,E,A,B" score="0"><feedback><p id="f593401daa6441aaa213793f03102f11">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="ef959ddfd288456dbc2db6d199ba019a">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="d1026eae1faf4065a4d4ef83296da72e">Incorrect. When making predictions, we want accuracy!</p><p id="b8e58bee766745e3b9533d0bb93ba46b">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,E,A,C" score="6"><feedback><p id="c74bdfeeeae943c89c79e409cfa03733">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and flexible. Sometimes, something must give.</p><p id="d10d331242914c0e9014e375fc3cbd12">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="a4322bf8efcc4b3683f7733e688255e5">Incorrect. When making predictions, we want accuracy!</p><p id="e0cf564ddd5343199655f4b1dcf6a277">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="D,E,B,C" score="6"><feedback><p id="c770ca2ab62c450181530e1c1f70d504">Incorrect. There are many cases where why such results were made more important than how accurate or flexible the results were.</p><p id="d7d999949abc412db29e886f81322ce6">Correct. You want to explain a relationship and should be able to interpret it.</p><p id="a42470d7d08c43f9b9f13791e11acbe5">Incorrect. When making predictions, we want accuracy!</p><p id="c87328ee44ac4b32a790518a6441bb68">Correct. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response><response match="A,B,C,D" name="AUTOGEN_{A,B,C,D}" score="0"><feedback><p id="f1e715fa2c624736b3f62a9d04e9ee71" /></feedback></response><response match="A,B,C,D,E" name="AUTOGEN_{A,B,C,D,E}" score="0"><feedback><p id="f6dae2289f3d4131bbc6edb372b12812" /></feedback></response><response match="A,B,C,E" name="AUTOGEN_{A,B,C,E}" score="0"><feedback><p id="d77cba548ce74118a35b734de4853287" /></feedback></response><response match="A,B,D" name="AUTOGEN_{A,B,D}" score="0"><feedback><p id="bfd90e0a8b4c47ce82874d2b765a0d0d" /></feedback></response><response match="A,B,D,E" name="AUTOGEN_{A,B,D,E}" score="0"><feedback><p id="b33eff1e29194d5fbed6e84fc8c2e3de" /></feedback></response><response match="A,B,E" name="AUTOGEN_{A,B,E}" score="0"><feedback><p id="c4d1f7d4279c45d5836d57edac967c87" /></feedback></response><response match="A,C,D" name="AUTOGEN_{A,C,D}" score="0"><feedback><p id="c4dd0c7c12a64a7fb069fc9ae7d15a3d" /></feedback></response><response match="A,C,D,E" name="AUTOGEN_{A,C,D,E}" score="0"><feedback><p id="ccca52e379cb4d838b2819a8b902fcba" /></feedback></response><response match="A,C,E" name="AUTOGEN_{A,C,E}" score="0"><feedback><p id="bd274eff27784733a28383a7c486815d" /></feedback></response><response match="A,D" name="AUTOGEN_{A,D}" score="0"><feedback><p id="ec9bebaa70fb4f359b480bcec3654bda" /></feedback></response><response match="A,D,E" name="AUTOGEN_{A,D,E}" score="0"><feedback><p id="bfdf9d8769c945bd8f9c2c360aebae13" /></feedback></response><response match="B,C,D" name="AUTOGEN_{B,C,D}" score="0"><feedback><p id="d1d1c79c85c845d2b6e0bea6fb3707d1" /></feedback></response><response match="B,C,D,E" name="AUTOGEN_{B,C,D,E}" score="0"><feedback><p id="effe4382d6c94879bb0b3981f0ae9720" /></feedback></response><response match="B,C,E" name="AUTOGEN_{B,C,E}" score="0"><feedback><p id="ad7d5b78286941f0a20c15dfa3628ac9" /></feedback></response><response match="B,D" name="AUTOGEN_{B,D}" score="0"><feedback><p id="a8e8caad548346d986cd7ade80377c00" /></feedback></response><response match="B,D,E" name="AUTOGEN_{B,D,E}" score="0"><feedback><p id="b4823f4ceb0c40ab9c076690d9e0dc17" /></feedback></response><response match="C,D" name="AUTOGEN_{C,D}" score="0"><feedback><p id="a1eeef26c0bd4fcfa15377217b443347" /></feedback></response><response match="C,D,E" name="AUTOGEN_{C,D,E}" score="0"><feedback><p id="a156aa14c9dd4a52b6a233ebf603a2e9" /></feedback></response></part></multiple_choice><multiple_choice id="a7b62edf34b9487fb083195415aedf42" grading="automatic" select="single"><body><p id="a4d8ee3a3a1544b2a6cc3e18f4e8c8c9">Which of the following statements is NOT true about metrics on clustering algorithms?</p></body><input shuffle="true" id="bec339d5330949e9a55c66595ce455ba" labels="false"><choice value="ff9bfac693ab4d8687adfc49f1746684">When the silhouette coefficient is close to 0, we can find overlapping clusters.</choice><choice value="b82f4de33c734ad7acc48cf5a047764d">The internal approach does not require ground truth data, while the external approach must have it.</choice><choice value="b51cb678ae924565a49fdca30135a7d4">The external approach for evaluating clustering includes Rand Index, Dunn Index, and as well as Jaccard Index.</choice><choice value="b09d286e9afc4785bbf198cca5ba11c2">If we only have two clusters, Rand Index can be viewed as the accuracy for binary classification.</choice><choice value="ac5a39a98f90429a867a77894eaadcb6">Jaccard index can be used to evaluate clustering algorithms as well as deep learning models like convolutional neural networks.</choice></input><part id="c54ef5a110984c698d9f5597eeae42ca"><response match="ff9bfac693ab4d8687adfc49f1746684" score="0"><feedback><p id="a4f07de63b3a421591d096e6c3815598">Incorrect. This statement is true. If we have the intra-cluster distance <em>a</em> and the mean nearest-cluster distance <em>b</em>, then the Silhouette score is calculated as (<em>b</em> - <em>a</em>) / max(<em>a</em>, <em>b</em>). When this score is zero, it means <em>a</em> is equal to <em>b</em> (the intra-cluster distance equal to the mean nearest-cluster distance). This is to say that we have two overlapping clusters.</p></feedback></response><response match="b82f4de33c734ad7acc48cf5a047764d" score="0"><feedback><p id="c5aa0dafb15d4833b339c8daa144e070">Incorrect. This statement is true. By definition, internal evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. External evaluation measures the results from a clustering task based on data not used for the clustering task.</p></feedback></response><response match="b51cb678ae924565a49fdca30135a7d4" score="10"><feedback><p id="d00888f5a5724ff5986d41af9d0b76fc">Correct. This statement is not true. Dunn Index is an internal approach.</p></feedback></response><response match="b09d286e9afc4785bbf198cca5ba11c2" score="0"><feedback><p id="f12e6a7a596b4609a0b22af04263e75f">Incorrect. This statement is true. Rand Index follows this formula: (TP + TN)/(TP+FP+FN+TN), which is the same as accuracy calculation.</p></feedback></response><response match="ac5a39a98f90429a867a77894eaadcb6" score="0"><feedback><p id="f992a3c63b2a4ad0aa167123bcbbbfc9">Incorrect. This statement is true. Jaccard Index is very commonly used in deep learning applications such as image segmentation.</p></feedback></response></part></multiple_choice><multiple_choice id="b9b7a2b5870749e8a1cdf1fb7f0669e2" grading="automatic" select="single"><body><p id="b1168870fb02454e83ca0fd79ddfddda">Which statement(s) best describe when accuracy is preferred over interpretability?</p></body><input shuffle="true" id="ef08a2ac91954b8db01c2b715cec37cd" labels="false"><choice value="aa0599c5d12e412eb309e0b532d957d4">Never, methods must always have both capabilities.</choice><choice value="c2ed90b38c494c75b29c021f5cb7a6b1">Accuracy should never be compromised. Always go with methods that have accuracy.</choice><choice value="bd7dd901ddd14c63b41f2bdfb4c00bfd">When trying to make inferences, e.g., you want to understand the relationship between y and x when x changes.</choice><choice value="d2e2248b825f40eea18383deb77eec00">When you are trying to make predictions, e.g., when you want to predict stock prices.</choice><choice value="e9b40739cab8453caa0ee9444c4bf95c">When trying to make causal inferences, e.g., you want to understand if x causes y or y causes x.</choice></input><part id="c8a72cb3331c4794adf80efcb07a6196"><response match="aa0599c5d12e412eb309e0b532d957d4" score="0"><feedback><p id="ed02b2baf4104caf8ca882b8758779b0">Incorrect. Having both capabilities is unrealistic for now. We might get to the point where methods are both interpretable and accurate. Sometimes, something must give.</p></feedback></response><response match="c2ed90b38c494c75b29c021f5cb7a6b1" score="0"><feedback><p id="d813d51ed01c48cda7f4877dbac3f285">Incorrect. There are many scenarios where interpretabiity is more important than how accurate the results are.</p></feedback></response><response match="bd7dd901ddd14c63b41f2bdfb4c00bfd" score="0"><feedback><p id="a4781d140feb48f0a9899388bbc3c4af">Incorrect. You want to explain a relationship between x and y, you care more about interpretability than accuracy.</p></feedback></response><response match="d2e2248b825f40eea18383deb77eec00" score="10"><feedback><p id="c7651d17c8f34db6995ca0f8a75d00c1">Correct. When making predictions, we want accuracy!</p></feedback></response><response match="e9b40739cab8453caa0ee9444c4bf95c" score="0"><feedback><p id="ca8619a5f38149a18fafb80f6f6ab73a">Incorrect. To make causal inferences, you first need to understand the relationship between x and y. This is the case where interpretability is much needed and not accuracy. Once an interpretable relationship is established, you can then perform the next steps to determine if the relationship is causal.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="fa1cded88b6147aa95567d3b6c8c3722"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="d6476c38c96f49f79c2fe0e067ce5f89"><title>Question 6</title><content available="always"><p id="ce5cabcf19944db7ab9d5b3a62938310" /></content><multiple_choice id="ad5c28e7a20d4c9091b8b7a5a636e7a5" grading="automatic" select="single"><body><p id="f17de2726c91468ab5687415818251f2">In regression, which of the following metrics should we maximize to ensure that we have a better model?</p><ol id="a2f9fc5195ef43349f37f536961fefde"><li><p id="eac91be7f23b4f738c0936dc85ad0b47">R-squared</p></li><li><p id="b2da094c596e42e9bf2e6fe750787482">Adjusted R-squared</p></li><li><p id="fed44a71dab94acbb32368f33cbaa1f0">Mean Squared Error</p></li><li><p id="adcb4080a6034584b7590790697312d7">Mean Absolute Error</p></li><li><p id="bd489260ce2a40938bd02ed1f7810e50">Root Mean Squared Error</p></li></ol></body><input shuffle="true" id="c0463ab310834564afff11031cd5abbf" labels="false"><choice value="fe7628cfb08a41f5ba0abe7166acb992">Only metric 1</choice><choice value="aafa1dd0ebdf4a0a95c9eb839d1701a2">Metrics 1 and 2</choice><choice value="b260198e4c5340b8943889eab8f53b0e">Metrics 1, 2 and 3</choice><choice value="bef78ae55a0741a2918d046979207336">Metrics 3, 4 and 5</choice><choice value="e8652995aa4341328f08b368022c3c2d">All of the five metrics</choice></input><part id="a74aa7ab9442411a88ddcfed98ec68ef"><response match="fe7628cfb08a41f5ba0abe7166acb992" score="0"><feedback><p id="ffbacc7ccdbd4d9d8838c1e1e9a83dc9">Incorrect. Larger values of Metric 1 and 2 indicate a better fit model.</p></feedback></response><response match="aafa1dd0ebdf4a0a95c9eb839d1701a2" score="10"><feedback><p id="ddf87392510149d585c7ba20b5affcfc">Correct. Larger values of Metric 1 and 2 indicate a better fit model. </p></feedback></response><response match="b260198e4c5340b8943889eab8f53b0e" score="0"><feedback><p id="c4e160618c1141adb9c8bb0e6b7b103c">Incorrect. Larger values of Metric 1 and 2 indicate a better fit model. </p></feedback></response><response match="bef78ae55a0741a2918d046979207336" score="0"><feedback><p id="ad22239a20224bcaa132174a286ab32d">Incorrect. Larger values of Metric 1 and 2 indicate a better fit model. </p></feedback></response><response match="e8652995aa4341328f08b368022c3c2d" score="0"><feedback><p id="d53f0619a1ed4224978022275e142c54">Incorrect. Larger values of Metric 1 and 2 indicate a better fit model.</p></feedback></response></part></multiple_choice><multiple_choice id="e900b4044dc94761ae9ff018a0c2dd7f" grading="automatic" select="single"><body><p id="d3de504448b043428286e5389e3a5792">Why are some classification metrics such as true positive, false negative, and F-1 score also useful in the computation of external evaluation metrics for clustering?</p></body><input shuffle="false" id="ea5abf1825fc4684b884d13e6d26ab5a" labels="false"><choice value="c49386e73ffb4c04adf4649e565a4d29">Because the internal evaluation metrics don\xe2\x80\x99t need ground-truth group labels to measure performance, similar to a regression setting.</choice><choice value="db61596b48524cda960e08addd9b9a80">Because the external evaluation metrics don\xe2\x80\x99t need ground-truth group labels to measure performance, similar to a classification setting.</choice><choice value="ea4925cfc50845bea86b36d7177a5de8">Because the external evaluation metrics need ground-truth group labels to measure performance, similar to a regression setting.</choice><choice value="fbf03aa916bc42268be6807b579bd784">Because the external evaluation metrics need ground-truth group labels to measure performance, similar to a classification setting.</choice><choice value="f0510f16c5df4103a5b8eb731b0b6b74">None of the above.</choice></input><part id="c38bea9f55284824a3331f099fb5cbde"><response match="c49386e73ffb4c04adf4649e565a4d29" score="0"><feedback><p id="f1e399236da84103b512a5c4ffddff2b">Incorrect. Because the external evaluation metrics need ground truth data to measure performance, similar to a classification setting.</p></feedback></response><response match="db61596b48524cda960e08addd9b9a80" score="0"><feedback><p id="e47446bacde844598181d96b87da67d5">Incorrect. Because the external evaluation metrics need ground truth data to measure performance, similar to a classification setting.</p></feedback></response><response match="ea4925cfc50845bea86b36d7177a5de8" score="0"><feedback><p id="a291094cb8024e9aa0511a5245ad8efe">Incorrect. Because the external evaluation metrics need ground truth data to measure performance, similar to a classification setting.</p></feedback></response><response match="fbf03aa916bc42268be6807b579bd784" score="10"><feedback><p id="dad65733f8164c8ca86ef6d39cfbfd6c">Correct. Because the external evaluation metrics need ground truth data to measure performance, similar to a classification setting.</p></feedback></response><response match="f0510f16c5df4103a5b8eb731b0b6b74" score="0"><feedback><p id="cfcd4f93e10d4918ad98e6b8ffa325dd">Incorrect. Because the external evaluation metrics need ground truth data to measure performance, similar to a classification setting.</p></feedback></response></part></multiple_choice><multiple_choice id="dc407445205c4551824f233d83b4db93" grading="automatic" select="single"><body><p id="ec9e6357fe1d4e3898e3e2a0732ee438">Machine learning is also widely used to analyze biomedical data. For example, RNA-seq data can be used to build models to separate different subtypes of breast cancer. Which of the following statements is NOT true?</p></body><input shuffle="true" id="c4862e5e3eb24a1495b19748f54ea6a8" labels="false"><choice value="b479f879e625409080df6eb3e22c46a1">If a research team wants to learn about the different mechanisms behind each subtype of cancer, the machine learning model should emphasize both accuracy and interpretability.</choice><choice value="e519d936815a4f17ab4b13b4aec6d6b9">If you build a model using data that lacks diversity (only a small fraction comes from minority groups), bias identification is important in model interpretation.</choice><choice value="d6dd44b1032b4e4d87865b0905792ae6">If we decide to use k-mean clustering for RNA-seq data, we can use Silhouette scores as a reference to choose the best number of clusters.</choice><choice value="d7f3ba418183452986a1714fa8749c0d">Given that we are doing clustering and we do not have true labels of each patient\xe2\x80\x99s cancer subtype, we are not able to use Dunn Index to evaluate clustering results.</choice><choice value="cbee7f6663df4f6c87044d7e6cfabd81">Hierarchical clustering is a popular method for analyzing RNA-seq data, but we should carefully choose the distance function and linkage method since the result will be vastly different given different combinations of distance function and linkage method.</choice></input><part id="e1b39d4901004daf9bf0bcaa36205c71"><response match="b479f879e625409080df6eb3e22c46a1" score="0"><feedback><p id="be079b930c0e4591bc1f830538ac3d7e">Incorrect. This statement is true. When we want to investigate the mechanism, we not only want to know which subtype a patient\xe2\x80\x99s cancer is, but we also want to know how the classification is made. This is an example of model explainability.</p></feedback></response><response match="e519d936815a4f17ab4b13b4aec6d6b9" score="0"><feedback><p id="cd9647f61ac04d32944102dc297204ab">Incorrect. This statement is true. The lack of data from some racial groups may cause some ethnic minorities to bear the higher risk of being misdiagnosed. This is why bias identification is important in model building.</p></feedback></response><response match="d6dd44b1032b4e4d87865b0905792ae6" score="0"><feedback><p id="e6c5de9b7f9c48b7ac4d083c39cc19b9">Incorrect. This statement is true. We can get Silhouette scores from different numbers of clusters from the algorithm, and choose a reasonable number of clusters with the highest Silhouette score.</p></feedback></response><response match="d7f3ba418183452986a1714fa8749c0d" score="10"><feedback><p id="cc3fcd5a06084b7c8a909484730ae279">Correct. This statement is not true. The external approaches require data labels, while internal approaches do not require labels. Dunn Index is an internal approach.</p></feedback></response><response match="cbee7f6663df4f6c87044d7e6cfabd81" score="0"><feedback><p id="ae7febaef26a449b9a8f719221acdf0b">Incorrect. This statement is true. Distance function determines how we calculate the distance between two points, and the linkage method decides how to measure the distance between two clusters.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="e5a9e1738f6f4c918f5f72fdb19e3afc"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="d187b840f8704071b2fa21e1b72c632d"><title>Question 7</title><content available="always"><p id="a7bc1c68e35146bf93ee30298b41d597" /></content><multiple_choice id="dbf17d2db5404979a73d4ce2f7955d3c" grading="automatic" select="single"><body><p id="dbbef662e7214dfc84f2c01cf10acfef">Among the following findings of experiment 1, which finding(s) contradicted the hypotheses of Poursabzi-Sangdeh et al. (2021) when they designed the experiment?</p><ol id="b9f56951ac53448f9ca89194f4e95d88"><li><p id="aa5b8aecc4ef45d78dceb51fe98902ab">(Simulation) The finding was that people who saw the clear, two-feature model could more closely simulate the prediction of the model shown to them than those in other conditions.</p></li><li><p id="e1d50062593c47a08653b77d52141f53">(Deviation) The finding was that people who saw the clear, two-feature model did not follow the model prediction more than those who saw the black-box, 8-feature model when it was beneficial for them to do so.</p></li><li><p id="e939718989544f9b9317e0cfc8aab46c">(Detection of mistakes) The finding was that participants who saw the clear models were not able to identify the mistakes on unusual data points as well as those who saw the black-box models.</p></li></ol></body><input shuffle="true" id="f163ee1100a7455eb1b29668fcadf201" labels="false"><choice value="cb0c32aed2e34c58bf5464d727436b17">Findings 1 and 2 contradicted the initial hypotheses</choice><choice value="bfc7fdbb8538407e8c2af417523cef96">Findings 2 and 3 contradicted the initial hypotheses</choice><choice value="a37849da054e45a0846c24abc93f4019">Findings 1 and 3 contradicted the initial hypotheses</choice><choice value="c12b8668982941068f0c8fc7024c4f4d">All three findings contradicted the initial hypotheses</choice><choice value="b449ceba621b46a28d0b7f4bd0e90b7b">None of the three findings contradicted the initial hypotheses</choice></input><part id="f4ab1a21f57e4ba0a33ed2247a62bdc5"><response match="cb0c32aed2e34c58bf5464d727436b17" score="0"><feedback><p id="a8722d2bc1684f499acbfb28b11eeffb">Incorrect. Paragraph 2 in section 3.2 shows that finding 1 was consistent with the authors\xe2\x80\x99 hypothesis.</p></feedback></response><response match="bfc7fdbb8538407e8c2af417523cef96" score="10"><feedback><p id="e636d41597e44d15a478f48cbed919ef">Correct. Paragraph 3 and 4 in section 3.2 show that findings 2 and 3 contradicted the authors\xe2\x80\x99 hypotheses.</p></feedback></response><response match="a37849da054e45a0846c24abc93f4019" score="0"><feedback><p id="ce1508c6ccb340f6a12482a7e1cc6e7a">Incorrect. Paragraph 2 in section 3.2 shows that finding 1 was consistent with the authors\xe2\x80\x99 hypothesis.</p></feedback></response><response match="c12b8668982941068f0c8fc7024c4f4d" score="0"><feedback><p id="ee157318886644c3899502ca0fa4f6dd">Incorrect. Paragraph 2 in section 3.2 shows that finding 1 was consistent with the authors\xe2\x80\x99 hypothesis.</p></feedback></response><response match="b449ceba621b46a28d0b7f4bd0e90b7b" score="0"><feedback><p id="fa5dd00e1e83414f8c13a91c1db17ac9">Incorrect. Paragraph 3 and 4 in section 3.2 show that findings 2 and 3 contradicted the authors\xe2\x80\x99 hypotheses.</p></feedback></response></part></multiple_choice><multiple_choice id="b01aed2c61374cc3896d67a280aac552" grading="automatic" select="single"><body><p id="dc0266a44e234a4d845109cac296098d">What was Poursabzi-Sangdeh et al. (2021)&apos;s primary motivation for conducting experiment 2?</p></body><input shuffle="false" id="befd5c799cdb4334a569934156d64fc6" labels="false"><choice value="da8581f0426f467c8bc98e896c5f140e">The sample size in experiment 1 was not large enough.</choice><choice value="cccf63070e2a40bab5b30cc83b4bac0d">The data for the housing features used in experiment 1 had poor quality.</choice><choice value="bb55916a83224a88ad31e4410800fded">The authors wanted to test the exact same three hypotheses in experiment 1 on a new group of participants.</choice><choice value="b6107c73375b43f19cd2f406c319bc32">The unusually high New York housing prices may influence people&apos;s perception of the models.</choice><choice value="dd2d9b8cb19d4f038c6adcddcd42d522">None of the above.</choice></input><part id="d205285e07d54d428c80d72b14b68c24"><response match="da8581f0426f467c8bc98e896c5f140e" score="0"><feedback><p id="b614b98fb1c84f3cbb5efddb322f4ab2">Incorrect. The paper did not mention any issue with the small sample size in experiment 1.</p></feedback></response><response match="cccf63070e2a40bab5b30cc83b4bac0d" score="0"><feedback><p id="f0ac194153ad4dafb254759eb9dbb34d">Incorrect. The paper did not mention any issue with the quality of the housing features in experiment 1.</p></feedback></response><response match="bb55916a83224a88ad31e4410800fded" score="0"><feedback><p id="c07b2b1ccf5343daa5ffdfc8cab99618">Incorrect. While the first two hypotheses in experiment 2 (H4, H5) were identical to those in experiment 1, the third hypothesis (H6) was edited to be more precise.</p></feedback></response><response match="b6107c73375b43f19cd2f406c319bc32" score="10"><feedback><p id="d4c925e0b01546ea9a05ddfa82ffd6fb">Correct. As mentioned in the first paragraph of Section 4, participants who lack familiarity with New York City\xe2\x80\x99s unusually high prices may think the model is always over-estimating.</p></feedback></response><response match="dd2d9b8cb19d4f038c6adcddcd42d522" score="0"><feedback><p id="e92be7461a864afebd754b2616a75dc5">Incorrect. As mentioned in the first paragraph of Section 4, participants who lack familiarity with New York City\xe2\x80\x99s unusually high prices may think the model is always over-estimating.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="bfef52d460224472a93de49a24fac793"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="b7e114bed1804d1ea7ec9b2140a69081"><title>Question 8</title><content available="always"><p id="c15ad91b1dbe438da252bc2243011399" /></content><numeric id="ecc48490397f4cb1a62d75133608ff05" grading="automatic"><body><p id="ebbf6e51eb5b4d16b4d95bc1b3b5fa83">You build a new supervised learning model to predict the chance of a minor traffic incident at an intersection, given a variety of additional information, such as demographics and driver safety metrics The base probability of any minor traffic incident at this intersection, without knowing these metrics, is 0.04. Using accuracy as a metric, a good choice for a baseline accuracy score of the new model so that it outperforms a majority-class classifier baseline is<input_ref input="e0ee21baf7444d9fa3b0c406afe0362b" /> (your answer should be a real number from 0 to 1, rounded to two decimal places).</p></body><input id="e0ee21baf7444d9fa3b0c406afe0362b" size="small" /><part id="ca26bb8caa994ae1a7668a43bbfa7d7d"><response match="0.96" score="10" input="e0ee21baf7444d9fa3b0c406afe0362b"><feedback><p id="c9d568c63503432f9730b99adfb47f51">Correct!</p></feedback></response><response match="*" score="0" input="e0ee21baf7444d9fa3b0c406afe0362b"><feedback><p id="d06179471d7f490a951ba2de3aad722d">Incorrect. With the probability of any minor traffic incident at the intersection being 4%, you will need a baseline accuracy score of 1 - 0.4 = 0.96 to outperform.</p></feedback></response></part></numeric><numeric id="c28cbf3b58174397b5f71a5d4f49a2a0" grading="automatic"><body><p id="dbc70821dd8541a9b6565511ed88392f">You build a new supervised learning model to predict the chance of a minor traffic incident at an intersection, given a variety of additional information, such as demographics and driver safety metrics The base probability of any minor traffic incident at this intersection, without knowing these metrics, is 0.07. Using accuracy as a metric, a good choice for a baseline accuracy score of the new model so that it outperforms a majority-class classifier baseline is <input_ref input="cd0c0668e0cd4594b5810cbf4627e31f" />(your answer should be a real number from 0 to 1, rounded to two decimal places).</p></body><input id="cd0c0668e0cd4594b5810cbf4627e31f" size="small" /><part id="ecb4d60e624d4da9b52814dd803b4400"><response match="0.93" score="10" input="cd0c0668e0cd4594b5810cbf4627e31f"><feedback><p id="b92aacada8f842b7ad498b28b9d1f5ec">Correct!</p></feedback></response><response match="*" score="0" input="cd0c0668e0cd4594b5810cbf4627e31f"><feedback><p id="a1001f89c3cc467da98f4990e93b59ee">Incorrect. With the probability of any minor traffic incident at the intersection being 7%, you will need a baseline accuracy score of 1 - 0.7 = 0.93 to outperform.</p></feedback></response></part></numeric></pool></selection></page><page id="f65c1d812f5a48f7b0bf4c111a8a7f79"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="bb7375db2f28494685b675b6430791af"><title>Question 9</title><content available="always"><p id="c6a25e06eae74a27908fbc6f8becdf3d" /></content><multiple_choice id="b3375af261d641688f0b35bd22a73eaa" grading="automatic" select="single"><body><p id="d609d4c124fc40289dae5ab5910ac890">In the binary classification setting, assume we have a simplistic model that always predicts the positive class, given any input data. Compute the following metrics when the model is evaluated on a dataset with 95 positive samples and 5 negative samples: accuracy, F-1, specificity, and precision.</p></body><input shuffle="true" id="bef056f3af3a4078a4352fbe7c4dfc5f" labels="false"><choice value="be06ec2bcd8444478a6e6ece09954c50">Accuracy = 5%, F-1 = 98%, Specificity = 97.4%, and precision = 95%.</choice><choice value="cca86f5ca9e0422e8b098a9208e72a45">Accuracy = 100%, F-1 = 98%, Specificity = 97.4%, and precision = 95%.</choice><choice value="a18630f6f61f454ebca01704d1daddb4">Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</choice><choice value="f19846ba1312450f82c038d0c958536f">Accuracy = 5%, F-1 = 97.4%, Specificity = 50%, and precision = 95%.</choice><choice value="cf7e1be1bb5e49f3a0747901265e5163">Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 5%.</choice></input><part id="e81521a3322d498d8e142c3263a25f0e"><response match="be06ec2bcd8444478a6e6ece09954c50" score="0"><feedback><p id="b0a1f7ad51414d95b35382945d36a18c">Incorrect. Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</p></feedback></response><response match="cca86f5ca9e0422e8b098a9208e72a45" score="0"><feedback><p id="cfcc0ea72f244825b4f819ec53fa5f7e">Incorrect. Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</p></feedback></response><response match="a18630f6f61f454ebca01704d1daddb4" score="10"><feedback><p id="ae2dcf4ef87a463ead1add5d1ee2ef5f">Correct. Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</p></feedback></response><response match="f19846ba1312450f82c038d0c958536f" score="0"><feedback><p id="d3638d81005c4cd793e7711b3c3a774b">Incorrect. Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</p></feedback></response><response match="cf7e1be1bb5e49f3a0747901265e5163" score="0"><feedback><p id="a3b69d6eae904782968fff8c25d2a488">Incorrect. Accuracy = 95%, F-1 = 97.4%, Specificity = 0%, and precision = 95%.</p></feedback></response></part></multiple_choice><multiple_choice id="c6ef748a25de467fbbad85de721b736b" grading="automatic" select="single"><body><p id="e7796aa8af7742689f2dfa2947837b7d">A research team collects data from type II diabetes patients and healthy individuals. They wish to find the best model to predict whether a person will develop type II diabetes. And they start with a very simple regression model, with two factors, BMI and gender:</p><p id="ac9a04c08015461093132d26387d98f6">Model 1: \\( Y = \\beta_0+ BMI \\cdot X_1 + \\epsilon \\)</p><p id="ed4bcbaa458b4751ba5b9b4e7fd4f545">Model 2: \\( Y = \\beta_0 + BMI \\cdot X_1 + Gender \\cdot X_0 + \\epsilon \\)</p><p id="cd0ceef2ae6048edae1ece5f32f886a6">Which of the following statements is NOT true?</p></body><input shuffle="true" id="faf1ab80b44d4271b76892a83c0f2679" labels="false"><choice value="e239d855ff7141ddab58e7555e1bc18c">Adjusted \\(R^{2}\\) of model 2 must be greater or equal to adjusted \\(R^{2}\\) of model 1.</choice><choice value="dfe823e8942d4e81be5e56f0da68f5df">\\(R^{2}\\) of model 2 must be greater or equal to \\(R^{2}\\) of model 1.</choice><choice value="cffbffb0d01a43f89a0c8c148c18209a">If the research team finds that gender has no effect in predicting whether a person will develop type II diabetes, and the estimated coefficient of gender is 0, then \\(R^{2}\\) of model 1 and model 2 are equal.</choice><choice value="d3ff83a80f25485bb5981fae628cc19d">When comparing model 1 and model 2, it is better to use adjusted \\(R^{2}\\) than using \\(R^{2}\\) since they have different numbers of predictors.</choice><choice value="ae2562a8c8854d19992321849553753a">In addition to \\(R^{2}\\) and adjusted \\(R^{2}\\), AIC and BIC are also common methods for evaluating this model&apos;s performance.</choice></input><part id="c03407dfeab74d78ab43d6ef3771975a"><response match="e239d855ff7141ddab58e7555e1bc18c" score="10"><feedback><p id="f8bb29a9d2154b70a169ad0c4577c3c9">Correct. This statement is not true. Adjusted \\(R^{2}\\) considers the number of estimators, and adjusted \\(R^{2}\\) will only increase when the new predictor improves more than expected.</p></feedback></response><response match="dfe823e8942d4e81be5e56f0da68f5df" score="0"><feedback><p id="b7debe398d3f4ff6861db5d8235fa5f7">Incorrect. This statement is true. When a new estimator is added, the \\(R^{2}\\) will typically increase.</p></feedback></response><response match="cffbffb0d01a43f89a0c8c148c18209a" score="0"><feedback><p id="c53d07a5af334822af80e672b06a5a06">Incorrect. This statement is true. If the estimated coefficient is equal to 0, then it does not contribute to the variation of the dependent variable and has no effect on \\(R^{2}\\).</p></feedback></response><response match="d3ff83a80f25485bb5981fae628cc19d" score="0"><feedback><p id="f59ee09f435745efb7180e947f29b7f8">Incorrect. This statement is true. Since model 1 and model 2 have a different number of estimators, it is better to use adjusted \\(R^{2}\\) since adjusted \\(R^{2}\\) takes into account how many estimators are in the model where \\(R^{2}\\) does not.</p></feedback></response><response match="ae2562a8c8854d19992321849553753a" score="0"><feedback><p id="a39805ec3f474f45ab6428d1f8882517">Incorrect. This statement is true. AIC and BIC can be applied to both linear and non-linear models.</p></feedback></response></part></multiple_choice></pool></selection></page><page id="d3b929f8ff584c959a93613c007970bd"><title>Assessment Page Title</title><selection count="1" strategy="random" exhaustion="reuse" scope="section"><pool id="cf6c1dac4ed6409bb933c674d714b025"><title>Question 10</title><content available="always"><p id="a15b222aced949f5b584985f0522ee49" /></content><multiple_choice id="a963407e8f9a47c799b02e23ba8f2f9d" grading="automatic" select="single"><body><p id="e42345c59cb64f788445ba0bd6cd2a30">You are building a virus detection algorithm to detect whether a computer program is virus (positive) or not (negative). You want to make sure that the algorithm doesn\xe2\x80\x99t miss any virus, i.e., it should be correct whenever it determines that a program is not a virus. Which metric should you focus on to evaluate this behavior?</p></body><input shuffle="true" id="f201f98189a84de484ab33ba2efef5e1" labels="false"><choice value="af2996bfe41b4e49bab6a8e69a4964d4">Accuracy</choice><choice value="ab5e0f899dd04721859a713941e1c580">Specificity</choice><choice value="dbeabc7456f5472abae75f9529cbc348">Recall</choice><choice value="f500f40c0ce748db990d9da81cc7f4d0">Precision</choice><choice value="b79e5de21ac14b3382c8c811064a6c8d">F1</choice></input><part id="fd998352fc1847568cc5205eb8bd1350"><response match="af2996bfe41b4e49bab6a8e69a4964d4" score="0"><feedback><p id="fa5c39822af34115b3cff9d336443f4a">Incorrect. You want to maximize the true negative rate, so you should focus on specificity.</p></feedback></response><response match="ab5e0f899dd04721859a713941e1c580" score="10"><feedback><p id="e98ecdb115d04bce972be77be8c3eca4">Correct. You want to maximize the true negative rate, so you should focus on specificity.</p></feedback></response><response match="dbeabc7456f5472abae75f9529cbc348" score="0"><feedback><p id="e72b7a09d4bd4de0bce05a08f61fa019">Incorrect. You want to maximize the true negative rate, so you should focus on specificity.</p></feedback></response><response match="f500f40c0ce748db990d9da81cc7f4d0" score="0"><feedback><p id="bb4d4282210d40abadf10e93b5a81e4a">Incorrect. You want to maximize the true negative rate, so you should focus on specificity.</p></feedback></response><response match="b79e5de21ac14b3382c8c811064a6c8d" score="0"><feedback><p id="bbdbfe794d094fb5aae10bf27529e88a">Incorrect. You want to maximize the true negative rate, so you should focus on specificity.</p></feedback></response></part></multiple_choice><multiple_choice id="eaf5aaf4c91a483e9e10d936185538cf" grading="automatic" select="single"><body><p id="f683d368a0974a8f84d9c59ddbfb9e35">You are building a facial recognition algorithm to allow people to unlock their phone (the phone scans the user\xe2\x80\x99s face and unlocks if it recognizes that the face belongs to an authorized user). If the face is not recognized, the user will be prompted to try again or enter their passcode. Given an input face, the algorithm is said to make a positive prediction if it recognizes the face as belonging to an authorized user and unlocks the phone. Select the correct statement.</p></body><input shuffle="false" id="d960870c44534cd298225461e9d8a396" labels="false"><choice value="b03985232298425896f3a061be87db5b">A 60% recall value means that, in 60% of the times the phone is unlocked, it is unlocked by an unauthorized person.</choice><choice value="ee5b70b75e6e44d189916d309db12418">A 60% recall value means that, in 40% of the times the phone is unlocked, it is unlocked by an unauthorized person.</choice><choice value="c06191fd953a485d8cd8af8044e7a95b">A 60% precision value means that, in 60% of the times an authorized user tries to unlock the phone, the phone does unlock.</choice><choice value="d4900bebee7d40aa847bcde99198a912">A 60% precision value means that, in 40% of the times an authorized user tries to unlock the phone, the phone does unlock.</choice><choice value="f7668bc8699447359291e46f7287240c">None of the above statements are true.</choice></input><part id="db8bdf76acee47778d96c199ca508aaa"><response match="b03985232298425896f3a061be87db5b" score="0"><feedback><p id="b7179a891661466fa4e9dd07e759232d">Incorrect. This situation corresponds to a 40% precision value, not 60% recall value.</p></feedback></response><response match="ee5b70b75e6e44d189916d309db12418" score="0"><feedback><p id="d335dbd21e204f9e81bc51a3fe3c61cb">Incorrect. This situation corresponds to a 60% precision value, not 60% recall value.</p></feedback></response><response match="c06191fd953a485d8cd8af8044e7a95b" score="0"><feedback><p id="dc786fcc21574a4fb59757395f2e299b">Incorrect. This situation corresponds to a 60% recall value, not 60% precision value.</p></feedback></response><response match="d4900bebee7d40aa847bcde99198a912" score="0"><feedback><p id="ebc8db06fe544ae9b917d52af94995f5">Incorrect. This situation corresponds to a 40% recall value, not 60% precision value.</p></feedback></response><response match="f7668bc8699447359291e46f7287240c" score="10"><feedback><p id="e95de69c662e47c298e0129ef8ca17c3">Correct. None of the other statements are correct.</p></feedback></response></part></multiple_choice><multiple_choice id="d3196c02312c4766a59cfc59e200e36c" grading="automatic" select="single"><body><p id="a8ac4f04e86a40ea8c856e4fc335223f">You are building a document retrieval algorithm for lawyers who query the system with an intention to retrieve all the documents relevant to a current case they are working on. The algorithm would match this query against a repository of documents related to past cases to retrieve relevant documents. You want to make sure that the algorithm doesn\xe2\x80\x99t miss retrieving any relevant documents, i.e., it should be able to retrieve as many relevant documents as it can. Which metric should you focus on to evaluate this behavior?</p></body><input shuffle="true" id="a6a7c153120c4c239cc59006a2a6c3ed" labels="false"><choice value="efb4e6b3a17147d8ba4c55ba0ca2f843">Accuracy</choice><choice value="f1e4050189a8462286985f4d8f73d76a">Recall</choice><choice value="c827013900284b8bbb38187d77951ff1">Precision</choice><choice value="e3076924bc3f4eaa9634c81ee0cb850b">F1 Score</choice></input><part id="d60c5afecdbb4ce0a48d9d7e3945afca"><response match="efb4e6b3a17147d8ba4c55ba0ca2f843" score="0"><feedback><p id="f94a2a1692c24df3ae4b04f5c7c7789e">Incorrect. You want to maximize recall, which is the number of relevant documents that are retrieved.</p></feedback></response><response match="f1e4050189a8462286985f4d8f73d76a" score="10"><feedback><p id="e9daac0b987b41ba81b39282c31df485">Correct. You want to maximize recall, which is the number of relevant documents that are retrieved.</p></feedback></response><response match="c827013900284b8bbb38187d77951ff1" score="0"><feedback><p id="dd2b5e07337a44098ef21f69b120a237">Incorrect. You want to maximize recall, which is the number of relevant documents that are retrieved. Whereas, precision is used the maximize the number of retrieved documents that are relevant.</p></feedback></response><response match="e3076924bc3f4eaa9634c81ee0cb850b" score="0"><feedback><p id="c6fabbf9803746d78d764dcaaef8b3d9">Incorrect. You want to maximize recall, which is the number of relevant documents that are retrieved.</p></feedback></response></part></multiple_choice></pool></selection></page></assessment>\n'