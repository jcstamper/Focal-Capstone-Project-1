b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="cd0dc533028541da8dac5675eb0babb0"><head><title>112-Analysis of Language</title></head><body><section id="ab6741063ffd43268188225395f8ef5e"><title>Levels of Representation in Natural Language Processing</title><body><p id="a4752096761f46678a12007e0f2ff55b"> </p></body></section><section id="dd8297e4f2984c29a78b09438241a591"><title>Learning Objectives</title><body><p id="a7f0bd468ac64463b94d6bbe980127b2"> </p></body></section><ol id="f609832b794b4852af01a787cfd8ef8a"><li><p id="a301a26d171c4bf68d1c5be0deb6c0db">Distinguish between different levels of abstract language representations  (e.g., morphological, lexical, syntactic, semantic) that are typically used in any language processing  pipeline.</p></li></ol><p id="a51b0318960949f5ac6e3eff1c65af3c">A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations  typically depend on the task and what is needed in downstream processing in the pipeline.</p><p id="ef7bd166befb4a548858b548fd953c2b">A typical classical NLP pipeline uses at least the representaion levels as shown in the following figure.</p><image id="b36a8a11634944e9971de73852598289" src="../webcontent/image-cd0dc533028541da8dac5675eb0babb0-1.png" alt="" style="inline" vertical-align="middle" height="355" width="468"><caption><p id="f63a384ef0124fb6900829288a51e081" /></caption><popout enable="false"></popout></image><p id="f469af257beb491e8e1d920e1b1464cc" /><p id="d1a0cffc56ea49c8ae039102d8daf7b7" /><p id="d81b6311044e488eaea5580963c1bef4" /><p id="bb04af0b394140a7b564aa824c1ebb72"><em>Phonetic and Phonological Representations</em></p><p id="d0076035a2b34260942a079f909a7090"><em style="italic">Phonetics</em> is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while  <em style="italic">phonology</em> is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc. </p><p id="ca35a3b3f3d24c30b3ac2c429af1262c">For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking, and classify segments of such of waves into abstractions called <em style="italic">phonemes</em>. Sequences of such phonemes are then transcribed into orthographic symbols making up words  taking into context usually through language models</p><p id="a5bb788df2c040e69d59a3b2b851183b"><em>Morphological Representation</em></p><p id="cdfed7265f9f403fad089833191f17ec">Morphology is the study of word structures, especially how <link href="https://www.thoughtco.com/what-is-a-morpheme-1691406" target="new" internal="false">morphemes</link>, which are the smallest units of linguistic representation  come together  and make up words which can then be used to satisfy the semantic and syntactic contraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morhemes). </p><p id="fd45a9fcdb704c1b85a5061504efe406">In many languages of the world, words typically consist one or more morphemes and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in a orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, for a word such as books can be segmented into morphemes as book+s and and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun <em style="italic">book</em>) or book+Verb+Pres+3PSg (third person singular form of the present form of the verb (to) <em style="italic">book</em>). </p><p id="f8c33d5a31bd4561bff40e8d8c485377"> <em>Lexeme Representation</em></p><p id="a87bb40580a14dac9068644c7a754c08">A morphological representation does not necessarily capture all the information in a word (or sometime in a sequence of words). The lexeme representation typically adds additional information to a word representation such as the <em style="italic">sense</em> of the root word (e.g., when we use the word \xe2\x80\x9cbanks\xe2\x80\x9d \xe2\x80\x93 are we referring to \xe2\x80\x9cbanks on the Wall Street\xe2\x80\x9d or are we referring to the \xe2\x80\x9cbanks of the river\xe2\x80\x9d? At this level, we also perhaps conjoin words that work together (e.g, <em style="italic">look up </em>or <em style="italic">piss off</em>) and treat those a single lexeme.  </p><p id="b3689477f353414383b9a3c75afe5039"><em>Syntactic Representation</em></p><p id="f278234fa2ba41c99f552d7f2f59fdc7">As one may already guess,<em> </em>not every sequence of words constitutes a valid sentence in a natural language. Consider for instance the following sentences:</p><ul id="a1cb8f276e204c6c93d9168c9be1740b"><li><p id="b13166d32eaa44c7b38f3e5128f02032">I want a flight to Tokyo</p></li><li><p id="f107544ce6b44ac6bbaf4817aa9894ca">I want to fly to Tokyo</p></li><li><p id="ba635ee5474a495caf06bb1e33a6b888">I found a flight to Tokyo</p></li><li><p id="a787a452297a42fc93b81c7f349a1bf0">I found to fly to Tokyo</p></li></ul><p id="f6b0b68abefb467ea09d31073db87a5d">The first three look fine with our understanding of valid English sentences but the last one does not.  Furthermore we sort of know that in the first sentence \xe2\x80\x9cto\xe2\x80\x9d goes with \xe2\x80\x9cTokyo\xe2\x80\x9d, \xe2\x80\x9ca\xe2\x80\x9d goes with \xe2\x80\x9cflight\xe2\x80\x9d and \xe2\x80\x9cto Tokyo\xe2\x80\x9d goes with \xe2\x80\x9ca flight\xe2\x80\x9d and \xe2\x80\x9cI\xe2\x80\x9d and \xe2\x80\x9ca flight to Tokyo\xe2\x80\x9d go with \xe2\x80\x9cwant\xe2\x80\x9d, the main verb of the sentence.  Such relationships are hierarchical and  can be captured with lingustic computational formalisms called <em style="italic">grammars</em>.</p><p id="b53ea23b0f53460ba6b27372e70dadf4">Grammars assign structure to valid sentences in a language. But at the syntax level,validity is only about the structure and not the meaning of a sentence.  For example, the sentence \xe2\x80\x9cColorless green ideas sleep furiously\xe2\x80\x9d is a syntactically perfectly valid sentence but semantically it is nonsense.</p><p id="f2c3e19f0b35458ea1e329df128022a7">The syntactic representation of sentences are hierarchical: two commonly used representations are <em style="italic">constituency syntax trees</em> based on grammars expressed using context-free grammar formalism rules and <em style="italic">dependency trees</em> based on lexical relationships between words.</p><p id="c8284d8ce39540b890496903fa63f5b0">For example, the following tree representation captures the structure of the sentence \xe2\x80\x9cA boy with a flower sees a girl with a telecope.\xe2\x80\x9d The various symbols such as <em style="italic">NP</em> (noun phrase) or <em style="italic">VP</em> (verb phrase) are names of various intemediate structure types as defined by the underlying  grammar.</p><image id="f21a28ba06804f0c94d7aeba508df41a" src="../webcontent/image-cd0dc533028541da8dac5675eb0babb0-2.png" alt="" style="inline" vertical-align="middle" height="234" width="468"><caption><p id="e3c6d18441d046d9894c2afaeb44b920" /></caption><popout enable="false"></popout></image><p id="d26e3e9a2f8b4f47bf529ae33a4324e5">Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.</p><p id="d4e8259da3ba430c8e5e532c3430d4b4">The sentence can also have the following tree representation.<image id="df3a3f1e39624e5ba77c4703113530ef" src="../webcontent/image-cd0dc533028541da8dac5675eb0babb0-3.png" alt="" style="inline" vertical-align="middle" height="256" width="468"><caption><p id="a0f6c4ba04ba49419a91c1312cbcf955" /></caption><popout enable="false"></popout></image></p><p id="f27fad68b4d845a2b7756eb1a3bf6908">This is for the interpretation where the girl is carrying a telescope!</p><p id="e2abcfc388f944cfa9a782b240fa5f9f">This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all  inputs (remember the two possible interpretations of \xe2\x80\x9cbooks\xe2\x80\x9d above which needs further context to resolved during actual processing.) Resouting of such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.</p><p id="e19e350c41a44e69a379e99eee8e28ea">A more recently commonly used syntactic representation relies on <em style="italic">dependency relationships</em> between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between head words and dependents, with a label denoting the relation as shown here.</p><image id="fa3f28a19dca4ab486da94cf06279653" src="../webcontent/image-cd0dc533028541da8dac5675eb0babb0-4.png" alt="" style="inline" vertical-align="middle" height="133" width="370"><caption><p id="e87ebdd4727843eeb562376f3fa893f6" /></caption><popout enable="false"></popout></image><p id="a412113e3275432a8c8d6dc09b6dea40">Here \xe2\x80\x9csaw\xe2\x80\x9d is the main meaning carrier of the sentence. \xe2\x80\x9csaw\xe2\x80\x9d has a <em style="italic">subject</em> \xe2\x80\x9ckids\xe2\x80\x9d and a <em style="italic">direct object</em> \xe2\x80\x9cbirds\xe2\x80\x9d. \xe2\x80\x9cfish\xe2\x80\x9d is related to \xe2\x80\x9cbirds\xe2\x80\x9d as a <em style="italic">prepositional object</em> which itself is related to \xe2\x80\x9cwith\xe2\x80\x9d which is a <em style="italic">preposition</em>.</p><p id="f83bfcd97d9848d88accb588b830bd81"><em>Semantic Representation</em></p><p id="dbcc10a94dd5407c9061eedf3d11548f">Loosely speaking, this level represents the \xe2\x80\x9cmeaning\xe2\x80\x9d of a sentence, sometimes  compositionally scaffolding on the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties and event in a \xe2\x80\x9cworld model\xe2\x80\x9d and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as \xe2\x80\x9cEverybody has something they  like.\xe2\x80\x9d would be represented by a logical form such as $\\forall x \\exists y likes(x, y)$.  The truth value of such a sentence can then be computed based on the description of the world model.</p><p id="dba0160578124ec88cc16c4c00388f62">A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using <em style="italic">semantic roles</em>. Such representations assign the same semantic representation to syntactically different sentences if those are expressing essentially the same event.  For example all these sentences </p><ul id="b4ea079c6aca48299fc7e39df89332bc"><li><p id="de85e72483984acbbc7752f51325fbc8">Warren bought the stock.</p></li><li><p id="efafc38ef9a7469ea27c7133051ca900">Someone sold the stock to Warren.</p></li><li><p id="daab5c01bc8048baa037c05664dd048f">The stock was bought by Warren.</p></li></ul><p id="dca02f5c66ec491191a99d95558602da">are describing the same \xe2\x80\x9cselling\xe2\x80\x9d event where the <em style="italic">buyer</em> is Warren, and <em style="italic">stocks</em> are sold and the <em style="italic">seller</em> is not known or not expressed explicitlybut it is inherent .  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.</p><p id="c6b9dad126f54b03b77cca85cb0f4aab">Much more recent approaches to semantic representation especially in deep learning contexts rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine transtion system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.</p><p id="fb0901bc5d8a4ba4b8c9e5120bc019a8"><em>Pragmatics</em></p><p id="c4007b02b6d04756a4be9413384f863b">Pragmatics deals with understanding how the context an utterance is made in or a sentence is used in, contributes to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual  information also includes  intonation, physical gestures, and social identity.  For example an utterance such as \xe2\x80\x9cCan you pass the salt? \xe2\x80\x9c in a dinner setting is really not a question on someone\xe2\x80\x99s ability of passing the salt, but is rather interpreted as a gentle request. </p></body></workbook_page>\n'