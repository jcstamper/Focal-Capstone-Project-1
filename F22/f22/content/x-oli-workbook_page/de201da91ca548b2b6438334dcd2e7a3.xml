b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="de201da91ca548b2b6438334dcd2e7a3"><head><title>Transformer Architecture</title></head><body><p id="f108ac7a78d447269b81a1aaf9b096f3">Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.</p><image id="c692813099d04c8e9b238729b6babe20" src="../webcontent/image-de201da91ca548b2b6438334dcd2e7a3-1.png" alt="" style="inline" vertical-align="middle" height="423" width="650"><caption><p id="c3c06d25e0b74c2c9888bed8d5e33463">Figure 6: Transformer architecture.</p></caption><popout enable="false"></popout></image><section id="e00c99554c024f41b511283568586a89"><title>Encoding</title><body><p id="faeb0745cd0d4539964e9e2ade143a40">Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.</p><image id="bce38dcedb7244e1948264c683282efb" src="../webcontent/image-de201da91ca548b2b6438334dcd2e7a3-2.png" alt="" style="inline" vertical-align="middle" height="358" width="650"><caption><p id="d0e63384ae5941df8324f72830112ef8">Figure 7: Transformer Encoder Inputs.</p></caption><popout enable="false"></popout></image><p id="fd11effebeff400eb1850ee4a88480cb">Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer \xe2\x80\x93 that helps the encoder look at other words in the input sentence as it encodes a specific word.</p><p id="d021e6b555454529917e3ff8fe3ba033">The self-attention layer begins by creating three vectors from each of the encoder\xe2\x80\x99s input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.</p><p id="dbb33564df954e8dbb0742db38f0d8de">The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (<em>query</em>) is free to take as much information using the dot-product mechanism from the other words (<em>values</em>), and it can pay as much or as little attention to the other words as it likes by weighting the other words with <em>keys</em>.</p><p id="b6dd55b1cef14d509362d765425f4806">The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a <link href="https://arxiv.org/abs/1607.06450" target="new" internal="false">layer-normalization</link> step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.</p><image id="f1a86d302b164a36827008093e2f27cc" src="../webcontent/image-de201da91ca548b2b6438334dcd2e7a3-3.png" alt="" style="inline" vertical-align="middle" height="471" width="650"><caption><p id="e5ce38df73ca4e31aca66212d3afc04c">Figure 8: Transformer Encoder Architecture.</p></caption><popout enable="false"></popout></image></body></section><section id="ad33ed43bc3c46c0bb26810d5e7314b0"><title>Decoding</title><body><image id="c9d9965caadb435e9ee89aad8c876833" src="../webcontent/image-de201da91ca548b2b6438334dcd2e7a3-4.png" alt="" style="inline" vertical-align="middle" height="369" width="650"><caption><p id="e8b70b167ca24b5a965f0c70cde20b7c">Figure 9: Transformer Encoder-Decoder.</p></caption><popout enable="false"></popout></image><p id="c56b717b8ba44707bb73c808cfe0acb4">Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.</p><p id="bc0cd3d5eba6427491f787b7d41afe7b">In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated. </p><p id="af50d0d651254a3292e0d8330f81e56d">After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The \xe2\x80\x9cEncoder-Decoder Attention\xe2\x80\x9d layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.</p><p id="bb10b8de336c492ca93e675d16ec4931">At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.</p><p id="c25a7d28a7204290bd4acc5ca97e0c6a">Here are some additional sources for more details on Transformers.</p><p id="d80aefce617a4dad9780f8202215424f" /><ul id="c2c99c8a482d40df9e130c0bbc65515c"><li><p id="b3240e7ba62a484bb54cac5295d1cc37"><link href="http://nlp.seas.harvard.edu/annotated-transformer/" target="new" internal="false">The Annotated Transformer</link></p></li><li><p id="acb8883167ef4d90906d99d907966c29"><link href="https://jalammar.github.io/illustrated-transformer/" target="new" internal="false">The Illustrated Transformer </link></p></li></ul></body></section></body></workbook_page>\n'