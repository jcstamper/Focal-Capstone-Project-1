b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="a485a8554ea04ec48e57ffa11682c30a"><head><title>Regularization</title><objref idref="baaf5d368db14e95bab130ac824e8eaf" /></head><body><p id="a8a6f7e17adc4822ad7bc924137f22eb">The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets. </p><p id="ec5e277903f7410c84590590867a8666">Regularization is a general approach to help select a balanced model to trade off between a high bias and a high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve,  hence, the need for the trade-off to minimize the model&apos;s total error.</p><p id="c72a9c419a3b4ad1a5ec7d7eec71879d">There are three popular regularization techniques, each of them aiming at decreasing the total size of the parameters of the model (e.g., coefficients in a regression):</p><ul id="f45a8a4ab0cf4c3a927e685d0363c7a2"><li><p id="c6af143c7336422f9d01ae8a1fcf0fa1">Ridge Regression, which penalizes the sum of squares of the coefficients (L2 penalty).</p></li><li><p id="f9432895b75b4b8face8adb994e8ce0e">Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).</p></li><li><p id="e0756d1e02be4b22bdaf85dadd39ed2a">Elastic Net, a convex combination of Ridge and Lasso.</p></li></ul><image id="b3cef748e1e7482697119286285f6c0d" src="../webcontent/main-qimg-583e319a860c9cc4a187e005fb4b7353.png" alt="" style="inline" vertical-align="middle" height="394" width="650"><caption><p id="ec2c87c25d32477f9bc88b9b89ae12ca">L1 and L2 Regularizations. </p></caption><popout enable="false"></popout></image></body></workbook_page>\n'