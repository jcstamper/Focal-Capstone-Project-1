b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="e4fb0de27ddb427893600030b3b08e5c"><head><title>Statistical Inference</title><objref idref="f4d0b27f5b9e41ef9f78810b14467651" /></head><body><p id="c1d3853d8f4548b3b0d7d18da115c231"><em>Statistical Inference</em></p><p id="a26631e8d2ee4864ab80befa4dfeb18c"><em style="italic">Statistical Inference</em> is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. </p><table id="f385335310f74e5caa8645b62088b77c" summary="" rowstyle="plain"><cite id="i433904e666aa4a709cd31fc58216ba7e" /><caption><p id="cce87e8df4054712af7d93090909cd8f" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="cc9d9d4525ba45dfa4186e368ee3fea1"><em>Reminder: </em>The characteristics of the sample dataset are called <em style="italic">statistics!</em> The characteristics of your population are known as <em style="italic">parameters.</em></p></td></tr></table><p id="c912cd87236e4388972b19576eb60d09">Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision making purposes.</p><p id="b8dbee45133e46dc97dec03b0c472082">If you recall from a previous unit, you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise. You can use statistical inference to draw scientific conclusions and test set hypotheses. Significance of a sample data set or descriptive statistics is often in question during the EDA process, using statistical inference techniques can give significance to your conclusions from EDA. Statistical inference techniques are categorized under <em style="italic">Estimation</em> and <em style="italic">Hypothesis Testing.</em> Let us explore these methods:</p><p id="f04b37d032e0403b9b23f04b90de2805"><em>Sampling Distribution</em></p><p id="d4bd1bf39b974485a31123ab2ade8ebd">You typically draw a sample dataset from your population as it is quite difficult to perform analysis on an entire population. Let us consider a quick example, assume we are analyzing the income data of all Neurologists in the United States of America. We can make inferences about the population mean income by calculating the mean of income on a sample of 2,000 Neurologists. This mean is the <em style="italic">Sample Mean x\xcc\x84. </em>We also refer to this as the <em style="italic">point estimator </em>of the population mean. If the mean of our sample is $258,900, then we refer to this number as the <em style="italic">estimate </em>of the population mean. </p><p id="f7b36bc1f7f544cc87980037a5a62549">Let&apos;s expand this further! The American Academy of Neurology conducted a study that estimated the number of Neurologists in the US at 16,400. We can draw another sample that will result in a different mean. Consider you draw multiple samples and record each sample mean, you will have what is called a <em style="italic">sampling distribution. </em>If you continued to draw samples from this population, the average value of your point estimator will equal the population mean. We can say that a point estimator is <em style="italic">unbiased </em><em>if</em> its expected value equals that of the population.</p><p id="f8a6c990d55d4e02a4555555e7debc7c">Keeping with the example above, let us derive the variance of your sample mean. If we continued to sample our neurologist population, the variance of the sample mean will be the variance of our population, divided by 16,400. Note that the variability between observations is usually larger than variability between sample means. This is because your sample contains a range of observations. Finally, you will derive the <em style="italic">standard error </em>of the sample mean. This is the population standard deviation divided by the square root of the sample size or simply, the standard deviation of the sampling distribution. </p><p id="ebfc06804e45473f98d4cb381cace518">Standard Error = Population Standard Deviation/</p><p id="e89be5a3a76145d4845bbfc44c34471f">The standard error is important because it measures how accurate the sample distribution represents the population.</p><ul id="bfbe8882c0f94d4593140f5972616e9d"><li><table id="a6d559f7504e49898198850fb73d7793" summary="" rowstyle="plain"><cite id="i8a41048f628240aea7af71fe183541e6" /><caption><p id="ef186c6d1e2b48259f58662a3f63eb77" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="fa712b60ddd14657a4aa61c1b7aba5a5"><em>Please note</em> that the sampling distribution is considered normal if the population mean is normally distributed. This is highlighted because you can not use most statistical inference techniques if the sampling distribution of your sample mean is not normally distributed.</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="c658953db04647569e8e1464f046238a">There is a popular theorem that addresses the situation when your population is not normally distributed. This is known as <link href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="new" internal="false"><em>The Central Limit Theorem!</em></link></p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="ff89910e38bd43efb0f7cd1a241e086b">Statistical inference is not solely applied to means, it is also applied to proportions.</p></td></tr></table></li><li><p id="a60c39603c764fbba4f559bc62222f7e">You will work with proportions in clustering analysis tasks. If a telecommunications company is assessing the proportion of customers who sign up for a contract after receiving a promotional advertisement, the parameter of interest is the <em style="italic">population proportion p. </em>As the data scientist tasked with this analysis, you will make an inference about the population proportion by drawing a sample. In this case, the point estimator is the sample proportion <em style="italic">P hat or p\xcc\x82. </em></p><p id="c9b5c6cd98684234b3157362e07e0f25">Reading: P<link href="https://en.wikipedia.org/wiki/Population_proportion" target="new" internal="false">roportions</link>.</p></li></ul><p id="e5abfb9edcbf4b829caa668e5299e33e"><em>Confidence Interval</em></p><p id="d392ac7c97304285b3e7a6433be68f6c">When we provide a range of values of estimates for a population parameter, we are referring to the <em style="italic">Confidence Interval (CI). </em>You can think of it as the range of likely values for a population parameter with a specified level of confidence. The sampling distributions of your sampling mean or sampling proportion must be normally distributed to derive an accurate CI. The sampling distribution of the sampling mean and sampling proportion will be normally distributed if the sample size is large (in most cases that is n is greater than or equal to 30). The sampling distribution of your statistic (mean or proportion) is needed to derive your CI.</p><p id="cc3f538489ac42f19b8974396c4e4eef">You will use the <em style="italic">margin of error </em> to account for the standard error of your point estimate and your desired confidence interval. Consider this example:</p><example id="da482ed6226f49c4811582aad0eabbe5"><title>Confidence Interval (CI)</title><p id="abab6f7fb46b4565be26114c61e751ca">Consider that we are measuring the heights of 40 randomly selected male soccer players, our sample mean is 175cm. We calculated the standard deviation of the athletes heights and it totaled 20cm. Let us calculate the CI. </p><p id="bd15007efdf34888b711ca967662d572">n = 40, mean = 175, s = 20.</p><p id="a2dd909c9d8c4c318fd9e492616af400">You will decide on the CI to use (95%) and then find the Z value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value. </p><p id="ed50d790f03f4d5e85a56e6f11d2cf37">The Z value for 95% CI is 1.960.</p><p id="e7eeca9a04b74916a24ba55b0ac45e66">We calculate the 175 \xc2\xb1 1.960 \xc3\x97 20/\xe2\x88\x9a40</p><p id="e4877907ad8540d3add1b5e9d8686af0">175cm \xc2\xb1 6.20cm</p><p id="d41b76043b5e4c84b53bf740516dac6e"><em>168.8cm to 181.2cm</em></p><p id="d487af00376441c284ddf8d6223cf601"><em style="italic">Source</em><em style="italic"><sup>1</sup></em></p><quote><em>Degrees of Freedom: </em>Usually the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate for the true value of the standard deviation, the sample mean follows the t distribution with mean and standard deviation . The t distribution is also described by its degrees of freedom. For a sample of size n, the t distribution will have n-1 degrees of freedom. The notation for a t distribution with k degrees of freedom is t(k). As the sample size n increases, the t distribution becomes closer to the normal distribution, since the standard error approaches the true standard deviation for large n. </quote><p id="cc80b239ff46444fb0b3b1ba0de07e25">Reading: <link href="https://www.itl.nist.gov/div898/handbook/prc/section1/prc14.htm" target="new" internal="false">Confidence Interval.</link></p></example><p id="d64a7ff459014455821bcf1b233652eb"><em>Hypothesis Tests</em></p><p id="a07078f2e1f64b0ea86ae78a26f66944">As you conduct research and complete data science projects, questions will arise about the likelihood of occurrences. As we have seen so far, statistical inference helps to ground your insights with statistical significance and does its best to rule out the possibility of chance. We have looked at Estimation and Confidence Intervals to help make inferences from your data. Now we will explore the oldest statistical inference, <em style="italic">Hypothesis Testing.</em></p><p id="ca4629223f08482a9594f3361261fb08">A <em style="italic">hypothesis </em>is &quot;an interpretation of a practical situation or condition taken as the ground for action&quot;. Similar to the other techniques, you are looking to draw a conclusion about a population using a sample data set. When you apply statistical hypothesis testing, the end results are always favorable because (according to Enrico Fermi) you make a measurement or a discovery. </p><p id="a91eefb85bb2487fab7b53ba55ff0c9b">According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120 day skin care line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage, this is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:</p><ul id="c90d50754a144fe8a008ac3b1a220d3a"><li><p id="bbd7d35deea44f20962d7ecf3702b257">Identify the population parameter of interest. </p></li><li><p id="be1573ad7a3a4cfd8e02e4d91923f200">Determine whether you will be conducting a <em style="italic">one-tailed or two-tailed test.</em></p></li><li><p id="bbdd82c339294dab88f2bd481eace254">Define a <em style="italic">null hypothesis, </em>often denoted as <em style="italic">H</em><sub><em style="italic">0. </em></sub>The null hypothesis is considered the status quo or in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.</p></li><li><p id="c6fe7be5ece74ff3a66f6f71127e37ac">...then define an <em style="italic">alternative </em>hypothesis, denoted as <em style="italic">H</em><sub><em style="italic">A. </em></sub>This would be the <em>opposite </em>of the null hypothesis. </p></li></ul><p id="dc687e99914b4d62b9eb6f26103f7790">The example above does not cover the entirety of identifying your null and alternative hypothesis. You must know that if proven, your alternative hypothesis is a call to action i.e. if you reject your null hypothesis, then the status quo has been changed and the decision makers must take action. How do we test our hypothesis statistically? </p><p id="ffafa8568c914f7abc5a1ffde7b249c7"><em>Reading</em>: We do this by using the <link href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests" target="new" internal="false">one- and two-tailed tests</link>.</p><p id="f64a59be2aeb4d119664af79995a29ba">Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.</p><ul id="b39d69da4f9e40be9459ad5c424c8bd3"><li><p id="b72f66496802475ca184e72fb233c6d2"><em style="italic">Type I error </em>occurs when you reject the null hypothesis when it should be accepted. </p></li><li><p id="e29bfc7f3b364b8d9be56ab30b538097"><em style="italic">Type II error </em>occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected. </p></li></ul><p id="bcf7ef3ed68541338ddd0cfa0d4e00f5">Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin damaging treatment product, that would lead to loss of customers and possible lawsuits. </p><table id="bb0342c1a91a4346b386c5bbdfa14493" summary="" rowstyle="plain"><cite id="i443f69edb1dc4e0b9bcd9f30c4247351" /><caption><p id="ca037555f3f149049138ce8682ac75c6" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a9cfe9f754d9459ab0e739e6cadb0754">How do you ensure that both errors do not occur? <em><em style="italic">Collect more data! Either increase your sample size or collect more data over a longer period of time. </em></em></p><p id="ddd92baf426f42218b7bbc24c6f0b49c">Collecting more data does not entirely mean that you will reduce both errors but it ensures that you commit one over the other at a lesser magnitude. </p></td></tr></table><p id="a09aa321e1614390b3359926506d9be8">In the module, we will continue the process by learning how to extract features of variables within the dataset to improve the performance of analytic solution(s). </p><wb:inline idref="newb324b652108541a29dcf9dd666144660" purpose="learnbydoing" /></body></workbook_page>\n'