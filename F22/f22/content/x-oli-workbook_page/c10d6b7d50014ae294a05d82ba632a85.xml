b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c10d6b7d50014ae294a05d82ba632a85"><head><title>Hypothesis Testing</title></head><body><section id="a970147304104475acda46a3e9f5ae20"><title>Introduction to Statistical Hypothesis Testing</title><body><p id="c070f4df86864826a92ff09fa75496e1">When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly \xe2\x80\x9cdifferent enough\xe2\x80\x9d for you to pick one model over another.</p><p id="cedcfee081644f7e904d446ac2cb0638">To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but \xe2\x80\x9cgood enough\xe2\x80\x9d classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are <em>no</em> associations between the features and the label.</p><p id="e5338c9ea07c4225bb4b73c5f6c5242c">Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.</p><p id="e7ff4a64ef00467fbfa841fa704f5431">In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a <em>hypothesis test</em> on your performance measures.</p><p id="fcd13c4d51e64ecf9f6774ce0d392af5">A <em>hypothesis test</em> is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a <em>population parameter</em>, and decide if you have a <em>statistically significant result.</em></p><table id="d2fab96f951e49889f28e5f6189038f9" summary="" rowstyle="plain"><cite id="ib994815edf7f4babad04a5274f5a41e3" /><caption><p id="fb59c872904f41c0bdf78c662e5b8146" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="de2f112218204d1ab7c7f16e4262aa5d">&#x1f4a1; Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.</p></td></tr></table><p id="a471f26056d444738fe11db226e5de7c">Performing a hypothesis test involves three major steps:</p><ol id="bf257b8e50254307b15902c63572ab1b"><li><p id="c1b5963b7c994074860d35d343e7720e">Deciding what your <em>Null Hypothesis</em>, \\(H_{0}\\) and what your <em>Alternative Hypothesis </em>are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to \xe2\x80\x9cdisprove,\xe2\x80\x9d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.</p></li><li><p id="eaddb7e830474a6196d8ab6b5f5c7157">Computing some sort of \xe2\x80\x9ctest-statistic\xe2\x80\x9d. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.</p></li><li><p id="effae8790bee496e9c1dd4353ab2980d">Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence \xe2\x80\x9cthreshold\xe2\x80\x9d, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.</p></li></ol><table id="a513747537e3491e85e7236160312777" summary="" rowstyle="plain"><cite id="if7f70f02212342508d38deb41ada3b43" /><caption><p id="d8da32c90302475d9cc166ac37641631" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a69a9cdc90d94097a09c6199a7ea8b92">&#x1f4a1; It is important to note that \xe2\x80\x9crejecting the null\xe2\x80\x9d does NOT mean \xe2\x80\x9caccepting the alternative\xe2\x80\x9d. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.</p></td></tr></table><p id="cb71919092ea4774b3dd5dcc695b9154">When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are <em>unlikely</em> given the assumptions and evidence; they are <em>NOT</em> iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.</p><p id="b3734fcef65b4cc984d550d63e0124cc">That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.</p><p id="e4649fe31ec84e32912c6a77b5dd5e1e">Without further ado, let\xe2\x80\x99s discuss the first statistical test of this module and one of the forerunners of hypothesis testing: <em>Welch\xe2\x80\x99s t-test.</em></p></body></section><section id="d2c3df67d54c4c6a967f9a20e93d282d"><title>Welch\xe2\x80\x99s t-test</title><body><p id="c01b8af2597a4420831907179061cbcb">Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.</p><p id="e56beb3726ed499498e029642e14e39c">If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.</p><p id="c231e6f25dca429c95dae1cecfcb3d38">We could try to test if they are just different from each other with the following hypotheses:</p><ol id="f7ce38cedeb1448ca172b3181b2cbca6"><li><p id="d9dcbcd31a3549ed940049863a601828">\\(H_{0}: \\mu _{\\alpha}=\\mu _{\\beta}\\)</p></li><li><p id="a85a4b9b56af4a04a4b6e0d77c4d4148">\\(H_{A}: \\mu _{\\alpha}\\neq \\mu _{\\beta}\\)</p></li></ol><p id="eb8cf3445c36464ab4f82cba3eeae615">Or we could test that one is strictly larger than the other:</p><ol id="af5e62eb0bf5445cb0fc397e66b74fdf"><li><p id="e235eb66bd4b479b881dca3943943994">\\(H_{0}: \\mu _{\\alpha}&lt; \\mu _{\\beta}\\)</p></li><li><p id="a7a65bcb1c434ba58cc3fe184fa6302a">\\(H_{A}: \\mu _{\\alpha}&gt; \\mu _{\\beta}\\)</p></li></ol><p id="fe854cc625b64298bc61b2b25ccca7ee">The first kind of test is known as a \xe2\x80\x9ctwo-tailed t-test,\xe2\x80\x9d while the second is known as a \xe2\x80\x9cone-tailed t-test.\xe2\x80\x9d Either way, we\xe2\x80\x99ll end up following the same procedure, so we\xe2\x80\x99ll continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a \xe2\x80\x9cp-value,\xe2\x80\x9d or the probability that such a test statistic can happen given the null hypothesis.</p><p id="b98a3e5e5fd042b2a814cdb6952c52c5">In our case, we have two averages and want to look at their differences. For the student\xe2\x80\x99s t-test, we shall use the aptly named \xe2\x80\x9ct-test statistic\xe2\x80\x9d:</p><p id="b18da9ff8a714da497784a6888ac8df7">\\[ T = \\frac{\\mu_\\alpha - \\mu_\\beta}{\\sqrt{\\frac{\\sigma^2_\\alpha}{n_\\alpha} + \\frac{\\sigma^2_\\beta}{n_\\beta}}} \\]</p><p id="de37865396cf46d381955880a9a628be">In particular, we are going to use what is known as \xe2\x80\x9cWelch\xe2\x80\x99s t-test statistic,\xe2\x80\x9d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer <em>degrees of freedom, i.e., approximately fewer samples</em>. To calculate the degrees of freedom, we simply need to compute the following:</p><p id="c2b4f89b8c9f4ee299025593f7fd9520">\\[ \\nu = \\frac{(\\sigma^2_\\alpha + \\sigma^2_\\beta)^2}{\\frac{\\sigma^4_\\alpha}{n^2_\\alpha(n_\\alpha-1)} + \\frac{\\sigma^4_\\beta}{n^2_\\beta(n_\\beta-1)}} \\]</p><p id="cd3a9bed484341e787279849ef0cc0a7">With these values, we can then compute the <em>p-value</em> or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.</p><p id="aaa3807618194684b7866d18d4b95040">While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.</p><p id="d905a817a9614caf869a13e6c90be6a2">However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.</p><p id="cb5cc81336a04d2dacbfc009482bac17">Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.</p></body></section><section id="f94ef1a20f22495bb21bd2f4ef63a85a"><title>Paired t-tests in Model Comparison</title><body><p id="e6a0fca258114821ba847c66b8eb2ea8"><em>McNemar Test</em></p><p id="b3e6c12c11664426a9d7b35b8ebf233b">To help combat some of the issues with Welch\xe2\x80\x99s t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\\text{accuracy}\\).</p><p id="b930d17c36e44290bf20eabe9c8e33a7">If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:</p><ol id="f3bd4fe84d304cc88e3cdcc9331ddd66"><li><p id="cdcad87b53a9459b86e05ff1f2a5f9ab">\\(H_{0}: E _{\\alpha}=E _{\\beta}\\)</p></li><li><p id="c02bfba558544f249edf0a0302fe82da">\\(H_{A}: E _{\\alpha}\\neq E _{\\beta}\\)</p></li></ol><p id="a6bd627e2fb34603a1b7263ecf8afa31">For this test, our test metric is actually much simpler:</p><p id="b60d107c4e574b82bcb93f7cb2504b53">\\[ \\frac{(|E_\\alpha - E_\\beta|-1)^2}{E_\\alpha + E_\\beta} \\]</p><table id="f1bd50b31e6b4234a2824864f7ea34b8" summary="" rowstyle="plain"><cite id="ibfc6e4d5db334dcf8514e9f0c834524f" /><caption><p id="bfd6ae2e053b4721a6c93999b5be5a13" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="a62071b648f24a1e819fc0ce3603d8e3">&#x1f4a1; In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.</p></td></tr></table><p id="b23a5b2b2e3d4e568965542a154d4e10">Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.</p><p id="fa860fcd3da543419a4e04404651385b">While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welch\xe2\x80\x99s t-test. Namely, it just assumes the samples you have tested are <em>independent</em> or that no datum\xe2\x80\x99s feature-label pairing depends on another datum\xe2\x80\x99s feature-label pairing.</p><p id="fbc9f10c11c8423e910f059e45e1ae11">On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welch\xe2\x80\x99s or another paired t-test variety.</p><p id="eb85c6103b1c45e4b47797a44fd99b27">However, both of these tests do suffer a single, incredibly critical flaw.</p><p id="b7827f1a9f944845876814b68f271fb1">Namely, they suffer from \xe2\x80\x9c<em style="italic">p-hacking</em>.\xe2\x80\x9d</p><p id="f4f51a2cafa3424881a952648979836b"><em>P-Hacking</em></p><p id="a65c39788f08429084779058dcc3665b">From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.</p><p id="cb775326d8bf4ef09ae1f77f7bca3a58">One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics <em>on the same data</em> could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.</p><p id="b4bd3036a33d48df96301165ae4741da">Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.</p><p id="a65b66649ef641848ad6c149349e61d6">For our problem, this is especially grave. Consider that, for <em style="italic">n</em> models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.</p><p id="c479393ae3f841dba7226ceaccb872fe">Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?</p></body></section><section id="fbd05d3d613b4eb280df671b55a60616"><title>The Friedman Test</title><body><p id="b984b46008e5477f898ea094513125d5">To correct this issue, we must introduce the concept of the Friedman test. If we have <em style="italic">n</em> data sets to compare with and algorithms to compare, we first define the concept of a \xe2\x80\x9crelative rank\xe2\x80\x9d between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.</p><table id="a2306ad18eab4bde9f988c93cc65be6a" summary="" rowstyle="plain"><cite id="i28632fd2a7d9426083b231f77b030bed" /><caption><p id="cd833920368b4570b8f5d44fbd925989" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="cd10129cac96477ea2451048274a71ec">&#x1f4a1; If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.</p></td></tr></table><p id="b9785997293947dab9a2d7dc72c72f7b">With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.</p><p id="b88308ca8b0846838da7150a84722e97">The hypotheses are the following:</p><ol id="ea9c5d09ef494466bef9ad80d92c7ff0"><li><p id="eebc0e36f7294d98a8bb770b901b795d">\\(H_{0}: r_{1}=r_{i}=...=r_{k}\\)</p></li><li><p id="ab7244cf8b184cd6bf04513a89b563d4">\\(H_{1}:\\) They are not all equal.</p></li></ol><p id="a91218a0cace476d93b33eb755738cc0">and the associated statistic is:</p><p id="f11dadbc937a4687a51316a4533ebe83">\\[ \\frac{12n}{k(k+1)} \\sum_{i}^{k}(r_i - \\frac{k+1}{2})^2 \\]</p><p id="d88f3dabfb084c48bb48c64bef454d45">For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.</p><p id="ad29191234bb430785d47b395da8aa29">With this test, the main problem comes from what happens after you reject the null. The test itself simply states that \xe2\x80\x9cthere is likely some difference between the ranks of each algorithm\xe2\x80\x9d. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a \xe2\x80\x9c<em style="italic">post hoc</em> test\xe2\x80\x9d to find the best-performing algorithm, assuming the Friedman test\xe2\x80\x99s null hypothesis was successfully rejected.</p><p id="f8f7507773544b148ac061b4d5c30c90">There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of <em style="italic">post hoc</em> tests:</p><ol id="a7793831403448e5a0b6ca86e20626d4"><li><p id="d5bb10c6797345e58b14444b7ea52838"><em>Tests that perform all pairwise comparisons:</em> Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.</p></li><li><p id="cb298d67644843a9b3bcd0aa83c9fd8f"><em>Tests that compare with a baseline:</em> When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of \xe2\x80\x9cwhich of the models I\xe2\x80\x99ve tested are better than the baseline\xe2\x80\x9d? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.</p></li></ol><p id="f6c3aa23c1f94a59915efe41ff95e59f">Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.</p></body></section><section id="e4859da1d0ea4243a8be47ed2c9cae24"><title>Errors in Hypothesis Testing</title><body><p id="d839d13f66aa46b9a72b6d9055579a9b">According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:</p><ul id="d0d913f39d574a91891959c2df40aee1"><li><p id="b24288d6f6ce4c1191777bfa9d88eecd">Identify the population parameter of interest.</p></li><li><p id="dc313634cd9a46aba435ff87792b2a9c">Determine whether you will be conducting a <em style="italic">one-tailed </em>or <em style="italic">two-tailed test</em>.</p></li><li><p id="c635e0cd01ad46b3a2d32d49954dfdef">Define a <em style="italic">null hypothesis</em>, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.</p></li><li><p id="b9c3016264954ab2bd00b8b476583e85">...then define an <em style="italic">alternative</em> hypothesis, denoted as \\(H_{A}\\). This would be the <em>opposite</em> of the null hypothesis.</p></li></ul><p id="ed328ed9df5b49e1957ec7bdb4722262">The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?</p><p id="c6e83974c84f415da8e19840d0431c7d">Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.</p><p id="a2a1588b85764e7ca14b67b9fcd6b33f"><em style="italic">Type I error </em>occurs when you reject the null hypothesis when it should be accepted.</p><p id="cae897086dae45d68be2596bd838a2da"><em style="italic">Type II error </em>occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.</p><p id="db42f990184d411691da6b7f98b712aa">Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.</p></body></section><section id="c6561a9f024a4866943c6d0fb309d747"><title>p-value</title><body><p id="e957a3bbc39d4075912bd3f13b4c2e93">As you may have wondered by now, what the p-value is and its role in hypothesis testing. The <em>p-value</em> is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.</p><p id="cf9c1fc02de24a15a4ad4a5f58f0edd7">Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)</p><p id="e5045f3b4dc64f1c862c45744864f047">Here, given a z-score of -2.878, we can calculate the p-value as,</p><p id="c8638a5861ef477d81cd71df103a85ad">p-value = \\(2\\times P(z&lt; -2.878)\\)</p><table id="aa3685b94540439393bc70d56296d63c" summary="" rowstyle="plain"><cite id="i28ada06365724ceca0630d1bb6631e18" /><caption /><tr><td colspan="1" rowspan="1" align="left"><p id="d7d9b7613dd844aaabb14473ae4def7a">&#x1f4a1; Note: Since we\xe2\x80\x99re conducting a two-tailed test, we can then multiply this value by 2.</p></td></tr></table><p id="e3516a38104646daa2cf96e44205d962">If you locate -2.878 in a <link href="http://www.z-table.com" target="new" internal="false">z-score table</link>, you get a value of 0.002.</p><p id="cd081a56a0f34357bf1fc2e0db74005d">p-value = \\(2\\times 0.002\\)</p><p id="d48fea0cfd8f46398d0f5cabe16847e0">p-value = 0.004</p><p id="cabb9a4c7e184475baf7475a0a69358a">So, we have our p-value &lt;  \\(\\alpha_z\\)</p><p id="ebc5e79862414c10bb9080b6d941116d">Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.</p></body></section></body></workbook_page>\n'