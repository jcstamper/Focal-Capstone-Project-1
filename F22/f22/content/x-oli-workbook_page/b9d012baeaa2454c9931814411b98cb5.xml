b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="b9d012baeaa2454c9931814411b98cb5"><head><title>Converting Text Data into Numerical Features</title><objref idref="c2a8f98843b244db8e81deb14f7ad85f" /></head><body><p id="ca14b31154e947abb37e503e38a793b4">Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.</p><p id="def669c6da7846fda825f309285fe244"><em>1. Bag-of-Words</em></p><p id="d1ef1449139f494684b4df9a51d21af5">Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts.  It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document.  Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.</p><p id="c9c18eb5a9694af3aee237a9c7b5477e"><em>2. Term-Frequency </em></p><p id="e40b8715ab9f43788c5b3a2a4edd5492">Instead of representing the entire corpus as a one-dimensional list of numbers indicating word counts, term frequency takes into account the word frequencies for each member document in the corpus.</p><p id="a539d3b78f9f432192774a109d854406">Consider the corpus = [ \xe2\x80\x9cJack ate an apple\xe2\x80\x9d, \xe2\x80\x9can apple on the table\xe2\x80\x9d, \xe2\x80\x9cJack likes the apple\xe2\x80\x9d ]</p><p id="d16b0315c3a9477c80a8b6444cf7ff42">Bag-of-words will create a one-dimensional vector for this entire corpus:</p><table id="e195a15d7e304136b4c657d912819044" summary="" rowstyle="plain"><cite id="ibf3351455b99488f895bbb1b13b4cd77" /><caption><p id="e1c983c7a2a847999670b16042cec945" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="da264a6815dd44de84e216cdd764c718">Jack</p></td><td colspan="1" rowspan="1" align="left"><p id="da38d03324fa4a26bc9f338e4cc13548">ate</p></td><td colspan="1" rowspan="1" align="left"><p id="e4c432c088bf492f98b62386ed07e61a">an</p></td><td colspan="1" rowspan="1" align="left"><p id="d8e5404e658247718d3de2de7d549138">apple</p></td><td colspan="1" rowspan="1" align="left"><p id="e0ff5f869ec3462f9bbbcc1ac098e1ed">on</p></td><td colspan="1" rowspan="1" align="left"><p id="c384c642501844139dc1f3b9bfdf67f9">the</p></td><td colspan="1" rowspan="1" align="left"><p id="b979553d494140f9a1e3b51235d01d79">table</p></td><td colspan="1" rowspan="1" align="left"><p id="b3f135c40d1945dc9a6802ea7d707588">likes</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="f851dbfa0ce047528bd26fd59d3af525">2</p></td><td colspan="1" rowspan="1" align="left"><p id="d1c91bb6752043d5a943e82de25ef0ea">2</p></td><td colspan="1" rowspan="1" align="left"><p id="a3522d1d4a0b4fd8858af9ba12f051d2">2</p></td><td colspan="1" rowspan="1" align="left"><p id="e5168ce4de434e51b0cbfcc622e9bea8">3</p></td><td colspan="1" rowspan="1" align="left"><p id="a4d1a91376d3439fa6ce4e327134fc59">2</p></td><td colspan="1" rowspan="1" align="left"><p id="ec994b2a00034a89b837cc114df68978">2</p></td><td colspan="1" rowspan="1" align="left"><p id="d56eb0e923774c5782616b9b56674926">1</p></td><td colspan="1" rowspan="1" align="left"><p id="c37ece1f7d54450b98647a7e754414f2">1</p></td></tr></table><p id="e16c431119c641aa9b9e8cbf4961d035">However, the term-frequency matrix will have a row corresponding to each of the three documents:</p><table id="b1d84ecf94a04bf8a15b231901ae3072" summary="" rowstyle="plain"><cite id="i96d52e7fbfc0478b9b726965fbb4f56c" /><caption><p id="d780eb99996d4712bd1bb7b3f4d5229d" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e449c4af70264414b9b67ff08b160405" /></td><td colspan="1" rowspan="1" align="left"><p id="add4dd410197485aadb9156a56bc3440">Jack</p></td><td colspan="1" rowspan="1" align="left"><p id="fdac8e9642524b6589c664126e37666c">ate</p></td><td colspan="1" rowspan="1" align="left"><p id="a17833510475439a9636f5381e3a0dbd">an</p></td><td colspan="1" rowspan="1" align="left"><p id="efd67f3acc08409eb7d7d667f54a77b9">apple</p></td><td colspan="1" rowspan="1" align="left"><p id="f4c1acca4dc74567b02d9ae42e00e5ac">on</p></td><td colspan="1" rowspan="1" align="left"><p id="f88d4d7f7eb647e093ec07bccdd6d327">the</p></td><td colspan="1" rowspan="1" align="left"><p id="d5f0c24410334be985f96760b5b4e6d5">table</p></td><td colspan="1" rowspan="1" align="left"><p id="d36dbaca710a438fb612478b37e6856b">likes</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="d0ed8f74176440b5b07942f493a59dac">Document #1</p></td><td colspan="1" rowspan="1" align="left"><p id="a28bf5de05bb4adeaa670de86473c02d">1</p></td><td colspan="1" rowspan="1" align="left"><p id="de3093d65d804b30875ac94c84eccc46">1</p></td><td colspan="1" rowspan="1" align="left"><p id="e54ff0c14b1b4ea58fcc98e8f570a168">1</p></td><td colspan="1" rowspan="1" align="left"><p id="a431281d7d0f40b194f8164263fb460e">1</p></td><td colspan="1" rowspan="1" align="left"><p id="b593853b9e0045779f86911563f4df19">0</p></td><td colspan="1" rowspan="1" align="left"><p id="d0cb0fc94c5b4eccb42aee32f4aeab02">0</p></td><td colspan="1" rowspan="1" align="left"><p id="a3c3c579a0324947b32f82cc13736a37">0</p></td><td colspan="1" rowspan="1" align="left"><p id="c7738b9c40864ee5bf596448ca50f36c">0</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="a3120511b0eb487aa699fcce090b1008">Document #2</p></td><td colspan="1" rowspan="1" align="left"><p id="a58b02c6cd8f4a47823b3ded7dfbbd15">0</p></td><td colspan="1" rowspan="1" align="left"><p id="e8297b02c6fe4fa5a32ddfc598d532a2">0</p></td><td colspan="1" rowspan="1" align="left"><p id="c28fb6fa636a4a70bc6e213b2fa49a0f">1</p></td><td colspan="1" rowspan="1" align="left"><p id="ed59c5b69fb04b4ca8aa648a5b4489bb">1</p></td><td colspan="1" rowspan="1" align="left"><p id="a37f6d4952c24114a31e102eb72ec38d">1</p></td><td colspan="1" rowspan="1" align="left"><p id="aba0c0a006e34db18ebc02fe1cabaf37">1</p></td><td colspan="1" rowspan="1" align="left"><p id="add38c4a29ba4b9f91fa6f11a7237212">1</p></td><td colspan="1" rowspan="1" align="left"><p id="d0bdf3e4924340c5844f8ef74edbe44f">0</p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="a8ef2fdffa4d480388c796c05cf61a7b">Document #3</p></td><td colspan="1" rowspan="1" align="left"><p id="c87b6e943f4a46a8b2d9a5bd1bb45675">1</p></td><td colspan="1" rowspan="1" align="left"><p id="cafbb742817a43809ea399559034a1df">0</p></td><td colspan="1" rowspan="1" align="left"><p id="c7e598226fd241b3ac7a65621963fe3d">0</p></td><td colspan="1" rowspan="1" align="left"><p id="eff463646a6d4caaaeccdb3736668d5a">1</p></td><td colspan="1" rowspan="1" align="left"><p id="be86ae66c1584aa4bed857a33eb74f2a">0</p></td><td colspan="1" rowspan="1" align="left"><p id="db9beb42155e457388709baf99741ed4">1</p></td><td colspan="1" rowspan="1" align="left"><p id="d81896a4fda941198d30f1709b150b01">0</p></td><td colspan="1" rowspan="1" align="left"><p id="b4e6d208012a4e6f8c7274666f299850">1</p></td></tr></table><p id="e0344662041340e8a793814a4074b14e"><em>3. Term-Frequency: Inverse-Document Frequency </em></p><p id="bfa7a586511846689d3c3349b19c121e">Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to \xe2\x80\x9cnormalize\xe2\x80\x9d these high-frequency values. In tf-idf, for any word not in the corpus, we can <link href="https://stackoverflow.com/questions/58371573/how-tf-idf-model-handles-unseen-words-during-test-data" target="new" internal="false">either ignore it or consider its frequency under another/foreign word column</link>.</p><p id="db3d7a60a67842fbb8a08e75570e0670">Each word has an inverse document frequency associated with it. Hence,</p><p id="d80b2f58893f4a90aa324a1ff80510d0">\\[IDF_{j}=log(\\frac{\\text{number of documents}}{\\text{number of documents with the word j}})\\] </p><p id="d671654a7c884a9ca23c34c5802b6060">Intuitively, the IDF of a word that appears in fewer documents is higher. (Now, using the above formula, what is the IDF of a word that appears in every document? What could be the highest IDF value a word could get?)</p><p id="e68ac9cd3e834ecd9ea665a8014a75f7">Finally, tf-idf for a word j in document i is calculated by multiplying the IDF score for word along j with the Tf for word j in document i : </p><p id="a102ea7518d6446b91ff942951611454">\\[ TF - ID_{ij}= TF_{i}\\times IDF_{j} \\]</p><p id="f6269c2bdb9949d096c8f6ad9a0413cc"><em>4. Word Embeddings</em></p><p id="a86a2b9035574eefbb55d51ee37126fd">All the above feature construction methods are easy to visualize and understand. However, it represents individual words as dimensions and thus tends to suffer from the curse of dimensionality. Word embeddings are representations of words in a meaningful low-dimensional space whose dimensionality is a fixed number independent of the word count in the corpus. Intuitively, Words that are placed closer in this space are expected to be similar in meaning. For instance, the position of \xe2\x80\x9cSeattle\xe2\x80\x9d is closer to \xe2\x80\x9cBoston\xe2\x80\x9d than it is to \xe2\x80\x9ctalk\xe2\x80\x9d in the below illustration of word embeddings.</p><image id="c62faa0d8c6c49b0b14e2d66c5a3d40c" src="../webcontent/image-b9d012baeaa2454c9931814411b98cb5-1.png" alt="" style="inline" vertical-align="middle" height="253" width="450"><caption><p id="c9b64f31f46d4b06adfcd851d9ed7bca">Figure 1. Word Embeddings (Source: <link href="https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/" target="new" internal="false">IBM Research Blog</link>)</p></caption><popout enable="false"></popout></image><p id="c8162a86203a460192de48c7ad05fc19">As discussed in an earlier module, word embeddings can be generated by training a model from scratch or through pre-trained models like BERT (introduced later in the course), which brings down the training time significantly. Due to the advantages of word embeddings over other methods in capturing context and minimizing memory used for feature representation, it is increasingly used for deep learning and advanced NLP tasks, some of which will be discussed in the next section.</p></body></workbook_page>\n'