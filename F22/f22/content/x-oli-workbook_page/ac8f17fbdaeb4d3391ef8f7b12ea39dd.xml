b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="ac8f17fbdaeb4d3391ef8f7b12ea39dd"><head><title>Model Selection Methods</title><objref idref="fc59dc700a3e4c95abcf79e8bd3bd4f9" /><objref idref="fc8171c21545480a80174095c242779a" /></head><body><section id="f2388fe3a6fb45e984784775e762b18a"><title>Model Selection for Prediction</title><body><p id="ed977c087c0e48f78b1e22e04f04035d"> </p></body></section><p id="d30aa3813769492c96eae4263069026b">To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:</p><table id="de5620314eb24bdf90da31a7c816334b" summary="" rowstyle="plain"><cite id="i522f8aa7c1a74f729ceb6241c1693ddd" /><caption><p id="d0d17f25be6741ba8755e98d202d1bc1" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c01bed07327542f4adc437f02890bef7"><em>Input</em>: candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub></p><p id="a2a2ccf5a7ac46598f269c6c5292bb02"><em>Procedure</em>:</p><p id="a8292b4bf711476da313e0a4aaca0901">Split the dataset into <em>train set</em> and <em>test set</em>.</p><p id="cfdc9a02f536443593a685d33fe82744">For each candidate model M<sub>i</sub>:</p><p id="ac5776f12306441081680bd0e1737601">\t(i) Train M<sub>i</sub> on the <em>train set</em>.</p><p id="b134c8d5b4a94b2d817fa31a4c73a123">\t(ii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em></p><p id="f28be553facd4c8b895b6d17b88e7b87"><em>Output</em>: the model M<sub>j</sub> with the best performance on the test set.</p></td></tr></table><p id="d021c3ba482841d0b54c689719528730">If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.</p><table id="ecc8fccb93b14d8c9c1e22febc52514a" summary="" rowstyle="plain"><cite id="idc4754f052844175814d1a6e1c0bfdb5" /><caption><p id="fe74c2c60a9a4b21b42e537930288e30" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="e7499b5dd6f44f11b794a99c9f29c527"><em>Input</em>: candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub> and hyperparameter space S.</p><p id="e20bb20f15b84cbfb3f3e29db7efe8f4"><em>Procedure</em>:</p><p id="f4f2ca2543de4acd9330dc90e6f8a760">Split the dataset into <em>train subset</em>, <em>validation subset </em>and <em>test set</em>.</p><p id="ca582b92a8164124ac9a4c59163c9d82">For each candidate model M<sub>i</sub>:</p><p id="f12d77d3406745c4aeb42e0f2ea0bfc1">\t(i) Pick the hyperparameters that give the best performance on the <em>validation subset </em>when M<sub>i</sub> is trained on the <em>train subset</em>. We call these the <em style="italic">best hyperparameters</em>.</p><p id="ccae4b8c62ac4a83b5be74f7612150ac">\t(ii) Retrain a new model M<sub>i</sub> using the <em style="italic">best hyperparameters</em> on the combined data from the <em>train subset </em>and <em>validation subset</em>.</p><p id="e491b0f5a32644c594037293bceb103e">\t(iii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em>.</p><p id="d48225e008644c0aa2ad5e5cb6899c22"><em>Output</em>: the model M<sub>j</sub> and the associated <em style="italic">best hyperparameters</em> with the best performance on the test set.</p></td></tr></table><p id="e459f77eadad4f5e92288f15b841c25e">Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search), if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.</p><p id="f76668b0ea0841fb89c9983a5c6a7e45">While the above procedure is sufficient to demonstrate prediction models\xe2\x80\x99 validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative to is perform k-fold cross validation as in the inner loop of the model selection procedure.</p><table id="ffa7668cf6f54a348c568393aefdb3cf" summary="" rowstyle="plain"><cite id="ib4ec9c40768c4a61881b61ffbc5d9e7b" /><caption><p id="f0ef082d6467438e85b981ee964d8f3f" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="f7243c6ca0b849b0909dd3eb81a2adce"><em>Input</em>: number of folds K, candidate models M<sub>1</sub>, M<sub>2</sub>, \xe2\x80\xa6, M<sub>l</sub> and hyperparameter space S.</p><p id="a83f28b92c4f406681ffe43324f400a1"><em>Procedure</em>:</p><p id="c221370a4a7a49e5a4438908b02a0728">Split the dataset into <em>train set</em> and <em>test set</em>. Split the <em>train set</em> into K folds.</p><p id="f9bb9f8925304addbad7cc184ed7afc4">For each candidate model M<sub>i</sub>:</p><p id="dfd3b50ab8234fbcaf8600104a6da4ec">\t(i) Pick the hyperparameters that give the best cross-validated performance for M<sub>i</sub> on the <em>train set</em>. We call these the <em style="italic">best hyperparameters</em>.</p><p id="ec76bab14ce243718673460d5f1f744d">        (ii) Retrain a new model M<sub>i</sub> using the <em style="italic">best hyperparameters</em> on the <em>train set</em>.</p><p id="cb0fbdb56d8040e0929b60c9546ddb44">\t(iii) Evaluate M<sub>i</sub>\xe2\x80\x99s performance on the <em>test set</em>.</p><p id="a2fe8490dd84455e9f423ea4d01cea8a"><em>Output</em>: the model M<sub>j</sub> and the associated <em style="italic">best hyperparameters</em> with the best performance on the test set.</p></td></tr></table><section id="c4ceb924d12542cb909b467c9cd39c9a"><title>Model Selection for Inference</title><body><p id="ffbe2532865e41a691bc1dfd5a2c8b10"> </p></body></section><p id="d593c95c31d641aab6eef45498e8de65">In inference, models are fit on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split; instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple / interpretable. We introduce a number of popular metrics below.</p><p id="c97cd54137ac473895550e7ec8d96831"><em>Akaike Information Criterion (AIC)</em>. Derived from frequentist statistics, the AIC score of a model <em style="italic">h</em> is computed as</p><p id="c2c63e861e4b4769b7801bc28fbc6ca5">\\[ AIC(h)=(2K_{h}-2LL(h))/N \\]</p><p id="aaf12d673d774cc6b13bfd81a8519840">where K<sub>h</sub> is the number of parameters in <em style="italic">h</em>, <em style="italic">LL(h)</em> is the maximum log likelihood of <em style="italic">h</em> on the dataset, and <em style="italic">N</em> is the size of the dataset. For regression, LL is the mean squared error, and for binary classification, LL is the logistic loss. A model with smaller AIC value is considered better for inference.</p><p id="ce122c1c262545f4818a456f9f6d51e1"><em>Bayesian Information Criterion (BIC)</em>. Derived from Bayesian statistics, the BIC score of a model <em style="italic">h</em> is computed as</p><p id="fc51d52d43544900bdf92e05cbf84455">\\[ BIC(h)=K_{h}\\times logN-2LL(h) \\]</p><p id="bfe28f052adb4896a9b5077f4eefc798">where the variables <em style="italic">K</em><em style="italic"><sub>h</sub></em><em style="italic">, N</em> and <em style="italic">LL(h)</em> are defined similarly as in AIC. A model with smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.</p><p id="c561b81e841e42f88069d9dd8d76500e"><em>Minimum Description Length (MDL)</em>. Derived from information theory, the MDL score of a model <em style="italic">h</em> is computed as</p><p id="da2e3dc36b3c47bd8daac7b079a6a37f">\\[ MDL =L(h)+L(D|H) \\]</p><p id="df89b1106f214b2a834a5757aaa22648">Where <em style="italic">L(h)</em> is the number of bits required to represent the model h, and <em style="italic">L(D|h)</em> is the number of bits required to represent the model predictions on the dataset. A model with smaller MDL value is considered better for inference.</p></body></workbook_page>\n'