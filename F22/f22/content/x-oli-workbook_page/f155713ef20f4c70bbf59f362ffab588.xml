b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="f155713ef20f4c70bbf59f362ffab588"><head><title>Common Language Processing Tasks</title><objref idref="ae4be08dd2cb4617b63d67112ddfcf45" /></head><body><p id="a36ba713672a47e1a5856ab7e16c83ae"><em style="italic">Language data</em> could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.</p><p id="af853cec384e4ebc8f78df10bc9dd7df"><em>1. Tokenization</em></p><p id="d7bcccf842ad40d18c3ff523470d743c">Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence &quot;Today is a good day.&quot;, when tokenized, would yield [&apos;Today&apos;, &apos;is&apos;, &apos;a&apos;, &apos;good&apos;, &apos;day&apos;,  &apos;. &apos;]. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in \xe2\x80\x9cDr. Smith\xe2\x80\xa6\xe2\x80\x9d the token is \xe2\x80\x9cDr.\xe2\x80\x9d and not \xe2\x80\x9cDr.\xe2\x80\x9d In English, sometimes tokenizers choose to split contracted words, e.g., \xe2\x80\x9cJohn\xe2\x80\x99s\xe2\x80\x9d is split as John and \xe2\x80\x9c\xe2\x80\x99s.\xe2\x80\x99\xe2\x80\x9d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them. </p><p id="d4af1d92424e4975a08e7c75571907b8">Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as <em style="italic">diacritization</em>.</p><p id="e6b1ac3d57d54523a53d013db13e7f11"><em>2. Stop word removal</em></p><p id="c6d76ffc7d5d4ac2859f723c44747811">Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as \xe2\x80\x9ca,\xe2\x80\x9d \xe2\x80\x9cthe,\xe2\x80\x9d \xe2\x80\x9cin,\xe2\x80\x9d \xe2\x80\x9con,\xe2\x80\x9d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10\xe2\x80\x93100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online. </p><p id="c1caa62559694eda9d2be7e2e0c9b864"><em>3. Morphological Analysis, Lemmatization,  Stemming</em></p><p id="ca747d4c3d9240b9b274b40ca294e95b">Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.  </p><p id="e35db3f931cd4f2d82a71f279036b8dd">Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as \xe2\x80\x9cstopped,\xe2\x80\x9d a morphological analyzer would need to know about the root words \xe2\x80\x9cstop\xe2\x80\x9d and the suffix \xe2\x80\x9c-ing\xe2\x80\x9d and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. \xe2\x80\x9cstops\xe2\x80\x9d).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like \xe2\x80\x9ceasiest\xe2\x80\x9d would be segmented as \xe2\x80\x9ceasy+est\xe2\x80\x9d using an orthographical rule in English that changes a stem final \xe2\x80\x9c-y\xe2\x80\x9d to an \xe2\x80\x9c-i\xe2\x80\x9d in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating \xe2\x80\x9cgo\xe2\x80\x9d as the root words for words such as \xe2\x80\x9cwent\xe2\x80\x9d or \xe2\x80\x9cgone.\xe2\x80\x9d</p><p id="c54673b237b149d1b3ebe5c13cb8494d">Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.</p><p id="a65f0d727604439f948352a6a5ec369d">The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.</p><p id="afb6cef409164fd48d0c4f7fd4bb405d">When full morphological information is not necessary or not available, a \xe2\x80\x9clighter\xe2\x80\x9d operation called <em style="italic">stemming</em> can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  &quot;change,&quot; &quot;changing,&quot; &quot;changes&quot; to &quot;chang.&quot; <link href="https://tartarus.org/martin/PorterStemmer/" target="new" internal="false">Porter Stemmer</link> is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.</p><p id="a0b7589fe951495386d8dcb30e36d21b">A slightly more accurate version of stemming is called <em style="italic">lemmatization</em> which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma \xe2\x80\x93 the standardized form to look the word up in a dictionary. In the examples above, it should return \xe2\x80\x9cchange\xe2\x80\x9d as the lemma.</p><p id="a6685917ce664e14973e53a927707e31">Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.</p><p id="d4cb7906b26c4615bdc33ffaad1c40c2"><em>5. Part-of-Speech Tagging</em></p><p id="a4b140f843784fedaf9974d23b53b312">Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a word\xe2\x80\x99s morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc. </p><p id="a2473a3ea3584cc6a87fd8f60e755c16">For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.</p><p id="ce704e705afe46359456a908d1d324ba">The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, \xe2\x80\x9cbooks\xe2\x80\x9d would get both VBZ (third person present tense verb) and NNS (plural common noun), while \xe2\x80\x9cwent\xe2\x80\x9d would be VBD (past tense verb), and \xe2\x80\x9cgone\xe2\x80\x9d would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as \xe2\x80\x9cbooks\xe2\x80\x9d earlier).  For example, the word \xe2\x80\x9cword\xe2\x80\x9d has 6 possible part-of-speech categories:</p><ol id="b10f6e5ffb5047f89e3dce144bf5f532"><li><p id="d98050d61ddb4cf9b940bf31d745f19c">Adverb (RB) \xe2\x80\x9c...up and down Florida\xe2\x80\xa6\xe2\x80\x9d</p></li><li><p id="ac2c5233216548298c5aee1f797a7aef">Particle (RP) \xe2\x80\x9c ..keep the ball down\xe2\x80\xa6\xe2\x80\x9d</p></li><li><p id="e4b8a1044759454ea01749a3b17c8b03">Preposition (IN) \xe2\x80\x9c..down the center\xe2\x80\x9d</p></li><li><p id="aa9700cfb9384a8d8032367d3da7ad59">Adjective (JJ) \xe2\x80\x9c..down payment..\xe2\x80\x9d</p></li><li><p id="a9dd062e46a741cfa7a63a622e583c1d">Verb (VBP) \xe2\x80\x9cwe down five glasses of beer every night\xe2\x80\x9d</p></li><li><p id="f79c31dd5fec4c4fa4b90c663d3ca54b">Noun (NN) \xe2\x80\x9cthey fill the comforter with down\xe2\x80\x9d</p></li></ol><p id="c8ffaa45cdfc436fb14b8688d3916169">In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,</p><quote>He can can the can.</quote><p id="aa3913913bab4343abce2d117f20c1e0">the first \xe2\x80\x9ccan\xe2\x80\x9d is a modal verb (MD), the second \xe2\x80\x9ccan\xe2\x80\x9d is a tenseless verb (VB), and the third \xe2\x80\x9ccan\xe2\x80\x9d is a singular noun (NN).</p><p id="e371cef96b714a0699840301c5203b12">The task of determining the contextually correct POS tag for a word in a sentence is called <em style="italic">part-of-speech-tagging </em>(POS tagging).</p><p id="d209e7951b6b4eea866e55c3440ae2a3">POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank. </p><p id="fcb588864a134b41b109da0910799511">Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.</p><p id="a6b82bc7b6da4eafbc19a509e5115355">Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like <link href="https://en.wikipedia.org/wiki/Word-sense_disambiguation" target="new" internal="false">statement disambiguation</link>, <link href="https://en.wikipedia.org/wiki/Sentiment_analysis" target="new" internal="false">sentiment analysis</link>, etc.</p><p id="c31f30eb63ae405ebd114df9675be26b"><em>6. Named Entity Recognition</em></p><p id="bdd5bfd06a9247e0948688c1eaf3bee4">While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.  </p><image id="dba6d307ae0341a18b60a174dbf6a4cd" src="../webcontent/image-fe6a3bc73e93485e942e02d0ce2bf2b1-1.png" alt="" style="inline" vertical-align="middle" height="397" width="600"><caption><p id="ea1c19bea7e04b11b48dd2586d18bfc5">Figure 1. Named Entity Recognition with <link href="https://spacy.io/api/entityrecognizer" target="new" internal="false">spacy</link>.</p></caption><popout enable="false"></popout></image><p id="f5c72b4fac30492ca4a8530813fe2cb0"> NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that <em style="italic">begins</em> a person-named entity, while I-PER labels any token of a person&apos;s name (of a length greater than one)  that is <em style="italic">inside</em> a named entity.  O labels any word that is <em style="italic">outside</em> a named entity. Thus with <em style="italic">k</em> different categories of named entities, there is a set of \\(2k+1\\) labels.  </p><p id="e7c5aa6b758542b2b4da64f67e0602a0">In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, \xe2\x80\x9cThe\xe2\x80\x9d would get B-ORG label, and \xe2\x80\x9cJustice\xe2\x80\x9d and \xe2\x80\x9cDepartment\xe2\x80\x9d would get the I-ORG label.</p><p id="dbdc0071802145eab97c749e290103b9">The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.</p><image id="d02504dfca064321a81ae97e6ad51f4a" src="../webcontent/image-fe6a3bc73e93485e942e02d0ce2bf2b1-2.png" alt="" style="inline" vertical-align="middle" height="220" width="600"><caption><p id="ddfa358a9f9741b5a3c8a0e0e103db31">Figure 2: NER as a classifier \xe2\x80\x93 From \xe2\x80\x9cJurafsky and Martin, Speech and Language Processing, 2nd Edition.\xe2\x80\x9d</p></caption><popout enable="false"></popout></image><p id="d31c4d48e649400c8cb207b5e2cbbd16">A training set of labeled named entities is used to train a classifier which then performs NER on new sentences. </p><p id="ad0237d86c214e83af1028af115d54ea">NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.</p><image id="a83ea5b4a63c47dcbeccc91b4e3f2307" src="../webcontent/image-fe6a3bc73e93485e942e02d0ce2bf2b1-3.png" alt="" style="inline" vertical-align="middle" height="313" width="600"><caption><p id="d9077441e4104c9f85ca7fc276cda605" /></caption><popout enable="false"></popout></image><p id="eab737168d3943cfac33528d682b5315"><em>7. Parsing</em></p><p id="ae95cb0b26724b68aa8eaad4ebbc88ea">Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.</p><p id="a60ce48e4970467bb39c1d5ccc311d43">Thus if the input sentence is \xe2\x80\x9cA boy with a flower sees a girl with a telescope.\xe2\x80\x9d  the parser would generate the following two parse trees:</p><image id="cc85f34dab0549019ad1e270da1d5ba3" src="../webcontent/Screen_Shot_2022-10-04_at_21736_pm.png" alt="" style="inline" vertical-align="middle" height="343" width="600"><caption><p id="f33c23fb193f4812a648f985eb435ade" /></caption><popout enable="false"></popout></image><image id="e83d0e8cd6b24efd8fb126a3e6d3e693" src="../webcontent/Screen_Shot_2022-10-04_at_21658_pm.png" alt="" style="inline" vertical-align="middle" height="343" width="600"><caption><p id="b6a59dda5c5b4ccebc0380b1e7b8f254" /></caption><popout enable="false"></popout></image><p id="bd46310cb49e4d31b0187d34b004f077">but it would not be able to tell which of these parses is the \xe2\x80\x9ccorrect\xe2\x80\x9d one. </p><p id="e3e3a30e3df44007871990b84f7ffab8">Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.</p><p id="f48b16ccb3bf4cd28ec299ea3707efd0">For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.</p><p id="f6325d0f7cc641dfa6b93ace395576bb">Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.</p><p id="eb1a668779cb47dc841dab55e699c0d7">Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.</p></body></workbook_page>\n'