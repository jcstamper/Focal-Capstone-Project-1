b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="afe43b3ca23848d29a3af1d12f9c426d"><head><title>Principal Component Analysis</title><objref idref="bb8cd74aab6c47c39d8c9786d3b7eb30" /><objref idref="e2347f3cd48240c28451f5daa9cbb8c9" /></head><body><p id="c65f2bbd83d24c9ea198599da81e9272"><em>Dimensionality</em></p><p id="c2cc325a8a264eeba95b244ad66772e2">As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.</p><p id="dc2b50fdab8743a19b664a4cd06f4ad8"><em>Feature Extraction</em></p><p id="a6d97c4fa187475aaa22ddf711368fc3">So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.</p><p id="a76d0b41deed4cf6b5c2e1cc205f6bf1"><em style="italic">Principal Component Analysis (PCA)</em> is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.</p><p id="e492c9fc835a490bb498ea75a66e99a5">You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.</p><p id="ed2dcd80994c40558a99153bc4244335">PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a \xe2\x80\x9csmall\xe2\x80\x9d number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.</p><p id="b97f5196fd4c4583926a794b9caca5a5">The following steps are used when performing PCA:</p><ul id="cc8a24b6cd864e88aa7d1e44c8c6abd1"><li><p id="deb2ecfdb44d4aeabacec7412b2c53fe">Standardize the data.</p></li><li><p id="dfbaf27855be401d8ab6a909c779741c">Compute the Covariance matrix of dimensions in the data.</p></li><li><p id="c3cd86b68973443abebe14d866100aef">Compute the Eigenvectors and Eigenvalues from the covariance matrix. <em style="italic">Eigenvector</em> is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.</p></li><li><p id="b177e27804454088998d1376526b703f">Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.</p></li><li><p id="a29bec7e349d45868f9507a3c0394fff">Construct the projection matrix W from the selected k Eigenvectors.</p></li><li><p id="fc6afd3cda984847bcc83e550017b4b8">Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.</p></li></ul><table id="ce9fc51513604828a7627ccef562e26d" summary="" rowstyle="plain"><cite id="ib4369003e6934a2ea528e87435d08425" /><caption><p id="b96b6f7b47db46fba21b56e091f77e78" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="c9be03c8b36e4c84865417a93d4e7da0"><em>PCA in Python Example: </em><link href="https://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/dimensionality_reduction/projection/principal_component_analysis.ipynb" target="new" internal="false">Principal Component Analysis in three (3) steps.</link></p></td></tr><tr><td colspan="1" rowspan="1" align="left"><p id="e37ad350c3414f499deb60d473e0aaee"><em>Reading: </em><link href="https://www.nature.com/articles/nmeth.4346" target="new" internal="false"><em style="italic">A Brief Article</em><em><em style="italic">-</em></em>Principal Component Analysis (Lever, Krzywinski, and Altman, 2017)</link></p></td></tr></table></body><bib:file><bib:entry id="adba2ed55b3648b78bb7d30daea0955f"><bib:book><bib:author>Hoffman, K. &amp; Kunze. R.</bib:author><bib:title>Linear Algebra</bib:title><bib:publisher>Englewood Cliffs</bib:publisher><bib:year>1971</bib:year><bib:edition>Second</bib:edition></bib:book></bib:entry></bib:file></workbook_page>\n'