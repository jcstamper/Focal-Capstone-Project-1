b'<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="c73cb4303aec4dc8a75b8960603e99d5"><head><title>Regression</title></head><body><p id="bc02b11b2faa4b9193ec2e3899deb54d">When your output variable is a continuous value, you are able to make predictions using the widely known <em style="italic">regression analysis</em>. The input variables for a regression task can be categorical, discrete, or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s).</p><table id="a9bd8cfc8d364652ac86df352310d046" summary="" rowstyle="plain"><cite id="ic7abb5ba5888463390dee627888fcb2e" /><caption><p id="f59a4888bac74c289ab8b9243430da6a" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="fbbb9f5c41754644bc10e66e4a80bcde"><em>Thought: </em>The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses; those techniques include <em style="italic">kNN</em>, among others.</p></td></tr></table><p id="cc10efd4a1b44b8c84348944195122aa">Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables <em style="italic">x</em> and <em style="italic">y</em>, one can again use regression techniques. </p><p id="bda8d7c64bfd40488fc68490b42e4736">If the independent variables are highly correlated, we can say that the variables are multicollinear. If the correlation between two independent variables is 1 or -1, then you have perfect multicollinearity. One can detect <em style="italic">multicollinearity</em> when there are large changes in the estimated regression coefficient when an independent variable is added or removed.</p><p id="f544e65aa88a44eabf4b2c281014656b">A regression model will have certain components, including the independent variables, often denoted as <em style="italic">X</em> and the dependent variable <em style="italic">Y</em>. A regression model also accounts for random error \\(\\epsilon\\); the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters <em style="italic">\xce\xb2</em>. Your goal with a regression model is to estimate the function <em style="italic">f</em>(X, \xce\xb2) with the best fit to the data. The function <em style="italic">f </em>should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.</p><p id="a66f5f952d3e476c8519007ed683057b">When performing regression analysis, you might encounter data that has <em style="italic">outliers</em>. This is not handled during the data understanding phase. It can affect the results of your regression analysis.</p><p id="fcbe386c7bbc459b9b0f565e782e9854">Let us explore the different types of regression techniques in this section with the goal of exploring each technique further in the subsequent sections.</p><section id="b2e9c7fca29e4553b077334e80bef783"><title>Linear Regression</title><body><p id="f45c3dc8d0b540629b7af4a06aaed016">This regression technique is used to model the relationship between independent variable <em style="italic">x</em> and dependent variable <em style="italic">y</em>. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and <em style="italic">k</em> is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.</p><p id="defebbbdceb441eaa72100d990458fd0">There is the simple linear regression model that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the multiple linear regression method to account for multiple predictors.</p><p id="f46c491bee1d499787d7b6b9bbb77b7f">This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution.</p><p id="df0d452e6255473f87ef0140ba136be3">Let us represent a regression model as:</p><p id="d917a6382fcd4458ad36767859c4dbde">\\[y=\\beta_0+\\beta_1 x_1+\\cdots+\\beta_{\\mathrm{k}} x_{\\mathrm{k}}+\\varepsilon\\]</p><p id="a368bed48bed4766bd832a375afd13cd">The regression function for multiple linear regression is:</p><p id="ba1d15ee9c424417b5893dfec077f157">\\[f\\left(x_1 \\ldots x_k\\right)=\\beta_0+\\beta_1 x_1+\\cdots+\\beta_k x_k\\]</p><p id="c8d6c9a1c72446cabb21813375ac3925">Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of\\(X_{1} \\ldots X_{k}\\) with <em style="italic">Y</em> are the constants \\(\\beta_1, \\ldots \\beta_k\\), also known as the coefficients of the variables. One can interpret this to mean \\(\\beta_i\\) is the change in the predicted value of your dependent variable Y per unit of change in \\(X_{i}\\), with other things being equal. Consider \\(\\beta_0\\)  as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error $\\epsilon$ in the equation.</p><p id="d18c7f6ccf934e02bdd1fea9672e68c1">You estimate \\(\\beta_1, \\ldots \\beta_k\\), and \\(\\beta_0\\) using the Least Squares method. This method will minimize the sum of squared residuals (a residual is a difference between an observed value and the fitted value given by a model). The least squares method can be linear or ordinary, or nonlinear. Ordinary Least Squares choose the parameters of a linear function of a set of independent variables by the principle of least squares. Non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters; that is, it will approximate the model by a linear model and refine its parameters by iterations.</p><p id="b49e7e9b8a8a4d5681e51a13501af39d">The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.</p><image id="e1e7859a1e864cb38ee0f912d444e752" src="../webcontent/OLS.jpg" alt="" style="inline" vertical-align="middle"><caption><p id="d73312dd8a4243fea5abb7c5bf1d2875"><em style="italic">Ordinary Least Squares Source</em><em style="italic"><sup>3</sup></em></p></caption><popout enable="false"></popout></image><table id="cce3848141c6410cb9e24d0c0160f22c" summary="" rowstyle="plain"><cite id="i2216bca809b047a8aec6d0c30653af3c" /><caption><p id="b3fcf77325494ee1917494fa4a16c8c1" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ce45a4b5d53f4a63bc1368a5000ce1b0"><em>Reading: </em><link href="http://people.duke.edu/~rnau/testing.htm" target="new" internal="false">Four Principal Assumptions</link>. These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights. </p></td></tr></table></body></section><section id="c6726d883c2c47fca9e74846b80dabbb"><title>Polynomial Regression </title><body><p id="e5d49b3579ef4d19a6bb69ef57e1f30c">When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in <em style="italic">x</em>, this is called a polynomial regression. Pay attention to the figure below. You will note that using a linear regression line to fit the data would result in a high value for the error.</p><image id="dc9237461e9b4707ba7bb2b593ffe20c" src="../webcontent/Polynomial.gif" alt="" style="inline" vertical-align="middle" height="375" width="500"><caption><p id="edfb70724475467aa9e35b6b15fca844"><em style="italic">Trying to fit a simple linear regression line. Source</em><em style="italic"><sup>4</sup></em></p></caption><popout enable="false"></popout></image><p id="a37a6ea2407e45edad62e52b84f73a36">Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the <em style="italic">y</em> and <em style="italic">x</em> variables. So a polynomial regression can fit a broader range of functions. However, it is sensitive to outliers, and those outliers can affect the result of a polynomial regression analysis.</p><image id="eae7a7ed598d429b8be46f4728391d46" src="../webcontent/Poly.gif" alt="" style="inline" vertical-align="middle" height="375" width="500"><caption><p id="ab6ff42ba9134db6ba8c26d46ebc6906"><em style="italic">Polynomial regression with lower error. Source</em><em style="italic"><sup>4</sup></em></p></caption><popout enable="false"></popout></image></body></section><section id="f154a185287b49b0b10cf2affa78e018"><title>Stepwise Regression</title><body><p id="f3de050c74d14ad19d3d90b8e4548285">When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. <em style="italic">Stepwise Regression</em> will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:</p><ul id="b5c3c3cc00074efc8f7768f1e7c360ce"><li><p id="ef75787844094bdc9a3548e2f79c430a">Backward elimination tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the &quot;most statistically insignificant deterioration of the model fit.&quot; This technique should not be used if the number of predictors is more than the observations in the dataset.</p></li><li><p id="a57e2f1bfb9d4cb18980df6d20b898bd">Forward selection is the reverse of backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit.</p></li></ul><p id="fd668ff887f344b7a75390964d98c5e4">We also have the mixed selection technique, which can be considered a hybrid selection method with the backward elimination and forward selection techniques.</p><p id="f037ca14ad3845e7be3c3c2445154055"><em style="italic">Model Accuracy</em></p><p id="cdeed868313f4131b394c52be193e9e3">Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the <em style="italic">residual standard error </em>(RSE is the standard deviation of error \\(\\epsilon\\)), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the \\(R^2\\) is independent of your response variable, unlike the RSE.</p><p id="b1268186d5ee416fb022868992c27c73">\\(R^2\\) is calculated using the total <em style="italic">sum of squares</em> which is the total variance in <em style="italic">Y</em>, and RSS is the discrepancy between the data and an estimation model.</p></body></section><section id="bbae6bc3ff224cd6b3bf4016eadd5fee"><title>Selecting the Right Regression Method</title><body><p id="ba730720a68d42a1891749424a9ca22c">The goodness of fit of a model will show how the model fits the data that it is trained with, and it will highlight any lack of balance between observations in the dataset and those that will be introduced to the model (new values). To select the right method, one can use the different metrics below:</p><ul id="aa2690f0dbb5486798b65e940e9fd278"><li><p id="b88feae27d104138870a00ea2ea6df14">AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.</p></li><li><p id="f700ec0fb3914713a0f24481dc46c603">BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.</p></li><li><p id="df5895cd3348400b84fa7145461c7f75">\\(R^2\\) will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability, and 1 means the model explains full variability.</p></li><li><p id="b4c36ce73d9b4a339e24e669e01b9fee">Adjusted \\(R^2\\) addresses the issue highlighted with n independent variable that has a strong correlation with the dependent variable increases the adjusted \\(R^2\\) and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted \\(R^2\\) is a suitable criterion to use.</p></li><li><p id="a7f6b03c5e8b48a990bc841d2c40ab04">Mallow&apos;s Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.</p></li></ul><p id="dbecae44e8ff4d63bf9be0108310e287">We will continue to learn more about regression analysis in an upcoming module, and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge of Regression analysis.</p></body></section><table id="ab9aa82d0691470ea04c05f389ba533a" summary="" rowstyle="plain"><cite id="i06e4417ed4504e2c8fddc0beb66fcdb1" /><caption><p id="c1d0dda4bfaf4d2c938dbf4c726e4ee4" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="ec88dd1f7f054d578588e6ea1fb00426"><em>Additional Reading: </em><link href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf" target="new" internal="false">Elements of Statistical Learning</link></p></td></tr></table></body><bib:file><bib:entry id="a68a18e5010e4e2baa960e8c03bdb1f0"><bib:inbook><bib:author>Murphy, K.P</bib:author><bib:title>Machine learning: A probabilistic perspective.</bib:title><bib:chapter>7</bib:chapter><bib:publisher>MIT Press</bib:publisher><bib:year>2012</bib:year></bib:inbook></bib:entry></bib:file></workbook_page>\n'