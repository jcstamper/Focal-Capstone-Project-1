Unit,Module,Title,Text,Subheaders
Deep Learning and Model Deployment,Model Deployment,Quiz 10,,
Data Science Project Planning,Developing a Vision,Module 4 Summary,"This module focuses on the foundation of a documentation set which is the vision document. This document not only provides a summary of the project but also clears up any confusion and ensures that you, your colleagues, and others are all on the same page.
The project's high-level scope and purpose are represented in the vision document. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective.
To develop a vision for the project, the project team develops a vision document and related artifacts. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution.
Different components of the vision document aim to answer questions such as: What is the real-world problem that you are trying to address? How do you come up with an overarching framework for your proposed solution? If working on a technical data science solution, one can formulate your hypothesis along with one of the following themes: Is it possible to build such a framework? Is the proposed solution cfast /good enoughd? Can the proposed solution significantly outperform the state-of-the-art baseline measured by a certain metric?
Next, the vision document focuses on the traceability part of the solution by highlighting the system features you plan to implement from the proposed solution to the problem. Finally, the scope will outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline.",
Analytic Algorithms and Model Building,Model Selection,Model Selection for Prediction,"To replicate the setting of performing prediction on unseen data, we can reserve a random portion of our dataset for testing and only use the remaining data for training. The model selection procedure can then be expressed as follows:
Input: candidate models M1, M2, , Ml
Procedure:
Split the dataset into train set and test set.
For each candidate model Mi:
(i) Train Mi on the train set.
(ii) Evaluate Mis performance on the test set
Output: the model Mj with the best performance on the test set.
If hyperparameter tuning is also part of the model selection process, the train set can be further split into a train subset and validation subset.
Input: candidate models M1, M2, , Ml and hyperparameter space S.
Procedure:
Split the dataset into train subset, validation subset and test set.
For each candidate model Mi:
(i) Pick the hyperparameters that give the best performance on the validation subset when Mi is trained on the train subset. We call these the best hyperparameters.
(ii) Retrain a new model Mi using the best hyperparameters on the combined data from the train subset and validation subset.
(iii) Evaluate Mis performance on the test set.
Output: the model Mj and the associated best hyperparameters with the best performance on the test set.
Here we note that step (i) can be performed by iterating through all possible hyperparameter values in the space S (grid search) if S is finite and computational resources are not a problem. Alternatively, we could sample the hyperparameter values uniformly from S (random search). When there are multiple hyperparameters, random search is preferred because it allows us to explore distinct values for each hyperparameter at each trial.
While the above procedure is sufficient to demonstrate the prediction of a  models validity, it relies on only two random splits, which may skew the model selection outcome if we get a bad split (e.g., if most of the outliers happen to be in the test set). A more rigorous alternative is to perform k-fold cross-validation as in the inner loop of the model selection procedure.
Input: number of folds K, candidate models M1, M2, , Ml and hyperparameter space S.
Procedure:
Split the dataset into train set and test set. Split the train set into K folds.
For each candidate model Mi:
(i) Pick the hyperparameters that give the best cross-validated performance for Mi on the train set. We call these the best hyperparameters.
(ii) Retrain a new model Mi using the best hyperparameters on the train set.
(iii) Evaluate Mis performance on the test set.
Output: the model Mj and the associated best hyperparameters with the best performance on the test set.",
Data Science Project Planning,Requirements Gathering,Successful Requirements Gathering,"The requirements gathering process is not linear. It is important to consider an evaluation at each step of the process. Doing this will ensure that errors are identified and fixed early, as it can be costly and time-consuming for errors to go undetected. However, it is a best practice to conduct validation and verification exercises for your requirements after they have been defined. It is important to confirm that requirements meet the needs of the business and users, and the requirements can be traced back to the defined business and analytic objectives.
Requirements should be validated to ensure that they are complete, correct, traceable, verifiable, and testable. It seems logical to perform validation when the solution has been developed but before it is delivered to the client. This is because you can test the actual solution. But what happens when you find errors that require the project team to start from the beginning of the development process? Time and money have been wasted! This grave mistake can be avoided if previously defined requirements are validated.
Validating requirements will most likely involve testing without implementation, but one should remember that the project team will also validate and verify the requirements throughout the solution development process. Validating requirements at this stage will ensure that solution developers will produce the right deliverables and save project time and cost.
Requirements can be validated by following the steps below:
Review requirements. The requirements management plan will be peer-reviewed to identify that each documented requirement is verifiable and unambiguous. The peer-review process should be well-defined so that reviewers can discuss their interpretations of requirements. This will reveal any ambiguity. The reviewers will produce a summary of the defects document. This document will be used by the business analyst and project team to revise requirements.
Prototype requirements. Prototyping can help the project and client team determine if the requirements can be considered complete. Simulations are a popular prototyping technique, but at this stage of solution development, creating simulations will be time-consuming. There are less time-consuming prototyping techniques that can help validate requirements. Proof-of-concept (POC) prototypes and paper mockups can be used to demonstrate the feasibility and completeness of requirements.
Test requirements. Requirements can be validated by users. Acceptance tests are done to assess whether a solution will meet predefined criteria called acceptance criteria. Acceptance criteria are set by end-users of the solution. A project team might worry about the users' inability to define the acceptance criteria. The project team can guide users in thinking about how the solution meets their needs. Acceptance criteria can be defined using the S.M.A.R.T goals. Once acceptance criteria are set, users can conduct acceptance tests.
A successful requirements gathering exercise is not yet deemed successful when the activities in the requirements management plan have been completed. After the results of the requirements gathering process have been documented and validated, a formal sign-off will confirm that all parties approve and accept the defined requirements. This is a signal that the project team can officially begin developing a solution/solutions.",Validating Requirements
Exploratory Data Analysis,Feature Engineering,t-SNE,"In the last section, we explored Principal Component Analysis (PCA) as a dimensionality reduction technique to provide a low-dimensional representation of data through a linear transformation (a fancy term for multiplying by a matrix). While it is quite useful for tabular data, it is not always the best tool for the job, particularly as it requires us to preserve all pairs of distances between different data points. This leads to potentially poor performance when data tend to be clusters or classes, where we know the distance between neighbors might be much more important than the distance between data points cfartherd apart. Lets consider MNIST, a well-known digit recognition dataset. We can apply PCA to the dataset to get the following visualization:
Source: https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/
As we can see, while some groups are visible, most of the digits are clumped together, making them really hard to distinguish. Given that MNIST has 10 distinct classes, what we would like is an algorithm that lets us differentiate between local distances and global distances a little better to make classes more visible.
Enter t-distributed stochastic neighbor embedding or t-SNE. t-SNE is a popular statistical dimensionality reduction technique that is primarily used for the visualization of clusters of points in higher dimensions. t-SNE was introduced originally as a stochastic neighbor embedding method in 2002 by Geoffrey Hinton and Sam Roweis, and the t-distribution modification was introduced as an improvement over the original method in 2008 by Laurens van der Maaten and Geoffrey Hinton. Practically speaking, it is a really important technique, as it allows us to achieve non-linear embeddings of our data.
How t-SNE works is that it tries to model the distribution of points in the higher-dimensional space as a set of Gaussian distributions, where we define the probability of some data point i picking another point j to be neighbors through the following equation:
\\[ p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_j\ight\\|^2 / 2 \\sigma_i^2\ight)}{\\sum_{k \eq i} \\exp \\left(-\\left\\|\\mathbf{x}_i-\\mathbf{x}_k\ight\\|^2 / 2 \\sigma_i^2\ight)} \\]
(If youre familiar with deep learning, you might notice the similarities between this method and the softmax function. Its also important to note that the distance function here is more of a suggestion, and you can use any custom distance function you want with most t-SNE implementations).
After creating this high-dimensional model, we then optimize a low-dimensional embedding, with the lower-dimensional embedding using a t-distribution instead with the following similarity function:
\\[ q_{i j}=\\frac{\\left(1+\\left\\|\\mathbf{y}_i-\\mathbf{y}_j\ight\\|^2\ight)^{-1}}{\\sum_k \\sum_{l \eq k}\\left(1+\\left\\|\\mathbf{y}_k-\\mathbf{y}_l\ight\\|^2\ight)^{-1}} \\]
By minimizing the distance between these distributions, we can get a good lower-dimensional embedding for our high-dimensional data, thus allowing us to see a cgood enoughd representation of the potential internal clustering of the data. While we can use multiple distance functions between distributions, we tend to use KL-divergence as it is relatively easy to optimize and relatively easy to intuit. Effectively, it tells us that two distributions are different if they associate largely different probabilities with the same data. By minimizing this, we can ensure that our lower-dimensional embedding roughly approximates the high-dimensional data.
If we go to our MNIST example, using t-SNE gets us the following results, which make the clusters a lot easier to see:
Overall, t-SNE can be a far better visualization tool than PCA. However, it is very important to know the limitations of any new tool you consider for data science, especially data visualization. Given that visualizations can be misleading, it is important to treat a potentially better tool with skepticism.
In the case of t-SNE, the problem lies in the optimization process. Firstly, it is important to note that there are several parameters to the algorithm which need to be carefully considered in order to get good results out of the algorithm. The most important parameter is known as the perplexity, and this roughly is the number of neighbors you want to consider as cclosed to any data point. Larger datasets require larger perplexity, but too large of perplexity can remove the presence of any potential clusters, as you can see in the following example:
Secondly, note that it is important to not really think too much about the distances between clusters and the relative size of any clusters reported. Given that t-SNE is stochastic, the distances between points tend to mean very little, if anything, and instead, it is important to look at the data in its entirety.
Thirdly, it is important to note that t-SNE is sensitive to data scaling. Applying a standard scalar or another scaling system is important to get interpretable results, as otherwise, certain features will be scaled in a very different manner than any other.
Lastly, and most importantly, t-SNE is random. As the loss function is non-convex, different initializations can lead to different visualizations. Treat t-SNE as an exploratory tool more than anything, and do not make the mistake of trying to use it ahead of classification. Should you want to use a non-linear technique in this manner, consider looking into other tools like UMAP instead.
Hinton, G. E., & Roweis, S. (2002). Stochastic Neighbor Embedding. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Advances in Neural Information Processing Systems (Vol. 15). Retrieved from https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf
Maaten, L.van der and Hinton, G. (2008) Visualizing data using T-Sne, Journal of Machine Learning Research. Available at: https://jmlr.org/papers/v9/vandermaaten08a.html
van der Maaten, L. (2014). Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15(93), 3221. Retrieved from http://jmlr.org/papers/v15/vandermaaten14a.html",References:
Collecting and Understanding Data,Sparse Matrix,Introduction to Sparse Matrices,"A matrix (2D array) is a common data structure that encodes the relationship between elements stored in rows and columns. For example, a movie review site may store the rating history of its users in a matrix, where the cell at row i and column j is the rating score of user i for movie j. While this information can also be stored in a database-style table where every row contains the user id, movie id, and corresponding score rating, the matrix format allows for more complex computations over the entire rating data. As you will see in Project 2, an example of such computations is using least-square errors to build a recommendation system (i.e., given a users movie rating history, which movie would they want to watch next?)
However, the disadvantage of the matrix format is that matrices can be very sparse in certain domains. Here sparsity refers to the fact that the majority of entries are unknown or missing. In the example above, every user is only able to watch and rate only a very small portion of the entire movie catalog, so most cells in the matrix would be empty. In general, if the number of non-empty cells is roughly equal to or lower than the number of rows or columns in a matrix (e.g., if a 5 x 5 matrix only has about 5 non-empty cells), this matrix is considered sparse (although this is not a hard-and-fast rule).
Empty cells can be assigned a placeholder value, such as 0 (if the data is assumed to be positive) or null/NaN (if the data is assumed to be signed). In either case, the primary issue is that sparsity leads to a waste of memory  and computational resources:
The placeholder values still consume actual memory. For example, storing a 10000 x 10000 sparse matrix of integers takes about 381MB, even if most entries are 0 and do not carry actual meaning.  There is no point in representing data that does not exist!
Many computations on sparse matrices yield trivial results due to addition/subtraction or multiplication with 0s; however, they still need to be carried out by the computer.
Alternate matrix representations have been devised to reflect the underlying sparsity and avoid the above issues. In this module, we will introduce a number of approaches, along with their mechanisms, strengths, and weaknesses. Then, we provide general pointers to applications of sparse matrices in different areas of data science and machine learning.",
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Regularization,"The ultimate goal of any supervised machine learning problem is to find a model or function that predicts a target or label and minimizes the expected error over all possible inputs and labels. Minimizing error over all possible inputs means the function must be able to generalize and make accurate predictions on unseen inputs. In other words, the fundamental goal of machine learning is for the algorithm to generalize beyond the training sets.
Regularization is a general approach to help select a balanced model to trade off between a high bias and a high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve,  hence, the need for the trade-off to minimize the model's total error.
There are three popular regularization techniques, each of them aiming at decreasing the total size of the parameters of the model (e.g., coefficients in a regression):
Ridge Regression, which penalizes the sum of squares of the coefficients (L2 penalty).
Lasso Regression, which penalizes the sum of absolute values of the coefficients (L1 penalty).
Elastic Net, a convex combination of Ridge and Lasso.
L1 and L2 Regularizations.",
Analytic Algorithms and Model Building,Model Selection,Module 18 Summary,This is a new page with empty contents.,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Manipulating and Measuring Model Interpretability,"[Required Reading] Paper: Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021, May). Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-52). (Requires CMU credentials to access)
The first author is a Senior Program Manager who studies AI and ethics in research and engineering. The remaining authors are affiliated with the Computational Social Science research group at Microsoft, which studies how to help laypeople (e.g., users of Microsoft products) make sense of numerical data.
The paper is targeting ML researchers and practitioners who build machine learning systems that interact with the end-user.
Machine learning models are increasingly used to aid decision-making in high-stakes domains and to influence peoples everyday decisions. However, people are reluctant to use these models due to concerns about their underlying mechanisms and fairness. In response, a prolific line of research on machine learning interpretability has emerged. This paper is one such research work.
The paper contributes a novel perspective that interpretability is a latent property that cannot be directly measured. However, it can be influenced by measurable properties and has a measurable influence on peoples behavior.
This perspective addresses the existing lack of consensus on the definition of interpretability in current literature. In addition, the paper reports an unintuitive result that clear models with fewer features are not better than complex or black-box models in their ability to help people make beneficial decisions or detect errors.
The overall experimental procedure is to various factors that may influence a models interpretability and measure their effect on peoples behaviors, with a focus on the following aspects:
RQ1: How well can people simulate a models prediction?
RQ2: To what extent do people follow a models prediction when its beneficial for them to do so?
RQ3: How well can people detect when a model has made a mistake and correct it?
The primary task in each experiment is to predict the price of apartments in New York City, with the help of a linear regression model. The study participants always have access to all 8 features in the dataset, but they may see a clear model (with explanations on how the prediction is derived) or a black-box model (with no such explanations). In addition, the models shown to the participant may have 2 or 8 features, thereby allowing for a 2 x 2 experiment setting (clear vs black-box and 2-feature vs 8-feature).
For each of the 12 apartment data points used in the study, participants were first shown its configuration (i.e., feature values) alongside the model (whose internals were either clear or black box) and were asked to guess what the model would predict for the apartments selling price. They were then shown the models prediction and asked for their own prediction of the apartments selling price. For RQ1, the authors measured the difference between peoples guesses of the models prediction and the actual prediction result. For RQ2, the authors measured the extent to which people deviated from the models prediction in their guess for the apartments ground-truth selling price. RQ3 used the same metric as in RQ2 but only applied to the last 2 apartments which had unusual configurations (such as 3 bathrooms squeezed into 726 square feet) and which the ML models made prediction mistakes.
Based on the papers findings:
A clear model with a smaller number of features was easiest for participants to simulate.
There were no significant differences in the participants trust of the models across the 4 experimental conditions. Participants did not trust the clear model with 2 features more than the black-box model with 8 features.
When participants see unusual examples, they are less likely to correct inaccurate predictions made by clear models than by black-box models. In other words, too much transparency can be harmful, possibly due to cognitive overload.
There are two important takeaways from the paper:
Machine learning interpretability is not purely a computational problem. An interdisciplinary approach is needed, and a human-centered focus is likely the key.
Intuition alone is not sufficient to interpret models. More empirical studies that cover a wider range of domain models, factors, and outcomes are needed.","Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Deep Learning,"A few samples of completely fake faces generated by a generative adversarial network (GAN). Source: ThisPersonDoesNotExist.com
Deep learning applications have been widespread in recent years with the increasing availability of data and compute resources. Deep learning is an area of machine learning that draws inspiration from how the human brain functions as a model of computation. Like most machine learning algorithms, deep learning also involves a transformation of data from an input to an output: for example, using speech samples as inputs to predict the speaker or taking some text in one language and translating the text into another language. As the problem becomes more and more complex, classical machine learning approaches fail to adequately learn such transformation or mapping of input to output from the data. Deep learning algorithms overcome this using many successive transformations of input data, thus the deep in deep learning.
At a very high-level abstraction, we can view neurons as aggregating signals from their inputs and sending processed signals to their outputs. Such input signals can be visual signals detected by the retina or acoustic signals detected by the ear. Again at an abstract level, the neuron can be considered computing a weighted summation of the inputs weighing each input by a synaptic weight and doing a final non-linear transformation on the sum.
Inspired by this, neural networks consist of neurons that are connected to each other via weights. Each neuron receives weighted inputs from neurons in the previous layer, these are summed and passed through a nonlinearity function, commonly called the activation function, and the resulting output is passed on to neurons in the next layer, again, connected with some weight. The deep nature of deep neural networks refers to having a very large number of layers of such connected neurons.
In this module, we discuss some of the key concepts in deep learning.",
Exploratory Data Analysis,Feature Engineering,Quiz 3,,
Collecting and Understanding Data,Data Collection,Validity and Bias,"It is critical in any research to have a clear and unambiguous definition of the population of interest. That is, it pays to be explicit, rather than vague, about the nature of the population we are interested in studying. Doing so will ensure proper inference or conclusions about the data we study. Regardless of the study design, any analysis could suffer from potential incorrect actions taken in any part of the data science lifecycle.
Validity measures how much the intended test interpretation (of the concept or construct that the test is assumed to measure) matches the proposed purpose of the test. This evidence leading to the assessment of validity is based on test content, response processes, internal structure, relations to other variables, and the consequences of testing.
Threats to validity refer to specific reasons for why we can be wrong when we make an inference in an experiment because of covariance, causation constructs, or whether the causal relationship holds over variations in persons, setting, treatments, and outcomes. In an observational study, the threat to validity can arise by the inability to account for whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.
There are four types of validity:
Statistical conclusion validity refers to the appropriate use of statistics (e.g., violating statistical assumptions, restricted range on a variable, low power) to infer whether the presumed independent and dependent variables covary in the experiment.
Construct validity refers to the validity of inferences about the constructs (or variables) in the study.
Internal validity relates to the validity of inferences drawn about the cause-and-effect relationship between the independent and dependent variables.
External validity refers to the validity of the cause and effect relationship being generalizable to other persons, settings, treatment variables, and measures.
In this module, we will discuss external and internal validity in more detail.
As data scientists, once we have defined the population of interest for the study, we must work hard to ensure that the data we will collect or the data given to us is representative of that population. For example, to investigate the impact of class size on high school student achievement, we need to decide whether it is possible to obtain a simple random sample of students from the population of students who are enrolling in formal education institutions in the United States. Alternatively, we might decide that we only want to study students in public schools, private schools, charter schools, etc., or that we want to study all high school students regardless of age. No matter what the sampling plan is, it is critical that the data we use are a representative sample of the population we want to study. Doing so is crucial to ensure the external validity of the study. External validity refers to the ability to generalize the findings or results to a known population of interest. Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.
Sampling bias is bias in which data is collected in a way that some members or groups of members in a population are systematically more likely or less likely to be selected in a sample than others. Sampling bias results in discriminatory data with over- or under-represented instances that are related to the study design or data collection method and can occur in both probabilistic and nonprobabilistic sampling. A study measuring the completion rate of graduate students in the United States with a sample of students from one socioeconomic background, race, or gender will undermine the external validity of that study. This means the results of the study can not be truly generalized to the entire population of graduate students in the United States.
As data scientists, we want to conduct sound research that produces meaningful, impactful, or novel results for stakeholders. To produce such results, we need to ensure confidence in the ability to draw inferences from the data about the population of interest established in the study after ruling out any alternative explanations. Failure to do so would result in internal validity threats. Threats to internal validity are problems in drawing correct inferences about whether the covariation (i.e., the variation in one variable contributes to the variation in the other variable) between the presumed treatment variable and the outcome reflects a causal relationship.
Table 1 (adapted from Creswell (2012)) displays the threats to internal validity, their descriptions, and suggestions for data scientists to avoid such a threat.
Type of Threat to Internal Validity
Description
Suggested response by Data Scientist
History
Time passes between the beginning of the experiment and the end, and events may occur between the pre-test and post-test that influence the outcome. In educational experiments, it is impossible to have a tightly controlled environment and monitor all events.
The data scientist can have the control and experimental groups experience the same activities (except for the treatment) during the experiment.
Maturation
Individuals develop or change during the experiment (i.e., become older, wiser, stronger, and more experienced), and these changes may affect their scores between the pre-test and post-test.
A careful selection of participants who mature or develop in a similar way for both the control and experimental groups helps guard against this problem.
Regression to the mean
Participants with extreme scores are selected for the experiment. Naturally, their scores will probably change during the experiment. Scores, over time, regress toward the mean.
The data scientist can select participants who do not have extreme scores as entering characteristics for the experiment.
Selection
Participants can be selected who have certain characteristics that predispose them to have certain outcomes (e.g., cognitive ability, receptiveness to treatment, or familiarity with a treatment)
Random selection may partly address this threat.
Mortality (also called study attrition)
Participants drop out during the experiment for any number of reasons, and drawing conclusions from scores may be difficult.
The data scientist can recruit a large sample to account for potential dropouts or compare the outcome of those who drop out with those who continue.
Diffusion of treatments (also called cross-contamination of groups)
Participants in the control and experimental groups communicate with each other. This communication can influence how both groups score on the outcomes.
The data scientist must keep the two groups as separate as possible during the experiment.
Compensatory equalization
When only the experimental group receives a treatment, an inequality exists that may threaten the validity of the study. The benefits (i.e., the goods or services believed to be desirable) of the experimental treatment need to be equally distributed among the groups in the study.
The data scientist can provide benefits to both groups, such as giving the control group the treatment after the experiment ends or giving the control group a different type of treatment during the experiment.
Compensatory rivalry
Participants in the control group feel that they are being devalued, as compared to the experimental group, because they do not experience the treatment.
The data scientist can try to avoid this threat by attempting to reduce the awareness and expectations of the presumed benefits of the experimental treatment.
Resentful demoralization
When a control group is used, individuals in this group may become resentful and demoralized because they perceive that they receive a less desirable treatment than other groups.
The data scientist can provide treatment to this group after the experiment has concluded or provide services equally attractive to the experimental treatment but not directed toward the same outcome as the treatment.
Testing
Participants become familiar with the outcome measure and remember responses for later testing
To overcome this threat, the data scientist can measure the outcome less frequently and use different items on the post-test than those used during earlier testing.
Instrumentation
The instrument changes between a pre-test and post-test, thus impacting the results of the outcome.
The data scientist must standardize procedures so that the same observational scales or instrument is used throughout the experiment.
Statistical bias is the bias that leads to a systematic discrepancy between the true parameters of the population of interest and the statistical features used to estimate those parameters. Bias made can be consciously or unconsciously, and it will affect the performance of a data science model but, most importantly, the analytic solution and the decisions made after the implementation of that solution.
Statistical bias results from violations of external validity or internal validity of a study. In the previous module, we explored sampling bias that undermines external validity. In this module, we will explore additional common statistical biases that you need to be aware of and take into account during the data understanding process.
Selection Bias, a threat to internal validity, occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about. Selection bias is usually a concern of studies using convenience samples.
Self-selection Bias occurs when individuals select themselves to be included in a study. Self-selection bias is a threat to the external validity of the study since such bias is usually untrollable during the data collection phase. Self-selection bias is often associated with certain characteristics of the sample that induce such individuals to be included in the resulting study sample. Take the example of a survey. If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.
Confirmation Bias. Your prior knowledge, beliefs, and values can play a role in the data that is used to build your analytic solution. This is because, as humans, we are prone to use our personal beliefs and experiences to guide us through daily life and decision-making. This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.
Information Bias. Also known as measurement bias, it occurs when data is collected, measured, or interpreted wrongly. Misclassification of observations is an example of information bias. For example, an observation with attributes similar to the stereotypical female student is recorded as female when that observation is actually from a male student. Another example is the misclassification of patients. In the context of the COVID-19 pandemic,  groups under the age of 45 are seen as low risk. o during a screening exercise, those in that age group might not be screened and therefore classified as negative. The data collected then has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.
Confounding Bias occurs when incorrect inferences are made about the subject matters while failing to account for a potentially confounding variable, an exogenous factor that causes the subject matters of interest.",
Advanced Natural Language Processing,Language Representation and Transformers,Introduction and Evolution of Language Representations,"Machines dont understand characters, words, or sentences. They can only process numbers. Most natural language processing tasks begin with converting textual to numerical data that machines can understand. A good representation is critical for the success of downstream tasks. The NLP module provided an introduction to the most straightforward text representation techniques like bag of words, term frequency (tf), and term frequency-inverse document-frequency (tf-idf). However, these techniques had the following two significant limitations:
The individual items in a vocabulary (terms) were represented as dimensions, and thus the representations suffered from the curse of dimensionality: the representations grew with the size of language vocabulary.
No useful information like context and word order that can be useful for the downstream tasks could be encoded within the numerical representations themselves.
These shortcomings lead to the emergence of word embeddings. Word embeddings is a term used for the representation of words, typically in the form of fixed-size real-valued vectors that encode the semantics of words essentially by capturing the contexts in which they appear. Words that are closer in the vector space of the word embedding vectors are expected to be similar in meaning, i.e., vector representations of semantically similar words have a smaller distance than dissimilar words. For example, learn, and study will be closer than the pair learn and eat. Operations on these on these embeddings could also be used to derive meaning. For example, subtracting the embedding of Germany from the embedding of Berlin and then adding the embedding of France would get you an embedding close to the embedding for Paris.
These embeddings are often learned automatically from large text corpora and are based on the idea that contextual information alone can help in generating a viable representation of linguistic items. Since the semantics are captured solely using raw text data, its a great idea to use embeddings that are pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. It turns out that using such pre-trained embeddings vastly improves performance in multiple tasks.
Word2Vec (2013) and GloVe (dGlobal Vectors for Word Representationd) (2014) are two early models to generate word embeddings that are still used widely. Word2vec embeddings are based on training a shallow feedforward neural network and leveraging occurrence within local context (neighboring words). Glove embeddings, on the other hand, are learned based on matrix factorization techniques and leverage global word-to-word occurrence counts, leveraging the entire corpus. In practice, both these embeddings give similar results for many tasks.
Neither of them, however, handles polysemy very well. These models output just one embedding for each word, combining all the semantic representations of the different senses of a  word into that one vector. For example, embeddings for the different occurrences of the word ccelld in the sentence, cHe went to the prison cell with his cell phone to extract blood cell samples from inmates,d would be the same.
This problem was solved by the ELMo (""Embeddings from Language Model"") model developed in 2018. Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning an embedding to each word. It uses a bi-directional LSTM architecture. ELMo is trained through the task of predicting the next word in a sequence of words - a task called Language Modeling.
Another issue is that the same general word embeddings (or contextualized word embeddings) are often not enough to get a good performance in all kinds of NLP tasks. The representations often need task-specific fine-tuning to obtain better results.
ULM-FiT (cUniversal Language Model Fine-tuningd), also introduced in 2018, proposed an effective inductive transfer learning method that can be applied to any NLP task and further demonstrated techniques that are key to fine-tuning a language model. It proposed a three-stage process:
General Domain LM Pre-Training, where the language model is trained on a general-domain corpus to capture general features of language in different network layers.
Target task Discriminative Fine-Tuning where the trained language model is fine-tuned on a target task dataset using discriminative fine-tuning and a changing learning rate schedule to learn task-specific features.
Target task Classifier Fine-Tuning where the classifier is fine-tuned on the target task using gradual unfreezing (unfreezing weights from the last to the first layer in different learning epochs) and repeating stage 2. This helps the network to preserve low-level representations and adapt to high-level ones.
Finally, the Transformer architecture released in 2017 revolutionized the NLP field. The release of the Transformer paper and code and the results it achieved on tasks such as machine translation made it replace LSTM models, which were most prevalent in NLP at that time. Well discuss the  Transformer model and the motivation behind its architecture in detail next.",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Introduction to Computer Vision,"Computer vision is the study of how to equip computers with (super) human-level perception, or more specifically, how to analyze or manipulate pixel values in a meaningful way. While computer vision models take input feature matrices and output scalars or vectors, much like the standard machine learning paradigm, the fact that their inputs are images (2D or 3D matrices) presents several interesting challenges.
Through the course of this module, we will be building upon the Deep Neural Network concepts introduced in the previous module. The power of neural networks, particularly multilayer perceptrons, lies in the ability to automatically extract a hierarchy of features at different levels of abstraction for classification tasks. As a result, neural networks preclude the need for feature engineering as standard machine learning methods require. However, one of its downsides is the presence of too many parameters and the inability to incorporate particular input structures required to model data from specific domains.
Figure 1. Rivian Pickup Truck.
Let us consider the image analysis task, where we would like to figure out what is happening in a given image or a sequence of images. In this image of a pickup truck (Figure 1), we get important signals regarding the objects within the truck, such as wheels, headlights, doors, and so on. The spatial proximity of the key features helps us understand what is happening in this image. This is the task of image understanding in computer vision. The input to all the computer vision models is the raw pixels in the images, which are just matrices of numbers representing the intensity levels of various spatial locations. From the computer's view, an image (Figure 2) is just a big matrix with a number (or tuple of numbers) at every pixel (Figure 3).
Figure 2. What a person sees.
Figure 3. What a computer sees.
A straightforward approach would be to flatten the image's pixel values and feed them as inputs to a multilayer perceptron. However, by doing so, we lose the valuable signal captured by the spatial structure of the image. Therefore, while learning useful visual features in computer vision, the key idea is to preserve and use the spatial structure of the image. One way is to use some spatial filters to extract a spatially adjacent set of pixels in the image and then feed those image patches to a multilayer perceptron. This idea sparked the need for a mechanism for weighting those extracted patches from the image to highlight their relative importance. In addition, it is sensible to use spatial filters of varying sizes to extract features at different resolutions. The algorithm we described was formalized as the convolutional neural network (CNN).
In this module, we will introduce the basic CNN architecture and classic architectures such as LeNet, AlexNet, VGGNet, and ResNet, which you will be implementing during the Computer Vision task in one of the Projects during the course. We will also brief the current state of CNN research and a few contemporary applications of computer vision.",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Module 12 Summary,"Statistical Inference is the process of drawing inferences from your data using probability theory. Statistical inference is used to draw scientific conclusions and test hypotheses.
If n samples are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed.
As the sample size n increases, the standard error approaches the true standard deviation for large n. This is because the standard error is an estimate of the true value of the standard deviation.
A hypothesis test is a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset.
A hypothesis test is used to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.
The two error types in hypothesis testing:
Type I error occurs when you reject the null hypothesis when it should be accepted.
Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.
A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.",
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Quiz 4,,
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,The Law of Small Numbers,"Figure 1. Kidney Cancer Rate in Each U.S. County (2007-2011). (Source: https://dataremixed.com)
Although we know that we should not make inferences from not enough data, we often forget that. We tend to overgeneralize from small samples. Sometimes, this is called the law of small numbers. Now, there is no statistical law of small numbersit is used here as a satire, playing on the law of large numbers. The law of large numbers states that, under general conditions, as the sample size gets large, the mean of the sample will be near the mean of the overall population with a very high probability.
In his book Thinking, Fast and Slow, Daniel Kahneman describes examples of cognitive biases of fast thinking. Drawing naive conclusions and making inferences about the population from such a small sample size is one of them. The book provides a great illustration of the dangers of acting as if the law of small numbers is actually a law.
A study of the incidence of kidney cancer in the 3,141 counties of the United States reveals a remarkable pattern. You may find out that the counties in the United States with the lowest incidence of kidney cancer are mostly rural rather than urban. Now, you can probably imagine why that's true if you're healthy or living out of the countryside with better air, or maybe you're eating from the food that you're growing, and you're getting better nutrition. You can come up with some interesting reasons why this rural lifestyle would lead to a lower rate of kidney cancer.
Now consider also the counties in which the incidence of kidney cancer is highest. You may find these ailing counties tend to be also mostly rural, sparsely populated, and located in the Midwest, the South, and the West. So now you're going to maybe come up with some reasonable explanation: ""Oh, well, It is easy to infer that their high cancer rates might be directly due to the poverty of the rural lifestyleno access to good medical care, a high-fat diet, and too much alcohol, too much tobacco.d
None of these explanations is correct. What's really going on is that the rural counties have fewer people. So we have a smaller sample size and, therefore, more variation in our observed kidney cancer rate even though there isn't any difference in the actual cancer rate because we have more variation for the small counties. Both the lowest and the highest rates come from the counties with a small population, the rural counties.",
Advanced Natural Language Processing,BERT,Module 25 Summary,This is a new page with empty contents.,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Cluster Analysis,"When we want to study the hidden structure of data and identify different groups within that structure, we use the Cluster Analysis technique. Once groups are constructed, it is safe to assume that data points within each group have similar features and are very dissimilar to data points in other groups. Cluster analysis is looking to define structure within a dataset.
There are different types of clustering techniques. We will briefly define the general idea at this point and fully explore it in an upcoming module.
K-means clustering is a widely used clustering technique that computes the distance between data points in a group and the center of the group. The number of clusters (k) is decided before the process begins, and K-means clustering can only be applied to numerical variables. This is because it solely uses Euclidean distance as a similarity measure to form clusters.
It is not uncommon to begin by randomly selecting a number of observations from the data as the initial cluster center, the remaining observations will then be assigned to the nearest cluster center. The algorithm continues to assign and reassign observations to their closest clusters by computing the cluster centroids (the middle of a cluster). The reassignment is done to minimize dispersion within clusters.
Hierarchical clustering connects data points to form clusters based on their distance and is also known as connectivity-based clustering. Each data point is considered its own cluster at the start of the process, and then the algorithm groups clusters based on similarity until true clusters are formed. This is also known as agglomerative clustering.
There is another approach of hierarchical clustering that puts all data points in one cluster and then separates them based on dissimilarity until different clusters are formed. This is called divisive clustering.
Hierarchical clusters are formed and represented using a dendrogram (shown below). The y-axis of a dendrogram marks the distance where clusters merge, and data points are placed on the x-axis.
Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the kind of clustering technique used to meet your analytic objective. The next sections will focus on the different types of clustering techniques and the evaluation techniques that apply to each technique.
Reading: Evaluating Clustering Results",Example of a Dendrogram. (Source: Mathlab).
Deep Learning and Model Deployment,CPU vs. GPU,Practical Data Science Optimization,"Throughout the course of this module, you have learned a lot about the underlying hardware that makes or breaks data science operations. While choosing underlying hardware goes a long way to ensuring that your system works within the budgets you allocated for yourself, you should also be prepared also to optimize your code accordingly. Given that times for jobs can range from overnight to multiple days, even a meager 10% speedup can provide more time to tune your model and extract bigger insights into your problem.
Even if you have experience optimizing C or C++ code, data science code can be much trickier to optimize. To better understand why, lets consider a couple of notable factors at play.
Most languages in which data science is done are interpreted rather than compiled. When you interpret code, you do not have the same ability to statically optimize the code as a compiled language. As a result, some of the automatic optimizations you might expect to happen will not, and this will result in slowdowns compared to C++.
For most data science stacks, you will need to use matrix-manipulation libraries instead of implementing every bit of an algorithm from scratch. These libraries provide fast implementations of specific operations in a pre-compiled binary, ensuring that the actual code is much faster than what is possible in the interpreted language as is.
Sometimes, the code you are looking at will have layers upon layers of code underneath it that could be the source of your performance issues. Even simple data science projects will have libraries for linear algebra operations linked in, along with the tree of libraries your project requires. This added complexity can make it difficult to understand where the potential problems are and make the relationship between code and performance harder to reason with.
Despite these factors, however, there are principles that can be followed to improve performance actively. These are not going to be surefire ways to optimize code, but tend to lead to better performance more often than not.
Many data science operations require the use of some matrix or data-frame manipulation library. Vectorization is the process of writing your code in the language of that library. This involves reducing the number of imperative programming constructs you use, from if-statements to for-loops, and increasing the number of functional programming constructs you use, such as reduce and map functions. The general goal of this step is to specify what you want the library to do rather than how you want the library to do it, as the libraries you work with can then optimize the performance accordingly.
In particular, your goal should be to do at least the following:
Remove for-loops and replace them with maps: For-loops in interpreted code are much slower than for-loops in pre-compiled code. If you can replace a for-loop with a map function or a function without any side-effects that take in each element of the matrix as input, applies some operation without any side-effect, and returns that element, then your code will speed up accordingly.
Use conditional indexing instead of if-statements: Like the above tip, moving branching code from your interpreted environment to the pre-compiled environment will generally be faster. If you are able to make some function that associates some element of a matrix with a Boolean without side effects, using conditional indexing to express what you want makes it easier for the library to perform its job for you.
You could also look at trying things like stride manipulation or pivoting, but the main goal of vectorization is to utilize the resources involved as effectively as possible. The more effectively you can use the library you have access to, the faster you will be able to make your code.
If your operation deals with processing large amounts of data, it might pay to make your code friendly to multiprocessing or multithreading libraries. Here, you will create either separate copies of your program, called processes, or separate execution environments which share data, called threads. In doing so, you can likely parallelize disk operations that might be the main bottleneck of your program.
Alternatively, you could also try switching your programming model to use something like Spark or Hadoop Map-Reduce. These tools utilize the cMap-Reduced framework for computation. In essence, you deconstruct the pipeline you wish to parallelize into separate phases, consisting of the following phases:
Mapping: Here, you separately process each line of a data file, and apply some operation to turn it into a key, value pair.
Reducing: Here, you take all of the data for a given key, and process the values together, producing some output to then map again.
While it can be challenging to construct the pipeline in this manner, it is necessary to learn how to rephrase the calculations you want to go into these varying programming frameworks, and it is part of your job. If you take courses on cloud computing or ML on Large Datasets, later on, youll be exposed to these tools and be forced to grapple with these concepts in more detail than we have time here to cover.
These frameworks can automatically parallelize the job with those functions given, allowing you to process more data faster.
If the above steps are not enough, you could write sections of your code in a compiled language and then create functions that use that code in your interpreted language. While this is generally not advised unless you know the code is the main bottleneck, it can provide large speedups at the cost of technical complexity.",
Collecting and Understanding Data,Sparse Matrix,Types of Sparse Matrices,"Throughout this section, we will make use of the following matrix as an example:
We refer to the above representation, where the entire matrix with missing values is written out, as the dense matrix format. Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them, as described below. Note that to be consistent with common library implementations, we will use zero-based indexing when referring to row and column indices.
COO is a straightforward that stores a matrix as three lists: a list of non-zero values, a list of the non-zero values row indices, and a list of the non-zero values column indices. In this way, the matrix A is represented as a tuple of three lists (in addition to the matrix shape):
Here <![CDATA[data[i]]]>, <![CDATA[row[i]]]> and <![CDATA[col[i]]]> represent the actual value, row index, and column index of the i-th non-zero value in the matrix respectively. Note that although we say i-th value, there is no ordering constraint here  the non-zero values can be arranged in any other in <![CDATA[data]]>, as long as their row and column indices are also arranged accordingly.
Updating entries in COO is simple: new entries can be appended to the end of the three lists while zeroing an entry means finding its locations in the three lists and removing those data points. COO doesnt support efficient arithmetic operations, but it can be quickly converted to other sparse formats that support these operations.
DOK uses a dictionary representation that maps the location (row index and column index) of every non-zero element to its value. With the example matrix A,
its DOK representation is
This format allows for fast element access by row and column index. It can also be quickly converted to and from the COO format, although it doesnt support efficient arithmetic operations.
The compressed sparse row format stores a matrix as three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of the column indices of the non-zero values
<![CDATA[row]]>: a list of m+1 values, where m is the number of rows in the original matrix.
<![CDATA[row[i]]]> denotes the number of non-zero entries that appear in the rows above the i-th row in the original matrix, where row indexes start from 0, and <![CDATA[row[m]]]> denotes the number of non-zero entries in the entire matrix.
With the example matrix A,
its CSR representation is
While the <![CDATA[data]]> and <![CDATA[col]]> lists are the same as COO, notice that the row column contains:
0 at index 0, as there are no non-zero entries above the first row.
2 at index 1, as there are 2 non-zero entries above the second row ( 7 and 5 ).
2 at index 2, as there are still only 2 non-zero entries above the third row.
4 at index 3, as there are now 4 non-zero entries above the fourth row (7, 5, 1, and 3).
6 at index 4, as there are 6 non-zero entries in the matrix.
With this setting, note that the length of <![CDATA[data]]> and <![CDATA[col]]> are the number of non-zero entries in the matrix, while the length of <![CDATA[row]]> is always m+1. In addition, it is always the case that <![CDATA[row[0]]]> is 0 (because there are no non-zero entries above the first row) and <![CDATA[row[m]]]> is the number of non-zero entries in the matrix. In addition, the order is important, as entries that appear in earlier (above) rows need to be listed before those in later (below) rows.
This compressed row representation is what gives cCSRd its name, as the rows are compressed to save space. (Think about why this compresses the space, and in what cases this might not compress space in the sparse representation).
This format allows for efficient row access and arithmetic operations (including elementwise matrix operations and matrix-vector products). For example, a matrix-vector product Mx involves computing the dot product between every row of M and x:
\\[M x=\\left(M_{1} \\cdot x M_{2} \\cdot x \\quad \\ldots M_{m} \\cdot x\ight)^{\op}\\]
We know that the non-zero entries in row Mi are the ones at indices <![CDATA[(i, col[row[i]]), (i, col[row[i]+1]), , (i, col[row[i+1]-1])]]>, so only these entries should be multiplied by the corresponding entries in x.
At the same time, column access is slow with CSR, and conversion to other sparsity formats is generally (but not always) expensive. Consider the case where the number of elements is rather few. In this case, what is the time-complexity of conversion to COO?
The CSC format behaves similarly to CSR, but with the columns being compressed instead of the rows, but in a very similar format. Its underlying representation consists of three lists:
<![CDATA[data]]>: a list of the non-zero values in the matrix
<![CDATA[col]]>: a list of n+1 values, where n is the number of columns in the original matrix.
<![CDATA[col[i]]]> denotes the number of non-zero entries that appear in the columns before (to the left of) the i-th column in the original matrix
<![CDATA[row]]>: a list of the row indices of the non-zero values.
With the example matrix A,
its CSC representation is
Similar to CSR, the order is important here, as entries that appear in earlier (left) columns need to be listed before those in later (right) columns.
This format allows for efficient column access and arithmetic operations (including elementwise matrix operations and matrix-vector products, although CSR is faster for the latter). For example, a matrix-vector product Mx can also be expressed as a linear combination of the columns of M, where the coefficients are the entries in x:
\\[ M x=x_{1} M_{(1)}+x_{2} M_{(2)}+\\ldots+x_{n} M_{(n)} \\]
We know that the non-zero entries in column M(j) are the ones in indices <![CDATA[(j, row[col[j]]), (j, row[col[j]+1]), , (j, row[col[j+1]-1])]]>, so only these entries should be multiplied with the corresponding entries in <![CDATA[x]]>.
At the same time, row access is slow with CSC, and conversion to other sparsity formats is, again, cgenerallyd expensive. Whats key here to note is, again, that in certain cases, conversions might be readily easy to do. As CSC and CSR compress rows and columns in a similar fashion, can you think of an example where converting between one to another does not require accessing the underlying data at all but just moving references?
Here we provide a brief preview of how sparse matrices are used in different data science domains. We will discuss these domains in more detail in their corresponding modules later on.
A crucial component of natural language processing is converting text data to numerical features which can then be used for subsequent modeling and training. Many techniques that perform this conversion yield feature vectors with very large dimensions but also high sparsity. For example, given a corpus C, the bag-of-word technique transforms an input document into a binary vector \\( v \\in\\{0,1\\}^{|C|} \\) where \\( v_{i} \\) is 1 if the i-th word in the corpus is present in the document. The size of this vector is the size of the corpus itself, which can easily reach tens of thousands for real-life documents.
At the beginning of this module, we have mentioned the user-movie rating matrix as an example of a very large but sparse data structure. This kind of matrix data format is typically used as input to recommendation algorithms, which attempt to predict missing data based on present data (e.g., predict a users rating of a movie they havent rated, based on their past ratings of other movies). A standard technique for performing such predictions is collaborative filtering, which attempts to approximate the original user-movie rating matrix \\( X \\in R^{m \imes n} \\) as a product of two lower-ranked matrices U and V, i.e., \\( X \\approx U V \\) where \\( U \\in R^{m \imes k}, V \\in R^{k \imes n} \\) and \\( k \\ll m, n \\). This factorization involves complex computations over the rows and columns of X, which motivate the need to store X in a sparse format.
The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a very large number of features. If, however, we expect that only a small subset of features carry predictive power, we can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.","Natural Language Processing,Recommender Systems,Sparse Modeling"
Exploratory Data Analysis,Feature Engineering,Principal Component Analysis,"As you develop analytic models or perform exploratory data analysis, you will encounter datasets with a large number of variables. A small dataset can also become quite large post data cleaning -think about when you transform variables by creating new variables, e.g., dummy variables. Considerations for a dataset with a large number of variables include issues with over-fitting and computing costs. We think about the dimensionality of a model when we consider the number of variables used by the model. The mathematician R. Bellman defined the curse of dimensionality as the problem caused by the exponential increase in volume associated with adding extra dimensions or variables to a space. This just means that when there are more features in a dataset, you are prone to more errors. A dataset with a large number of features could have lots of redundancy and noisy data with little benefit to your overall analytic objective. How can you address the curse of dimensionality without losing useful information? We use the technique of dimensionality reduction, sometimes referred to as feature extraction or factor selection. This technique is implemented using mathematical modeling.
So far, we have talked about techniques that focus on features of an observation. As you know by now, feature engineering informs the models that you will build, and its techniques involve looking at the features of the data. Now, we will explore a technique that is considered a model-based feature engineering technique.
Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset. You might be asking yourself why we would reduce a dataset when we have talked about the importance of more data for better interpretation and solid performance from models. When you have a dataset with a large number of variables, you have to assess the relationship between those variables, identify variables that might violate the assumptions of your chosen ML model, and generally select the variables that are useful to your task. With PCA, you will be reducing the dimension of your feature space to remove any redundancies or irrelevant features.
You use the PCA technique when you want to ensure variables in the dataset are independent of each other. It is a useful technique to use when there are variables that need to be dropped. There are other techniques for dimension reduction, including Linear Discriminant Analysis (LDA), and those techniques will be mentioned in a future unit as well as in your upcoming Machine Learning courses.
PCA is a linear transformation technique as it finds a low-dimensional representation of your high-dimensional data. PCA involves performing the eigendecomposition on the covariance matrix. It will seek out a csmalld number of dimensions in the dataset that are useful to the analytic task. PCA is considered to be an unsupervised technique and will be mentioned in that unit as well.
The following steps are used when performing PCA:
Standardize the data.
Compute the Covariance matrix of dimensions in the data.
Compute the Eigenvectors and Eigenvalues from the covariance matrix. Eigenvector is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it. Meanwhile, an eigenvalue is known as a characteristic value1 or a set of scalars.
Sort eigenvalues in descending order and choose the top k Eigenvectors that correspond to the k largest eigenvalues.
Construct the projection matrix W from the selected k Eigenvectors.
Transform the original data set X via W to obtain the new k-dimensional feature subspace Y.
PCA in Python Example: Principal Component Analysis in three (3) steps.
Reading: A Brief Article-Principal Component Analysis (Lever, Krzywinski, and Altman, 2017)","Dimensionality,Feature Extraction"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Defining an Analytic Objective,"Once a set of business goals has been identified, one can proceed to formulate analytic objectives that state how the application of analytical methods to data can facilitate reaching the business goal. An analytic objective can typically be phrased along with the following template:
As an incremental step towards business objective O
We work towards solving problem P
by focusing on specific tasks T
and applying analytic methods M in conjunction with data D
to create valuable functionality F and/or produce insight I
This formulation of an analytic objective can also be considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances (comparable to treatment versus control condition). In fact, in academic settings, the effectiveness of data science methods will regularly be scrutinized using statistical tests, and it is good practice to principally strive for similar methodological rigor in industry applications. While data science teams and projects in the private sector may use different terminology and specific technical or other circumstances may differ, this pattern of formulating analytic objectives is a very good point of departure for framing data science work and effectively communicating with clients and domain experts. It is the connecting element between the problem, the technique to be applied, the associated requirements and evaluation, as well as the business objective. Also, one should keep in mind that projects often comprise multiple analytic objectives that form a larger plan.
In order for you to become proficient in working with this definition, we will now move to examine its components.",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Objective / Loss Functions,"An objective function quantifies how well or badly a model is performing. Typically, objective functions in deep learning are defined such that a lower value is better. Because of this nature, they are often called loss functions. There are many functions that could be used to estimate the error of a set of weights in a neural network, and they often depend on the choice of the activation function in the final layer of the model. A function with a smooth, differentiable, and high-dimensional curve that the optimization algorithm can reasonably navigate to perform iterative updates to network weights is a desirable choice.
Following are some of the commonly used loss functions based on the problem at hand:
Regression Problem: A problem where the model predicts a real value, the last layer in the model consists of a single node with a linear activation function, and the Mean Squared Error (MSE) can be used as a loss function.
Binary Classification Problem: A problem where an example has to be classified into one of two possible classes, the final layer in the model consists of a single neuron with a sigmoid activation, and a Binary Cross Entropy function can be used as a loss function.
Multi-Class Classification Problem: A problem where the input has to be classified into one of more than two possible classes, the final layer in the model consists of the same number of neurons as the output classes and a softmax activation. The Cross-Entropy function can be used as a loss function in this case.
For a deep learning problem, once a loss function has been defined, an optimization algorithm is used to update the network parameters (weights and biases) based on the obtained loss value. The loss is usually the sum of the loss values obtained for each example in the training dataset and is minimized by an optimization algorithm. An optimization algorithm iteratively calculates the next point using the gradient at the current position, then scales it by a learning rate and subtracts the obtained value from the current position. This is known as cmaking a stepd and refers to the update in network parameters mentioned above. The value is subtracted in the case of a minimization objective, which is the most common case in deep learning and can be added for a maximization objective.
Following are some common optimization algorithms along with their advantages and disadvantages:
Gradient Descent is the most basic but also one of the most used optimization algorithms. It is used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm that is dependent on the first-order derivative of a loss function. It calculates which way the weights should be altered so that the function can reach a minimum. Through backpropagation, the loss is transferred (propagated!) from one layer to another, and the models parameters, also known as weights, are modified depending on the losses so that the loss can be minimized.
\\[ w:=w-\\eta \abla Q_{i}(w) \\]
Easy to compute
Easy to implement
Susceptible to getting stuck in a local minima
Convergence is slow as updates are calculated after calculating the gradient for the entire dataset
Computation for entire dataset requires a large memory
Stochastic Gradient Descent (SGD) is a variant of Gradient Descent where model parameters are updated more frequently as opposed to one single update. Model parameters are updated after the computation of loss on each training example chosen in a random order, hence the title stochastic.
\\[ w:=w-\\eta \abla Q_{i}(w) \\]
Converges in lesser time because of frequent updates
Lesser memory requirements for calculating updates
Less likely than Gradient Descent to get stuck in a local minima
High variance in parameter updates due to high frequency in updates
Learning rate needs to be correctly chosen and adjusted for effective training
To overcome the issues in Gradient Descent and Stochastic Gradient Descent, Mini-Batch Gradient Descent performs loss calculation and parameter updates for a given batch. A batch is a fixed-sized subset randomly sampled from the training dataset. Thus, the dataset is divided into multiple batches, and parameter updates are calculated after processing each batch.
Frequent updates and lesser variance as compared to SGD
Moderate amount of memory requirements
Learning rate needs to be correctly chosen and adjusted for effective training
Learning rate is constant for all parameters which might not be desirable
Susceptible to getting trapped in a local minima
The addition of momentum to SGD addresses the problem of high variance in parameter updates due to frequent updating. Historical parameter updates are multiplied by a momentum term and added to the current calculated update. SGD oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster and smoother.
Reduces the high variance in model updates by SGD
Faster convergence than Gradient Descent
The momentum hyperparameter needs to be additionally tuned
Learning rate needs to be correctly chosen and adjusted for effective training
The adaptive gradient descent algorithm uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features.
\\[ \\mathrm{w}_{\\mathrm{t}}=\\mathrm{w}_{\\mathrm{t}-1}-\\eta_{\\mathrm{t}}^{\\prime} \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}(\\mathrm{t}-1)} \\]
\\[ \\eta_{\\mathrm{t}}^{\\prime}=\\frac{\\eta}{\\operatorname{sqrt}\\left(\\alpha_{\\mathrm{t}}+\\epsilon\ight)} \\]
Reduces the need to manually modify learning rate
Tends to have faster convergence than Gradient Descent and SGD
The learning rate might be decreased aggressively and monotonically, resulting in a very small learning rate
RMSProp addresses the issue of varying gradient values. Some gradients might be quite large, and some might be quite small. In this case, the monotonically decreasing learning rate, as in the case of AdaGrad, might not be ideal. The algorithm focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square. As a result, if there exists a parameter due to which the loss function oscillates a lot, the update of this parameter is penalized.
\\[ v(w, t):=\\gamma v(w, t-1)+(1-\\gamma)\\left(\abla Q_{i}(w)\ight)^{2} \\]
\\[ w:=w-\\frac{\\eta}{\\sqrt{v(w, t)}} \abla Q_{i}(w) \\]
Requires lesser tuning than other optimization algorithms
Faster convergence
The initial learning rate needs to be set manually and needs to be carefully chose as the suggested value does not work for all tasks
AdaDelta is an extension of AdaGrad, which tends to remove the decaying learning rate problem. Instead of accumulating all previously squared gradients, AdaDelta limits the window of accumulated past gradients to some fixed size w. An exponentially moving average is used rather than the sum of all the gradients in this case. AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.
\\[ \\mathbf{s}_{t}=\ho \\mathbf{s}_{t-1}+(1-\ho) \\mathbf{g}_{t}^{2} \\]
\\[ {x}_{t} = {x}_{t-1} - {g}_{t}^{\\prime} \\]
\\[ \\mathbf{g}_{t}^{\\prime}=\\frac{\\sqrt{\\Delta \\mathbf{x}_{t-1}+\\epsilon}}{\\sqrt{\\mathbf{s}_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} \\]
\\[ \\Delta \\mathbf{x}_{t}=\ho \\Delta \\mathbf{x}_{t-1}+(1-\ho) \\mathbf{g}_{t}^{\\prime 2} \\]
Elevates the learning rate decay problem in AdaGrad
Computationally expensive
Adam works with momentums of first and second order to update the learning rate, but unlike RMSProp, which only uses the momentum of the first order. Also, instead of maintaining a single learning rate through training as in SGD, Adam optimizer updates the learning rate for each network weight individually. The Adam optimizer is known to combine the benefits of RMSProp and AdaGrad.
\\[ m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\ight)\\left[\\frac{\\delta L}{\\delta w_{t}}\ight] v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\ight)\\left[\\frac{\\delta L}{\\delta w_{t}}\ight]^{2} \\]
Rapid convergence
Rectifies vanishing learning rate and high variance
Computationally expensive
Equations Reference: Link
(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)
(https://awesomeopensource.com/project/Jaewan-Yun/optimizer-visualization)","Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages,Advantages,Disadvantages"
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Architectures,"The CNN architecture incorporates a number of layer types as follows:
The input layer accepts a 3D matrix of size W1
The convolutional layer accepts a 3D matrix of size W1 and has four hyperparameters: the number of filters K, the spatial extent F, the stride S, and the amount of zero padding P. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F+2 P}{S}+1, \\quad H_{2}=\\frac{H_{1}-F+2 P}{S}+1, \\quad D_{2}=K \\]
The pooling layer accepts a 3D matrix of size W1 and has two hyperparameters: the spatial extent F and the stride S. It then outputs a 3D matrix of size W2 where \\[ W_{2}=\\frac{W_{1}-F}{S}+1, \\quad H_{2}=\\frac{H_{1}-F}{S}+1, \\quad D_{2}=D_{1} \\]
The fully connected layer is identical to a fully connected network layer. It accepts a K-dimensional vector and outputs an l-dimensional vector, where l is the number of nodes in this layer (if the input is a 3D matrix, this matrix is flattened to become a vector).
LeNet is perhaps one of the first successful applications of CNNs was made in 1998 by Yann LeCun et al. They proposed a CNN architecture called LeNet for the task of document recognition. In particular, the task they considered was to recognize handwriting. The architecture of LeNet (Figure 9) contains two convolutional layers separated by two pooling layers, and then finally, two fully connected layers to perform the eventual classification. The convolutional filters used were of size 5x5, with a stride of size 1, whereas the pooling layers were 2x2 with a stride of 2.
Figure 9. Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.
Figure 10. Data Flow of AlexNet
AlexNet was the first successful application of CNNs to the ImageNet dataset and is considered a breakthrough in the application of deep learning to computer vision. It was the first CNN-based winner of the ImageNet challenge. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state-of-the-art at that time. The architecture of AlexNet (Figure 10) contains five convolutional layers with max pooling and three fully connected layers before making 1,000 class prediction problems via the softmax function. AlexNet contained eight layers and was also the first to use the fast and efficient Rectified Linear Unit (ReLU) activation functions and used extensive data augmentation. The original AlexNet uses 11x11 convolutional filters with a stride of size 4. Figure 11 shows the first layer of the AlexNet convolutional filter.
Figure 11. Image filters learned by the first layer of AlexNet.
Several follow-up CNN architectures improved on AlexNet by using even smaller convolutional filters and even deeper networks. A noteworthy successor to AlexNet was the architecture called VGGNet from Oxford University. It cut the error rate of AlexNet on ImageNet in half as it got an error rate of just 7.3 percent. VGGNet was twice as deep as the AlexNet as it had 16 layers and used smaller convolutional filters of size 3 by 3. In total, VGGNet had a staggering 138 million model parameters. The main rationale for using smaller filters and more layers is that the stack of smaller filters has the same receptive field as some larger ones, but more layers allow us to incorporate more non-linearities and potentially fewer model parameters overall.
Figure 12. Comparison of AlexNet and VGGNet.
VGGNet improves over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3X3 kernel-sized filters one after another (Figure 12).
He et al. (2015) introduced the Residual Network or ResNet as ""shortcut connections"" that allow layers to be skipped. ResNet researchers showed that a 56-layer neural network has both higher training as well as a higher testing error compared to a 20-layer network. One reason for this surprising finding is that the valuable predictive signal attenuates as it passes through many layers and the associated activation functions. The solution to this problem and the key idea behind a ResNet is to fit the residual value of the signal instead of the actual desired mapping. Doing so allows us to train a staggering 152-layer residual network with an error rate of just 3.57 percent on the ImageNet dataset, which is actually better than human-level accuracy on this task.
Figure 13. Comparison of Normal CNN layer and Residual layer.
Figure 13 compares how a ResNet and a regular CNN operationalize a residual layer. The left figure shows a standard layer in a CNN where it tries to fit the actual desired mapping H of x. A residual layer, in contrast, fits the residual F(x) = H(x) - x. Architecturally, it is acquired using a residual or a short-circuit connection, as shown in the right figure. It is common to use residual connections after every couple of convolutional layers, as seen on the architectural diagram of ResNet-18 in Figure 14.
Figure 14. ResNet-18 architecture.
Now that we have seen several milestone architectures for CNNs, let's now see the current state of CNN research. In recent years, the trend has been to go deeper as extra layers of non-linearities give significant accuracy boosts. In addition, recent algorithms use smaller filters as they can have similar receptive fields as some of the larger filters but simultaneously are parsimonious in terms of model parameters. Further, it is also becoming increasingly common to residual connections in state-of-the-art CNN architectures these days. Figure 15, taken from a 2017 paper by Canziani et al., shows the accuracies of different models on the ImageNet dataset. It is a remarkably rapid area of research with successive innovations, and most modern-day architectures achieve accuracies better than humans on the ImageNet dataset.
Figure 15. Complexity comparison of deep neural network models. Source: Canziani et al. (2017)","LeNet,AlexNet,VGGNet,ResNet,The Current State of CNN research"
Data Science Project Planning,Design and Plan Overview,Quiz 1,,
Collecting and Understanding Data,Data Structures and Algorithms,Fundamental Abstract Data Types,"Algorithms represent data and relations between data items using a variety of abstract data types. The most commonly used abstract data types are the following:
Sequences: Sequences, also called one-dimensional arrays. Many programming languages and lists in Python. Typically arrays are fixed size. Items are identified by positions or indices, starting with 0. Given an index, any item can be accessed in constant time; old items can be replaced with new values similarly. Languages like Python allow the extension of an array if one needs to add new values to the end of the array. Further, Python lists can hold data values or different underlying types.
Sets: Sets let one represent the equivalent of finite sets in mathematics, with elements coming from some domain where equality between the elements in the domain is defined in some way. The operations on sets are the typical operations one does on mathematical sets:
Intersection and union of sets,
Subtraction of one set from another set,
Inserting elements into a set or removing an element from a set,
Checking if a given element of the domain is a member of a set,
Computing the size of a set.
In ordered sets, the elements are assumed to be orderable based on a \\(<\\) relationship. With sorted sets, we can also do operations such as
Find the \\(i^{th}\\) smallest element of a set,
Find the ordered position in the set of a specific value \\(x\\) (same as finding the number of elements in the set less than a given \\(x\\)).
Tables: Tables are an extension of sets. Each element in a table \\((k, v)\\) consists of a key \\(k\\) and an associated value \\(v\\). Key values in a table should be unique. Set operations like union, intersection, and difference on tables are done based on the key values with some provisions for conflicts. But one mainly uses tables typically for finding the value \\(v\\) associated with a key. Similarly, tables can be ordered based on a key if there is a need.
Graphs: Graphs are the most versatile abstract data types. They are typically used to represent a set of items (called nodes) along with asymmetric or symmetric relations between those items (called edges). For example, graphs can be used to represent
Social networks with nodes representing people and edges representing cfriendshipd or cfollowsd relations between them.
Transportation networks with nodes representing cintersectionsd and edges representing the roads between the intersections, with a distance measure associated with each road.
Neural networks with nodes representing neurons and edges representing weighted connections.
There is a whole set of operations one can do on graphs that can compute all kinds of useful information about the data and the relations. Here are some of such operations:
What is the shortest distance between any two intersections?
Who are the people on a social network with more that 100 connections?
Trees: Trees are special cases of graphs and are used to represent hierarchical relations such as parentchild relations. This restriction usually allows for more memory-efficient representations or time-efficient operations for specific classes of operations.
Priority Queues: Priority queues are essentially sets where each element consists of a value, and a priority, and the only operations one can do are
Inserting a value with its priority
Finding or deleting a value with the maximum priority.
While we listed these as abstract data types, some of these types are used to implement others, usually in a nested way. For instance, sets and tables can be used to implement graphs, while trees (specifically their balanced binary variants) can be used to efficiently implement sets, tables, or sequences. More details on these are beyond the scope of this section and are typically the topic of an introductory course and book on data structures and algorithms.
One can also refer to https://en.wikipedia.org/wiki/Abstract_data_ type for more details on abstract data types.",
Data Science Project Planning,Requirements Gathering,Functional and Non-Functional Requirements,"Reminder: The data science process emphasizes understanding business needs and objectives and defining analytic objectives to meet the expectations of the client. The requirements gathering process will involve identifying the stakeholders, eliciting needs, and defining requirements. The difference between the requirements of a traditional IT project and those of a data science project is the focus on the requirements for the analytic solution.
Functional requirements define the functions of a system and how users will interact with the system. Functional requirements are derived from the user and system requirements that are needed to satisfy the business requirements. In essence, defining the right business requirements will result in useful functional requirements that can be used to develop the proposed system. As mentioned earlier, user requirements are captured in use cases, and those use cases can help the project team define the functional requirements. A use case will describe the interaction between the system and its users, also known as actors. The interactions between the system and the user are known as goals.
Not all functional requirements are implemented in the first iteration of solution development. This is why functional requirements are organized by priority. High-priority functional requirements must be implemented to meet the business objectives. Medium and low priority functional requirements are important but typically classed as requirements that will not affect the current business objectives. These requirements may also be implemented in later iterations or updates to the system.
Traditional functional requirements considered in the software and application development process include Business Rules, Process Flows, Audit Tracking, Transaction Handling, Reporting Requirements, Administration Functions, Authorization Requirements, and Data Management.
Reading: IEEE ANSI 830 Documentation on Functional Requirements.
Requirements can be captured in different formats, including user stories, use case specifications, the voice of the customer, and business rules. This unit will focus on defining functional requirements from use case specifications.
Initial user requirements are often written too broadly to unambiguously define what the proposed system should do at each step in a solution. The danger is that the software provider will produce a system that may not meet the business objectives because it misunderstands what the customer would consider an acceptable solution. Different forms of use case analysis are typically used to capture, discuss, and verify the details of a solution with the customer. For each expected capability or interaction (functional requirement), we work with the customer to write detailed use specifications and gather them into a single document.
The use case specification provides a textual description of a use case. As mentioned earlier, it will decompose a user requirement into functional requirements. The use case specification details the steps involved in a goal or action. Figure 1 below shows the sections of a use case specification:
Figure 1. Use Case Specification
A business analyst (BA) has distributed questionnaires to elicit the needs of stakeholders for a proposed system for their customers. The BA analyzed the information from the questionnaire and defined some user requirements. One of the user requirements is customers ability to update their billing address in the new system. This requirement describes what customers can do with the solution, but it is still too ambiguous and does not tell a developer what the system should do at each step of this requirement. We will illustrate how we can simply decompose that user requirement into functional requirements.
Use Case/User Requirement: Update billing address.
(1) The user shall be able to view the billing addresses in the system.
(2) The user shall be able to update a billing address in the system.
(2) The system shall display updated customer service and billing addresses.
Functional requirements considered in the software and application development process include the business rules, user and system authorization levels, authentication, and regulatory requirements.
Non-functional requirements (NFR) describe the performance and behavior of a system. They are also referred to as operational requirements. The NFRs for a traditional IT project will describe the attributes of a system, including the system's scalability, usability, maintainability, performance, reliability, availability, capacity, interoperability, and security.
Availability
Refers to a property of software that is there and ready to carry out its task when you need it to.
Interoperability
Interoperability refers to the degree two or more systems can usefully exchange meaningful information via interfaces in a particular context.
Modifiability
Modifiability refers to the ease of modifying the system with minimal changes to the architecture.
Performance
Performance refers to the software systems ability to meet timing requirements. When events occurinterrupts, messages, requests from users or other systems, or clock events marking the passage of timethe system, or some element of the system, must respond to them in time.
Security
Security refers to the systems ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized.
Testability
Software testability refers to the ease with which software can be made to demonstrate its faults through (typically execution-based) testing. Specifically, testability refers to the probability that the software will fail on its next test execution, assuming that it has at least one fault.
Usability
Usability is concerned with how easy it is for the user to accomplish the desired task and the kind of user support the system provides. Over the years, a focus on usability has shown itself to be one of the cheapest and easiest ways to improve a systems quality (or, more precisely, the users perception of quality).
Scalability
Scalability refers to the ease of adding new resources to a system to cope with increasing demands on its use.
Observability / Monitorability
These refer to the ability of the operations staff to monitor the system while it is in operation.
Portability and Compatibility
These refer to the compatibility of the hardware, systems, application software, and solution with other applications and processes within the existing environment.
Modules and Architecture
These refer to the technical considerations for the system, including operating system compatibility and the programming development environment employed by the developers.
Non-functional requirements focus on the user experience and take into account the system and application software and data compliance rules. Framing a non-functional requirement for a data science project will include the above-mentioned attributes and additional requirements that are related to machine learning models and AI analytic solution(s).","Functional Requirements,Decomposing User Requirements into Functional Requirements,How can a customer perform this task within the system?,Functional Requirement for Customers (1):,Functional Requirement (2):,How will the system confirm that the billing address has been updated?,We know that a requirement must be verifiable, traceable, and unambiguous to be considered a complete requirement. The Business Analyst must ensure that these functional requirements meet the criteria for a good requirement.,Non-Functional Requirements"
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Attention is All You Need,"[REQUIRED READING] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [pdf]
The content of this paper is essentially similar to the Language Representation, and Transformers module introduced earlier in the course. However, because this is a foundational work in modern NLP, we have opted to cover it again in this research paper module. Here we will focus more on understanding how the paper was presented to the research community and which areas it contributes to.
The authors are a group of researchers and engineers from Google Brain, Google Research, and the University of Toronto, with expertise in deep learning and natural language understanding. Prior to this paper, they worked on various language research projects related to Googles services, such as language translation, speech tagging, and language inference. Interestingly, the majority of the authors have left their affiliations at the time of this paper to found their own NLP startups.
The paper targets ML researchers and engineers who build large-scale language models. It is pivoting a switch from the RNN/LSTM paradigm to a new transformer architecture that was shown to achieve state-of-the-art performance in language translation.
Since the resurgence of deep learning in 2012, many advances have been made in neural network architectures and methodologies, although they mostly apply to computer vision (e.g., AlexNet, VGGNet, ResNet). There wasnt a similarly impactful innovation for natural language processing - RNNs are designed to handle sequential data but suffer from exploding/vanishing gradients; LSTMs address this issue but require three times more matrix computations. In addition, these recurrent models can only process data sequentially and do not benefit from the powerful parallel processing of modern GPUs. This paper is part of an effort to build new neural architectures that address these issues.
There are two primary innovations from the paper.
Positional Encoding is a novel way of representing word order in a sentence. Given the sentence cI like data science,d an RNN knows that cliked comes after cId because it processes the tokens sequentially and therefore receives cId as input before clike.d Transformers, on the other hand, construct inputs that consist of both the original tokens and their index locations in the sentence, i.e., [(cId, 1), (cliked, 2), (cdatad, 3), (cscienced, 4)]. In addition to learning the embedding of the tokens, they will also learn the encoding of these index locations and, therefore, the importance of word ordering (the paper actually uses fixed formulas for the positional encoding, \\(PE_{(pos, 2i)}\\) and \\(PE_{(pos, 2i+1)}\\), because they were shown to produce similar results to the learned positional embeddings). Note also that this approach enables the parallel processing of all tokens because their ordering within the input sentence has already been represented by the index locations.
Self-attention is a mechanism that relates different positions of a single sequence to compute a representation of this sequence. At a high level, self-attention allows a neural network to understand a word in the context of the other words around it  for example, it may know that cbackd has different meanings in cI came back from workd and in cmy back hurtsd because it attends to the token ccamed in the first sentence and churtsd in the second. While self-attention has been used in prior works in conjunction with recurrent or convolutional neural networks, the innovation of this paper lies in using self-attention alone, without the associated recurrent or convolutional structure, to achieve state-of-the-art results. This is also where the paper title cAttention is all you needd comes from. As an unrelated note, the template cX is all you needd subsequently became popular in the machine learning literature, with a recent paper from CMU that both make use of it and poke fun at it.
The paper proposes the Transformer model architecture (Figure 2) for language translation, whose training procedure can be summarized as follows. Given an input sequence of tokens (e.g., an English sentence) and an output sequence of tokens (e.g., a French sentence):
Step 1: Convert each input sequence token to its vector embedding. Add this vector to the positional encoding vector, i.e., \\(PE_{(pos, 2i)}\\) or \\(PE_{(pos, 2i+1)}\\) to yield the word vector with positional information for each token.
Step 2: Pass the input sequence word vectors into the encoder block, which consists of a multi-headed attention unit and a fully-connected feedforward neural network unit. The attention unit generates an attention vector for every token in the input sequence to represent how much the token is related to other tokens in the same sentence. This process is performed \\(h = 8\\) times with different, learned linear projections to different dimensions. Thus, every input token yields \\(h\\) attention vectors, which are then concatenated to form a single vector (the name multi-headed refers to the fact that multiple vectors are concatenated in this step). These attention vectors are then passed to identical but independent feedforward neural networks in parallel, outputting an encoded vector for every input token.
Step 3 is similar to Step 1 but carried out on the output sequence tokens.
Step 4: Pass the output sequence word vectors into the decoder block, which contains a masked multi-headed attention unit, followed by a multi-headed attention unit and a feedforward unit. The first attention unit generates an attention vector for every token in the output sequence to represent how much the token is related to other tokens before and including it in the same sentence (this is where the term cmaskedd comes from  we mask away the tokens after the current token because those are our prediction goals). These attention vectors for the output tokens, combined with the output of the encoder block, are passed into the second attention unit. This unit generates an attention vector for every token in both the input and output sequence to represent how much the token is related to every other token in both sequences (in other words, this unit relates every input English token to all the other input English tokens and to all the output French tokens).
Step 5: The output from step 4 is fed to a standard linear classifier, represented as a fully connected layer with a softmax activation function. The layer outputs the probability that each word in French is the next output (in other words, this is a multi-class classification problem where the classes are all the French words, and the word with the highest probability value is predicted to be the next output token).
Additionally, batch normalization is applied after every unit to smoothen the data and make it easier to learn with larger learning rates.
During inference, the same process as above applies, but the output sequence is replaced by an empty sequence with only a start-of-sequence token (because this is the prediction goal). The transformer will predict the next token one by one and add each predicted token to the output sequence, so that it can be used as the basis for the next token prediction. This is the same inference technique used by Sequence2Sequence models, except that at each timestep, we use the entire output sequence generated so far, rather than only the most recent prediction.
The above architecture was evaluated in two tasks: English-German translation and English-French translation. Details about the training process and hyperparameters used can be found in Section 5 of the paper. Results from the experiment (Table 2) showed a better BLEU scores than previous state-of-the-art models at a fraction of the training cost.
The paper presents a novel Transformer architecture that significantly improves upon the standard RNN/LSTM variations in both performance and efficiency. Transformers have been extensively used in both natural language processing and computer vision research following the publication of this paper (which has been cited more than 47,000 times, based on Google Scholar). It also led to the release of large-scale pre-trained Transformer models, such as BERT, GPT-3 and T5, which anyone can utilize for their own projects.","Who are the papers' authors? Why are they qualified to write about this topic?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summarize the paper\xe2\x80\x99s experiments and findings,What are the implications of the paper\xe2\x80\x99s findings? What can an outsider like us expect to learn from this paper?"
Data Science Project Planning,Design and Plan Overview,Diagrams in Design Document,"It would be helpful to think about what you would like to showcase in your diagrams. The following classifications of diagrams can help you get started.
Modules And Their Relationships - How code is structured
Runtime Components and Connections - How data flow during runtime
Deployment Diagrams - What is Infrastructure for the architecture
You can use these types of diagrams at any layer of abstraction, i.e., you can use these types of diagrams to describe  a high-level view of the system or a sub-system, as is apparent in the exemplar documentation.
These diagrams are useful if you would like to show modules and their relationships with each other. Modules represent a static way of structuring the system. In these diagrams, we do not care much about how the system behaves at runtime.
Modules allow us to answer questions like:
What are the different business functions in your system?
What other modules does each business function depend on?
Are there any external dependencies for each module?
Does any module inherit behavior from another module?
Figure 1. Execution Graph of a Completed Run of the Continuous Training Pipeline (ACAI, MCDS Capstone Project, 2020)
Notice how model dependency is represented in this execution graph, as it is important to this project.
When you want to showcase how a system functions during runtime, you can use Components and Connectors. Each runtime element is related to another element via a Connector, which should be adequately described in your writeup as well as in the legend.
Examples of runtime elements, i.e., components are:
Services
Peers
Clients
Servers
Filter Systems
Examples of connectors also called vehicles of communication are:
Call-return style connector
Data pipelines
Process synchronization operators
These diagrams help us answer the following type of questions:
What are the major components of your system at runtime?
Does the system  have shared data stores? What is the nature of these stores, i.e., are they persistent or transient, etc.?
How does data progress through the system?
Does anything run in parallel?
This is a good example of using components and connectors to provide a high-level view of the flow of data. We can see that connection types have been labeled separately to represent the different types of data transfer that can happen in the system.
Figure 2. A Typical ML Workflow in ACAI (ACAI, MCDS Capstone Project, 2020)
Representing deployment models can be confusing because they look very similar to runtime diagrams.
The important distinction between the two is that deployment diagrams are meant to display the interaction of the solution with non-software structures like CPUs, file systems, networks, development teams, etc.
In a deployment model, you will need to make infrastructural considerations for your solution. Your deployment model will help answer questions like:
What cloud instance type does your solution execute on?
What type of cloud data store are you expected to use?
Who deploys the solution?
What type of queuing system are you expected to use?
In the following example, we know that we have docker containers that interact with a Job Monitor, a Log Server, and a Launcher.
A Job Registry requires the use of an SQL server. In your diagrams, you can be specific about particular SQL servers like MySQL/Postgres if the requirement is clear.
Figure 3. Overview of Execution Engine Architecture (ACAI, MCDS Capstone Project, 2020)",Friendly reminder: Don\xe2\x80\x99t forget to include a legend in your diagrams. It greatly increases the readability of your diagrams and helps with communication.
Model Evaluation,Metrics and Interpretation,Module 19 Summary,This is a new page with empty contents.,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Module 2 Summary,"This module focused on formulating the analytic objective, which is also considered a research hypothesis because it effectively posits that a certain technique improves a metric with regard to a population of problem instances.
A well-framed analytic objective will include the following elements:
Understanding the business objective,
Identifying the problem,
Focusing on a well-defined task,
Checking the proposed method against the target insight
Proposing data collection and curation methods, and
Framing the analytic objective.
Analytic objectives can be reframed to focus on specific tasks, in which case an incremental step would become the primary goal.
An analytic objective should state what specific functionality, insight, or resource is gained from leveraging that data and methods you propose, relative to the current situation, and assuming the project is successful as proposed.",
Advanced Natural Language Processing,Language Representation and Transformers,Module 24 Summary,This is a new page with empty contents.,
Model Evaluation,[Research Paper] Manipulating and Measuring Model Interpretability,Quiz 7,,
Collecting and Understanding Data,Data Collection,Module 7 Summary,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. Data collection also consists of attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants.
Data scientists need to develop a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from an exploratory analysis of data that is organically available to highly planned efforts. The manner in which data is collected is arguably more important than the availability of that data itself.
Validity is the development of sound evidence to demonstrate the intended test interpretation.  In an observational study, the threat to validity concerns whether the observed changes can be attributed to the exposure or intervention and not to other causes and whether we can generalize that exposure more universally causes the outcomes.
Threats to internal validity are problems in drawing correct inferences about whether the covariation between the presumed treatment variable and the outcome reflects a causal relationship.
Threats to external validity are problems that threaten our ability to draw correct inferences from the sample data to other persons, settings, treatment variables, and measures.
Statistical bias results from violations of external validity or internal validity of a study. Common statistical biases that you need to be aware of and take into account during the data understanding process are:
Selection Bias: a threat to internal validity occurs when there is a mismatch between the data selected for the study and the subject matter that the data scientist wants to make inferences about.
Self-selection Bias: If the response rate of a survey is not perfect, it is likely that certain characteristics of those individuals are related to the reason why they responded to the survey.
Confirmation Bias: This type of bias occurs when we favor evidence that confirms our personal beliefs, values, and hypotheses.
Information Bias: The data collected has a misclassification bias and is not accurate. One way to control information bias is to implement blinding.
Confounding Bias: occurs when incorrect inferences are made about the subject matter while failing to account for a potentially confounding variable.",
Collecting and Understanding Data,Data Structures and Algorithms,Module 9 Summary,"Functionality is the most vital characteristic of an abstract data type that every user must understand. It is often communicated via an API that specifies the operations and input-output data for each API call, as well as a description of the function that the call delivers.
A specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. Operations such as vector addition or the dot product of two vectors can be then implemented using this representation.
Concrete data structures are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.
Asymptotic analysis is the study of how an algorithm grows as a function of the size of the input data to the algorithm. The basic idea is to model how the growth rate of two functions compares to large input. When analyzing an algorithm, we build a mathematical model of how the number of steps the algorithm executes depends on the size of the input.
Big-O Notation: We say a function f(n) is O(g(n) if there are positive constants c and n0 such that for n  n0 f(n)  cg(n). That is, beyond n0, f(n) grows at most as fast as c  g(n). That is, c  g(n) always dominates f(n) in growth.
It is important to understand how the time and memory cost grow with the size of the input, i.e., the time and space complexity for an algorithm. There is often a trade-off between runtime and space complexity, which typically comes in the form of storing intermediate values (using more space) to avoid re-computing them (reducing runtime).
Learning to use the right data structure for different use cases is an essential skill for writing efficient programs and building complex applications.
An algorithm represents data and relations between data items using a variety of abstract data types. Some of the most commonly used abstract data types include Sequences, Sets, Tables, Graphs, Trees, and Priority Queues.",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Converting Text Data into Numerical Features,"Textual data cannot be used directly as model inputs as models typically require numerically represented features. Applying the text processing tasks mentioned in the previous section helps streamline textual data into a form that can be easily constructed to numerical features using any one of the methods, like bag-of-words, term frequency, word embeddings, etc., based on the use-case.
Bag-of-Words is a primitive feature construction method that can be employed for simple problems to obtain quick results. It translates the entire text corpus into a vector with word counts.  It also assumes that the vocabulary is fixed and the size of the vocabulary determines the size of the vector, with each entry in the vector representing how many times a word occurs in the document.  Since only a small number of the words in a vocabulary would be used in a document, such a representation would be very sparse, with many of the entries in the vectors being 0.
Instead of representing the entire corpus as a one-dimensional list of numbers indicating word counts, term frequency takes into account the word frequencies for each member document in the corpus.
Consider the corpus = [ cJack ate an appled, can apple on the tabled, cJack likes the appled ]
Bag-of-words will create a one-dimensional vector for this entire corpus:
Jack
ate
an
apple
on
the
table
likes
2
2
2
3
2
2
1
1
However, the term-frequency matrix will have a row corresponding to each of the three documents:
Jack
ate
an
apple
on
the
table
likes
Document #1
1
1
1
1
0
0
0
0
Document #2
0
0
1
1
1
1
1
0
Document #3
1
0
0
1
0
1
0
1
Term-frequency Inverse-Document Frequency (tf-idf) treats frequency counts for each document of a corpus distinctly like in Term Frequency (Tf). The Tf method discussed earlier assigns an importance value to all the words purely based on their frequency. Hence, features corresponding to words that appear more frequently, like stopwords, get assigned a large value in comparison to words that could potentially warrant higher importance in the corpus. Therefore, tf-idf is a way to cnormalized these high-frequency values. In tf-idf, for any word not in the corpus, we can either ignore it or consider its frequency under another/foreign word column.
Each word has an inverse document frequency associated with it. Hence,
\\[IDF_{j}=log(\\frac{\ext{number of documents}}{\ext{number of documents with the word j}})\\]
Intuitively, the IDF of a word that appears in fewer documents is higher. (Now, using the above formula, what is the IDF of a word that appears in every document? What could be the highest IDF value a word could get?)
Finally, tf-idf for a word j in document i is calculated by multiplying the IDF score for word along j with the Tf for word j in document i :
\\[ TF - ID_{ij}= TF_{i}\imes IDF_{j} \\]
All the above feature construction methods are easy to visualize and understand. However, it represents individual words as dimensions and thus tends to suffer from the curse of dimensionality. Word embeddings are representations of words in a meaningful low-dimensional space whose dimensionality is a fixed number independent of the word count in the corpus. Intuitively, Words that are placed closer in this space are expected to be similar in meaning. For instance, the position of cSeattled is closer to cBostond than it is to ctalkd in the below illustration of word embeddings.
Figure 1. Word Embeddings (Source: IBM Research Blog)
As discussed in an earlier module, word embeddings can be generated by training a model from scratch or through pre-trained models like BERT (introduced later in the course), which brings down the training time significantly. Due to the advantages of word embeddings over other methods in capturing context and minimizing memory used for feature representation, it is increasingly used for deep learning and advanced NLP tasks, some of which will be discussed in the next section.","1. Bag-of-Words,2. Term-Frequency,3. Term-Frequency: Inverse-Document Frequency,4. Word Embeddings"
Data Science Project Planning,Developing a Vision,The Vision Document,"The important components of a vision document are described as follows:
Problem Description. First, you need to determine the real-world problem that you are trying to address. You should also look into the literature to identify existing solutions and their limitations. Your goal is to propose an improvement to these solutions in some ways so as to yield tangible social or business impacts.
Proposed Solution. Next, come up with an overarching framework for your solution. An example solution formulation could be cA framework that supports a declarative description of a configuration space and automatically evaluates all the options, finding the best given some measure and labeled dataset.d Compare your solution with the existing solutions identified previously. What makes your solution better?
Scientific Hypotheses. Now you need to formalize the scientific hypotheses underlying your proposed solution. Keep in mind that hypotheses need to be testable assumptions. For example, if you make the assumption that the majority of your users are teenagers, but your platform doesnt collect users ages, then your assumption is not testable. Then, develop a plan to validate your hypothesis as you develop your solution. With respect to a technical data science solution, you can formulate your hypothesis along with one of the following themes:
cConstructived: It is possible to build such a framework.
cFormatived: The proposed solution is cfast /good enoughd
cEmpiricald: The proposed solution will significantly outperform the state-of-the-art baseline measured by a certain metric.
Major Features. For the next step, you should describe in more specific terms how the system features you plan to implement form the proposed solution to the problem (e.g., a list of major features or components of your solution). This is really important from a ""traceability"" perspective as it's easy to miss major features as you dive deeper into the project. Here are the important requirements for the proposed features:
Features should not be technical constraints. For example, cthe system will use an Amazon AWS load balancer to manage trafficd is a bad example, while cthe system will use a cloud-based load balancer to manage trafficd is a good example.
Features should be distinct in the sense that no two features should overlap.
Features should be traceable, with a unique name or identifier, so that they can be traced across different documents and project stages.
Scope. At this point, you will want to outline the boundaries of your project, with a focus on what will be delivered at the end of the project timeline. If there is something your project will specifically not do, you should note it here. For example, be clear about how much (or how little) data will be covered, whether your solution will meet certain run-time requirements in terms of responsiveness, whether your solution will be deployed as a web service or application or exist only as a code repository/Jupyter notebook, etc.",
Collecting and Understanding Data,Data Structures and Algorithms,Asymptotic Complexity,"Above, we alluded to concepts such as the time an operation takes or the memory a data structure requires. When we design an algorithm to solve a specific problem, such as sorting a set of numbers, we consider the best algorithm in terms of the time it takes to solve an instance of the problem, along with the maximum memory during execution that the algorithm will need.
In general, time depends on various specific aspects of the hardware we eventually execute the algorithm on. Thus, time depends on hardware parameters such as the clock speed of the CPU (e.g., 3.5 GHz), the speed of the memory interface along with the number and sizes of cache memory, and the speeds of other hardware units, such as GPUs, etc. This makes it hard to compare specific implementations of algorithms on different CPUs and is actually a very tedious and possibly intractable effort.
Instead, theoretical computer science has developed simple but effective mathematical tools to compare algorithms in terms of the number of relevant steps they execute as a function of the size of the input data to the algorithm. These tools are based on what is called asymptotic analysis.
The basic idea in the asymptotic analysis is to model how the growth rate of two functions compares to large input. In particular, as we increase the numeric argument of both functions to infinity, how do the functions behave? Does one grow faster, equally as fast, or slower than the other? In this comparison, we ignore what happens for small input values or any other constant factors (such as the speed of the underlying hardware).
The most important tool is based on the big-O notation. We say a function \\(f(n)\\) is \\(O(g(n)\\) if there is are positive constants \\(c\\) and \\(n_0\\) such that for \\(n \\geq n_0\\) \\(f(n) \\leq c\\cdot g(n)\\). That is, beyond \\(n_0\\), \\(f(n)\\) grows at most as fast as \\(c\\cdot g(n)\\), that is, \\(c\\cdot g(n)\\) always dominates \\(f(n)\\) in growth, as shown below.
So when analyzing an algorithm, we build a usually mathematical model of how the number of steps the algorithm executes depends on the size of the input. Based on the definition above, we express the number of steps with a function \\(f(n)\\), which captures the size of the \\(n\\). We then try to find the function \\(g(n)\\) such that \\(f(n)\\) is \\(O(g(n)\\).
Let's give a very simple example to explain this. Suppose we have an unsorted array of \\(n\\) integers, and we want to search if a given integer \\(x\\) appears in the array or not. A simple algorithm for this would a simple loop that searches if the next entry in the array is equal to \\(x\\). So in the worst case, we will need to look at each of the \\(n\\) integers until we know what the result is. Assume of these steps take, say, 3 operations; our function \\(f(n)\\) will be \\(3n\\). Now with the choice of \\(c=4\\) and \\(n_0 = 0\\) you can easily convince that \\(f(n)\\) is \\(O(n)\\). That is, the number of steps of our algorithm grows linearly as \\(n\\). While this is a very simple example, not every algorithm analysis will be simple as that. You can try and come up with an algorithm that searches \\(x\\) in a sorted array whose number of steps \\(f(n)\\) is \\(O(\\log n)\\). So ignoring the cost of initial sorting (which can be amortized over many searches if necessary), you can see that this latter algorithm uses much less time than the former one, as the function \\(\\log n\\) grows much much slower than the function \\(n\\).
The following are some additional important concepts regarding asymptotic analysis:
Upper bound is the asymptotically maximum time that a given algorithm needs for all inputs of size \\(n\\). Algorithms have upper bounds. For example, we say, ""mergesort has an upper bound time complexity of \\(f(n) = c_1 n \\log n\\)"" or ""selection sort has an upper bound time complexity of \\(f(n) = c_2 n^2\\)"".
Lower bound is the minimum asymptotically the minimum time that any algorithm for a problem needs for all inputs of size \\(n\\). For example, it can be shown that no sorting algorithm that works by comparing elements can take less than \\(c_3n \\log n\\) steps.
For more details on asymptotic analysis one can refer to any book on algorithm analysis, or to https://en.wikipedia.org/wiki/Asymptotic_analysis",
Collecting and Understanding Data,Ethics of Data Science,What is Data Science Ethics?,"As we practice data science, it is important that we do so with an ethical framework in mind. There are many ways we can cause unintended negative consequences through our work as data scientists if we blindly focus on the techniques we learn without considering their consequences. In this module, we will talk about how we could potentially make mistakes and how we can avoid making these mistakes.
Lets start with a simple definition of ethics. Ethics are rules that we, as fellow citizens, voluntarily follow because we believe they make the world a better place. Ethics guide us to distinguish right from wrong and are at the bedrock of human civilization.
Consider this simple example: Ethics stop you from shoplifting. There may be other things that could stop you from shoplifting, such as the possibility of being caught by the store owner,  but ethics would stop you even if there is no one in the store to catch you. What stops you is the fundamental principle of what you think is right. Ethics are the shared values of the society that compel us to think about how we want to be treated, which then translates into how we treat others.
Ethics are distinct from the law. Consider an example where you tell a friend a secret, and the friend promises not to tell anyone else. The friend then breaks that promise and spreads your story. The friend may not have broken any law and is unlikely to face any legal consequences. Nonetheless, we can agree that the friends behavior is unethical because it is a common principle of human relationships and interactions to keep ones promises, especially to ones own friends.
Ethics are not laws, but laws often follow the shared social values of ethics. Laws are created and enforced to ensure these common social values. However, despite having generally agreed upon shared values, not everyone will act in accordance with those values, either because they dont share them or because theyve justified to themselves in some way that theyre not truly violating them. For example, just because we have a shared principle saying it is wrong to shoplift doesnt mean there are no shoplifters in society. It merely means that we all have agreed that stealing is wrong as a society and can therefore pass legislation that punishes thieves. These laws are then enforced by our criminal justice system, which also determines the punishments for those who violate them.
One practical way to think about the benefit of ethics is through the lens of economics. In general, society as a whole does best when individuals maximize their own private returns. This is the fundamental principle of a modern free-market economy. However, there are situations where we see that the results of collective actions from individuals under the name of maximizing private benefits are a detriment to society.
When individuals act in their own interest while collectively accessing a public resource (called a commons), it is likely they will ultimately deplete or otherwise corrupt that resource. This situation is known as the tragedy of the commons, a term used for the first time by Garret Hardin in 1968 and published in Science Magazine (requires CMU credentials to access). The tragedy of the commons stresses that cthe population problem has no technical solution; it requires a fundamental extension in morality,d in which cpopulationd refers to members of society and cextension in moralityd refers to ethical principles. It explains individuals tendency to make decisions based on selfish needs that sometimes lead them to ignore the best interests of the group as a whole of which they are a part themselves.
In 2009, Elinor Ostrom became the first woman to be awarded the Nobel Memorial Prize in Economic Sciences for revisiting the tragedy of the commons and her work on governing the commons (requires CMU credentials to access). She showed cases where such ctragedyd was not inevitable and that if humans cooperated with one another and monitored and enforced rules to manage the commons, then the ctragedyd could be avoided.
Now that weve learned about what ethics are and how together we can avoid unintended negative consequences from selfish and unethical practices, in the subsequent modules, we will link this with data science and demonstrate the core values of ethical data science practices to motivate the discussion of how data scientists can apply ethical principles.",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Module 14 Summary,This is a new page with empty contents.,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Computer Vision Applications,"Jitendra Malik, a computer vision pioneer, proposed the 'Three R's' as the classic problems of computational vision: reconstruction, recognition, and (re)organization. You might have come across many computer vision applications in your daily life, which are most likely attempts to solve one of the three Rs. In this section, we brief a few common applications of computer vision.
Optical character recognition (OCR)
Figure 4. LeNet 5.
Optical character recognition or optical character reader (OCR) is a technology to convert images of text into actual text. The text images can be typed, handwritten, or printed into machine-encoded text such as a scanned document, a photo of a document, or an image that contains the text. Figure 4 shows the results of probably the first OCR task, LeNet. OCR is a commonly used method to digitalize printed text to reduce storage size and enable editability and searchability.
Object Recognition
Object recognition is a computer vision technique for identifying objects in images or videos. It might be relatively easy for a human to look at a photograph or watch a video and spot people, objects, scenes, and visual details. However, it is not as straightforward for a computer not only to recognize the items but also to understand what the items mean to the level of understanding of a human. Object recognition is a key technology behind driverless cars (Figure 5), enabling them to recognize a stop sign or to distinguish a pedestrian from a lamp post. It is also valuable for disease identification in biological imaging, industrial inspection, and robotic vision applications, just to name a few.
Figure 5. Object recognition technology from Teslas Autopilot and full self-driving capabilities.
Face Recognition
Figure 6. Apples Face ID.
Facial recognition is a specific case of object detection where the primary object is a human face. While similar to object detection as a task, where features are detected and localized, facial recognition performs not only detection but also recognition of the detected face. Facial recognition systems search for standard features and landmarks like eyes, lips, or a nose and classify a face using these features and the positioning of these items. The facial recognition system is used as an ID verification process to authenticate users for security purposes, such as Apple's Face ID (Figure 6), Clear airport security service, and the United States driver's license photo database.
Image augmentation applications on various social media services, such as Snapchat, use an algorithm to detect faces and perform augmentation to swap faces in an image for entertainment (Figure 7).
Figure 7. Snapchats Face Swap.
Vision-based Biometrics
Figure 8. Sharbat Gula, photographed in 1984 and 2002.
Sharbat Gula, one of the students in an informal school at the Nasir Bagh refugee camp in Afghanistan, was identified 18 years later from a photograph taken in 1984 when she was 12, using an analysis of her iris pattern image (Figure 8). Daugman (2004) computed IrisCodes from both of her eyes from the photograph in 1984 and 2002, respectively, and matched them using a Hamming Distance to confirm that the images are of the same person. You can read more about the story here.",
Exploratory Data Analysis,Feature Engineering,Feature Engineering,"Feature engineering is the process of using domain knowledge to extract features from raw data. Algorithms need specific features in the model development process. Feature engineering will ensure your dataset is compatible with your algorithm, thereby improving model performance. So far, we have highlighted the specialized nature of feature engineering and that there is no one suitable solution. However, there are foundational concepts that are essential to your understanding of feature engineering.
Figure 1. Mapping Raw Data to ML Features. (Source: Google Developer Course)
Do you remember this concept from an earlier module? It is useful during the data wrangling process as you cleanse your data and is equally used in feature engineering. Missing values in a dataset can negatively affect the performance of a model. Missing values can be caused by simple human errors, and privacy concerns, among others. How can we fix the problem of missing values? A simple but problematic solution is dropping rows or columns. A preferable solution is an imputation. It would help if you considered a default value for missing values in a row or column. Let us visit how you handle this with numeric and categorical data.
Numerical Data Imputation. If you are not dropping rows and columns with missing data, the numerical imputation method will allow you to replace missing values intuitively. For example, a column with numbers and some with "" - "" or ""NA"" can be replaced with a ""0"". Other methods used include using the median or mean values of that variable.
Categorical Data imputation. In some cases, replacing missing values with a zero will not make sense to the dataset. You can replace values in a categorical column with the cmost frequently  occurring value,d and you can impute cotherd in a situation where there is no dominant value in the categorical column.
Similar to imputation, binning can be applied to numerical and categorical data. Binning makes a model more robust, but there is a trade-off between performance and overfitting. Binning categorical data will have less of a negative effect on model performance than when binning is performed on numerical data. It is also used to capture noisy data when you have values that have variance.
In the context of image processing, binning is the procedure of combining a cluster of pixels into a single pixel. As such, in 2x2 binning, an array of 4 pixels becomes a single larger pixel, reducing the overall number of pixels.
You learned about how to visualize your data to detect outliers in an earlier module. This method is less error-prone. You can use some statistical and visualization methods to detect and handle outliers, including computing the z-score, using percentiles, and visualizing the data distribution of your dataset. These techniques were discussed in the ""Exploratory Data Analysis"" module.
Log transform, also known as logarithm transform, is used to handle skewed data and make the distribution of data less skewed. It is widely used because of its ease of use, and it decreases the effect of outliers in a dataset. Log transform is not usually applied to values that are less than or equal to zero.
One hot encoding is a process by which categorical variables are converted into a form that could be provided to a machine learning algorithm to make accurate predictions. This technique replaces categorical variables with different Boolean variables that indicate whether or not a category of the variable is part of the observation. Those Boolean variables are called dummy variables.
One hot encoding is easy to implement; it will retain all information of the categorical variable. This method does not add information that can make a variable more predictive.
Assume that a categorical variable education with labels less than high school and high school. We can generate the Boolean variable chigh school,d which becomes one if the person has high school or 0 if the person has less than high school.
When you have a variable with multiple categories, one-hot encoding might increase the dimensionality of your data. The binary encoding method can be used to create a smaller number of variables without losing information.
When you have ordinal data that is useful to your analytic solution, you can transform those features using ordinal encoding. Here we convert string labels to integer values. If you have an ordinal variable with string values that are satisfied, dissatisfied, highly satisfied, highly dissatisfied, not applicable, and somewhat satisfied, ordinal feature encoding will map the values to a corresponding integer. As you can see in the table below, all values are not integers.
Highly Satisfied
1
Satisfied
2
Somewhat Satisfied
3
Not Applicable
4
Dissatisfied
5
Highly Dissatisfied
6
When you split features, the features become easier to bin, and this improves model performance. There are many ways of splitting features, and it depends on the variable. If your dataset contains the variable address, you might split the column by extracting the street address, city, state, and postal code. You run the risk of increasing dimensions; in this case, we employ techniques that assess the value of the extracted dimensions.
Some machine learning algorithms need to have scaled continuous features as model inputs. Scaling is not necessary for most algorithms, but it can make continuous features identical with respect to range.
There are instances that require the use of scaled data, including algorithms that use gradient descent
Neural Networks and Linear Regression are some of those examples. The data is scaled before being fed to the model. Algorithms like k-Nearest Neighbors, clustering analysis like k-means clustering, and other distance-based algorithms would need data that is scaled.
Quick thought: How about tree-based algorithms like decision trees and random forests? They are not affected as they are not distance-based.
Normalization. This technique involves values ranging between 0 and 1. Prior to normalization, all outliers in the dataset should be handled.
Standardization. Also known as z-score normalization, it is useful for feature engineering in logistic regression, artificial neural networks, and support vector machine tasks.
Reading: Data Preprocessing in scikit-learn.","Importance of Feature Engineering,Feature Engineering Techniques,Imputation,Binning,Handling Outliers,Log Transform,Categorical Encoding,One hot encoding,Binary Feature Encoding,Ordinal Feature Encoding,Feature Split,Scaling,An optimization algorithm used to minimize some functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."
Data Science Project Planning,Design and Plan Overview,Module 6 Summary,"It is necessary to establish a design for the project as it can help identify various bottlenecks of the project. This documentation makes the developers begin thinking about the various risks and challenges involved in the data science project. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, and the system environment. A clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams. The developers are also prompted to consider the many risks and difficulties that the data science project may present.
Every business makes sure the software development life cycle is maintained to start the project because it is very crucial. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. This module has introduced you to an efficient software development life cycle - Agile Scrum, with components  below:
User stories: A high-level definition of a work request.
Sprints: Short spans of work where teams work on tasks determined in the sprint planning meeting. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service.
Stand-up meetings:  These are daily stand-up meetings where participants are required to stay standing, helping to keep the meetings short and to the point.
Agile board: This is a tracking system that helps your team track the progress of your project.
Backlog: This is the set of outstanding project requests that are added through your intake system. Managing your backlog is a vital role for project managers in an Agile environment.
Sprint retrospective: This is a meeting where the team gathers to discuss what went well and what didn't.
Demonstration: This is when the team demonstrates a working product to the stakeholder.",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Activation Functions,"The nonlinearity introduced in neural networks is key to their learning capabilities and what allows these models to approximate more complex functions. Activation functions are responsible for performing the nonlinear transformation on the weighted sum of inputs received from the neurons in the previous layers. Depending on the magnitude of the continuous value generated by an activation function, the neuron can be considered as cactivatedd or cinhibited,d thus affecting the transformations in the subsequent layers.
Following are some common activation functions:
The sigmoid function transforms an input to a value between 0 and 1. Assuming the \\(s\\) represents the weighted sum of the inputs, the logistic function, which is an example of a sigmoid function, computes
\\[\\frac{1}{1+e^{-s}}\\]
It is especially useful when we want to think of the output in terms of probability. The sigmoid function has some disadvantages in being used in intermediate layers of a deep neural network and is hence mostly used in the output or the final layer. One of the disadvantages of the sigmoid function is that for very small or very large input values, the gradient approaches zero, which slows down the learning process.
ReLU stands for Rectified Linear Unit. It is a piecewise linear function that will output the input directly if it is positive. Otherwise, it will output zero. It has the advantage of being faster to compute than some of the other activation functions and does not get saturated at high input values as opposed to the logistic function, which saturates at 1. ReLU is a common default choice for activation functions. However, the ReLU activation function does have the disadvantage of dying out for negative values, as the ReLU function is 0 when the weighted sum of inputs is negative. This results in stalling of the training when the weights in the network always lead to a negative output.
Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training and hence is considered a hyperparameter. The small slope for negative values prevents the stalling problem encountered in the ReLU activation.
Tanh, the hyperbolic tangent function is a shifted version of the sigmoid function where its range is between -1 and 1. The mean of the activations that come out of the function is closer to having a zero mean therefore, data is more centered, which makes the learning for the next layer easier and faster. The tanh function shares the same disadvantage as that of the sigmoid, where the gradient becomes close to zero for very large and very small values, thus slowing down gradient descent.
The softmax activation function is used in neural networks when we want to interpret the outputs of a  multi-class classifier as a probability distribution. This makes it easier to assign an input to the one class with the highest probability when the number of possible classes is larger than two. The softmax ensures that the sum of outputs for each class is equal to 1 for a given input.",
Data Science Project Planning,Requirements Gathering,Writing a Requirements Document,"The requirements document should not only state all the hardware and software requirements of the project but also other requirements such as functional and non-functional requirements and the intended group of users that the project aims to target. The most significant sub-headings that must be covered in a requirements document have been detailed below.
The first section is the introduction which would either include an introduction to the project or to the upcoming sections in the document. If the project is more research-oriented, the introduction should summarize the experiments that will be conducted during the entire duration of the project. For industrial or software-oriented projects, it should describe the various interfaces that will be developed for the system. The document will also mention how testing will be done and how various modules will be run together. This would make developers think about the various sub-sections involved in the overall project. The intended users section will detail the various groups of users involved in the project and their corresponding use cases so that the requirements of each group of users can be determined beforehand. This would help develop a solution that fits all the requirements of all the various stakeholders.
The next section in the requirements document will be the system functionality section. This section describes the goals of the project using multiple simplified diagrams, such as context diagrams and component diagrams, which can be used to illustrate these goals.
While developing this document, the non-functional requirements, including various quality attributes of the system like speed, reliability, and performance, will be considered. This will help developers handpick the most important non-functional requirements for the system, which would be achievable within the time frame of the project. The use cases of the project must be kept in mind when selecting the non-functional requirements. Other constraints like hardware and software resources, monetary resources, and human effort must also be taken into account before committing to the functional and non-functional requirements of the project. The requirements document would be the preliminary attempt at defining the overall resource requirement of the project for meeting the goals of the entire project. Developers should begin thinking about a design methodology while preparing this document.",
Problem Identification and Solution Vision,Problem Identification,Translating Business Needs to Data Science Tasks,"Business needs are general necessities for a company to remain competitive or expand in the long term. They can typically be stated in broader terms, e.g., acquiring new customers, reducing production costs, improving the quality of its products, or increasing brand awareness. At the start of a data science project, the data science team will work with a client to understand the specific business need.
To meet a particular need, the company will identify one or more specific business objectives it can work towards. These objectives are stated fairly concretely and often have time periods associated with them, after which, somewhat simplified, they will be considered reached (in case of success) or not reached (in case of failure). For example, a business may want to increase sales of a particular product by a certain margin through online advertising, be smarter about scaling production to market fluctuations, reduce travel costs for its logistics, or get a better sense of current trends in its customer base for better product development.
Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them. However, the objectives in question will typically be framed in terms of business terminology and assumptions. A data science team will first engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem, as well as the available data and resources, it will work with the client to develop a solution vision. This vision is effectively a walkthrough of what a full data-driven solution to the problem could look like. Based on this, one can identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.
In this context, an analytic objective states, in specific terms, what the data science project should produce (insight, resource, model, etc.) and serves as the main success criterion for the team. On the technical side, it drives the derivation of requirements for the project. On the business side, it is critical that analytical objectives stay firmly connected to the business objectives they facilitate.",
Analytic Algorithms and Model Building,Model Selection,Introduction to Model Selection,"Machine learning involves formulating a hypothetical mapping from the input features to the output space. It is often the case that many different implementations of the mapping could work (for example, classification can be carried out by logistic regression, support vector machines, or k-nearest neighbors), but the best mapping depends on the underlying data distribution and available training data. Model selection is a systematic process of identifying this best mapping and builds upon the following concepts:
A model is a set of assumptions you make about your data, which in turn defines the hypothesis space over which learning performs its search
The model parameters are the numeric values or structures that are derived from the learning process.
The model hyperparameters are the numeric values or structures that impact the learning process but are not selected by the learning process.
The learning algorithm specifies the way in which model parameters are updated or derived from the input data.
With these definitions, model selection can be considered the process of identifying the learning algorithm, hyperparameters, and associated pre-processing and post-processing steps that yields the best-fitting model for your data. The table below shows an example of two candidate models for binary classification.
Component
Model 1
Model 2
Hyperparameters
Learning rate \\(\\alpha =0.1\\), regularizer \\(\\lambda =1\\), number of iterations \\(N = 1000\\)
Learning rate \\(\\alpha =0.5\\), regularizer \\(\\lambda =0.01\\), number of iterations \\(N = 100\\)
Learning algorithm
Gradient descent over the logistic loss function with L2 regularization
Gradient descent over the logistic loss function with L2 regularization
Pre-processing
None
Normalize the data to have zero mean and unit variance
Post-processing
None
None
Here both models involve using regularized logistic regression to perform classification, but have different hyperparameter and pre-processing components, which in turn reflect different assumptions about the underlying data. For example, Model 1s higher regularizer value corresponds to the assumption that the dataset may contain outliers which the model should not overfit to (recall that higher regularizer enforces lower variance at the cost of potentially higher bias). On the other hand, Model 2s pre-processing step is suitable for datasets where the feature values have different scales and need to be normalized prior to gradient descent. In what follows, we will introduce ways to compare a set of candidate models to select the best one; however, we should first discuss what cbestd means.
Prediction is the process of developing models from available data to predict outcomes from new and unseen data. Here the focus is on generalization, and predictive models are evaluated against data they have not been trained on, using the standard performance metrics (e.g., MSE for regression, F1 score for classification). An example prediction problem is that of predicting the number of hospital beds needed in the event of a surge in Ebola cases using historical data from past outbreaks. Prediction accuracy is important in this case because it can help inform resource allocation to hospitals in case a new Ebola outbreak takes place.
Inference is the process of identifying relationships between independent variables (input features) and dependent variables (outcome values). Here the focus is on interpretability. Inference models are evaluated on both their goodness of fit and simplicity. An example inference problem is inferring peoples political inclinations based on their demographic information. Model interpretability is important here because knowing which factors have the largest influence on political inclinations can help a politician strategize his/her campaign for an upcoming election.
Due to their differing priorities, the best prediction models are typically very different from the best inference models. Prediction models are fitted on only the training set, tend to be complex with many features, and have good validity but low interpretability. In contrast, inference models are fitted on the entire dataset, prioritize retaining only the most salient features, and have low validity but good interpretability. Another way of viewing this difference is via the focus on accurately predicting unseen data (prediction) or explaining the underlying data generation process (inference).
Optional Reading: Integrating explanation and prediction in computational social science",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Tree-based Methods,"Tree-based methods are considered to be among the simpler methods for prediction and classification. Trees can be built using both numerical and categorical variables, and the tree method is rated highly as an interpretable method. Certain data science practitioners and thought leaders favor the simplicity of tree-based models because they can be seen to mirror an ""If-Then"" statement and are easily digestible to an individual with a growing statistics knowledge.
We will explore the different tree-based methods starting with one of the most popular methods: Decision Trees. Using a very simple example, let us build a decision tree: Decision Trees: scikit-learn.
A decision tree consists of a root node, leaf nodes, and branches. In decision sciences, it is an effective visualization that is easy to interpret, in data mining and machine learning, it is used to model predictions. The end goal of a decision tree method is to predict the value of a target variable based on several predictors. When you have a decision tree model with an outcome response containing a categorical value, you have a Classification Tree. When your outcome or target variable is a continuous value, you have a Regression Tree.
Additional Reading: Decision Trees for Decision Making
Building a classification tree involves recursive partitioning and pruning. Both concepts are used to ensure the model has a low error rate and that overfitting is not an issue.
Recursive Partitioning creates a decision tree that splits its entire dataset into smaller sets to accurately classify records within the dataset. C4.5 is one of the popular algorithms that employ recursive partitioning. It generates models that have more sensitivity and tend to be more accurate. Partitioning is done by repeatedly splitting and creating subsets until the tree is pure; that is, observations belong to a single class. Recursive partitioning splits each node on the decision tree to create decision rules that are easily interpretable, but overfitting can be an issue.
Another technique for building decision trees is the Chi-square automatic interaction detection (CHAID). This is used for both classification and prediction and can be used to capture the interaction between variables. It is most useful when you have a large dataset. Let us assume that you have received a credit card offer from Capital One as a preselected customer. CHAID can help Capital One's marketing firm to predict how your age, income, and credit score will affect your response to the interest rate offered.
Measures of Impurity. You can measure impurity using entropy and the Gini index. The Gini index is useful in measuring the degree to which a variable can be misclassified when it is randomly chosen. It varies from 0 to 1. 0 indicates that all elements are members of a class, while  1 denotes that elements are distributed (randomly) across various classes. It is best practice to select the feature with the lowest Gini index as the root node. Entropy is a measure of uncertainty within a model. Decision trees will always seek to minimize entropy.
Reading: Gini Index and Impurity Measures
Pruning. If you have dabbled in horticulture, you will be familiar with the term pruning. You prune a plant so that it grows without obstacles, but you can also prune a plant to redirect the growth and shape of the plant. You can think about pruning decision trees in a similar light. It is one of the solutions to avoid overfitting the training dataset. Once you have a large decision tree, you will prune the weakest branches to reduce the complexity of your model and improve accuracy. Pruning can be done using two techniques.
Cost complexity pruning will generate a series of trees. The tree is created by removing a subtree and replacing it with a leaf node with a value chosen as in the tree-building algorithm. The best tree is chosen by generalized accuracy, measured by a training set or cross-validation.
Reduced error pruning is done by replacing each node with the node's most popular class, however that replacement is temporary unless it does not negatively affect the prediction accuracy. It is an efficient technique for pruning.
Application: Decision Trees and NLP: A Case Study in POS Tagging.
When a full tree is built, it will result in a fully grown decision tree that represents the maximum number of splits that the CART method will make to identify pure subsets. Full trees tend to overfit and do not do best at generalizing well to new cases. Solving this requires pruning the tree. The least complex tree with the smallest validation error is called a Minimum Error Tree. The least complex tree with a validation error that is within one standard error of the minimum error tree is called a Best Pruned Tree.
The validation dataset is used to optimize the complexity of a tree by pruning a grown tree into a simpler tree. After pruning, the tree will generalize new cases well. Misclassification rate is a performance measure for classification trees and is used to identify the tree that has the lowest error or the minimum error tree.
We already indicated that decision trees are more explainable than linear regression models. A smaller tree can easily be interpreted by someone who is not in the field, and trees can use qualitative variables without the need to create dummy variables. The impurity measure for a regression tree is the sum of the squared deviations from the mean of the terminal nodes. The predictive accuracy of CART models is not as robust as other methods. Regression tree performance is evaluated using the root mean square error (RMSE).
Random Forests, Bagging, and Boosting can be used to improve this prediction accuracy and performance. We will learn about those next.
Bagging reduces variance in a decision tree method. This is achieved by averaging a set of observations and directly applied by producing multiple training data sets from the entire dataset, then using those training datasets to build a model for each set, then averaging the results retrieved from each model. This is likely to produce a model with low variance. Bagging will reduce overfitting issues and works quite well with high-dimensionality data. Out-of-Bag Error Estimation measures the prediction error of models that use bagging. It is also used to validate models created using random forests. It is computed on data that was not used in the analysis of a model, unlike the validation metrics.
Additional Reading: History of Random Forest Algorithm
Random Forests. This is an extension of bagging and makes some changes to bagged trees. When there is overfitting with decision trees, random forests will remedy this issue. Similar to bagging, a random forest will perform well because it consists of a large number of decorrelated trees (the focus is on the low correlation between trees). A Random Forest will build several decision trees and then merge them for better accuracy and predictive value. It is used for classification and regression tasks and it searches for the best feature within a random subset of features in a dataset. Random forest method will also evaluate the importance of features and scale the results of this assessment to show the importance of features. This is useful for feature engineering as you can eliminate the features that do not contribute to your task without losing information. Random forests create random subsets of features and combine those subsets which prevent overfitting. The number of features to be included can be derived by calculating the square root of the number of predictors. The downside to the random forest method is that it can be computationally slow in making predictions (but not slow to train).
Boosting. Similar to bagging, boosting can be used to improve the predictive accuracy of certain methods including decision trees. It differs somewhat from bagging as the trees built with this model are dependent on a prior tree (each tree depends on or fits the residual of the trees that preceded it). Each tree is created iteratively and the output of each tree is assigned a weight that is relative to its accuracy. This ensures that the overall predictive accuracy estimate of that method is improved.
Overfitting can occur in boosting if the number of trees becomes too large. When you take your machine learning class, you will learn more about the techniques that are used in Boosting, including one of the most popular: Adaptive Boosting (AdaBoost). AdaBoost is used to improve the performance of models. It is sensitive to outlier data, but on the upside, it is considered the best out-of-the-box classifier when used with decision trees. This is because the information that is collected by the AdaBoost algorithm about the training data is then fed into the tree algorithm so that the model can accurately classify observations that would have otherwise been difficult to classify. AdaBoost will select features in the dataset that will improve the model's predictive power, which is helpful for reducing dimensionality and improving computation time.
Ensemble methods were represented as an extension of the tree method; take note that they are also used for other methods.
Reading: Ensemble Methods-General Use","Building Classification Trees,Building Regression Trees"
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Cloud Computing,"In data science, you will work with large amounts of data with a variety of tools in interpreted environments. The sheer amount of data in todays data science systems requires a lot of experience to manage the underlying hardware, and doing so efficiently and effectively is usually prohibitively expensive to do so. Additionally, managing the security, fault-tolerance, and data governance of the systems you are working on can be incredibly difficult to do from an IT level.
In situations like these, especially when you might not know the scale of the system you want to build, cloud computing can be a great boon to your efforts. By purchasing either Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS), you can choose to rely on another company to manage some aspect of the underlying hardware/software issues and focus on data science. The main benefits of cloud computing are the following:
You are able to trade the cost of buying all of the hardware for your projects and managing them, for whatever cost the cloud provider asks you to pay. In practice, these costs will be based on your usage rather than the hardware itself, which can allow for savings in the long run.
You do not need to worry about capacities as much. As the main cloud providers have large data systems in place, you do not need to worry about infrastructure capacity or about ensuring that systems cache as intended. Instead, you can make use of the servers that are present to accomplish your goals.
You can purchase these services for potentially less than what you might have thought possible. As you share the cloud with other users, you are able to pay less for more power which, along with specialized chips only available in some cloud instances, can make the cloud much cheaper for the processing power given than other systems.
For all of these reasons, the cloud is becoming the place to do data science in, especially for large and unstructured data. As a result, however, you will need to understand the cloud and use it accordingly to manage your data. In particular, you will need to pay attention to the types of storage and processing you are purchasing from the cloud. In general, there are many ways to use the cloud, but the most important factor to consider when using the cloud to host data is the structure/usage patterns of the underlying data.
For unstructured data that has high-data temperature, i.e., data that you need to read and write a lot, you should try to use a block-based storage system. Systems like these work like network hard drives, which can make them incredibly useful when you wish to perform large-scale processing tasks and data cleaning tasks.
For large files that are low-write but high-read, an object-based storage system might be more beneficial. Object-based systems store files as key-value pairs, allowing people with the write key and access permissions to download the files accordingly. They are best for Binary Large OBject files (BLOBs) or when you might want to distribute a data file to many people to allow them to process it on their own systems.
For structured data that you might need to write to constantly, you might want to consider a relational database system. These systems enforce a lot of constraints on the underlying data but, in doing so, ensure that each operation has Atomicity, Consistency, Isolation, and Durability properties. These systems use the SQL, or Structured Query Language, as a basis for every operation available and are especially important when you want consistency in your data.
Alternatively, if your data is not structured enough to fit in the tight schema requirements of a relational database, one can try a non-relational database system. These systems dont need to adhere to the requirements of an  SQL schema structure and thus to the consistency provided but gain added flexibility and scale. Such databases allow you to store a wider variety of data and scale it up to more computers much easier than a traditional database system.
Each of these systems will be useful for different use cases, and you will need to carefully consider your use case before deciding on what to use. Besides these hosting concerns, you will also need to consider the following carefully:
Your Budget: As cloud resources are based on what you use, it pays to monitor your resources carefully and constantly check your usage limits. Most cloud providers have methods to monitor your usage, which can help you both understand and automatically stop extraneous usage accordingly.
Your Security: While cloud providers maintain the security of the infrastructure for you, you still need to keep in mind good practices for the security of the software you are running. Checking if your systems are vulnerable to the latest Log4J or SQL injection attack is not your main priority as a data scientist, but knowing best practices and discussing the architecture with experts is a huge priority, especially if the data in question is confidential user data.",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Hypothesis Testing,"When evaluating model performance, it is easy to fall into the trap of thinking that a better score is strictly better for model performance. Even if you use tools like k-fold cross-validation to get estimates of your prediction error or loss function, it is still quite challenging to confirm that you have not learned some constant model or that these performance estimates are truly cdifferent enoughd for you to pick one model over another.
To understand why this might be the case, consider a case where you have a hundred features, each randomly chosen from the set {0,1}, with your label also being uniformly at random chosen from {0,1}. If you run k-means cross-fold validation, your prediction error will not be 0.5, but usually much lower. This is due to the fact that your dataset is just a sample of the entire set of features that the problem can have. No matter how you try to classify the elements of the dataset, a trivial but cgood enoughd classifier will suggest strong performance due to random associations between the features and the label, despite the fact that, in this case, there are no associations between the features and the label.
Thinking about this problem more carefully, it becomes clear that most of the measures we discuss in machine learning to look at model performance have built-in uncertainty that we need to utilize to ensure that our systems work when deployed. These uncertainties can cause situations where the best-performing model on a training set might not be the best-performing model on a test set, no matter how you check the performance metric in question.
In general, no matter how great your dataset has become after cleaning and post-processing, there will still be some associations that come about from the dataset itself, which you will be unable to correct for. As a result, when comparing different models and different hyper-parameters for the same model, it can pay to take a page from statistics and do a hypothesis test on your performance measures.
A hypothesis test is simply a statistical procedure by which you can test some assumption about some fact about the true distribution of your dataset, known as a population parameter, and decide if you have a statistically significant result.
💡 Before you continue, it is important to note that these tests can be easily misused if not carefully thought about and reasoned with. Take your time through this chapter, as it is important to think carefully about if this is the tool you need for the problem at hand.
Performing a hypothesis test involves three major steps:
Deciding what your Null Hypothesis, \\(H_{0}\\) and what your Alternative Hypothesis are, \\(H_{A}\\). This will depend on the test you perform, but in general, \\(H_{0}\\) refers to what you wish to cdisprove,d and \\(H_{A}\\) refers to what you wish to demonstrate as more possible than the null.
Computing some sort of ctest-statisticd. This is a measure of how unlikely the observed metric is, given the null hypothesis, and depends heavily on what distribution we assume the metric has in our problem.
Looking up the p-value for that test statistic, and comparing it to some pre-defined confidence cthresholdd, \\(\\alpha\\). This \\(\\alpha\\) is the minimum likelihood threshold for failing to reject the null hypothesis. If we are lower than \\(\\alpha\\), we can reject the null, and tentatively suggest the alternative is more possible.
💡 It is important to note that crejecting the nulld does NOT mean caccepting the alternatived. All we are saying here is that, given our assumptions of the distribution of the metric in question, it is unlikely for the null hypothesis to hold. As a result, as the alternative hypothesis is the negation of the null, it is more likely to hold.
When performing these tests, you will need to be incredibly careful in the language you use to frame the results. These are tools to demonstrate that certain hypotheses are unlikely given the assumptions and evidence; they are NOT iron-clad rules that infallibly demonstrate that some fact about your data or the universe must hold.
That said, these tests can allow you to tentatively separate models based on their metrics, and suggest when a model is likely to perform better than another in general. This makes them useful in fields like Automated Machine Learning and when you want to compare models a little more thoroughly.
Without further ado, lets discuss the first statistical test of this module and one of the forerunners of hypothesis testing: Welchs t-test.
Something we generally wish to do when we compare different metrics or other values about data or models are means or averages. For example, if you had two different average cross-fold validation metrics, it would be nice to know if that difference is statistically significant, i.e., is it likely to have happened due to random chance or not.
If we want to compare these means \\(\\mu_\\alpha\\) and \\(\\mu_{\\beta}\\) against each other, we first need to define some sort of null and alternative hypotheses. Here, we have two options.
We could try to test if they are just different from each other with the following hypotheses:
\\(H_{0}: \\mu _{\\alpha}=\\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}\eq \\mu _{\\beta}\\)
Or we could test that one is strictly larger than the other:
\\(H_{0}: \\mu _{\\alpha}< \\mu _{\\beta}\\)
\\(H_{A}: \\mu _{\\alpha}> \\mu _{\\beta}\\)
The first kind of test is known as a ctwo-tailed t-test,d while the second is known as a cone-tailed t-test.d Either way, well end up following the same procedure, so well continue onwards with our next goal: figuring out what sort of test statistics we wish to compute. Generally, these test statistics come with their own particular distribution, from which we can calculate a cp-value,d or the probability that such a test statistic can happen given the null hypothesis.
In our case, we have two averages and want to look at their differences. For the students t-test, we shall use the aptly named ct-test statisticd:
\\[ T = \\frac{\\mu_\\alpha - \\mu_\\beta}{\\sqrt{\\frac{\\sigma^2_\\alpha}{n_\\alpha} + \\frac{\\sigma^2_\\beta}{n_\\beta}}} \\]
In particular, we are going to use what is known as cWelchs t-test statistic,d which is used when we have two averages with potentially different variances. This \\(T\\) value is distributed according to the t-distribution, which is essentially a more conservative estimate of the normal distribution, which is better when we have fewer degrees of freedom, i.e., approximately fewer samples. To calculate the degrees of freedom, we simply need to compute the following:
\\[ \u = \\frac{(\\sigma^2_\\alpha + \\sigma^2_\\beta)^2}{\\frac{\\sigma^4_\\alpha}{n^2_\\alpha(n_\\alpha-1)} + \\frac{\\sigma^4_\\beta}{n^2_\\beta(n_\\beta-1)}} \\]
With these values, we can then compute the p-value or the probability that our null hypothesis holds, given our parameters. If this p-value is less than some predefined value, then they are different, and we can be more confident that we have different results.
While this test is not the simplest test, it does give us our first method of comparing different model performances. If we have enough data, we can create multiple sets of test and training datasets and try this test on two models to see if they have differing performances.
However, there are some problems with this testing procedure as is. Firstly, we do make some key assertions about the distribution of our metrics, namely that they follow a t-distribution. Given that accuracy metrics might not necessarily be normally distributed, we will want tests that assume less when our models get better.
Additionally, we cannot use the test as is without heavily segmenting the dataset. If we do not have enough data or wish to apply something more sensible than simply splitting the dataset three ways and applying a k-fold CV to each section, we will need to account for that.
To help combat some of the issues with Welchs t-test, we can use the McNemar test instead. This test compares the error of two different models and determines if those errors are strictly the same or strictly different. Here, we let the error be simply \\(1-\ext{accuracy}\\).
If we let the error of model \\(\\alpha\\) be \\(E_\\alpha\\) and the error of model \\(\\beta\\) be \\(E_\\beta\\), then our associated hypotheses are:
\\(H_{0}: E _{\\alpha}=E _{\\beta}\\)
\\(H_{A}: E _{\\alpha}\eq E _{\\beta}\\)
For this test, our test metric is actually much simpler:
\\[ \\frac{(|E_\\alpha - E_\\beta|-1)^2}{E_\\alpha + E_\\beta} \\]
💡 In fact, this is the corrected McNemar Test, which helps when we are comparing high-accuracy measures.
Instead of following a t-distribution, this metric instead follows a Chi-Squared distribution with one degree of freedom. If you have at least 25 misclassified examples, this test is suitable for your data.
While there is no end-all-be-all hypothesis test in this space, it is worth mentioning that this test assumes far less about accuracy than the standard Welchs t-test. Namely, it just assumes the samples you have tested are independent or that no datums feature-label pairing depends on another datums feature-label pairing.
On the other hand, this test does only work for accuracy values. When you are trying to compare other loss metrics, you need to use Welchs or another paired t-test variety.
However, both of these tests do suffer a single, incredibly critical flaw.
Namely, they suffer from cp-hacking.d
From our introduction to hypothesis testing, remember that the p-value is simply the probability that our null hypothesis implies the result we have. Due to this definition, we run into problems when we try to take paired tests, which look at pairs of models or pairs of means and expand them to handle more than two models at a time.
One way to see this is to think of flipping a heavily weighted coin, where one side of the coin comes up \\(\\alpha\\) percent of the time and the other side comes up \\(100 - \\alpha\\) percent of the time. In this situation, even if we have a really, really low \\(\\alpha\\), comparing multiple metrics on the same data could result in some null hypothesis being rejected when, in all likelihood, the null hypothesis holds.
Such a result would lead to a false comparison, where we find a statistically significant conclusion, not due to our data analysis skills but simply due to flipping the coin enough times. In research, this has led to situations where published research had a result that came from finding a singular interesting conclusion after sifting through a number of conclusions that did not pan out.
For our problem, this is especially grave. Consider that, for n models, we would want to perform \\(n \\choose 2\\) comparisons. As \\({n \\choose{2}} \\approx n^2\\), the chances of having a poor comparison skyrocket as the number of models increases.
Given this problem, then, the question is, how are we going to correct it and thus ensure that our models are statistically significantly different from each other?
To correct this issue, we must introduce the concept of the Friedman test. If we have n data sets to compare with and algorithms to compare, we first define the concept of a crelative rankd between algorithms as the order in which the algorithms are ranked on a singular dataset. For example, if on the first dataset, a simple linear classifier gets first on our loss metric, it would have a rank of 1 on that dataset.
💡 If there are ties, you will need to change the rank slightly to compensate. If you are interested, feel free to look around for one of the many ways to handle this case.
With this, we can then define the Friedman test in terms of the average rank of the \\(i^{th}\\) algorithm, \\(r_{i}\\), among all datasets.
The hypotheses are the following:
\\(H_{0}: r_{1}=r_{i}=...=r_{k}\\)
\\(H_{1}:\\) They are not all equal.
and the associated statistic is:
\\[ \\frac{12n}{k(k+1)} \\sum_{i}^{k}(r_i - \\frac{k+1}{2})^2 \\]
For this particular test, if you have at least 15 datasets or at least 4 algorithms, you can quite easily use a Chi-Squared distribution to check statistical significance. If you have neither of these cases, you will need to use a table specific to the Friedman test to get the p-value.
With this test, the main problem comes from what happens after you reject the null. The test itself simply states that cthere is likely some difference between the ranks of each algorithmd. Thus, if you want to then pick the best algorithm out of the lot, you will need to do what is called a cpost hoc testd to find the best-performing algorithm, assuming the Friedman tests null hypothesis was successfully rejected.
There are a wide variety of these tests and many ways to display them. As calculating them can be relatively intensive, we will simply note that there are two types of post hoc tests:
Tests that perform all pairwise comparisons: Here, we compare all algorithms with each other, and determine which algorithms are better than each other. These tests work better than simply applying the paired-test, but still suffer from many comparisons.
Tests that compare with a baseline: When you are working on a challenge or on improving a model, typically you can look at it instead as a problem of cwhich of the models Ive tested are better than the baselined? These tests determine this, with the added benefit of only a linear number of comparisons on the number of algorithms used.
Overall, statistical tests like these help us make more sense of it, while accounting for some of the problems associated with multiple-comparisons testing. While they are computationally expensive and relatively difficult to run, they are key to having model evaluation strategies that make sense, and in making better sense of the training and tuning process for ML models.
According to Dermatology Associates, hyper-pigmentation is the number one skin health concern for Black females ages 18-45. Skincare Co. is one of the leading manufacturers of skin care products. Skincare Co. is looking to develop a 120-day skincare line to target this population and this skin health concern. You are the data scientist assigned to the project investigating the use of the ingredient hydroquinone in the product for the treatment of hyperpigmentation. Your preliminary research has found that administering hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will lead to permanent skin damage. This is different from claims that have been made about this ingredient (previous claims state that there will be no damage). This claim or belief has been formulated, and it should be tested with evidence that refutes or proves that it is true. You can use hypothesis testing to provide this evidence. To construct a hypothesis test:
Identify the population parameter of interest.
Determine whether you will be conducting a one-tailed or two-tailed test.
Define a null hypothesis, often denoted as \\(H_{0}\\). The null hypothesis is considered the status quo or, in the case of our example: The administration of hydroquinone on the skin of black females ages 18-45 for more than 90 days at a time will not lead to permanent skin damage.
...then define an alternative hypothesis, denoted as \\(H_{A}\\). This would be the opposite of the null hypothesis.
The example above does not cover the entirety of identifying your null and alternative hypotheses. You must know that if proven, your alternative hypothesis is a call to action, i.e., if you reject your null hypothesis, then the status quo has been changed, and the decision-makers must take action. How do we test our hypothesis statistically?
Let us also keep in mind that these tests are not error-proof! You want to be sure that you do not accept the null hypothesis when the null hypothesis should be rejected and reject the alternative hypothesis when it should be accepted. To avoid this, we consider the two error types in hypothesis testing.
Type I error occurs when you reject the null hypothesis when it should be accepted.
Type II error occurs when you accept the null hypothesis (or fail to reject the null hypothesis) when it should be rejected.
Considering our skin care manufacturer example above. A Type I error would mean that the company does not include this ingredient in their skincare line when they should have been able to do so. The company stands to lose customers to companies with products that include this ingredient that is effective in treating this condition with no side effects. The consequences of committing a Type II error would mean that the company includes hydroquinone in their new skincare line targeted towards hyper-pigmentation when they should not have done so. The cost of this error would mean producing a skin-damaging treatment product that would lead to loss of customers and possible lawsuits.
As you may have wondered by now, what the p-value is and its role in hypothesis testing. The p-value is a term you often encounter in hypothesis testing. The p-value of a test is the smallest \\(\\alpha_z\\) value at which the test would reject the null hypothesis. The smaller the p-value, the greater the evidence against the null hypothesis.
Consider the example where you are calculating the p-Value for a test statistic with z-score = -2.878. Assuming \\(\\alpha_z\\) = 0.05, should you reject or accept the null hypothesis? (consider a two-tailed test)
Here, given a z-score of -2.878, we can calculate the p-value as,
p-value = \\(2\imes P(z< -2.878)\\)
💡 Note: Since were conducting a two-tailed test, we can then multiply this value by 2.
If you locate -2.878 in a z-score table, you get a value of 0.002.
p-value = \\(2\imes 0.002\\)
p-value = 0.004
So, we have our p-value <  \\(\\alpha_z\\)
Hence, we can conclude that we should reject the null hypothesis as a p-value less than 0.05 is typically considered to be statistically significant.","McNemar Test,P-Hacking"
Data Science Project Planning,Design and Plan Overview,Overview,"Following the requirements document, it is vital to develop a design for the project. This is the most important documentation of the project as it provides not only a low-level design of the system but also dives deep into the implementation details of the system. Some key design considerations that developers should address include assumptions that they will be making throughout the project, the various constraints involved, the system environment, and the design methodology.
The data science project's design document should explain the entire system architecture of both the low-level and high-level components. A system architecture diagram can significantly simplify the explanation of the solution's architecture.
Figure 1. Overview of ACAI Architecture (MCDS Capstone Project, 2020)
While developing this architecture, the team can identify various bottlenecks of the project. Developers should be aware of the data used in the project and the various transformations that the data would go through. Thus a clear understanding of the data flow and design must be developed. This can be done by using various flow diagrams. Based on relevance, a number of diagrams like domain model, component, interface design, activity, entity, class, and sequence diagrams can be modeled in order to obtain an overall understanding of the design considerations that need to be made and to describe an overview of the implementation of the project. Context diagrams, problem diagrams, and frame diagrams can be used to outline the scope of the project. The dependencies in a project can be depicted via entity-relationship diagrams. Dataflow diagrams can be used to explain the flow of information from one module to another. Activity and sequence diagrams explain the interaction between systems or modules. Unlike these diagrams, use case diagrams document the user interactions with the system. State machine diagrams depict the system behaviors for various events.
Figure 2. Use Case Diagram (Source: https://venngage.com/blog/use-case-diagram/)
Apart from employing diagrams, it is also a good practice to make a list of tools and dependencies along with the suitable versions that the project may require.
This documentation also helps the developers think about various risks and challenges involved in the data science project. These risks could be domain, technical or business-related risks that are a part of the solution proposed for this project.",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Data Wrangling,"The quality of your data has a direct effect on the decisions made long after the models are developed. When data is gathered, it can present quality issues ranging from missing values to inconsistent formats. Data architects and engineers within organizations must clean the data gathered from internal and external sources to ensure that it is usable. Data that is collected from different sources are considered raw data. Raw data should be studied before it is used in an enterprise. Data is used for immediate analysis and model development with the goal of producing automated results or strategic decision-making. Data will move through different stages to ensure continuous use.
At this stage of the data science lifecycle, we are considering data in its raw form. One should also view all data (whether cleaned from its source) as raw data. We want to know how to enrich data to understand it further. Once you have completed this module, you will be able to discuss the techniques used to enrich data through a process called Data Wrangling.
Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. As mentioned earlier, data wrangling is also a best practice for an organization with a good data management framework. The data architects, engineers, and/or administrators will store data that has been processed to allow for enterprise-wide access and usage.
Data wrangling is a time-consuming process. As a data science team considers all data that has been extracted as raw data, the data wrangling process can assign value to a dataset after the data has been cleaned and transformed. Data wrangling is also part of the data understanding phase of the data science lifecycle and successful data understanding requires a clear understanding of the business and defining the business and analytic objectives and requirements for the analytic solution.
Despite its importance, data wrangling presents some challenges that are common in data science projects.
So far, you might have interacted with datasets from sources such as Kaggle, KDNuggets, or other avenues with ccleanedd datasets. You might also be collecting data from social media using built-in data-gathering tools to generate CSV files. You must consider these datasets as raw data. It is best practice to study the data to determine its quality.
Consider an organization that collects or purchases customer data from a marketing firm. The data from the marketing firm can be sent to the organization by a simple file transfer or through more automated sharing processes. The file from the marketing firm will contain formatted data that fits their data architecture and must be structured to fit the recipient organization's architecture.
A quick search about the data wrangling process will produce multiple definitions and perspectives. You might find that data wrangling is sometimes referred to as feature engineering. In this course, we separate both processes. When you perform data wrangling, you are essentially concerned with cleaning your data. Feature engineering will involve domain knowledge of the data and involves selecting the right features from the data to further improve the performance of your models.
The scikit-learn preprocessing package is widely used by data scientists and analysts for transforming and enriching data. Pandas, Numpy, Matplotlib, and Theano are other libraries in Python that support data cleaning and transformation.
Once data has been gathered, you will inspect it to assess its quality. You can inspect data using basic sorting techniques as well as creating visuals. Using visuals such as box plots to identify outliers in your dataset, sorting techniques will expose missing values and show the range of values in the different variables. Once data inspection is completed, you are ready to begin the preparation process.
Rather small or large observation within your dataset compared to other values in the dataset is called an outlier. Outliers will affect the performance of your model and, prior to getting to that point, your exploratory data analysis. When you have a large dataset, the outliers are not as noticeable as when you have a smaller dataset. Similar to missing values, you must handle outliers when you identify them in your dataset. You should refrain from removing them from the dataset until a proper investigation is completed. You can chandled outliers by following these steps:
Construct a Box plot or, as it is sometimes called, a box and whisker plot. This chart is used to graph the five-number summary. The five-number summary is then used to identify an outlier in your dataset. A five-number summary consists of five values: the maximum and minimum values in your dataset, the lower and upper quartiles, and the median. These values are then ordered in ascending order and plotted.
The box of a box plot contains the central 50% of the distribution, from the lower quartile to the upper quartile. The median is marked by a line drawn within the box. The lines extending from the box are called whiskers. They extend to the maximum and minimum, except for outliers, which are marked separately.
Box plots provide a visual summary of the data so that data scientists and analysts can identify outliers and other summary statistics, including the skewness and mean.
In box plots, the whiskers extend to the smallest and largest observations only if those values are not outliers; that is if they are no more than 1.5 IQR beyond the quartiles. Otherwise, the whiskers extend to the most extreme observations within 1.5 IQR, and the outliers are marked separately.
Why highlight outliers? It can be informative to investigate them. Was the observation perhaps incorrectly recorded? Was that subject fundamentally different from the others in some way? Often it makes sense to repeat a statistical analysis without an outlier to make sure the conclusions are not overly sensitive to a single observation. Another reason to show outliers separately in a box plot is that they do not provide much information about the shape of the distribution, especially for large data sets.
In practice, the 1.5 IQR criterion for an outlier is somewhat arbitrary. It is better to regard an observation satisfying this criterion as a potential outlier rather than a definite outlier. When a distribution has a long right tail, some observations may fall more than 1.5 IQR above the upper quartile even if they are not separated far from the bulk of the data.
Another way to measure position is by the number of standard deviations that a point falls from the mean. The number of standard deviations that an observation falls from the mean is called its z-score.
\\(z = (x-\\mu )/\\sigma\\)
The z-score of an observation \\(x\\) is a measure of the relative position of that observation within a dataset. You calculate the z-score by subtracting the mean from the value and dividing the result by the standard deviation. By the Empirical Rule, for a bell-shaped distribution, it is very unusual for an observation to fall more than three standard deviations from the mean. An alternative criterion regards an observation as an outlier if it has a z-score larger than 3 in absolute value. If an observation has a z-score that is more than 3 or less than -3, it is an outlier!
The data gathering process looks different for each data-related project and depends on your business and analytic objectives and your data source(s). The data you acquire during the gathering process will almost always need to be transformed into a usable format to meet the requirements of a data science task
One of the most common data quality issues is missing values in your dataset. This can happen due to human error or system issues during data collection. As you inspect your data and identify missing values, it is important to determine why the dataset has missing values. One should also be aware that a dataset that was extracted from an external source might not provide context on the reason behind the missing values. Even in those cases, a data scientist or data analyst should still investigate the missing values. The reasons behind the missing values will determine the techniques used to handle those values.
In statistics, missing data are classified into three categories. Those categories explain the likelihood of missing data.
Missing completely at random (MCAR) implies that missing data is not related to the data. The probability of data being missing is the same for all observations.
Missing at random (MAR) is the probability that the missing data is the same within certain groups.
Not missing at random (NMAR) means that the probability of data being missing varies for reasons that are unknown.
The common strategies that are employed in handling missing values are imputation and omission. Imputation replaces missing values in the dataset with other values. The replacement values are not random. One can replace missing values with the mean value. For example, if you have missing values in the age variable, you can replace the missing values with the mean age across all observations. This method will work if the group is homogeneous. But our dataset may not always contain homogeneous groups. In such cases, you will need to resort to other imputation techniques that we will discuss in the feature engineering unit. Those techniques include hot and cold deck imputation, regression imputation, and interpolation and extrapolation.
Omission is often the go-to technique when there are missing values. Omission involves excluding the missing values from the dataset. Remember, you will suffer a loss of data if you exclude values instead of finding other missing value-handling techniques. Omission can be done when the number of missing values is small.
Pairwise deletion is a type of omission. This means your analysis will be performed on just the available values, which is a smaller sample size.
Listwise deletion removes all data for an observation that has one or more missing values. This would mean your dataset would have observations with values for all variables.
You can also omit variables with missing values. Such variables need to be ones with little to no importance to your dataset and overall objective. For example, if we are predicting social media usage habits, and our dataset includes a shoe size variable with a missing value, we can likely remove that variable and its values from the dataset.
Subsetting. This process involves extracting portions of a dataset that are relevant to your model or analysis and is used in data wrangling to prepare data for exploratory data analysis. This technique can be used to remove observations with missing values. Subsetting can also involve excluding variables instead of observations. An example is looking at summary measures of three subsets of medical records for diabetes treatments where one subset is for successful treatments, another is for unsuccessful treatments, and the last is for inconclusive treatments.
When we discussed inspecting the data, there was mention of visualizing the data to identify outliers. Outliers are unusual values in the dataset. The value is unusual because it clies at an abnormal distance from other values in your dataset.d We will discuss using exploratory data analysis techniques to identify outliers in a future unit. In general, you should not immediately remove outlier values as they oftentimes can contribute valuable insights to your solution. Investigating the reason behind the outlier value is the first step in handling it.
As you learned previously, there are different types of data, and those types of data have specific data transformation techniques that accommodate them.
Categorical data is divided into groups or nominal categories based on a qualitative characteristic. Gender, race, and eye color might be variables in a dataset that is useful in predicting a health challenge. Usually, for processing purposes, such data may need to be transformed into a quantitative format. The following are techniques that are employed to transform categorical variables.
Category Reduction. Categorical variables can have many categories or levels. A variable with levels that are not useful can negatively affect your analysis and model. Some categorical variables will have levels that do not occur. It will be difficult to capture the interactions within those levels. A technique to handle these variables can include collapsing some of the categories or creating an ""other"" category for the categories with few occurrences.
Creating Category Scores. Ordinal data may need to be transformed into quantitative values for certain statistical techniques. Ranked values are an example. A dataset containing student evaluations would have responses that are ranked by different levels. One can transform that data by assuming equal increments between category scores. Responses to the question: cThe instructor provided out-of-class support for the coursed could be one of Always, Most Times, Sometimes, Hardly, Never. One can assign a score of 1-5, 1 being the highest and 5 being the lowest, or vice versa. The categorical variable can now be captured using quantitative values.
Creating Dummy Variables. Dummy variables are often referred to as binary variables. This technique allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary variables. Please note that there is no order or ranking.
Creating Dummy Variables for more than one category. What happens when you have a categorical variable containing more than one category? Consider a dataset with the variable hair color with data represented as brown, brunette, black, gray, and blonde. The hair color variable can still be transformed into dummy variables using the following steps:
For a variable with \\(k>2\\) categories, one will create \\(k-1\\) dummy variables. So for the example above, we will need 4 dummy variables. Lets call them black, brown, brunette, and gray. 4 is the number of categories of the variable. You will create 4 dummy variables (5-1).
One can now assign 0 or 1 to each category: for example, the black variable would get a value of 0 if the observation does not have black hair and 1 if the observation has black hair.
Keep in mind that the category that was not included in the creation of dummy variables still exists in the dataset. In this example, a dummy variable for blonde was not created. This simply means that all other categories will be compared to this category. Usually, you select the category with the most frequent occurrence as the category that will not transform into a dummy variable.
Categorical data is transformed into quantitative data so the data can be used for specific statistical techniques. Why would one need to transform quantitative data? If you remember, when data is gathered, it is usually noisy with missing values and sometimes needs to be converted to a structure that fits the data science task. This will ensure that you do not lose data or lose information during the analysis phase. One will also encounter quantitative data that needs to be transformed to allow one to glean insights and be usable with appropriate statistical techniques.
An exampleof a popular quantitative transformation is converting the date of birth to age.
Quantitative transformations are also useful when performing feature engineering. One will extract features from the quantitative data and transform them into formats that can be used by a machine learning model. These techniques will be explored in depth later but right now, let us take a look at the techniques for converting quantitative data during data wrangling.
Binning transforms a quantitative variable into a categorical variable. For example, values for age can be grouped into intervals; that is, one can create the following groups: 15-19, 20-24, 25-29, and 30-34, thereby reducing redundancy in the dataset and making it easier to capture outliers. Binning can also be done using unequal intervals.
Using Mathematics. One can create new variables using mathematical transformations on existing variables. For example, you can use techniques such as standardization, min-max scaling, and logarithmic transformation. We explore these mathematical transformation techniques in a future unit.
Data integration involves ingesting, transforming, and integrating the transformed data for access. The data is integrated to allow for analytic solution development, i.e., modeling and analysis. A popular example is integrating data into a data warehouse so that OLAP (Online Analytical Processing) servers, DSS (Decision Support) Systems, and other enterprise-wide analytic tools can access the data. The data can also be moved to data marts within the data architecture so that other parts of the enterprise can access the data. When there is a data warehouse, data integration can be done with the assistance of an ETL (extract, transform, and load) mechanism.
Once you have enriched and integrated your data, you are ready to explore it and perform feature engineering visually. You might find that feature engineering is an extension of the transformation process done during data wrangling.
In the next module, we will be taking an in-depth look into data exploration techniques. This is typically referred to as Exploratory Data Analysis (EDA). EDA is used somewhat analogous to descriptive analysis in statistics. The results of an EDA exercise can give insights into the project. This is why it is important to begin the data understanding process with wrangling. At this point in the data science lifecycle, data has been preprocessed for use during the EDA process and beyond. Remember, data wrangling is not just for analytic solutions.
The extent of the data understanding phase shows that data quality can truly make or break an analytic solution. The data wrangling process has now informed the data science team on the state of their dataset and might signal a need to source new data. If new data is sourced, then the data wrangling process is repeated in an iterative fashion.","Outliers,The z-score,Handling Missing Values,Imputation,Omission,Outliers,Transforming Categorical Data,Transforming Quantitative Data,Data Wrangling to Data Exploration"
Collecting and Understanding Data,Sparse Matrix,Module 10 Summary,"A matrix is a data structure that encodes the relationship between rows and columns. The disadvantage of this format is that matrices can be very sparse in certain domains. Sparsity refers to the fact that the majority of entries are unknown or missing. Sparsity leads to a waste of space and computational resources.
Equivalent representations of A as a sparse matrix instead aim to store only the non-zero (non-empty) values and operate on them.
The sparse format is suitable for storing not only the input data but also model parameters in certain domains. For example, in computational biology, we typically need to build predictive models (e.g., linear regression, logistic regression) over a large number of features. We can opt to store the model weights in a sparse vector/matrix format to optimize training and inference.",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Quiz 9,,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Quiz 5,,
Data Science Project Planning,Design and Plan Overview,Design Considerations,"Before we talk deeper about diagrams, it is important to outline what factors will affect your design, i.e., what you have to keep in mind when designing your solution.
For this section, it will be useful to go back to your requirements document. Most constraints will be apparent from the Non-Functional Requirements (NFRs) and Scope sections, but other sections like Intended Users may also bring up some interesting constraints.
Some examples are:
If your system is a prototype or a proof of concept, you may want to include that assumption in this section. Accordingly, in your design, the deployment model may not be really important as it is a research project. You can have a cbest-cased deployment model, i.e., a model that indicates the best way to deploy a full-scale version of the idea.
Information about the intended users of the system (e.g., working professionals or consumers) may affect the type of information your system has to expose. In your design, this will translate into specific outlets of information from your model.
Related to the above point, if your system deals with sensitive consumer information, your design will have to be careful of which outlets are exposed to the external world and what information is part of that outlet.
If performance is an important NFR, then a good assumption to have is the response time for your system (e.g., the requirement may indicate that an API call has to respond back within no more than 2 ms).
If the client has specifically asked for the system to be deployed on AWS/Azure/GCP or wants a specific platform to be used for the solution, one should make sure to include this requirement here. This will affect your design, as these platforms have their own constraints, which you may want to consider in your design.",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Module 11 Summary,"Statistics provides methods for planning how to gather data for research studies, summarizing the data, and Making predictions based on the data. Data is typically categorized as numeric or categorical. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.
Structured Data has organized facts that are presented in fixed formats and are easy to extract. Unstructured Data does not neatly fit in the row, and column structure or cannot be maintained in formats that are uniform.
Unlike structured data, unstructured data can be stored without a predefined schema. New-generation database frameworks, also known as NoSQL databases, have been developed specifically to handle this type of data.
Internal data is data collected and/or controlled by an organization, and external data is data that is collected from sources outside of an organization.
Data Wrangling is the process of cleaning, formatting, and enriching raw data to make it usable for analysis. Data wrangling is sometimes referred to as feature engineering. Feature engineering involves selecting the right features from the data to further improve the performance of your models.
Dummy variables are a technique that allows for categorical data to be transformed into 0s and 1s. A dataset containing customer spending data can have a categorical variable, gender, with two categories, male and female. The gender variable can be converted to binary/dummy variables.
The purpose of descriptive statistics is to make it easier to assimilate information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets. EDA uses non-graphical techniques and graphical techniques to explore the data.
Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight-line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables.",
Problem Identification and Solution Vision,Data Science Lifecycle,Roles in the Data Science Lifecycle,"As we have seen, the data science process involves multiple steps that require the expertise of team members in defined roles. The data science team is the talent that will help develop the analytical solutions that meet the business objectives of a data-related problem. Certain organizations can provide the structure and support for the different responsibilities within the process. In smaller organizations, personnel in the data science process might wear multiple hats to ensure the efficient execution of tasks and the development of solutions. Below, you will find the roles of a typical data science team. This list of roles will vary depending on the domain and size of the organization.
Data Scientist. This role involves solving business tasks using machine learning model development and statistical techniques. This individual identifies trends and patterns within the data and makes predictions based on trends. The data scientist will write code to support the data analysis and model-building process.
Data Engineer. The Data Engineer specializes in data structures and algorithms, as well as in working with data in databases and other large repositories.
Solutions Architect. This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.
Machine Learning (ML) Engineer. The ML engineer performs modeling tasks that are different than the tasks the data scientist performs in that the ML Engineer is further away from the domain side of the project. The ML Engineer spends a considerable amount of time programming and creating ML solutions but also needs to have strong statistical skills.
Data/Business Analyst. A data analyst has data gathering, analysis, and visualization skills. Like the data scientist, she provides insights from data to inform decision-making. She develops key performance indicators and utilizes business intelligence and analytics tools. Compared to data scientists, however, data/business analysts are typically firmly rooted in the business domain and are not necessarily as proficient in programming and advanced machine learning.
Software Engineer. The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.
Domain Experts. Also known as subject matter experts, domain experts are the actors who know the most about the problem on the business side. Their role is to define the framework for the data science project, and hence they are a key participant in the process. A domain expert will translate business needs and characteristics to the data scientists and eventually judge the solution as successful or not by assessing whether the business objective has been achieved or not.",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Language Models,"While methods like Bag-of-words and term frequency are simple yet highly effective techniques, they dont take the context of relative positivity between words into consideration. For example, cgood food and terrible serviced and cterrible food and good serviced mean completely different things, although frequency-based methods would model them to be the same. Language models take into account this additional relationship between words that helps represent language data more accurately.
Probabilistic language models are a category of language models that are constructed by calculating n-gram probabilities (an n-gram being an n-word sequence, n being an integer greater than 0). An n-grams probability is the conditional probability that the n-grams last word follows the particular n-1 gram (leaving out the last word). For instance, the Bi-gram (that is, n=2) model for the phrase cgood food and terrible serviced would require modeling conditional probabilities of every two consecutive words. With n=2, each word is modeled with one preceding word, like, P(word = cfoodd/dgoodd), P(word = cserviced/dterribled).
\\[ P(W_{n}|W_{n-1})=\\frac{P(W_{n-1},W_{n})}{P(W_{n-1})} \\]
Where the probability \\(P()\\) of a token \\(W_{n}\\) given the preceding token \\(W_{n-1}\\) is equal to the probability of their bigram \\(P(W_{n-1},W_{n})\\), divided by the probability of the preceding token.
Given a sentence in a language, a language model will use these probabilities to assign an overall probability to the sentence, which can be interpreted as a useful measure of  the plausibility of that sentence in the language (but not necessarily of grammaticality.)  For example, the sentences cBig blue skies look appealing.d and cColorless green ideas sleep furiously.d have the same grammatical structure, but to a speaker of the language, the first is a much more plausible sentence than the second one  a sentence she can say someone could use.
Such probabilities can be estimated from large-scale text corpora using maximum-likelihood estimation. Various smoothing methods are used to estimate probabilities for n-grams that have not been observed in the training data.  Such language models are useful in many language processing tasks, such as contextual spelling correction, part-of-speech tagging, etc.
These days people build (classical) language models using well-established toolkits:
SRILM Toolkit: https://www.sri.com/engage/products-solutions/sri-language-modeling-toolkit
CMU Statistical Language Modeling Toolkit: http://www.cs.cmu.edu/~dorcas/toolkit_documentation.html
KenLM Language Model Toolkit: https://kheafield.com/code/kenlm/
Each toolkit provides executables and/or API and options to build, smooth, evaluate and use language models.",
Collecting and Understanding Data,Data Structures and Algorithms,Data Structures,"When we design algorithms, we need to represent data items based on what is expected of them in terms of functionality in a formal abstract or mathematical sense. For example, we may need to represent our data as a set as in mathematics because the operations we will involve them in are things we normally do with sets:
inserting an element into a set
deleting an element from a set
intersecting, unionizing two sets
subtracting a set from another set
testing whether a set is equal to another one or is a subset of another one
testing if a set contains a certain element
Note that we have intentionally not said anything about what kind of an element a set contains. In fact, sets do not really care what types of elements are stored in them, except that they expect at least that you can test if two such elements are equal or not. In a way, we have abstracted sets from what is contained in them.
Modern computers provide standard representations for data such as integers, and floating point numbers, which are approximations to their mathematical analogs of mathematical integers and real numbers, differing only in the range or the precision of the numbers that can be represented. Our sets can be sets of integers or real numbers or any other structured data we can build from these, e.g., complex numbers, etc.
What is important for an abstract data type description are the following:
What are the mathematical descriptions of each of the operations one can do?
What are the types of data items that are input to each operation?
What are the types of data items that are output from each operation?
For instance, for the first operation above, we input an element and set and get a new set which is guaranteed to contain the given element after the operation. For intersection, we get two sets and return a set of only those elements that are in both of the given sets. Finally, for the last two operations, the output is of a binary-valued boolean type whose values can be true or false. Note that these specifications are independent of what the type elements are in the sets or how the sets are represented, or how each operation is implemented in code.
So ideally, the programmer decides on the abstract data types that will be used in the algorithmic solution to a problem by concentrating on the operations that will be needed. At this point, she does not need to worry about how those data structures are represented in detail and how the operations are implemented.
The most important idea one should remember about abstract data types is that abstract data types determine functionality. Functionality is the most important aspect of an abstract data type that any user of that abstract data type (i.e., a client programmer) needs to know. This is communicated through typically an API that names the operations and the input-output data to each call in the API along with some description of what function that call provides.
A data structure specifies how data of an abstract data type is represented and how the operations are implemented. For example, a set can be represented in many different ways:
We can represent a set as an array of elements. This is an efficient representation in terms of the memory required. On the other hand, many of the operations would be very inefficient. inefficiently. For example, to decide if the set contains a certain element, we have to start at the beginning and systematically compare it to every element in the array until we either locate it or exhaust the array. Other operations, such as the intersection of two sets or the insertion of a new element, would have additional complications in terms of steps required or memory allocated to represent the set. Yet, with suitable coding, all the operations can be implemented, but in this case, almost all operations would require a number of steps that is proportional either to the number of elements in the set (e.g., insertion or search) or is proportional to the product of the sizes of each set (e.g., intersection).
We leave it as an exercise to see how much time these operations would take if one implemented a set as a sorted array.
We can represent a set as a hash table. Hash tables consume additional memory in addition to the memory required to store the data but allow for a search for any element based on its key in expected constant time. So if you plan to do a lot of searching most of the time, that will be desirable. You can also insert new elements or delete old elements from a set in the expected constant time. However, the intersection of two sets would take a comparatively longer time, and so is the operation of finding the element with the minimum value.
There are many other concrete data structures that one can use to implement the abstract data type for sets.
Similarly, a specific concrete data structure can be used to implement multiple abstract data sets. For example, a pair of real numbers as a concrete structure can implement:
Complex numbers, where the pair either encodes the real and imaginary parts of a cartesian representation OR the magnitude and the argument of a polar representation of a complex number. All operations on complex numbers (e.g., exponentiation of a complex number to a complex number) would then be implemented in the said representations.
Two-dimensional vectors in a cartesian vector space where the two numbers represent the components of a vector along the x and y axes. One can then implement operations such as vector addition or the dot product of two vectors using this representation.
Note that it is the abstract data type that determines what operations to be done on the concrete representations are sensible or not. For instance, exponentiation of a complex number to a complex number makes sense in the domain of complex numbers but not in the domain of two-dimensional vectors, and similarly, the dot product of two vectors does not make sense in the domain of complex numbers, even though in both cases the very underlying concrete representations are the same.
The important point to remember about concrete data structures is that they are used to implement abstract data types correctly, but this implementation determines the cost of the abstract data.",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Classification,"Let us assume that you have a medical dataset that contains observations with features including patient age, weight, height, sex, and race, and you have been tasked with identifying whether a patient is diabetic or not). It would take a long time for you to review each observation and compare their feature value and symptoms to classical symptoms of diabetes. Using a data science approach, you can assign a diagnosis to each observation based on the historical data for that diagnosis. You would be a classification problem. Classification works with an existing dataset that has labeled outcomes and seeks to label the outcomes of a new, previously unseen dataset. Below, you will find the different types of classification problems. Later in the course, we will explore methods that can be used to solve classification problems.
Classification tasks that are binary will classify observations in a dataset into two defined categories. The observations are grouped based on the presence of characteristics unique to one of the two categories. An example would be making a decision on a credit card application (i.e., approve/deny).
Multi-class classification also referred to as multinomial classification, classifies observations into one of three or more classes. Each observation can only be classified as one of the multiple classes. That is, an observation can not be labeled as belonging to imore than one class. For example, if our task is to classify images with a single fruit in each, a classifier would classify each new image into one type of fruit, e.g., one of orange, pineapple, peach, and mango.
Unlike multi-class classification, which assumes that each observation belongs to one class, multi-label classification allows for observations to be classified under multiple classes hence the term multi-label classification.
A quick thought: Can you think about a scenario where observations can belong to multiple classes at once (thereby leading them to be labeled under those classes)?
Multi-label classification can be applied to, for example, classifying textual data. One important such application is document classification, the task of determining the topic of a document. It is conceivable that a document on politics can also be considered a business or an economics document  though perhaps not that strongly. Or, if you watch movies, you know that some movies can belong to multiple genres, e.g., Romantic Comedy, Romantic Drama, and Thriller Comedy, etc. Let us stick with this example and conceptually define how a multi-label classification task would pan out.
You are tasked to classify movies based on their plot. We can assume that we have defined our analytic objective, defined our requirements, and we have gathered and prepared our data. When you classify the observations in this dataset, you might find Movie A will belong to Romance and Comedy. Let us now look at the different multi-label classification techniques and see how they can handle problems with multi-labels without causing a dimensionality issue to your dataset and jeopardizing the performance of your model.
Multi-label classification does not have constraints on the labels that observation can have, and this makes it difficult to learn. Using the OneVsRest Technique, the classifier makes the assumption that labels are mutually exclusive and there is no consideration for correlations between classes.
Similar to OneVsRest, the Binary Relevance technique trains a separate single-label binary classifier for each class, i.e., for each class, an observation will either be predicted as belonging to that class or not. This technique ignores any correlation between classes.
The algorithm for your classification task can also adapt the algorithm to perform multi-label classification. A popular example is using a multi-label version of the k-Nearest Neighbors (kNN), a supervised learning technique we saw before that makes the assumption that similar data points are always close together.
Example: scikit-multilearn for MLkNN.
You can transform your task into a multi-class task by training all unique class combinations on one multi-class classifier.
X
Y1
Y2
Y3
X1
0
1
1
X2
1
1
0
X3
1
1
0
X4
1
0
1
Here we see that observations X2 and X3 belong to the same classes. This technique will transform our task into a single multi-class task and give a unique class to all possible combinations in your training data set.
X
Y1
X1
1
X2
2
X3
2
X4
3
So far, we have seen that OneVsRest, Binary Relevance, and Label Powerset techniques do not consider correlations between classes. The Classifier Chains technique will build a chain of binary classifiers to take into account any correlations between classes. The number of classifiers that are constructed equals the number of classes, i.e., if we have classes: comedy, drama, and romance, we will have three classifiers as well C1:C3.
We should mention Logistic Regression in this section because it is an important classification technique. You will learn more about it in the next module. Logistic regression uses a logistic function to model the probability of a class or event. Some questions you can answer with logistic regression are: Will you pass or fail a course, will you develop high blood pressure based on certain attributes, or will 18 to 35-year-old college-educated men from Pennsylvania vote for the Democratic or Republican presidential candidate in the 2024 presidential elections? The logistic regression model can have independent variables that are of diverse data types, but the response is categorical.","Binary Classification,Multi-Class Classification,Multi-Label Classification,Multi-label Classification Techniques"
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Data Visualization,Please see the Data Visualization primer on Sail().,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5a: Create Functionality & Insight,"Finally, the analytic objective should state what specific functionality, insight, or resource is gained from leveraging the data and methods you propose relative to the current situation and assuming the project is successful as proposed.
Valuable Functionality: In many practical scenarios, this may simply be that, for example, a predictive model can successfully solve the task and contribute to the problem solution, as stated. In more complex situations, the benefit gained may only be an incremental yet necessary step towards producing a model that solves the task. For example, the task may be to link passages in news text that state false information about certain historical events to a reference database of those events. Initially, one would need to detect whether passages talk about historical events at all before then discriminating which precise one they address. This detector, if implemented successfully, would add valuable functionality to the linking task.
It should be noted that analytic objectives can, of course, be reframed to focus on specific tasks, in which case an incremental step would become the main goal. In the example just given, one can change the analytic objective so that historic text detection becomes the main task in the first phase of the project. In fact, if the circumstances and the client allow for such a rescoping, then this may even be preferable. One of the main points of this unit is that explicitly formulating, discussing, and committing to analytic objectives supports a productive data science project lifecycle and ensures a proper alignment of the work with the business objective.
Valuable Insight: In some project contexts, the client may not need a piece of software that automates analytic functionality but rather requires insight from data in order to make decisions. This area of data science blends into what is commonly referred to as cbusiness intelligence.d
Valuable Resource: Data science projects can also produce resources to be used by the organization or further projects. For example, the project may be intended to collect and curate a competition dataset and publish it along with some baseline results to facilitate research on a certain topic.
Improvement over the current situation: Naturally, the project should improve over the currently available functionality, information, and resources. In industry settings, this is usually obvious. In academic and other research settings, however, you will be characterizing your project as improving over the state-of-the-art results. This can be done via a survey of related work and possibly some exploration of existing methods and datasets. While this will often be left implicit in the statement of the analytic objective, project proposals (especially academic ones) may require an explicit section on related work.
Assumption of project success: While it is natural that one would propose a project with confidence to succeed, it is worth noting that many data science projects (especially in academic settings) are exploratory to different degrees. The dataset may contain too much noise on top of the interesting patterns, or the computational effort may be too large. One should be mindful of what can be done to distill some value-added from the data, even if the main objective may fail due to factors beyond ones control.",
Model Evaluation,Metrics and Interpretation,Clustering Evaluation Metrics,"The previous page focused on the metrics for evaluating supervised learning problems. The presence of labeled data makes it somewhat straightforward to train and test the model's performance. Now, we will focus on metrics that can be used when labeled data is not present. There are two approaches to evaluating clustering. The Internal and External evaluation approaches. The internal approach involves summarizing the clustering task to a single quality score, while the external approach compares the clustering to a ground truth classification; ground truth is empirical evidence or data that is provable. Clustering can also be evaluated by an expert.
Internal Evaluation evaluates the clusters with high similarity within the cluster and high dissimilarity with other clusters and assigns the clusters a score. The cluster with the best score is seen to be the best. Internal evaluation although useful, can have its drawbacks. It gives insight into how one clustering technique performs against another but it can not speak to the validity of the results in the clusters. A sound example from Wikipedia illustrates this: k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means nor of an evaluation criterion that assumes convexity, is sound.Let's look at internal evaluation techniques that are used to assess the quality of clustering methods:
The Silhouette Coefficient shows how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance for each data point. A silhouette coefficient is 1 is best and -1 is seen to be the worst and means that the sample is in the wrong cluster, when the silhouette coefficient is close to 0, there is a presence of overlapping clusters.  For an excellent description and details on how to compute it, see https://en.wikipedia.org/wiki/Silhouette_(clustering).
Dunn Index is also used to evaluate clustering techniques and is very similar to the Silhouette coefficient. It is only dependent on the data within the clusters. A good clustering is one with a higher Dunn index. When using this evaluation technique, you want to be aware of a high computational cost when you have a large number of clusters. The Dunn index is computed by calculating the distance between each data point in a cluster and others in different clusters. The minimum of the pairwise distance is used to determine minimum separation (min.separation). The compactness of a cluster is measured by computing the distance between the data in the same cluster (max.diameter). Finally, the Dunn index will be: min.separation/max.diameter
If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, the Dunn index should be maximized.
See https://en.wikipedia.org/wiki/Dunn_index for more details.
External Evaluation measures the results from a clustering task based on data not used for the clustering task. Benchmarks are set from a set of pre-classified data. External evaluation techniques need ground truth data to evaluate clustering.
Rand Index tells you how similar a cluster or clusters are to a set benchmark. This is similar to a classification evaluation technique. You can calculate the Rand index as:
(TP + TN)/(TP+FP+FN+TN)
Purity is considered a no-frills technique that assigns each cluster to a class (usually one that occurs often in the cluster), the number of correctly assigned observations is divided by the overall number of observations to determine accuracy. Purity close to 1 is best and close to 0 is not optimal. A large number of clusters can lead to a higher purity. There is a tradeoff between the quality of clustering and the number of clusters when using purity as a metric. The normalized mutual information (NMI) can be used to measure and compare the quality of clustering between different clusterings with a varying number of clusters.
Jaccard Index is used in cluster analysis evaluation. It is defined as ""the size of the intersection divided by the size of the union of the sample sets."" The Jaccard distance measures dissimilarity between sample sets.
F-Measure is simply computed as the \\(\\frac{2*Precision*Recall}{Precision+Recall}\\). You might remember it from the classification metrics, it is also known as the F1 score.
The Dice Index, also known as the Sorensen-Dice index or Dice Coefficient, can assess the similarity of two samples. It ranges from 0 to 1. The dice index is a semi-metric version of the Jaccard index and gives less weight to outliers in a dataset. It is used to measure the lexical association score of two words.",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Regression,"When your output variable is a continuous value, you are able to make predictions using the widely known regression analysis. The input variables for a regression task can be categorical, discrete, or continuous data. So far, we have read about getting qualitative responses or output using classification techniques. Regression techniques return a quantitative response to a task. It is used to investigate the relationship between your input (independent) variables and your output (dependent) variable and predict the average value of an output variable given some independent variable(s).
Thought: The delineation above does not mean that all supervised techniques will either return qualitative or quantitative responses. Some techniques that we will explore in the next modules can return both types of responses; those techniques include kNN, among others.
Regression is one of the easier techniques to implement. We perform regression analysis because it can highlight the impact of independent variables on a dependent variable. For example, one can tell the effect of changes in temperature and terrain on the outcome of a football game. Regression analysis allows a data scientist to evaluate the best variables that can be used to construct a predictive model. Regression is used for forecasting tasks as well. When the goal is to infer relationships between the values of variables x and y, one can again use regression techniques.
If the independent variables are highly correlated, we can say that the variables are multicollinear. If the correlation between two independent variables is 1 or -1, then you have perfect multicollinearity. One can detect multicollinearity when there are large changes in the estimated regression coefficient when an independent variable is added or removed.
A regression model will have certain components, including the independent variables, often denoted as X and the dependent variable Y. A regression model also accounts for random error \\(\\epsilon\\); the random error is not found in the dataset. Instead, it is the difference between an expected outcome and an actual observation. It is usually an unpredictable occurrence that you can not account for in your dataset. Then, you have unknown parameters \xce. Your goal with a regression model is to estimate the function f(X, \xce) with the best fit to the data. The function f should be specified when performing regression analysis. This will ensure that you are deciding on the right regression methods to use.
When performing regression analysis, you might encounter data that has outliers. This is not handled during the data understanding phase. It can affect the results of your regression analysis.
Let us explore the different types of regression techniques in this section with the goal of exploring each technique further in the subsequent sections.
This regression technique is used to model the relationship between independent variable x and dependent variable y. When you have two or more independent variables, you will represent them as the vector \\(x=(X_{1t} \\ldots X_{kt})\\), and k is the number of inputs. The model is said to be linear because the output is expected to be a linear combination of independent variables.
There is the simple linear regression model that allows for predicting a response based on one predictor variable. Most times, you will be predicting a response with multiple predictor variables. Single linear regression does not allow for multiple predictor variables, so instead of training multiple simple linear regression models for each predictor, you use the multiple linear regression method to account for multiple predictors.
This model can also be used for classification if you replace the gaussian output with a Bernoulli distribution.
Let us represent a regression model as:
\\[y=\+\ x_1+\\cdots+\\beta_{\\mathrm{k}} x_{\\mathrm{k}}+\\varepsilon\\]
The regression function for multiple linear regression is:
\\[f\\left(x_1 \\ldots x_k\ight)=\+\ x_1+\\cdots+\\beta_k x_k\\]
Y is a straight-line function of each independent variable X. The slopes of the individual straight-line relationships of\\(X_{1} \\ldots X_{k}\\) with Y are the constants \\(\, \\ldots \\beta_k\\), also known as the coefficients of the variables. One can interpret this to mean \\(\\beta_i\\) is the change in the predicted value of your dependent variable Y per unit of change in \\(X_{i}\\), with other things being equal. Consider \\(\\\)  as the intercept (a prediction that your model will make if all the independent variables had zero values). You must also account for the random error \\epsilon in the equation.
You estimate \\(\, \\ldots \\beta_k\\), and \\(\\\) using the Least Squares method. This method will minimize the sum of squared residuals (a residual is a difference between an observed value and the fitted value given by a model). The least squares method can be linear or ordinary, or nonlinear. Ordinary Least Squares choose the parameters of a linear function of a set of independent variables by the principle of least squares. Non-linear least squares will fit a set of observations with a model that is non-linear in unknown parameters; that is, it will approximate the model by a linear model and refine its parameters by iterations.
The Performance of a regression model can be assessed using the coefficient of determination or \\(R^2\\). \\(R^2\\) measures the proportion of the variation in the dependent variable that is predictable from the independent variable(s). So, the larger the \\(R^2\\), the better the model can explain the variation of the response with various predictors.
Ordinary Least Squares Source3
Reading: Four Principal Assumptions. These assumptions justify the use of linear regression models for prediction modeling. These assumptions should be met to avoid producing misleading analytic solutions and insights.
When the relationship between the independent variable (x) and the dependent variable (y) is modeled as a degree polynomial in x, this is called a polynomial regression. Pay attention to the figure below. You will note that using a linear regression line to fit the data would result in a high value for the error.
Trying to fit a simple linear regression line. Source4
Now refer to the image below to see the outcome when you fit a polynomial line through the data points. The polynomial regression provides a better view of the relationship between the y and x variables. So a polynomial regression can fit a broader range of functions. However, it is sensitive to outliers, and those outliers can affect the result of a polynomial regression analysis.
Polynomial regression with lower error. Source4
When you have a regression analysis task, you might have multiple independent variables (and, in reality, you will), and you will need a method that fits the regression model with the most significant predictors. Stepwise Regression will increase the prediction power of a model with a minimum number of predictors. The process of fitting the model with the predictors is done automatically without human intervention. There are two techniques for stepwise regression:
Backward elimination tests the effect that each variable has on a model by deleting it. The deleted variables are those that have the ""most statistically insignificant deterioration of the model fit."" This technique should not be used if the number of predictors is more than the observations in the dataset.
Forward selection is the reverse of backward elimination. Variables are added to assess model fit and included if the variable shows a significant improvement to the fit.
We also have the mixed selection technique, which can be considered a hybrid selection method with the backward elimination and forward selection techniques.
Stepwise regression is prone to overfitting issues, and one way to guard against this is to check how significant the least significant variable will be based on chance. Model accuracy is tested using a validation set by calculating the mean error between the predicted value and the value in the validation set or holding-out sample. You can check the extent to which a model fits the data with the residual standard error (RSE is the standard deviation of error \\(\\epsilon\\)), i.e., the average amount that the response will deviate from the true regression line. A large RSE means the model was not a good fit for the data, and the \\(R^2\\) is independent of your response variable, unlike the RSE.
\\(R^2\\) is calculated using the total sum of squares which is the total variance in Y, and RSS is the discrepancy between the data and an estimation model.
The goodness of fit of a model will show how the model fits the data that it is trained with, and it will highlight any lack of balance between observations in the dataset and those that will be introduced to the model (new values). To select the right method, one can use the different metrics below:
AIC (Akaike Information Criterion). One chooses the model with the smallest AIC as the best model. The AIC puts more emphasis on the model performance on a training set and will tend to select more complex models.
BIC (Bayesian Information Criterion). A model with the lowest BIC is considered the best model. BIC is related to the AIC and is appropriate for models that fit under the maximum likelihood estimation. Unlike the AIC, BIC penalizes complex models.
\\(R^2\\) will increase as more dimensions are added to the dataset (this is considered a weakness of this metric). A value of 0 means that a model does not explain any variability, and 1 means the model explains full variability.
Adjusted \\(R^2\\) addresses the issue highlighted with n independent variable that has a strong correlation with the dependent variable increases the adjusted \\(R^2\\) and decreases it when a variable without a correlation to the dependent variable is added. When you have a model with more than one variable, the adjusted \\(R^2\\) is a suitable criterion to use.
Mallow's Cp is used to assess the fit of a regression model that has been estimated using ordinary least squares. The goal is to find the best model involving a subset of these predictors. Note that you want a small Cp.
We will continue to learn more about regression analysis in an upcoming module, and you are encouraged to locate the materials in the additional reading section to strengthen your knowledge of Regression analysis.
Additional Reading: Elements of Statistical Learning",Model Accuracy
Advanced Natural Language Processing,[Research Paper] Attention is All You Need,Quiz 11,,
Collecting and Understanding Data,Ethics of Data Science,Governance,"Data governance defines how data is accessed and managed within an organization. It is important because it facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis. An organization that handles data efficiently understands that data governance impacts data quality and the decisions made from the data available to the organization. This unit focuses on data governance as a component of data management and how it influences the development of analytic solutions in an organization and its decision-making. Any organization that stores and utilizes data should have a data governance strategy for internal data, or data stored within the organization, and external data.
Data governance is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, maps the location of data in the enterprise, reduces the scourge of data silos, and improves data management overall.
Reading: Data Governance in the Cloud.
Data governance is the responsibility of an entire organization; although it is often administered by the data management team, all users of data in an organization are considered stakeholders of the organization's data. The Data Governance Institute defines a stakeholder as an individual or group that makes or is affected by data-driven decisions within an organization. In addition to data governance policies, data stakeholders will have an influence on the state and use of data. As a quick reminder, the data science team works with different individuals in an organization to define business and analytic objectives during the data science project lifecycle, as well as to determine the requirements for the analytic solution. So why is data governance important to a member of a data science project team?
Data governance is more than just policymaking for data. Iit influences business strategy because data are now (more than ever) considered an asset to an organization. An organization that has embedded data governance principles into its data infrastructure or data management framework is an organization able to abide by data standards (whether industry-set or company-defined).
Data governance best practices for organizations are met when data has integrity, and data-related decisions and controls are transparent and can be audited. Another best practice gaining ground in the industry is for organizations to collect and store data that is unbiased. Unbiased data means something different to each organization and industry, but the general idea is that the data represent all members of a population that could be served by an organization. This best practice will positively influence the development of ethical models and algorithms for analytic solutions.
Reading: Data Governance and Its Implications for Ethical Models.
Data governance is necessary to ensure that data is safe, secure, private, usable, and in compliance with both internal and external data policies. Data governance allows setting and enforcing controls that allow greater access to data, gaining security and privacy from the controls on data. Some common use cases include:
Data stewardship. Data governance often means giving accountability and responsibility for both the data itself and the processes that ensure its proper use to cdata stewards.d
Data quality. Data governance is also used to ensure data quality, which refers to any activities or techniques designed to make sure data is suitable to be used. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.
Data management. This is a broad concept encompassing all aspects of managing data as an enterprise asset, from collection and storage to usage and oversight, making sure that data  are leveraged securely, efficiently, and cost-effectively before they are disposed of.",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Ranking,"A data science pattern that can be used to solve different data science tasks from machine translation to information retrieval is Ranking. Learning-to-rank is a technique used to train a model for ranking tasks. We do not always want to predict the probability of scenarios. We just might want to rank things. Ranking is used to solve information retrieval problems, including collaborative filtering, sentiment analysis, and document retrieval. The learning-to-rank technique is applied in supervised learning to rank results according to relevancy. When you are building a model using this approach, you must decide on the features used but also on the adequate relevance criteria.
Assume that you have a set of documents and that users pose queries to retrieve documents matching a query ranked based on a measure of relevance to a query.   We can use one of the following approaches:
Pointwise Approach is used under the assumption that each we can compute a numerical score that captures how much a document is relevant to a query. Once we know these scores, we can rank the documents.  Thus the learning-to-rank problem can be cast as a regression problem  given a (training set of ) query-document pair, learn to predict a relevance score. Ordinal regression and classification algorithms can also be used in a pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.
Pairwise Approach seeks to reduce the average number of wrongly ordered rankings in comparison to the ideal expected result (also known as the ground truth). Ranking using the pairwise approach becomes a classification or regression task. Every pair of documents is classified by a binary classifier which determines which one of the pairs is more relevant to the query. Then based on these pairwise rankings a global ranking is produced minimizing the number of out-of-order pairs in the final list.
Listwise Approach reviews the list of documents and produces an optimal ordering.  It tries to directly optimize the value of one of the above evaluation measures, averaging over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to the ranking model's parameters.
Microsoft Research has developed the three known learnings to rank algorithms that all use pairwise ranking:
RankNet uses gradient descent to update the weights or model parameters for a learning-to-rank task. This algorithm seeks to minimize the number of wrong orderings among a pair of results of a ranked list.
LambdaRank uses a cost function to train a RankNet which results in speed and accuracy improvements.
LambdaMART uses Multiple Additive Regression Trees (MART is an implementation of the gradient tree boosting methods for regression and classification) and LambdaRank to solve a ranking task.
Learning to Rank Algorithms (Source: Lucidworks)
Additional Reading: Application of LTR - Bayesian Product Ranking at Wayfair
Additional Reading: From RankNet to LambdaRank to LambdaMART",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 3: Focusing on a well-defined Task,"Once the team has identified the precise problem for which data should be leveraged, it is advisable to explicitly characterize the task to be done at the analytical level. To do this, one casts the cheartd of the problem into one or more categories of typical data science tasks:
Classification: Individual instances in the dataset have a categorical (i.e., non-numerical) label associated with them or will be labeled as such. The goal of the project is to develop a system capable of categorizing data points using these labels. A common variant of classification is sequence labeling, where individual data points are not independent but form a series. Typical examples of classification are sentiment analysis of text and labeling images as depicting certain objects (animals, cars, etc.).
Regression: Individual instances in the dataset have a numerical label associated with them whose magnitude carries a specific meaning. The goal of the project is to develop a model capable of predicting this target score for individual data instances. Like classification, regression is often applied to dependent series of data points. Typical examples of regression include predicting measurements in medical or demographic data over time.
Retrieval & Ranking: The dataset can be thought of as one or more collections of data points and queries. In response to the query, one or more cideald data points should be retrieved and presented in ccorrectd order. A typical example is a  search engine returning a ranked list of results in response to a query.
Recommendation: The dataset consists of users and items, as well as information about the preferences of users for specific items. The task is to find items to recommend to users towards the maximization of some utility. Typical examples are movie recommendations on Netflix and product recommendations on Amazon.
Clustering: The dataset is assumed to have some latent structure which should be discovered by partitioning the data points into groups that are close to each other in variants of the feature space. A typical example is the exploratory analysis of unlabeled data.
Anomaly Detection: The dataset is assumed to have some latent structure that should be discovered in order to identify instances that do not adhere to the pattern. A typical example is the detection of fake customer reviews in online retail data.
Domain-specific tasks: Aside from the generic task types explained above, some domains have developed specific task patterns and associated evaluation metrics that should be used if the analytic goal is sufficiently specific. This is particularly true of natural language processing and image analysis. For example, machine translation will commonly be characterized as a text generation task, and models will be evaluated using a specialized metric called the BLEU score. Similarly, models that segment a part of an image containing a specific object may be evaluated using average precision in conjunction with an cintersection over uniond threshold.
It should be noted that some of these categories overlap somewhat in that the methods associated with them can be used in different ways. For example, ranking problems can be solved using classification or regression methods. Still, if the primary nature of the problem is that items must be ranked, it should be evaluated using ranking experiments and metrics, even if it is doing classification cunder the hood.d
The purpose of identifying the type of task at this stage is not to firmly restrict the project to a narrow instrumentarium, but to characterize it for purposes of planning and communication. Specifically, a possible quantitative evaluation and the metrics used therein will typically closely correspond to the problem's primary nature.",
Problem Identification and Solution Vision,Problem Identification,Problem Identification,"Just like other business processes, a data science project is complex with moving parts that include understanding the business needs and objectives of the company that influences the stakeholders, existing system environment, and support structure. The data whose analysis supports the proposed solution may have to be gathered both from inside and outside the organization. Overall, this process of initiating and executing a data science project can be challenging, and it requires technical expertise as well as domain guidance. A Gartner study showed that 85% of data science projects fall short of expectations. Project expectations are defined by your client, and the data science team creates a solution vision that will meet their expectations. The path to defining those project expectations must be tread carefully to avoid project failure.
According to the Chaos Report, a group that tracks IT project failure, reports that data science projects might fail due to multiple reasons, including those that occur in general IT projects. However, there are some unique issues that data science teams must consider to ensure the success of their projects. Some of the reasons for data science project failure include:
Insufficient or inappropriate data,
Lack of technical data science skills,
Issues with project management,
Inaccurate interpretation of results, and
Mismanaged client expectations.
The above-mentioned issues are related to a misunderstanding regarding the clients business needs and to inadequate communication between the data science project team and the business stakeholders. It is important to identify and understand your clients business needs, environment, and current solution. This will increase the chances of meeting the business objectives and providing the right analytical solution.
In this unit, we will discuss how you derive analytic objectives from a clients or organizations business needs. In doing so, you need to broaden your understanding of the data science process beyond the gathering of data and the application of machine learning methods to a more or less clean dataset. While these aspects are certainly important parts of many data science projects, being a successful data scientist involves being mindful of the big picture to envision, design, implement and ultimately deploy creative solutions for real-world problems. You will find that this course introduces a data science approach that is grounded in scientific research, software engineering principles, and experimentation.
The remainder of this unit focuses on the ability to identify problems and envision a solution, which is among the most important skills a data scientist must possess.",
Data Science Project Planning,Requirements Gathering,Framing Analytic Requirements,"Once it is determined that an organization is Data Science ready, the project team will gather requirements that will enable the development of an analytics solution to meet the defined business need(s).
Let us explore a scenario with a two-year college that is seeking to improve the recruitment of new students and increase the retention of existing students by 10% over the next five (5) years. The college would like to use predictive modeling to achieve this objective. The predictive model should provide insights that can inform business processes related to recruitment and retention.
Upon meeting with project sponsors, assessing the client for data science readiness, and defining business and analytic objectives, the project team embarks on gathering requirements for this project. The business analyst (BA) identifies all necessary stakeholders and systems related to recruitment and retention in the college. The BA takes the lead in eliciting information about user needs and system constraints.
In this scenario, requirements elicitation involves interviewing stakeholders and document analysis. Once user and system requirements are defined, the project team can now define the requirements for the analytic solution.
While distilling analytic objectives for this project, the project team determined that an exploratory analysis of the colleges prospective student data is needed to perform segmentation for targeted advertising and recruitment efforts. Prior to performing a segmentation task or cluster analysis, the team defines the requirements for the data that will be explored.
Defining data requirements involves understanding the data needed to reach the business and analytic objectives. Defining the data requirements will also involve considerations for data sources, data management, and data extraction. The following questions must be answered to determine the data requirements:
How can data be used to meet the business and analytic objectives?
What data is needed for developing the analytic solution? Can the data be collected from credible sources?
How can the data be prepared to visualize and answer relevant questions?
Although the talent for developing models resides within the project team, the model requirements are defined collaboratively. Model explainability is important to ensure that a client understands the insights that can be gleaned from the results/outputs from the model(s).
Lets consider the scenario above. The client will benefit from models that predict the probability of a prospective applicant enrolling in the college. Without understanding the output of a predictive model, the client will not be able to decipher the observations that could have a positive yield from their engagement efforts. This can result in the college wasting funds in targeting prospective students, therefore leading to a failed solution.
Model requirements describe the behavior and attributes of a model. The figure below highlights questions that should be answered, defining requirements for a model/models:
Figure 1. Defining Model Requirements
Reports and Dashboards: ""Reports are often considered as a solution for business intelligence projects. If the college needs to generate reports, the project team will work with the college to define the requirements for reports by inquiring about (1) the current reports used in the college and the changes that need to be made to the reports, (2) data sources for the reports, (3) considerations for users generating reports i.e. how can an authorized user customize a report to display the data they need?"".
Similar to traditional IT projects, requirements should also be gathered for analytic solutions that are packaged. i.e., not developed in-house or outsourced.
Reading: Requirements for an IIoT (Industrial Internet of Things) Predictive Maintenance Solution.","Data Requirements,Model Requirements"
Data Science Project Planning,Requirements Gathering,What is a Requirement?,"The IEEE defines a requirement as a documented condition or capability needed by a user or system to meet a business need or achieve a business objective. Requirements become useful to the solution development process when they have been converted into specifications. Requirements must meet certain criteria to be useful for achieving business objectives. Let us define the general characteristics of a good requirement.
Reading: IEEE Guidelines on Software Requirements Gathering.
A good requirement should be complete and correct. You must identify the relevant stakeholders and define a set of needs, goals, and objectives for your project. As there is no perfect scenario, you must define the constraints that are applicable to a project. Those constraints might include cost, scope, existing systems and processes, time, and technology. Defined scenarios can also result in identifying all the stakeholders and their needs within the business context. A requirement should also be traceable. This refers to tracking the life cycle of a requirement from its development to its specification and deployment in various versions of the solution. Traceability can be supported in a straightforward way via a matrix which associates a unique identifier, a description, and a design specification element to each requirement, as shown below.
Requirement
Design Specification
1.0.0 Consumption reports should be integrated with the Dashboard API.
Data from the hourly consumption report will use data_integrate_trail in the Update event procedure.
A requirement should be unambiguous. Given that the requirements gathering process involves contributions from multiple stakeholders, the requirement should be explicit and clear to all. It should have the same meaning to everyone involved and not be open to interpretation. Unambiguous requirements must have defined acceptance criteria, metrics for success, expected outcomes, and acceptable values. The use of an active voice in its description will make a requirement clearer. Finally, a requirement should be verifiable. Testers should be able to verify that the requirement is implemented correctly. A requirement is considered complete when it is verifiable, unambiguous, and traceable.
Consider this requirement for a report-generating solution:
This requirement leaves room for interpretation, and this can lead to not meeting client expectations. A better representation of the requirement is to include a time frame, a responsible party, and a deliverable.
Reading: Traceability in Requirements and Other Artifacts.","Characteristics of a Requirement,The consumption reports will be generated and displayed.,The system will automatically generate the consumption reports every 30 minutes, and the results will be displayed on the consumption dashboard."
Collecting and Understanding Data,Data Collection,Where do data come from?,"Data collection is the process of gathering and organizing data that can meet defined business and analytic objectives. The data collection process in the data science lifecycle can be compared to the data collection process in scholarly research. Data collection should be conducted systematically to ensure that the data are valid and reliable. Data collection also involves attending to issues of validity, reliability, and ethics, such as provisions for informed consent from participants. We will explore these issues in upcoming modules.
As a professional who works with data, it is important to know where data come from and think about the analytical approaches that one will take to analyze data. Before going further into data analysis, we want to understand where the data that are provided to us come from or what approaches have been used to gather data for the study. We need to ask these questions to guide us in thinking about what process generated the data and the type of data that we may be working with or collecting. In general, there are two key types of data:
Organic or process data
Data collected from a designed study
Organic or process data are data that are generated by an automated computerized information system or extracted from images, video, or audio recordings. This type of data is generated organically as a result of some process continuously or over a period of time.
Examples of organic or process data:
Financial or stock market exchange transactions
Web browser history
Web or mobile application activity history
Netflix viewing history
Surveillance camera video recordings
The term cbig datad refers to these types of datasets comprising organically produced data from automated processes over time in massive quantities. . Data scientists mine these data to study trends and discover interesting relationships. But processing such massive quantities requires significant computational resources.  Thus compiling and processing such massive quantities of data efficiently and getting them ready for analysis are exciting research and practice areas in and of itself.
Data collected from a designed study as the name suggests derives data from specific studies designed to address particular research topics. The main difference between this type of data and organic data is that data collected from a designed study are collected based on a certain guided agenda by theory, prior knowledge, literature, etc., in order to test particular hypotheses or to attempt to answer predetermined research questions.
Here are examples of data that can be collected from a designed study:
Questionnaires and surveys. Questionnaires are used to collect data from a group of individuals. Questionnaires can be administered on paper or online. In general, it might be easier to distribute questionnaires online as there are efficient tools that can analyze the collected data. Questionnaires can have open-ended, closed-ended, rating, Likert-scale, or multiple-choice questions. Data cleaning is still a consideration with questionnaire data as errors can occur. For example, responses to open-ended questions can contain misspellings, among other errors.
Interviews. Interviews are open-ended question-answering dialogs between an interviewer and one or more interviewees. Interviews are guided by an interview protocol designed to provide instructions for the interview process, the questions to be asked, and the space to take notes during the interview.
Observation is the process of gathering open-ended, firsthand information by observing people and places at a research site. Data collected during these observations can support or complement the data collected during interviews and from questionnaires.
Focus groups can be used to collect shared understanding from several individuals as well as to get views from specific people. A focus group interview is a process of collecting data through interviews with a group of people, typically four to six. The researcher asks a small number of general questions and elicits responses from all individuals in the group. Focus groups are advantageous when the interaction among interviewees will likely yield the best information and when interviewees are similar to and cooperative with each other.
As you may have noticed from the provided examples, this type of data arises from a design data collection rather than organically from an automated process. So instead of knowing what a users Netflix viewing habits are over time, we might want to ask a group of individuals that are sampled from all the Netflix viewers and then interview or survey them about their opinions on a particular topic such as a new pilot feature or a newly added movie.
Figure 1: Drawing a representative sample from the population. (Source: https://www.voxco.com/)
In figure 1, we have a population of interest but we draw a representative sample of individuals from that population because it is usually difficult to measure everyone from that targeted population. There is another way to leverage process data in a designed study: we specifically design a way to extract a subset of the massive quantity of process data collected that can serve the purpose of the study design and research objectives.
You can appreciate the key differences in the data collected through a designed study. Such data are very rigorously designed data collections that people might be interested in looking at, as opposed to just large data sets that arise organically.",
Deep Learning and Model Deployment,Model Deployment,"The Three Environments for AI Professionals - Research, Development, and Production","The individuals who want to pursue skills required for data roles like Applied Engineer, Data Analyst, Data Engineer, Data Scientist, Data Solutions Architect, Machine Learning Engineer, Research Scientist, etc., are confused because the fields are relatively new, and there is a lot of overlap between these roles. Moreover, the definitions of the roles and skills required are different for different organizations because organizations have a different understanding of each role based on their requirements, organizational culture, and allocated budget.
This chapter introduces three different environments for AI professionals and focuses on different tasks and skills required for each environment. In general, a good AI professional needs to be aware of the basics of all three environments and be an expert in some tasks in at least one environment. Based on her interest and expertise in tasks of environments, she can further pursue skills in depth and expand their skill set. Awareness of these environments can help individuals avoid confusion while making career decisions in the dynamic data world.
AI professionals first collaborate with stakeholders and domain experts to understand and define the business problem. They might also present and validate assumptions related to the problem. Once the problem is defined, and assumptions are validated, AI professionals can start working in the research environment.
A research environment is where AI professionals define experiments and might use tools like Jupyter Notebook and Jupyter Lab to collect data, clean it, perform Exploratory Data Analysis (EDA), and present findings to the team. Each experiment's findings might help select and generate new features from data and build models that can potentially solve the problem. Later, metrics are defined to evaluate and select models across different experiments. Sometimes, an ensemble of models from different experiments might result in higher performance. The code might be very messy in this environment or phase. It might also be hard for others to run your code and/or reproduce your results successfully on their machines.
The different steps in the research environment include:
Define Experiments: Different experiments can be defined based on the definition of the problem. For example, suppose we have a classification problem. In that case, experiments might be defined based on different approaches like conventional supervised learning, weak supervision active learning, semi-supervised learning, pre-training, etc. Experiments are prioritized based on the type, project timeline, and quantity and quality of data.
Data Collection: For a given experiment, the AI professional might collect structured or unstructured data from existing proprietary databases, use open-source datasets, or extract data using python scripts like crawling text or images from relevant websites.
Data Cleaning: The steps in cleaning depend on the data, problem, and experiment. For example, AI professionals can impute missing values, normalize extreme values, remove duplicate samples, etc., to classify structured data.
Exploratory Data Analysis (EDA): The goal of EDA is to find patterns in cleaned data which helps in selecting relevant features for modeling and understanding relationships among them. EDA can also help identify how to further clean the data for modeling.
Feature Engineering: The knowledge from domain experts and EDA patterns help AI professionals create new features that might increase the performance of models in the experiment. Remember that generating relevant new features from existing features is called feature engineering.
Data Modeling: The goal of a model is to try to replicate domain experts decision-making process. AI professionals come up with mathematical algorithms and build models using relevant features to automate the decision-making process.
Tuning and Evaluation: Optimal hyperparameters can be found to maximize the model performance by comparing the metrics of each version of the model in an experiment on evaluation data.
Experiments Tracking and Evaluation:  Steps 2 to 7 are repeated for each experiment and evaluated at the end. Experiment tracking tools like Neptune AI and Weights and Biases can efficiently track experiment information with a good user interface.
The model in an experiment with the highest performance is selected for working further in the development environment.
A development environment is where AI professionals create components by cleaning and modularising code from, e.g., Jupyter notebooks, adding dependencies (PyTorch, Numpy, and Pandas, etc.), and packaging them. A component is an organized, modular, maintainable, and reusable code that performs one step, like data extraction in the AI/ML pipeline.
In applied machine learning, the AI/ML Pipeline automates performing a sequence of steps in components and interaction between the components defined by the AI/ML system design. The components include data collection, data preprocessing, model development and fine-tuning, post-processing on predictions, model evaluation, model deployment, maintenance, and monitoring.
Use a version control tool. Version control plays a crucial role in the development environment. Version control tools like perforce and assembla make the processes like creating a GIT repository, defining the code repository structure, and branching strategy easy.
Install IDE like PyCharm to automatically create virtual environments for projects and allow easy integration with GIT.
Convert Jupyter Notebook code into object-oriented code and save in .py files. Have appropriate variable names, add comments, and organize different files into components with proper hierarchy.
Create config files containing standard information across multiple components like input file location, model location, output file location, cloud or external API credentials, model parameter values, hyperparameters values, etc. Config files make adding new variables easy for all components across the pipeline and modifying and removing existing variables.
Write and automate tests for multiple components. Write modules to test each component individually (unit testing) and test the interaction between components (integrating testing).
Use a logger to log the message and time. Logging makes debugging easy, especially when the code base becomes huge and complex. A logging message can have a logging level like critical, error, warning, info, debug, or notset. Critical is an essential message to log, and notset is an unimportant message to log. Levels ensure the minimum level to log. For example, if you set clevel = logging.warningd, any message logged as critical, error, or warning is only logged, and other levels are ignored.
Unlike traditional software engineering where only changes in code are tracked (code versioning), data used for training, testing, and evaluation can also be tracked (data versioning) especially if data is large and dynamic. DVC, Delta Lake, and LakeFS are some open-source data versioning tools.
Often based on the requirement, a server is built using web frameworks like FastAPI, Flask, or Django to deliver predictions to other software components.
The packaged code is further used in the production environment.
Based on the size and timeline of the project, development and production environments are the same or different. Generally, the production environment is a phase where the models in the pipeline are scalable, monitored, and served in real-time by containers.
Design Optimization: In general, there is a lot of gap between the number of models and the quality of models in the research environment, development environment, and production environment. Hence, if required, the AI/ML system design created before in the development environment needs to be optimized and redesigned for production.
Containerization using Docker: Developers might use multiple components like Data Extractor, Elastic Search, Rest API, Messaging Queues, etc. Each component has its respective dependency libraries. Having components with different versions of a library in the same environment might lead to conflict. With the help of Docker, AI Professionals can standardize environments and run different containers for different components in isolation, where each container has dependent libraries for the respective component. An environment can be created by Docker using a DockerFile. DockerFile contains instructions like navigating to a respective folder, installing dependencies, setting environment variables, loading configuration parameters for the model, etc. Scaling is easy with containers because AI professionals can spin up new containers for the same component in seconds to satisfy the scaling requirements.
Continuous Integration and Continuous Delivery (CI/CD): CI/CD enables AI professionals to work together in a shared code repository where updates to a part of code by an individual are automatically pushed, built, tested, delivered, and deployed to the shared code repository, and code issues can be tracked and resolved respectively
Workflow Orchestration and Infrastructure Abstraction: Workflow Orchestration tools like Googles Kubernetes and Red Hat's Openshift can quickly spin up multiple containers on different machines on demand, manage resources like memory and compute for containers, have high container availability for the product. Depending on the organization, the infrastructure of the workflow orchestration tool is owned by separate teams like DevOps or the AI professionals themselves. Some AI professionals might find it tedious to work with infrastructure abstraction tools. They can use infrastructure abstraction tools like Googles Kubeflow and Netflixs Metaflow, which are built on top of workflow orchestration tools that allow them to focus more on models and stop worrying about low-level infrastructure.
Monitoring and Maintaining the Deployed Models: Unlike traditional software, AI/ML models are dynamic and degrade over time. Hence, it is essential to measure, monitor, and govern the different metrics and tune models before they negatively impact user experience and business value. In general, the models health can be measured by three different metrics.
Resource Metrics: These measure incoming traffic, CPU/GPU memory usage or utilization (Does server efficiently utilize resources?), prediction latency (Does server handle requests quickly?), throughput (Does server maintains good throughput and scales based on requests?), and cost (Are hosting and inference costs of the entire ML pipeline are as expected or more?).
Data Metrics: It is essential to check if the input data format is correct first instead of debugging the entire pipeline.
Anomaly Checks: Simple checks like having max and minimum values for each feature (age cannot be negative or 100000) can identify and validate extreme or anomalous data points in input data. Later, the team can brainstorm and find root causes for receiving these anomalies from users.
Data Quality Issues: Users might give synonyms (cGirld for cFemaled) or incorrect values (cMaild instead of cMaled) as input to the pipeline. In these cases, the model might fail to recognize the value in the feature cGenderd (data might be absent while training the model) and assign NaN (not a number) for the feature. Even though the model doesnt break, the predictions produced by the model might be wrong. Hence, testing new data that the model hasnt seen before is essential.
Data Drift: When we train a model with some static data, it assumes specific patterns based on the distribution of provided data. However, real-world data is dynamic. Because of these changes, the assumptions made by the model might no longer be valid, and the model might get biased, which leads to bad performance in real-time model evaluation. For example, water consumption in hospitals during COVID-19 was very high compared to historical data. Hence, we cannot use a model built on historical water consumption data during COVID-19. This phenomenon is called cData Drift.d Periodically detecting changes in the distribution of data using statistical tests can help to detect data drift.
Model Metrics: It is crucial to estimate the expected performance of models before deploying them into production and periodically check if expected KPIs are met. If model predictions or expected KPI values are bad compared to benchmarks, AI professionals might consider the Model Drift issue. Model Drift is a phenomenon where the relationship between features changes, and the model no longer gives accurate predictions. For example, the relationship between the births and deaths ratio changed during COVID-19 causing model drift. Model drift can be detected by periodically analyzing feedback from feedback loops and correlating it with what is affecting the business.
Monitoring: Based on Model Metrics and Data Metrics, if re-training is required, the AI professional might start repeating the research, development, and production environment to deploy and monitor the new model. Often, retraining is also a way to improve the model's performance to reflect the change in data over time. Some of the Data Monitoring tools include SuperwiseAI, ArtherAI , and VertaAL.
It is good for any individual who wants to make a career in AI to be aware of the basics of all three environments. This awareness can help individuals to identify the skills required to work in an environment. Based on their interest, they can choose to specialize in one or more of these three environments and eventually make their career decisions in the current rapidly changing data world.","Research Environment:,Development Environment:,Some best practices while packaging the code:,Production Environment:,Some of the tasks performed in the production environment are:,Conclusion:"
Collecting and Understanding Data,Sparse Matrix,Quiz 2,,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Means Clustering,"When we want to identify patterns in a dataset from unlabeled data, we use unsupervised learning to perform this task. Unsupervised learning is also referred to as self-organizing. We had already seen an example of unsupervised learning when we studied  Principal Component Analysis while discussing feature engineering. We will also look further into the different types of cluster analysis techniques.
You can categorize data according to characteristics using a technique called Cluster Analysis. If you think about how we reason and learn as human beings, we make sense of events, people, and things by placing them in groups. You have memories that are characterized as happy and sad, or people categorized into close friends, acquaintances, and mentors, among others. You might similarly consider clustering data to identify those with similarities as a method of exploring data. Applications of cluster analysis include market segmentation - the segmenting of customer data based on certain criteria, including transaction history. The different clusters created from the segmentation exercise are useful for targeted advertising or the application of customized marketing strategies that will elicit positive responses and increase sales and engagement.
Clustering (Source: www.pyarmy.com)
Hard Clustering divides data into a number of groups, and data points can only belong to one cluster. All clusters are independent of each other.
Soft Clustering groups data into clusters, but a data point can belong to more than one cluster to a degree.
Overlapping Clustering allows data to belong to more than one cluster.
Hierarchical Clustering organizes data in a hierarchical manner so that the hierarchies are represented by a dendrogram.
Reading: An Expansion on Clustering.
This method involves identifying the number of clusters k that the dataset will be grouped into. The data in each cluster will share similarities to the other data points within its cluster. Assume you have k = 5. Cluster 1 will contain data that is homogeneous but quite dissimilar to the records in cluster 5. The data within clusters adhere to distance measures to ensure that dispersion is minimized. k-Means Clustering technique abides by a number of distance measures, but the most popular is the Euclidean distance. Let us look at how clusters are created using this technique:
A set of k means is chosen. These are meant to capture the mean of all the observations within the cluster, in general.
Each data point is then assigned to the cluster with the nearest mean.
After a pass, using the points assigned to a cluster, new means are computed.
The last two steps are repeated.
The algorithm converges when the assignments to cluster no longer change. The algorithm is not guaranteed to find the optimum.
How do we decide k? Similar to kNN, there are empirically studied recommendations for the best k to select. You can also select k based on previous knowledge (this is hardly the case with this unsupervised task). You can use different values for k and then compare the results obtained from each value of k. It is good practice to also run the k-Means cluster method by using different values for k based on the number of clusters that are expected from the data in order to see how the sum of distances reduces with increasing values of k.
k can also be chosen by calculating the Within Cluster Sum of Squares (WCSS). This is the sum of squares of the distances of each data point in relation to the centroids in the data points cluster.
Assuming that we have 1000 observations in a dataset, and we have decided that k = 1000, the WCSS should be zero (0). This is because all the observations are considered centroids, and there is technically no distance between the observation and the centroid within the cluster since it is the sole data point in its cluster. This is certainly not a computationally sensible way to cluster data. Think about a dataset with over 100,000 observations. Also, think about the information to be gleaned from the cluster analysis; you will lack useful information.
When you randomly initialize with a range of k values for the 1,000 observations mentioned above, i.e., between 2-10. You can use the Elbow method to find out the optimum value for k. The Elbow method produces a graph that shows this optimum value at the ""elbow"" of the line, as shown below. You select k as the WCSS decreases; the figure below shows that after 5, the decrease in WCSS is quite small.
Elbow Method
Reading: k-Means Clustering-sklearn.
Additional Reading: K-Means Clustering Algorithm
K-Means Clustering and k-Nearest Neighbors have been known to cause confusion for data scientists who are new to the field. After all, we are discussing similarity measures and distances to an observation to classify or cluster into a class. The main difference is that one is an unsupervised technique, and the other is supervised. kNN is a supervised classification method that involves labeled data that is used to train a model to accurately predict the class of a new observation according to its closest or neighbor data points.
K-means does not provide a labeled dataset to the model for learning purposes. K-means will partition the data into a number of clusters. kNN works best with data that is of the same scale, but k-means does not need the same scale data to perform well. Remember when you learned about kNN being a lazy learner? K-means is an eager learner. It is slow to train, but it tends to deal with noise in the training dataset better than a lazy learner.",
Analytic Algorithms and Model Building,[Research Paper] Conceptual Complexity and the Bias/Variance Tradeoff,Conceptual Complexity and the Bias/Variance Tradeoff,"For selected units in this course, we will have paper reading modules that provide exposure to foundational research papers. The goal of these modules is to familiarize you with the styles of data science literature and the contexts in which advances in data science are introduced. We understand that reading technical papers can be challenging and time-consuming if you dont have prior experience. To facilitate your learning, we have included both the original paper and our synthesis of the papers key points below. Our expectation is that you can acquire a good understanding of the papers message by skimming through the original article and then reading our synthesis.
Beyond the specific content of each paper, we also encourage you to pay attention to the synthesis structure introduced below, which contains key questions that one should ask while reading through a scientific publication. You may find this outline useful in a future seminar course or in your own research projects.
[Required Reading] Paper: Briscoe, E., & Feldman, J. (2011). Conceptual complexity and the bias/variance tradeoff. Cognition, 118(1), 2-16. (Requires CMU credentials to access)
The first author is currently a Senior Research Scientist and the Chief Scientist of the Aerospace, Transportation, and Advanced Systems Laboratory with the Georgia Tech Research Institute. She conducts research and development projects that focus on behavioral and data science/analytics applications in various problem spaces, including computational social science, technology emergence and prediction, social network analysis, insider threat detection, terrorism and radicalization, business intelligence, and psychological profiling. She received a Ph.D. in cognitive psychology from Rutgers University in 2008.
The second author received his Ph.D. in 1992 from the M.I.T. Dept. of Brain and Cognitive Sciences and has been at Rutgers ever since. His main research interests are in visual perception, especially perceptual organization and shape, and in categorization and concept learning. In both these general areas, his focus is on mathematical and computational models of human mental function. In categorization and concept learning, he is similarly interested in how the mind organizes groups of objects into coherent collections and hierarchies. In experimental work, he has found that human learners, given a set of objects to be learned, tend to form categories that are as simple as possible. This idea opens up an enormous set of research questions about what perceptual features form the basis for categorization, how these features are selected in order to reduce representational complexity, and how these goals relate to the structure of the natural world.
The paper is targeting machine learning researchers and practitioners who build machine learning systems that interact with the end-user.
There are two popular psychological theories on how humans perform categorization. Exemplar theory states that people store the attributes of observed examples, called exemplars, along with their category labels in memory, and categorize a new object with the label of the most similar exemplar. For example, people would categorize an object as a bird if its similar to any type of bird that they have come across, e.g., parrot, sparrow, penguin. In contrast, prototype theory states that there is a central representation of each category, and people compare a new object against these central tendencies to determine its category. With the same bird classification task above, based on the prototype theory, one would label an object as a bird if it possesses the common, caveraged features of a bird, e.g., two legs, two wings, and lay eggs. Overall, the key difference between the two theories is whether a new object is compared to real instances (exemplars) or an abstract central representation (prototype) of a category.
While prior researchers have often regarded these theories as fundamentally disparate, the authors instead suggest that they can be viewed as two extremes on the same continuum of bias-variance and that the way humans actually perform categorization lies somewhere in the middle of this continuum. Their attempt to connect psychological theories of human cognition to statistical machine learning concepts of bias and variance presented a novel perspective at the time of the papers writing (keep in mind that back in 2010, statistical ML was not as popular as it is nowadays, especially to those in non-technical areas such as psychologists).
The paper presents a number of conceptual and empirical contributions:
The characterization of exemplar theory as the low bias, high variance extreme, and prototype theory as the high bias, low variance extreme on the bias-variance continuum.
A class of experiments to evaluate human learners position on this continuum when the complexity of the training data varies has not been systematically attempted before.
The proposal of a locally regularized model that had the best fit for human performance and therefore constitutes a reasonable explanation for how humans perform categorization.
The experiment reported in the paper has the following phases:
Generate data at different levels of complexity.  The authors first constructed five bivariate Gaussian mixtures, p1(x, y), p2(x, y), , p5(x, y), where pK(x, y) consists of K components (Equation 1). In this way, K ranges from 1 to 5 and denotes the complexity of the underlying distribution. For each K, the authors then generated ship flag images, each with a pre-defined label  either belonging to a pirate ship (positive) or a friendly ship (negative)  and having two quasi-continuous features, the width of the inner black rectangle and the orientation of the sword (Figure 4). These two features were generated from either the distribution with pdf pK(x, y) for positive images or 1 - pK(x, y) for negative images.
Obtain human categorization of the generated data. 13 undergraduate students were recruited for the study. Subjects were shown a sequence of flags that may belong to either a pirate ship or a friendly ship, and the flag features were sampled from one of the five Gaussian distributions in step 1. They were then asked to learn the categorization by first attempting to classify each flag on their own, then seeing feedback on the correctness of their answer, and then repeating these steps with the next flag.
Build models that represent the exemplar approach, prototype approach, and locally regularized context approach. The exemplar model is called GCM and is denoted in Equation 5. The prototype model is denoted in equations 6 and 7. The locally regularized model assumes the same functional form as the exemplar model, but the sensitivity parameter c (whose high value corresponds to more cexemplar-liked and the low value corresponds to more cprototype-liked) is modulated locally, i.e., its value is set independently at each partition of the feature space.
Fit the models to subject data. The fitted parameters are optimized to fit the ensemble of each subjects responses. This helps answer the question: as the complexity K varies, which models best reflect the human performance in this flag classification task?
Fit the models to concept data. The fitted parameters are optimized to maximize the likelihood of the training examples observed so far at each point in the experiment. In other words, the ground truth labels of the flags are used in this process instead of the human classifications like in the previous step. This helps answer the question: as the complexity K varies, which models performance is more closely correlated to human performance?
The important findings from the experiment are as follows:
Human subjects are proficient at categorizing simple concepts (K = 1), but their performance declines as the complexity of K increases, approaching random guessing at K = 4 or K = 5.
When fitting models to subject data, the exemplar model has a better fit than the prototype model across all complexity levels. At larger complexity levels (K = 4 or K = 5), the two models converge in performance, largely because they were fitted on human categorizations that were just random guesses.
When fitting models to concept data, the prototype models performance decreases much faster than human performance, whereas the exemplar models performance does not decrease fast enough to match human performance at higher complexity levels.
The locally regularized model, which represents a middle point in the bias-variance continuum, consistently fitted subject data better than the exemplar (low bias, high variance) and the prototype (high bias, low variance) model.
Circling back to the question of whether humans perform categorization by the prototype approach (compare a new object to an abstract prototype of each candidate category) or the exemplar approach (compare a new object to existing instances of each candidate category stored in memory), this papers finding suggests that humans adopt a middle ground. Humans dont assume there is only a single prototype for each concept but do not keep in memory a large number of exemplars for each candidate prototype either. Instead, they treat concepts as mixtures of several sub-concepts, each represented by a partition of the feature space with its own localized sensitivity parameter c.
From a cognitive standpoint, the paper shows that evaluation of human learning should be conducted at different levels of conceptual complexity. While theoretically disparate models, such as the exemplar model and prototype model, may have a similar fit with human learning on simple data, they quickly diverge at higher levels of complexity. Complexity should be systematically varied over a range of levels to reflect a comprehensive picture of general human learning.
From a machine learning standpoint, there remains the open question of whether machine learning should follow the process of human learning. While there have been attempts to connect the two, for example, with neural networks that replicate the neural structure of the brain, the similarities are shallow at best. Deep neural networks typically require a very large amount of training data, which is very different from how humans learn. In recent years, however, more attention has been paid to making machine learning more human-like, for example, by learning from a limited number of samples (few-shot learning and no-shot learning) or by increasing robustness to adversarial attacks. This paper shows yet another way that human learning can be connected to machine learning  while the bias/variance trade-off originates from statistical learning, it can also be used to explain the way humans perform categorization by balancing the performance accuracy and the number of sub-concepts that they can reasonably hold in memory.","Who are the paper's authors? Why are they qualified to write this paper?,Who is the audience of the paper?,Why is the paper\xe2\x80\x99s topic relevant at the time of its writing?,What is the paper\xe2\x80\x99s contribution? Which research gap is it trying to address?,Summary of the paper\xe2\x80\x99s experiments and findings.,What are the implications of the paper\xe2\x80\x99s findings?"
Advanced Natural Language Processing,Language Representation and Transformers,Traditional Sequence2Sequence Models,"Traditional sequence2sequence models transform an input sequence (source) to a new one (target), and both sequences can be of arbitrary lengths. They generally have an encoder-decoder architecture where both the encoder and decoder are recurrent neural networks with LSTM or GRU units.
The Encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or cthoughtd vector) of a fixed length. At a particular timestep, the encoder takes a word embedding and produces an output called the chidden state,d which is then fed as an input with the next word embedding at the next time step. The final output is the context vector which is then used by the decoder.
The Decoder is initialized with the context vector to emit the transformed output. At a particular timestep, the decoder takes the output from the last timestep (and the context vector for the first timestep) to generate the result one-word embedding at a time.
Figure 1: Traditional Sequence2Sequence model architecture.
A critical and apparent disadvantage of this fixed-length context vector design is the incapability to remember long sentences. Often it may  forgotten the first part of a long sequence once it completes processing the whole input.
Figure 2: Limitation of the traditional Sequence2Sequence model architecture.
The attention mechanism introduced in Bahdanau et al., 2015 tried to resolve this cbottleneck problemd.
Attention allows a model to focus on specific, most important parts of the sequence in the case of natural language processing or a vision model to concentrate visually on different regions of an image.
In the following example, when we see ceating,d we expect to encounter a food word very soon. The color term (dgreend) describes the food but is probably not related much to ceatingd directly.
Figure 3: Attention between words in a sequence.
In NLP, the attention mechanism in models try to imitate this behavior and provides a different amount of cattentiond to different parts of the text with respect to some reference element. We can explain the relationship between words in one sentence or in a close context.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.
In the sequence2sequence models with attention, at each decoder step, the model can decide which parts of the source are more important. In this setting, the encoder does not have to compress the whole source into a single context vector - it gives representations for all source tokens by passing the intermediate hidden states to the decoder (remember that each input RNN cell produces one hidden state vector for each input word). Through the training process, the model itself learns which input words to cattend tod at each step without the need to manually provide this information. The decoder weighs the encoder's hidden states to give higher importance to words  from the input sentence  that are most relevant to decoding the next word (of the output sentence). Adding attention to Seq2Seq RNN architectures perform better across multiple translation tasks than their counterparts without attention.
Figure 4: Attention in Seq2Seq RNN architectures.
Mathematically, attention in deep learning can be broadly interpreted as a vector of importance weights given to other elements in order to predict or infer one element, such as a word in a sentence. We estimate using the attention vector how strongly that target element is correlated with (or cattends tod) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.",
Model Evaluation,Metrics and Interpretation,Be Mindful of the End Goal,"Metrics are employed to objectively evaluate the performance of machine learning models. When selecting a metric, you should always have the end goal of the machine learning application in mind.
In practice, we are usually interested not just in making accurate predictions but also in using these predictions as part of a larger decision-making process. Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. The consequences of choosing a particular algorithm for a machine learning application are called the business impact. Despite being called cbusiness impactd, not losing track of the end goal is important in any scientific domain. This can be the high-level goal of avoiding traffic accidents or decreasing the number of hospital admissions. It could also be getting more users for your website, or having users spend more money in your shop. When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production in a real-life system.
In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidance capabilities of a self-driving car by just letting it drive around, without verifying it first; if your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using an evaluation metric that is easier to compute. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation and selection. The result of this evaluation might not be a single numberthe consequence of your algorithm could be that you have 10% more customers, but each customer will spend 15% lessbut it should capture the expected business impact of choosing one model over another.
This section covers a number of common metrics used in classification, regression, and clustering problems. It is imperative that you understand which metrics are suitable for which use cases because your choice of metric (which should be decided prior to model training) will have an impact on the entire subsequent pipeline. or example, you may get different best models in the model selection process with different metrics.",
Problem Identification and Solution Vision,Problem Identification,Evidence Value Proposition,"We use a framework to formulate questions that carefully define business needs and extract business objectives that can be met using data science techniques. This framework can help a data science team meet the expectations of the client and deliver a solution that meets the business needs. The Evidence Value Proposition (EVP) framework was developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology. This methodology, as shown in figure 1 below, shows five (5) steps guided by questions that help formulate business objectives.
Figure 1. Evidence Value Proposition Framework",
Collecting and Understanding Data,Ethics of Data Science,Module 8 Summary,"Data scientists have virtually unlimited access to data and analytical techniques with which to analyze that data. As data scientists, we should be thinking about whether we should do something just because it is technically possible.
Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.
Data governance defines how data is accessed and managed within an organization. It is beneficial because it provides a reliable and consistent view of enterprise-wide data. It ensures that there is a plan for improved quality of data, reduces the scourge of data silos, and improves data management overall.
Accountability is about all the little decisions made by a group of people who created a system at each step of the way. There is both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong - to design a system for someone whom we are responsible for.
Data scientists try their best to make predictions about the future based on the information in the present. In an important sense, all of a data scientist's work is bound up with information about the past. Data science involves making predictions and classifications and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. As a data scientist, it is important to watch out for this bias toward what is most prevalent or most ""normal"" about a given dataset.",
Model Evaluation,Metrics and Interpretation,Classification Evaluation Metrics,"A Confusion Matrix (contingency table) shows how well a classifier performs compared to the ground truth labels. It is often used in a binary classification setting and has the following components:
True Positives (TP) is the number of positive data points that are correctly predicted as positive.
True Negatives (TN) is the number of negative data points that are correctly predicted as negative.
False Positives (FP) is the number of negative data points that are incorrectly predicted as positive. This is also called the Type I error in statistics.
False Negatives (FN) is the number of positive data points that are incorrectly predicted as negative. This is also called the Type II error in statistics.
A trick for remembering these definitions is that the second term denotes what is predicted, and the first term denotes whether this prediction is correct (true) or incorrect (false). For example, false negative means the negative label is predicted but it is false (i.e., the ground truth label is positive).
Ground truth \\(y = 1\\)
Ground truth \\(y = 0\\)
Prediction \\(\\hat y = 1\\)
TP
FP
Prediction \\(\\hat y = 0\\)
FN
TN
In a multi-class classification setting with \\(K\\) categories, a similar confusion matrix with \\(K\\) rows and \\(K\\) columns can be constructed, where the entry at row \\(i\\) and column \\(j\\) denotes the number of instances that have ground-truth label \\(j\\) and are predicted as having label \\(i\\). In this case, the metrics TP, TN, FP, and FN are computed for each individual category. For example, if there are three categories A, B, and C, then the TP, TN, FP, and FN values for class A can be computed by treating A as positive and B, C together as negative.
Accuracy is the number of correctly classified instances, divided by the total number of instances in the dataset.
\\(\ext{Accuracy} = \\frac{TP+TN}{TP+FP+FN+TN}\\)
bbThis is an intuitive measure, but should only be used when there is an even distribution of ground truth labels. If the dataset is imbalanced (e.g., there are many more negative than positive data points), accuracy can easily be inflated even by simple models. For example, if a dataset has 90% negative instances and 10% positive instances, a naive model that always predicts negative labels can already achieve an accuracy of 0.9.
Recall, also known as sensitivity or true positive rate, denotes the fraction of all positive instances that are correctly classified as such:
\\(\ext{Recall} = \\frac{TP}{TP+FN}\\)
Recall is used when you want to optimize your model to detect positive instances as best as possible, potentially at the cost of many false positives. For example, cancer detection models may aim for high recall values because predicting healthy people as having cancer (false positive) is less costly than predicting people having cancer as healthy (false negative).
Specificity, also known as true negative rate, denotes the fraction of all negative instances that are correctly classified as such:
\\(\ext{Specificity} = \\frac{TN}{TN + FP}\\)
It can be interpreted similarly as recall, but in this case, you prioritize detecting negative instances as best as possible.
Prediction denotes the fraction of positive predictions whose ground truth label is also positive:
\\(\ext{Precision} = \\frac{TP}{TP + FP}\\)
Prediction is suitable when optimizing the confidence of the positive predictions in your model. For example, if the government decides to cover the health care cost of anyone who has cancer, they will choose a cancer prediction model with high precision, so that money is not wasted on false positives.
F1 score is the harmonic mean of precision and recall:
\\(\ext{F1} = 2 \\cdot \\frac{\ext{Precision} \imes \ext{Recall}}{\ext{Precision} + \ext{Recall}}\\)
Like accuracy, F1  provides a general measure of model performance without a bias for or against a certain type of error. The differences between accuracy and F1 score are as follows:
Accuracy is good for optimizing true positive and true negative, while F1 is good for optimizing false positive and false negative.
Accuracy is good for balanced datasets (with even distribution of the ground truth labels) while F1 is good for imbalanced datasets.
Matthews Correlation Coefficient is a correlation coefficient between the observed and predicted binary classification. A value of +1 means a perfect prediction, 0 indicates that the classifier did the same job as you would if you randomly guessed the label, and finally -1 means the classifier misclassified all observations. MCC is symmetric, meaning that no class is more important than another (if you switch the positive and negative labels, the value of MCC is unchanged).Resource: MCC in skikit-learn.
\\[\extrm {MCC} = \\frac{TP\imes TN - FP\imes FN}{\\sqrt{\\left(TP+FP\ight)\\left(TP+FN\ight)\\left(TN+FP\ight)\\left(TN+FN\ight)}}\\]
If your model outputs a probability value \\(\\hat y\\) that an input data point has a positive label, it can be evaluated by the logistic loss
\\(L(\\hat y, y) = y \\log(\\hat y) + (1 - y) \\log (1 - \\hat y)\\)
where \\(y\\) is the ground truth label. The logistic loss is a value between 0 and 1; the lower the loss, the better your model is. In contrast to the metrics introduced so far, the logistic loss is differentiable and often used as the target loss function during model training.
The ROC curve is a chart that shows the performance of a classifier by highlighting the true positive rate against the false-positive rate at certain thresholds. Alternatively, it can be considered as expressing the sensitivity as a function of the false-positive rate. Area Under the ROC Curve, otherwise known as AUC, measures the entire area underneath the ROC curve, and it is the measure of the classifier's ability to distinguish between classes. It also provides a measure of performance across different thresholds. The AUC measures how well predictions are ranked and the quality of the prediction. AUC might not be useful for certain scenarios, such as it does not tell you much about the ""cost of different errors,"" instead of giving similar weight to all errors. The general interpretation of the chart is that the higher the AUC, the better the model is at its task of distinguishing between classes, e.g., the model has predicted observations that are apples as apples and observations that are not apples as not apples.",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Introduction to Exploratory Data Analysis,"We've seen that statistical methods are descriptive or inferential. The purpose of descriptive statistics is to summarize data and to make it easier to assimilate the information. The exploratory data analysis (EDA) process comprises visualizing data to allow a data scientist or a data analyst to explore datasets to gain insights from the data. EDA uses non-graphical techniques and graphical techniques to explore the data. Non-graphical techniques include using summary statistics to describe the data, and graphical techniques are used to describe the frequency distribution of the dataset. Both techniques can be used to show the skew of the data distribution and the extreme outliers.
Summarizing data is dependent on the types of data present in your dataset. It is difficult to describe a large data set in its raw form and use specific techniques to summarize and describe the data, including Describing Central Tendency and Assessing Measures of Spread and Relationships.
One can use the location in the data space, the shape of the distribution, and the spread of the data in a dataset to understand its aggregate properties. some of the concepts below can seem like a review of a first course in Statistics, but one should pay attention to the reason for using these techniques in exploring the data. Furthermore, these concepts are important when using statistical inference to draw conclusions on an unknown population parameter.
Location. During the EDA process, one describes the data using a central value. The Mean, sometimes called the arithmetic average, is one such value and is the sum total of all observations divided by the number of observations in the data. The whole population of data may have a population mean value \\(\\mu\\), or if you are only exploring a (smaller) sample, you can talk about a sample mean \\(\\overline{x}\\) In addition to the standard arithmetic mean, there are also other central values such as the geometric mean, and harmonic mean.
The Median is the mid-value of a dataset. To compute a median value, one first sorts the data in ascending order. The median value in a dataset with an odd number of elements is the value in the middle. For example, for the (sorted) set {1, 3, 5, 7, 9}, the median will be 5. On the other hand, s the median of a dataset with an even number of elements observations is defined to be the average of the two middle values. For example, for the (sorted) set {1, 3, 5, 7, 9, 11}, the median is defined to be the average of 5 and 7 = 6.
Mode is the value that occurs most frequently in the dataset. A uni-modal variable is one that has just one mode, and a bimodal variable has two modes. If your data has more than two modes, it can be referred to as multi-modal. The mode is quite useful when summarizing categorical variables.
Percentile. You may remember this nifty word from your GRE scores or height and weight data from your health records. The percentile tells you the position of a value in the dataset. If someone is 175cm in height and she is in the 10th percentile of height measurement for her gender, it means that among all the height data collected for that gender, she is taller than 10% of those values. The 50th percentile is considered to be the median. Quartiles are values that split the data into quarters.
The are several measures to describe the spread, variability, or dispersion of a dataset
Range of a set of values in a dataset can be calculated by subtracting the minimum value in your dataset from the maximum value. Notice that the range only considers two values and ignores all other values of a variable.
Mean Absolute Deviation is the average distance between each value and the mean of a dataset., that is
\\[\\sum_i\\frac{\\mid x_i - \\mu\\mid}{N}\\]
where \\(N\\) is the number of values and \\(x_i\\) is the \\(i^{th}\\) value in the data set.
This measure of dispersion can tell you how values are spread out in a dataset and determine whether the mean is a useful indicator of the values within the data. The larger the mean absolute deviation, the more spread out the data. When working with time series forecasting methods, one uses the mean absolute deviation to measure the performance of a forecasting model. Variance, typically denoted by \\(\\sigma^2\\), is defined as the averaged square deviation of the values in a data set from the mean that is
\\[\\sigma^2 = \\sum_i\\frac{(x_i - \\mu)^2}{N}\\]
Standard deviation, \\(\\sigma\\), is simply the square root of the variance. It is the most commonly used measure of the amount of variation or dispersion of a set of values.
A low standard deviation tells you that the values are close to the mean, and a high standard deviation means there is a spread. As one performs exploratory data analysis and even while developing models, the importance of the standard deviation can not be overstated. Despite its mention as a way to summarize data, the standard deviation is also used to cmeasure the confidence in statistical conclusionsd and to draw statistical inference conclusions on data and hypotheses.
Interquartile Range (IQR), similar to the range, does not consider all observations when looking at the spread of values in a dataset. IQR describes 50% of values in your dataset when arranged in ascending order. The IQR is the difference between the values in Quartile 3 and the values in Quartile 1. You can use this measure to identify a value that is an outlier.
Shape. Now that you can explain the measures used to explore data by describing its central value and its spread from the mean, and identifying outliers, let us describe the distribution of a dataset and assess whether it is normally distributed. Normally distributed data is useful when making statistical inferences. How can we assess the distribution of our data:
Skewness measures the degree to which the distribution of data lacks symmetry. A dataset with 0 skewness is considered normally distributed. Data does not always have a skewness of 0; however, if you have found skewness to be between -0.5 and 0.5, you can ascertain that your data is symmetrical. If skewness is between -1 and -0.5 or 0.5 and 1, then your data is moderately skewed. If skewness is < -1 or > 1, your data is highly skewed.
Figure 1. Symmetrical Dataset with Skewness = 0 (Source: BPI Consulting LLC)
Kurtosis looks at the outliers within the distribution. This measure of shape will describe the distribution of data by showing whether the tails of the distribution are more or less extreme than the normal distribution.
Covariance describes the linear relationship between two variables in your sample or population data. Covariance can be negative, meaning your variables have a negative linear relationship, zero (0), meaning the variables have no linear relationship, or positive, meaning a positive linear relationship exists between the variables.
Correlation or correlation coefficient describes how strong the association between two variables, x, and y, is in terms of how closely the data follow a straight line trend. It is a different measure than covariance because it describes both the direction and strength of the linear relationship between the variables. The correlation value lies between -1 and 1: \\(\\left | r_{XY} \ight |\\leq 1\\)
The correlation equals 1 if \\(x_i=y_i\\) for all \\(i\\) and equals -1 if \\(x_i=-y_i\\) for all \\(i\\).
More generally, if the scatterplot of x and y is a straight line, then the correlation is either 1 or -1. If the line slopes upward, there is a positive relationship between x and y, and the correlation is 1. If the line slopes down, there is a negative relationship, and the correlation is -1. The closer the scatterplot is to a straight line, the closer the correlation is to 1 or -1.
A high correlation coefficient does not necessarily mean that the line has a steep slope; rather, it means that the points in the scatterplot fall very close to a straight line.
Figure 2. Scatterplots for Four Hypothetical Datasets.
Figure 2 gives additional examples of scatterplots and correlation. Figure 2a shows a strong positive linear relationship between these variables, and the correlation is 0.81. Figure 2b shows a strong negative relationship with a sample correlation of -0.81. Figure 2c shows a scatterplot with no evident relationship, and the correlation is zero. Figure 2d shows a clear relationship: As x increases, y initially increases but then decreases. Despite this discernable relationship between X and Y, the sample correlation is zero. the reason is that, for these data, small values of Y are associated with both large and small values of X. This final example emphasizes an important point: The correlation coefficient is a measure of linear association. There is a relationship in Figure 2d, but it is not linear.
One important note on correlation is that two variables having an association does not mean there is a causal relationship between them.","Summarizing Data,If the mean and median of your data set differ greatly, you should check that variable for outliers!"
Model Evaluation,Metrics and Interpretation,Model Interpretation Strategies,"As a data scientist, model interpretation means more than one thing to you and your clients, and in most cases, it will mean different things to both parties. A data scientist is interested in understanding the results of a task and how it can assist the client and their end-users in making decisions. A great resource by Marco Ribeiro explains end-user empowerment as the secret weapon to building trust in a model. The example given is of a doctor using a model to predict whether a patient has the flu or not. There is a middle ""man"" between the prediction and the explanation of the prediction. This explanation is what the decision-maker (doctor in this case) will use to make the decision on the right diagnosis and treatment.
Interpretability is important to data science and machine learning because it directly affects human decision-makers and their understanding of the predictions made by models. It is not enough to trust the predictions of a model based on prescribed metrics (which we cover in the next module). Instead, it is often important to know what is predicted and why the prediction was madeunderstanding the why will make the problem clearer and affect problem-solving for future challenges.
Doshi Velez & Kim (2017) have explained in great detail some of the reasons why interpretability is important, the most important being the ever-growing and unsatisfied curiosity of humans (and, by extension, our thirst for learning). Bias identification is another reason why interpretability is important. Why does a model grant loans to one person and not to another with similar credit scores and income? Detecting bias can also lead to better acceptance. Finally, the data scientist and machine learning engineers can debug and audit models when those models are easily interpretable.
Interpretability is not needed if a model does not have an impact of much significance or if the context in which it is applied has been extensively investigated (although this does not help with detecting bias. The studies conducted can still be laden with bias).
The next module is an overview of the assessments or metrics that typically concern you as the data scientist. These metrics are useful tools in deciding whether a model will be considered trustworthy.
Reading: Should you trust that model?
The authors of the above article proposed a technique to explain the predictions and usefulness of any machine learning model. They have tested this technique with a number of classifiers, including neural networks for text and image classification.
Local Surrogate Models""explain individual predictions of black box models.""
Shapley Value is concerned with explaining a prediction by assessing the importance of features to the task.
Additional Resource: Sara Hooker: The Myth of the Perfect Model
Throughout this course, you have learned about understanding your client's needs and developing and implementing the right analytic solution to meet those objectives. At this stage, we want to think through the interpretability of models. This will be helpful for fixing issues with the model and explaining why a model produced its results.
Interpretability is a very important research area in data science and machine learning. We want to explain why a model produces certain results and what happens when there are changes within a model, also known as explainability. Interpretability ensures that a data scientist can measure the effects of any trade-offs within a model.
Let us turn our attention to the accuracy of a model and how its results can proffer better solutions and decisions. As you know by now, errors can be the difference between a useful solution and a solution that will lead to loss of money and with how data science solutions are integrated into everyday life, lives. Accuracy can be defined as the measurement used to determine the best model for a task. If the model can properly generalize to new data, it will produce better results (such as predictions).
There are certain sectors that are restricted by laws and standards in their use of certain techniques in the banking and education industry. Some of these restrictions protect the consumers data and ensure that bias is not introduced into the decision-making process. As you read on the last page, the more we learn more about interpretability and employ interpretability strategies, these issues might become a thing of the past. Accuracy is very important in these sectors, and as we have learned, accuracy will typically lead to less interpretability. The big question is, ""how can we retain interpretability while improving accuracy?""
Hall (2016) has recommended the following steps:
Train black-box models and use them as benchmarks.
Use different regression techniques.
Use black-box models in the deployment process.
Train small interpretable ensemble models.
Create nonlinear predictors using black-box techniques.
Explain black box models better using variable importance measures.
Figure 1. Accuracy versus Interpretability (Source: Rane-20181)",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 5b: Framing Common Forms of Analytical Objectives,"Now that you have studied the elements of a properly framed analytical objective, we shift towards explaining three basic archetypes of hypotheses that will cover a fair amount of projects one encounters in data science. They are provided here as purely illustrative example instances of the general template on which you can base your own formulations.
Not all framings of analytic objectives will include every individual element, as some of them may not be necessary depending on the situation. In industry settings, the problem and task may be merged, and the added valuable functionality may be evident from a model that performs its function well. In academic settings, the overarching interest may be that of advancing state of the art in research, and hence the statement may either not include an explicit business objective or state it as a problem solution vision.
A constructive analytical objective states that it is, in principle, possible to develop a desired functionality from the available methods and data without the need to fully optimize its performance yet. One can think of it as a proof-of-concept or prototyping endeavor.
In order to increase sales from the companys online store (Business objective)
...we work towards increasing the click-through rate of its advertising through targeted content (Problem)
...by classifying website visitors into youth, middle-age, and senior demographics (Task)
...using supervised learning models on curated internal datasets (Method)
(Business objective omitted due to project being primarily research)
In order to enable more effective search of audio collections (Problem)
We demonstrate the feasibility of a system that retrieves audio pieces from short natural language descriptions of their sounds (Task)
using neural models on a dataset of short clips of classical music and their descriptions (Method)
towards developing suitable multi-modal audio-textual encoding (valuable functionality)
In scenarios where the feasibility of an analytical task has been established, projects may be targeted toward improvement over the state-of-the-art in some performance metrics by using innovative methods/features/data. This is typically the case if one works on leaderboard-type datasets where there are models.
The client is a logistics company that wants to speed up its automatic package sorting (Business objective)
We focus on the problem of handwritten address recognition from shipping label scans (Problem and Task)
We want to combine neural image recognition with language models on company-internal data (Method and Data)
To improve performance beyond the current model based on standard convolutional neural networks without language information (Valuable functionality)
(Business objective omitted due to project being primarily research)
For the task of span-based question answering from text (problem and task merged because span-based question answering is a common leaderboard task) 
We want to combine graph-based knowledge bases with neural attention models (Method)
To improve over state of the art performance on realistic news text (valuable functionality and data)
Exploratory objectives are typically formed when data is available that is related to a problem of interest but needs to be surveyed before it can be used in projects pursuing constructive or benchmarking objectives.
The client runs a complex semi-automatic manufacturing pipeline and wants to make it more efficient (Business objective)
Specifically, he would like to see whether some parts of the process statistically interdepend so that bottlenecks and critical components can be identified (Problem and Task)
We want to conduct a qualitative survey and basic statistical analysis on a dataset of production machinery sensor readings provided by the client (Methods and Data)
Towards identifying correlating events across the production process that can be used for process optimization.
(Business objective omitted due to project being primarily research)
The development of AI dialogue systems suffers from a lack of clear training signal of how satisfied the user is with the chat bots replies (Problem)
We want to conduct a sparse labeling of conversation quality and produce basic topic models for a dataset of chat protocols (Methods and Data)
In order to develop a per-topic quality scoring rubric for the eventual annotation of a larger dataset. (Task/Valuable Insight)","Constructive,Benchmarking,Exploratory\r"
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Bias-Variance Decomposition and Trade-off,"As we explore the different methods, we will dive deeper into calculating the error rate for the models trained using those methods.
The bias is a measure of how closely the model can capture the mapping function between inputs and outputs and measures the average accuracy of the model arising from erroneous assumptions in the learning algorithm. A high bias can cause an algorithm to miss the relevant relationships between features and target outputs (underfitting). The bias is always positive.
Low Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are weak.
High Bias indicates that assumptions regarding the functional form of the mapping of inputs to outputs are strong.
The variance of the model is the amount the performance of the model changes when it is trained on different training data. It is an error from sensitivity to small fluctuations in the training set and measures the average consistency of the model. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting). The variance is always positive.
Low Variance indicates that changes to the training dataset cause  small changes to the model.
High Variance indicates that  changes to the training dataset cause large changes to the model.
How big is the data-intrinsic noise? This error measures ambiguity due to your data distribution and feature representation. You can never beat this. It is a property of the data.
Variance refers to the amount by which \\(\\hat{f}\\) would change if you estimated it using a different training data set. Bias refers to the error that is introduced by approximating a real-life problem, which may be complicated by a simpler model. For example, real life does not present scenarios that have a simple linear relationship. This means linear regression will present some bias in the estimate of f.
When you decompose bias-variance, you will analyze an algorithm's ability to predict outcomes for data that your model has not seen. The bias-variance tradeoff is encountered while working with some supervised learning techniques. The premise is that your model will adequately learn the training data, and it should properly generalize well to new data.
As seen in the figure below, a supervised learning method that can represent training data well but experiences overfitting is considered a high variance method. A method with high bias will not adequately learn the training data, and this leads to underfitting. High variance models are typically more complex, and those with high bias tend to be simpler.
Bias-Variance Diagram.
Let us look at a very well deconstructed mathematical representation of Bias-Variance by IBM's Aditya Prasad:
Note that the bias and variance of an estimator are mathematically related to each other and also to the performance of the estimator. Let us define an estimators error at a test point as the cexpectedd squared difference between the true value and the estimators estimate.
Whenever an expected value is referenced, this means the expectation over all the possible models, trained individually over all the possible data samples. For any unseen test point \\(x_{0}\\), you will have:
\\[\\operatorname{Err}\\left(x_0\ight)=E\\left[\\left(Y-g\\left(x_0\ight)\ight)^2 \\mid X=x_0\ight]\\]
Referring to \\(f\\left(x_0\ight)\\) and \\(g\\left(x_0\ight)\\) as f and g, respectively and skipping the conditional on X:
\\[\\begin{aligned}&\\operatorname{Err}\\left(\\mathrm{x}_0\ight)=\\mathrm{E}\\left[\\left(\\mathrm{Y}-\\mathrm{g}\\left(\\mathrm{x}_0\ight)\ight)^2\ight] \\\\&=\\mathrm{E}\\left[(\\mathrm{f}+\\epsilon-\\mathrm{g})^2\ight] \\\\&=\\mathrm{E}\\left[\\epsilon^2\ight]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{g})^2\ight]+2 \\cdot \\mathrm{E}[(\\mathrm{f}-\\mathrm{g}) \\epsilon] \\\\&=\\mathrm{E}\\left[(\\epsilon-\\mathrm{o})^2\ight]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\ight]+2 \\cdot \\mathrm{E}[\\mathrm{f} \\epsilon]-2 \\cdot \\mathrm{E}[\\mathrm{g} \\epsilon] \\\\&=\\mathrm{E}\\left[(\\epsilon-\\mathrm{E}[\\epsilon])^2\ight]+\\mathrm{E}\\left[(\\mathrm{f}-\\mathrm{E}[\\mathrm{g}]+\\mathrm{E}[\\mathrm{g}]-\\mathrm{g})^2\ight]+\\mathrm{o}-\\mathrm{o} \\\\&=\\operatorname{Var}(\\epsilon)+\\mathrm{E}\\left[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])^2\ight]+\\mathrm{E}\\left[(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})^2\ight]+2 \\cdot \\mathrm{E}[(\\mathrm{g}-\\mathrm{E}[\\mathrm{g}])(\\mathrm{E}[\\mathrm{g}]-\\mathrm{f})] \\\\&=\\operatorname{Var}(\\epsilon)+\\operatorname{Var}(\\mathrm{g})+\\mathrm{Bias}(\\mathrm{g})^2+2 \\cdot\\left\\{\\mathrm{E}[\\mathrm{g}]^2-\\mathrm{E}[\\mathrm{gf}]-\\mathrm{E}[\\mathrm{g}]^2+\\mathrm{E}[\\mathrm{gf}]\ight\\} \\\\&=\\sigma^2+\\operatorname{Var}(\\mathrm{g})+\\operatorname{Bias}(\\mathrm{g})^2\\end{aligned}\\]
\\[\extit{Generalization Error} = \extit{Bias}^2 + \extit{Variance} + \extit{Irreducible Error}\\]
So, the error of the estimator at an unseen data sample \\(x_{0}\\) can be decomposed into the variance of the noise in the data, bias, and the variance of the estimator. This implies that both bias and variance are the sources of error in an estimator.
Reading: Bias-Variance Tradeoff.
bbThe bias and the variance of a models performance are connected.
Ideally, we would prefer a model with low bias and low variance, although in practice, this is very challenging. In fact, this could be described as the goal of applied machine learning for a given predictive modeling problem. Reducing the bias can easily be achieved by increasing the variance. Conversely, reducing the variance can easily be achieved by increasing the bias. This relationship is generally referred to as the bias-variance trade-off. A model will present a high error when there is high bias and also when there is overfitting, or there is high variance and low bias. The model can not generalize to new or unseen data. We want a model that is balanced between bias and variance to ensure the error is minimized.
The variation of Bias and Variance with the model complexity.
This is similar to the concept of overfitting and underfitting. More complex models overfit while the simplest models underfit.
As shown in the figure above, an ideal and balanced model is one that has a low bias and low variance. You can work against overfitting (high variance) with dimensionality reduction techniques. This way, the model is simplified. Trade-offs can be optimized using a technique that will be discussed on the next page, Cross-Validation. The figure below gives you a visual representation of bias-variance with the training dataset.
Bias-Variance and Training-Test Data.
Additional Reading: Bias-Variance Tradeoff.",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Levels of Representation in Natural Language Processing,"A language processing system will rely on different representation choices for capturing relevant aspects of the language input and output. These representations typically depend on the task and what is needed in downstream processing in the pipeline.
A typical classical NLP pipeline uses at least the representation levels, as shown in the following figure.
Phonetics is the study of speech sounds as physical entities (their articulation, acoustic properties, and how they are perceived), while phonology is the study of the organization and function of speech sounds as part of the grammar of a language. Knowledge of phonetics and phonology is pivotal for applications that require understanding or generating speech data, like digital voice assistants, text-to-speech generators, etc.
For instance, speech recognition systems analyze (representations of) waves of air pressure (originally) generated by a human speaking and classify segments of such waves into abstractions called phonemes. Sequences of such phonemes are then transcribed into orthographic symbols making up words taken into context, usually through language models.
Morphology is the study of word structures, especially how morphemes, which are the smallest units of linguistic representation that come together and makeup words that can then be used to satisfy the semantic and syntactic constraints of a sentence. Morphemes can themselves be meaningful words that can appear by themselves in the language (free morphemes)  or can be affixes that can only appear when combined with other morphemes (bound morphemes).
In many languages of the world, words typically consist of one or more morphemes, and these morphemes can combine in many different ways to build words (suffixation, prefixation, infixation, interdigitation, etc. A typical morphological takes in an orthographical representation of a word and generates a representation of all possible morphological interpretations of that word.  For instance, a word such as books can be segmented into morphemes as book+s, and then this segmentation can be interpreted as either book+Noun+Pl (the plural form of the noun book) or book+Verb+Pres+3PSg (third-person singular form of the present form of the verb (to) book).
A morphological representation does not necessarily capture all the information in a word (or sometimes in a sequence of words). The lexeme representation typically adds additional information to a word representation, such as the sense of the root word (e.g., when we use the word cbanks,d  are we referring to cbanks on the Wall Streetd or are we referring to the cbanks of the riverd? At this level, we also perhaps conjoin words that work together (e.g., look up or piss off) and treat those as a single lexeme.
As one may already guess, not every sequence of words constitutes a valid sentence in a natural language. Consider, for instance, the following sentences:
I want a flight to Tokyo
I want to fly to Tokyo
I found a flight to Tokyo
I found to fly to Tokyo
The first three look fine with our understanding of valid English sentences, but the last one does not.  Furthermore, we sort of know that in the first sentence, ctod goes with cTokyo,d cad goes with cflight,d and cto Tokyod goes with ca flightd and cId and ca flight to Tokyod go with cwant,d the main verb of the sentence.  Such relationships are hierarchical and can be captured with linguistic computational formalisms called grammars.
Grammars assign structure to valid sentences in a language. But at the syntax level, validity is only about the structure and not the meaning of a sentence.  For example, the sentence cColorless green ideas sleep furiouslyd is a syntactically perfectly valid sentence, but semantically it is nonsense.
The syntactic representation of sentences is hierarchical: two commonly used representations are constituency syntax trees based on grammar expressed using context-free grammar formalism rules and dependency trees based on lexical relationships between words.
For example, the following tree representation captures the structure of the sentence, cA boy with a flower sees a girl with a telecope.d The various symbols, such as NP (noun phrase) or VP (verb phrase), are names of various intermediate structure types as defined by the underlying grammar.
Here the structure is for the interpretation of this sentence where the boy is using the telescope to see the girl.
The sentence can also have the following tree representation:
This is for the interpretation where the girl is carrying a telescope!
This brings out another major issue in NLP:  there are usually a multiplicity of representations for almost all inputs (remember the two possible interpretations of cbooksd above, which need further context to resolve during actual processing). Rerouting such ambiguities at every level of linguistic representation is probably the hardest problem in NLP.
A more recently commonly used syntactic representation relies on dependency relationships between lexical items, forgoing any use of the intermediate structure or phrase types in the trees and representing lexical relations between headwords and dependents, with a label denoting the relation as shown here.
Here csawd is the main meaning carrier of the sentence. csawd has the subject ckidsd and a direct object, cbirds.d cfishd is related to cbirdsd as a prepositional object which itself is related to cwith,d which is a preposition.
Loosely speaking, this level represents the cmeaningd of a sentence, sometimes compositionally scaffolding the structure of a sentence as described by a syntactic representation. Early approaches to semantic representation have assumed rather discrete representations of entities, properties, and events in a cworld modeld and have employed formalisms such as formal logic to capture what is called the truth-conditional semantics of a sentence.  A sentence such as cEverybody has something they  like.d would be represented by a logical form such as \\(\\forall x \\exists y\\  likes(x, y)\\).  The true value of such a sentence can then be computed based on the description of the world model.
A less formal but potentially more useful approach to semantics has been flatter but still hierarchical representations using semantic roles. Such representations assign the same semantic representation to syntactically different sentences if those express essentially the same event.  For example, all these sentences:
Warren bought the stock.
Someone sold the stock to Warren.
The stock was bought by Warren.
are describing the same csellingd event where the buyer is Warren, stocks are sold, and the seller is not known or not expressed explicitly, but it is inherent.  Thus the semantic representation for these sentences will be the same.  There have been many similar approaches proposed along the same lines differing in the types of roles and granularity of how events are represented.
Much more recent approaches to semantic representation, especially in deep learning contexts, rely on embeddings computed by either running the embeddings of individual words through an encoder (e.g., in a machine translation system) or usually by even just adding up the embeddings of individual words to get a representation of the sentence.
Pragmatics deals with understanding how the context in an utterance is made, or a sentence is used to contribute to the overall meaning and communicative intent and which aspects of a context are relevant to the interpretation of the utterance of a sentence.  Such contextual information also includes intonation, physical gestures, and social identity.  For example, an utterance such as cCan you pass the salt? c in a dinner set is really not a question of someones ability to pass the salt but is rather interpreted as a gentle request.
Thus pragmatics requires representation of all aspects of the context, including the set of all propositions that all discourse participants in agree on for the purpose of going on with the discourse.
A sequence of natural language sentences incrementally describes a local model of entities and the (evolving) relations between them. This model is known as the discourse model, and we, as the understander of the text, interpret linguistic expressions in the sentences with respect to this mental model that the understander of the text builds incrementally as we read,  containing representations of the entities referred to in the text, their properties and the relations among them.  This mental model already assumes a jointly agreed world model (e.g., everyone cknowsd New York City or cBill Clintod), and one introduces entities that will be mentioned by naming them the first time they need to be mentioned and then as the text develops uses a variety of linguistic referring expressions to refer to these entities as needed.
Furthermore, not every possible sequence of sentences constitutes a meaningful discourse. Consider the following two sequences of sentences:
Eric is a pathetic programmer. He only knows Java. Worse still, he always optimizes the outermost loop first. However, the incompetence of his managers ensures him a steady, six-figure income.
Worse still, he always optimizes the outermost loop first. Eric is a pathetic programmer. However, the incompetence of his managers ensures him a steady, six-figure income.  He only knows Java.
Clearly, only the first of these cmakes sensed; the second is not something we are likely to see feel that while we probably understand each sentence, we have a feeling that the whole thing does not cmake sense.d
A sentence sequence has to exhibit hard-to-define properties to be interpreted as a discourse: They have to have cohesion and coherence. Cohesion refers to the degree to which two passages of speech/text are cheld togetherd by formal devices like shared words and discourse markers that indicate continuity or lack of continuity. On the other hand, coherence refers to the degree to which passages in a text have cmeaningful relationships.d
Recent work in NLP has been using a representational paradigm based on a real vector representation of words. Such representation represents not only the identity of words (as a lexicon would) but also their semantics by capturing aggregate contexts words appear in to represent word semantics.  The idea of such representations is actually quite old and goes back to what is known as the distributional hypothesis, first put forward in the 1950s.  This hypothesis basically states that cWords that occur in similar contexts tend to have similar meanings.d
Such representations have been instantiated with the notion of embeddings which can be computed directly from the distributions of words in large amounts of text using a variety of algorithms, such as word2vec or glove embedding algorithms. Recent NLP algorithms that make use of the meanings of words use embeddings. Basic embeddings can be static since the computations rely on the orthography of individual words. Thus words with multiple meanings, such as cbook,d cbank,d or cdown,d get an embedding that lumps the semantics of all different meanings into one vector. Recent large transformer models such as BERT can compute contextualized embedding from static embeddings as input when a sentence is an input.  These contextualized embeddings capture different uses of an ambiguous word and are typically different for each distinct user/meaning of a word.","Phonetic and Phonological Representations,Morphological Representation,Lexeme Representation,Syntactic Representation,Semantic Representation,Pragmatics,Discourse,Representation in Neural Models"
Data Science Project Planning,Design and Plan Overview,Working in a team: Exploiting Agile Scrum,"Many different frameworks can be used to adopt an efficient software development life cycle for your data science project. These methodologies have been tested and have established pros and cons so that teams dont have to spend too much time choosing among them. Agile Scrum is quite popular in major technology companies. It is important to remember that many factors affect the decision to choose a framework, including the time to complete a project, the number of team members, the cultural setting of the organization, etc. However, these frameworks are not rigid and can be modified to suit the teams interests and style of working.
Lets dive into how Agile is usually implemented in the industry and how to apply it to a data science project.
Lets first understand the roles in the Agile Scrum framework:
Scrum Master. The Scrum Master ensures that each sprint stays on track. In a project team, one of the team members can assume this role. In general, the Scrum Master need not be the most senior person or the team lead. Usually, a Scrum Master also helps to remove or resolve any issues or challenges that may come up.
Product owner. The role of the product owner is to define the goals of each sprint, manage and prioritize the team backlog, and be the voice of the customer. This role ensures that the team prioritizes work that will be useful for a user. In a project team, anyone in the team can take up his role.
Team members. The people on this team are the ones who execute the work in each sprint. These teams, usually of three to seven people, can be composed of different specialties and strengths, or they can be teams of people with the same job roles.
Stakeholders. This is an informational role only. The stakeholders should be kept up-to-date on the product and sprint goals, have the opportunity to review and approve work during a sprint and provide feedback during the sprint retrospective.
User stories: A user story is simply a high-level definition of a work request. It contains just enough information so the team can produce a reasonable estimate of the effort required to accomplish the request. This short, simple description is written from the users perspective and focuses on outlining what the stakeholder wants (their goals) and why.
For example, if you are building an application that guides a user with directions, here is an example of a user story:
As a user, I want to be able to navigate my way using the directions so that I can reach my destination.
As a user, I want to be able to change the destination so that I can reach a new destination.
Sprints:  Sprints are a short span of work, usually taking between one to three weeks to complete, where teams work on tasks determined in the sprint planning meeting. As you move forward, the idea is to repeat these sprints until your product is feature ready continuously. Once the sprint is over, you review the product, see what is and isnt working, make adjustments, and begin another sprint to improve the product or service. This feature helps you build products that generate value for the stakeholder.
Stand-up meetings: Daily stand-up meetings (typically under 10 minutes), also known as cdaily Scrum meetings,d are a great way to ensure everyone is on track and informed. These daily interactions are known as cstand upd because the participants are required to stay standing, helping to keep the meetings short and to the point. Regular sync-up helps in mitigating any new challenges early on.
Agile board: An Agile board helps your team track the progress of your project. This can be a whiteboard with sticky notes, a simple Kanban board, or a function within your project management software (like JIRA). If you love sticky notes, you can, for instance, consider MURAL to build your board virtually, and all pending activities can go onto this board.
Backlog: As project requests are added through your intake system, they become outstanding stories in the backlog. During Agile planning sessions, your team will estimate story points for each task. During sprint planning, stories in the backlog are moved into the sprint to be completed during the iteration. Managing your backlog is a vital role for project managers in an Agile environment. In a project team, this will be the teams collective effort.
Sprint retrospective:  This is a meeting where the team comes together to recognize what has worked and what has not worked. The team makes efforts to make sure the concerns are dealt with to ensure the smooth completion of the project.
Demonstration: This is when the team demonstrates a working product to the stakeholders in order to show them the value generated from the project and to keep them happy.",
Exploratory Data Analysis,Statistical Inference & Hypothesis Testing,Statistical Inference,"Statistical Inference is the process of drawing an informed conclusion about an aspect of your entire dataset using statistical methods. Those conclusions are typically drawn using exploratory data analysis or summary statistics. The goal of this process is to use probability theory to make inferences about your data. This is the first step of learning about the attributes of your population from the sample that you have drawn. Understanding statistical inference ensures that you analyze your data properly and eventually draw the right conclusions for decision-making purposes.
If you recall from a previous unit, you learned that the objective of your data science project could be to explore the data and gather insights from that exploratory exercise. You can use statistical inference to draw scientific conclusions and test hypotheses. The significance of a sample data set or descriptive statistics is often in question during the EDA process, but using statistical inference techniques can give significance to your conclusions from EDA. Statistical inference techniques are categorized under Estimation and Hypothesis Testing.
Voter preference is a variable that varies among voters. Likewise, the sample proportion voting for a given candidate is a variable. If a sample was randomly drawn from a larger population, the act of random sampling makes the sample itself a random variable. Before the sample is obtained, its value is unknown, and that value varies from sample to sample. If several random samples of size n=2705 each were selected, a certain predictable amount of variation would occur in the sample proportion values. This distribution is called a sampling distribution. The sampling distribution of a statistic is the probability distribution that specifies probabilities for the possible values the statistic can take.
Each sample statistic has a sampling distribution. There is a sampling distribution of a sample mean, a sampling distribution of a sample proportion, a sampling distribution of a sample median, and so forth. A sampling distribution is merely a type of probability distribution. A sampling distribution specifies probabilities not for individual observations but for possible values of a statistic computed from the observations. A sampling distribution allows us to calculate, for example, probabilities about the sample proportion of individuals who voted for the Republican in an exit poll. Before the voters are selected for the exit poll, this is a variable. It has a sampling distribution that describes the probabilities of the possible values.
Suppose a student decides to record her commuting times on various days. She selects these days at random from the school year, and her daily commuting time has the cumulative distribution function in Figure 1.
Figure 1. Cumulative Distribution Function of Commuting Time.
Because these days were selected at random, knowing the value of the commuting time on one of these randomly selected days provides no information about the commuting time on another of the days. That is because the days were selected at random, and the values of the commuting time on each of the different days are independently distributed random variables.
The situation described is an example of the simplest sampling scheme used in statistics, called simple random sampling, in which n objects are selected at random from a population (the population of commuting days) and each member of the population (each day) is equally likely to be included in the sample.
The n observations in the sample are denoted \\(Y_{1}\\), , \\(Y_{n}\\), where \\(Y_{1}\\) is the first observation, \\(Y_{2}\\) is the second observation, and so forth. In the commuting example, \\(Y_{1}\\) is the commuting time on the first of her n randomly selected days, and \\(Y_{i}\\) is the commuting time on the \\(i^{th}\\) of her randomly selected days.
Because the members of the population included in the sample are selected at random, the values of the observations \\(Y_{1}\\), , \\(Y_{n}\\) are themselves random. If different members of the population are chosen, their values of Y will differ. Thus the act of random sampling means that \\(Y_{1}\\), , \\(Y_{n}\\) can be treated as random variables. Before they are sampled, \\(Y_{1}\\), , \\(Y_{n}\\) can take on many possible values; after they are sampled, a specific value is recorded for each observation.
Because \\(Y_{1}\\), , \\(Y_{n}\\) are randomly drawn from the same population, the marginal distribution of \\(Y_{i}\\) is the same for each i = 1,.., n; this marginal distribution is the distribution of Y in the population being sampled. When \\(Y_{i}\\) has the same marginal distribution for i = 1,..., n, then \\(Y_{1}\\), , \\(Y_{n}\\), are said to be identically distributed.
Under simple random sampling, knowing the value of \\(Y_{1}\\) provides no information about \\(Y_{2}\\), so the conditional distribution of \\(Y_{2}\\) given \\(Y_{1}\\), is the same as the marginal distribution of \\(Y_{2}\\). In other words, under simple random sampling, \\(Y_{1}\\) is distributed independently of \\(Y_{2}\\), , \\(Y_{n}\\).
When \\(Y_{1}\\), , \\(Y_{n}\\) are drawn from the same distribution and are independently distributed, they are said to be independently and identically distributed (or i,i.d.).
The sample mean, \\(\\bar{y}\\), is a variable because its value varies from sample to sample. In practice, when we analyze data and find \\(\\bar{y}\\), we don't know how close it falls to the population mean \\(\\mu\\) because we do not know the value of \\(\\mu\\). Using information about the spread of the sampling distribution, though, we can predict how close it falls. For example, the sampling distribution might tell us that with high probability, \\(\\bar{y}\\) falls within 10 units of \\(\\mu\\).
For random samples, it fluctuates around the population mean \\(\\mu\\), sometimes being smaller and sometimes being larger. In fact, the mean of the sampling distribution of \\(\\bar{y}\\) equals \\(\\mu\\). If we repeatedly took samples, then, in the long run, the mean of the sample means would equal the population mean \\(\\mu\\). The spread of the sampling distribution of \\(\\bar{y}\\) is described by its standard deviation, which is called the standard error of \\(\\bar{y}\\). The standard error of \\(\\bar{y}\\) is denoted by \\(\\sigma _{\\bar{y}}\\).
For a random sample of size n, the standard error of \\(\\bar{y}\\) depends on n and the population standard deviation \\(\\sigma\\) by \\(\\sigma _{\\bar{y}}=\\frac{\\sigma }{\\sqrt{n}}\\).
Because of random sampling error, it is impossible to learn the exact value of the population mean of Y using only the information in a sample. However, it is possible to use data from a random sample to construct a set of values that contains the true population mean \\(\\mu _{y}\\) with a certain prespecified probability. Such a set is called a confidence set, and the prespecified probability that \\(\\mu _{y}\\) is contained in this set is called the confidence level. The confidence set for \\(\\mu _{y}\\) turns out to be all the possible values of the mean between a lower and an upper limit so that the confidence set is an interval, called a confidence interval.
Consider this example:
Consider that we are measuring the heights of 40 randomly selected male soccer players, our sample mean is 175cm. We calculate the standard deviation of the athletes' heights to be 20cm. Let us calculate the CI.
n = 40, mean = 175, s = 20.
You will decide on the CI to use (95%) and then find the z-value for the selected CI. A 95% CI means that 38 of the 40 confidence intervals will contain the true mean value.
The z-value for 95% CI is 1.960
We calculate the 175  1.960  20/\\(\\sqrt{40}\\)
175cm  6.20cm
Usually, the standard deviation for the population of interest is not known. In this case, the standard deviation is replaced by the estimated standard deviation s, also known as the standard error. Since the standard error is an estimate of the true value of the standard deviation, the sample mean follows the t-distribution with mean and standard deviation. The t-distribution is also described by its degrees of freedom. For a sample of size n, the t-distribution will have n-1 degrees of freedom. The notation for a t-distribution with k degrees of freedom is t(k). As the sample size n increases, the t-distribution becomes closer to the normal distribution since the standard error approaches the true standard deviation for large n.","Sampling Distribution,Random Sampling,I,I.D.,Standard Error,Confidence Interval,168.8cm to 181.2cm,Degrees of Freedom"
Advanced Natural Language Processing,Language Representation and Transformers,Transformer Architecture,"Both the encoder and the decoder units in a Transformer are made up of multiple individual encoders and decoders. The units are all identical in structure but they do not share weights.
Figure 6: Transformer architecture.
Since the input is not processed sequentially, we need a method to account for the order of words in the input sequence. For this, the transformer begins by adding a vector to each input embedding. These vectors follow a specific pattern that is either generated using a fixed function or learned, which helps the transformer  determine the position of each word or the distance between different words in the sequence.
Figure 7: Transformer Encoder Inputs.
Each of the position-encoded inputs is then passed into the encoding stack. Each encoder in the stack is broken down into two sub-layers, as shown in Figure 6. The inputs first flow through a self-attention layer  that helps the encoder look at other words in the input sentence as it encodes a specific word.
The self-attention layer begins by creating three vectors from each of the encoders input vectors. So for each word, it creates a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that are trained during the training process.
The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best-matched videos (values). The attention operation can be thought of as a retrieval process as well. The query vectors of a particular input \\(x_i\\) when multiplied with the keys of the other inputs (\\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\)) give weights that represent how much \\(x_i\\) attends to those other inputs. These weights are then normalized using a softmax layer and multiplied with the value vectors for \\(x_1 \\dots x_{i-1}, x_{i+1} \\dots x_n\\). The products are then added to get a weighted sum of attention values. In essence, each token (query) is free to take as much information using the dot-product mechanism from the other words (values), and it can pay as much or as little attention to the other words as it likes by weighting the other words with keys.
The outputs of the self-attention layer are then fed into a feed-forward neural network. The exact same feed-forward network is independently applied to each position of the input sequence. The self-attention and feed-forward sublayers in each encoder have a residual connection around them and are followed by a layer-normalization step. The final results of the first encoder are then fed into the next one, and the process continues till the last encoder.
Figure 8: Transformer Encoder Architecture.
Figure 9: Transformer Encoder-Decoder.
Decoding is shown in Figure 9. Decoding looks very similar to encoding in terms of the components. However, unlike encoding, it works sequentially. Decoders can only pay attention to the words in the sentence that they have already generated (so, only the words before the word currently being generated). For example, when we have predicted the first three target words, we give them to the decoders, which then, along with all the outputs of the last encoder, try to predict the fourth word.
In the figure, the input of the first decoder (from the bottom) are the embeddings of the output sequence (added with the positional embedding) already generated. Future positions are masked (by setting them to -inf). This way, the self-attention layer is only allowed to attend to earlier positions in the output sequence. For the first timestamp, everything is masked, and the masks are removed one by one in successive iterations as the output sequence words are generated.
After applying self-attention and then layer normalization, the results are fed into an encoder-decoder attention layer (which is something different from the encoder stack). This block tries to find the cross-attention between the encoded input sequence and the generated output sequence till a particular timestep. The entire output of the top encoder of the encoder stack is transformed into a set of attention vectors K and V for use in this layer. This layer helps the decoder focus on appropriate places in the input sequence at a given timestep to generate the next output. The cEncoder-Decoder Attentiond layer works just like multiheaded self-attention, except it creates its queries from the layer below it and takes the keys and values from the output of the encoder stack. Output from the encoder-decoder attention layer then goes through feed-forward and layer normalization layers. All these components are contained in a single decoder block. The decoding results are then bubbled up through all the other decoders in the stack. The final decoded result from the last decoder is then fed into a linear layer which maps the output to a vector of the size of the vocabulary, which is then normalized using the softmax layer turning the values into probabilities. The word corresponding to the highest probability becomes the next predicted word in the output sequence.
At a high level, the following steps repeat the process until a special symbol is reached, indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did to produce successive output tokens.
Here are some additional sources for more details on Transformers.
The Annotated Transformer
The Illustrated Transformer",
Problem Identification and Solution Vision,Problem Identification,Module 1 Summary,"A data science project does not begin with building models; one must consider the needs of the client and set objectives to meet those needs. A data scientist must approach a project with a methodology. Similar to scientific research, a data science project follows frameworks that will guide the problem identification process. A data science team will work with a client to understand the business need(s). Those needs will are then translated to data science tasks.
Business objectives will be defined by the company to help meet business needs. These objectives are stated fairly concretely and often have time periods associated with them, after which they will be considered reached (in case of success), not reached (in case of failure). Given a businesss needs and objectives, one can now evaluate whether data science methods can be used to facilitate the company's efforts to meet them.
A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the companys domain experts. Once the team has familiarized themselves with the problem as well as the available data and resources, they will work with the client to develop a solution vision. Finally, the data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.
This module introduced you to the Evidence Value Proposition framework (EVP). The EVP is a suitable framework that can be used to ensure that defined business objectives are met and has been developed to help determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.
When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Applications of Natural Language Processing,"Natural language processing has numerous practical applications within todays world, such as question-answering systems or language translators. Here are a few of the more well-known applications:
As the name suggests, sentiment analysis is used to identify the sentiments in a fragment of a natural language text. Expressions like sarcasm, threat, exclamation, etc., are often very hard to be recognized by the computer. A use case of sentiment analysis is companies identify the opinion and sentiments of their customers based on online reviews and feedback.
Information Extraction is the task of extracting pre-specified types of facts from written texts or speech transcripts and converting them into structured representations (e.g., databases). An example is when your email extracts only the relevant data of a meeting invite for you to add to your Calendar.
Machine Translation is the procedure of automatically converting the text from one language to another language while keeping the meaning intact. In earlier days, machine translation systems were dictionary- and rule-based systems, and they saw very limited success. However, starting in mid 199os first due to statistical models based on large parallel corpora and then to deep neural networks coupled with special processing hardware such as GPUs and TPU (Tensor Processing Units), machine translation has become fairly accurate in converting text from one language to another. I
Natural language generation (NLG) is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services. NLG also encompasses text summarization capabilities that generate summaries from one or more documents while maintaining the integrity of the information. Natural language generation systems have evolved over time from static text generation with the application of hidden Markov chains, recurrent neural networks, and transformers to enable more dynamic text generation in real time.
Topic modeling refers to the task of identifying topics that best describe a set of documents. These topics are not predefined and will only emerge during the topic modeling process, which makes it an unsupervised approach. We will delve into the implementation of one of the popular topic modeling techniques known as Latent Dirichlet Allocation (LDA) in Project 5.","1. Sentiment Analysis,2. Information Extraction,3. Machine Translation,4. Natural Language Generation,5. Topic Modeling"
Data Science Project Planning,Requirements Gathering,Module 5 Summary,"The difference between a traditional IT project and a Data Science project is the focus on the analytical data, model, and operations requirements.
Requirements are capabilities needed to achieve an objective. Requirements are gathered from stakeholders of a project, including the client, end-users, and the data science team. Good requirements are correct, complete, unambiguous, verifiable, measurable, and traceable.
Requirements analysis involves activities that identify business needs, evaluate the feasibility of solutions, and establish constraints.
Requirements management is an iterative set of activities to help ensure that elicitation, documentation, refinement, and changes of requirements are done during the data science project life cycle.
Requirements gathering involves fact-finding that describes the current state of the business and the business objectives that inform the proposed solution. Requirements gathering is done to ensure that business needs and solutions are well defined.
There are three types of requirements highlighted in this unit. The business, the system, and user, and the solution requirements.
Poorly documented requirements lead to assumptions and miscommunications between the project team and client stakeholders. Communication issues can lead to defects in solutions and unmet expectations. These are issues that a project team should avoid. This can be achieved by defining requirements in the early stages of solution development. Requirements are well-documented statements that define the needs of the users and systems that should be implemented to address a business need. The requirements gathering process can become complex if it is not managed properly. This process is managed by the business analyst.
There are different types of requirements including business and solution requirements. Business requirements describe the cwhyd behind the implementation of a solution, while the user requirements describe the tasks that users will be able to perform with the system, and the solution requirements specify the behavior of the system and its characteristics.
Similar to a traditional IT project, it is a best practice to collect requirements for analytic projects. The requirements for an analytic project include determining how data, models, and results or outputs of the models will support meeting the analytic and business objectives.
There are techniques that are used to gather requirements including conducting interviews, brainstorming sessions, and facilitated workshops, other techniques used in addition to the above listed include document analysis and observations. Once requirements are elicited, they are analyzed and translated into written requirements for understanding. Finally, requirements are validated by the project and client team to signal the commencement of solution development.",
Deep Learning and Model Deployment,CPU vs. GPU,Quiz 8,,
Collecting and Understanding Data,Ethics of Data Science,Introduction to Data Science Ethics,"For much of human civilization, people did the best they could to thrive under challenging circumstances. These were times when there werent many rules to follow, so people did and took what they could and werent exactly ethical, but arguably this ethic was necessary to survive difficult times. That is also where we see the situation of the tragedy of the commons, mentioned in the previous module. We also saw that the tragedy could be overcome  that is, we can begin to produce a better civilization for all. In this improved civilization, we no longer do things just because we can or feel we have to. We now have to follow rules of ethics that give us more benefits collectively than they cost us.
We have virtually unlimited access to data as data scientists today, and we have unprecedented analytical techniques with which to analyze that data. So the question we should be thinking about is whether we should do something just because it is technically possible. Are there things that are possible to do but which we can agree would not be right to do? This may sound strange, but it is a question that modern science has already begun considering and continues to do so.
Think about how data science creates impacts and the tremendous excitement about how data science applications provide better ways of doing things in society. Think about the power you have as a data scientist and how that power influences peoples lives, potentially for the better. With that great power comes great responsibility. It is crucial as data scientists that we be responsible when exercising that great power.
The difficult thing about being an ethical data scientist is not about understanding ethics. It is the connection to how one applies ethical principles to data science practice; it is about doing cgoodd or ethical data science.
By Rijksdienst voor het Cultureel Erfgoed, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=23391637
This is the Amsterdam Civil Registry Office in the Netherlands. Buildings like these were record offices that contained cabinets full of records about the Dutch population. During World War II, following the Nazi invasion and occupation of the Netherlands in 1940, the Nazis took over the registry office and used these records to identify members of the Dutch resistance. The records about their birth were used as a potent weapon to identify members of the Dutch population who were Jewish and to therefore decide whom to send to extermination camps.
The Dutch resistance saw that the mere existence of the Civil Registry Office was dangerous in the hands of the Nazi war machine. So they planned a bombing of the Office in 1943 in an effort to destroy records that would help the Nazis identify Jews. Figure 2 is of a plaque in Amsterdam that lists the names of the people in the Dutch Resistance who bombed this record office. They were the ones that identified links between these records and what was happening in the war, took actions to prevent it, and paid the price. They were captured and executed.
By Design: Willem Sandberg. Photographer: Frans Willemsen - Own work, CC BY-SA 3.0 nl, https://commons.wikimedia.org/w/index.php?curid=32468279
We hope this is a good place to start the conversation about doing good data science and the motivation to do so. In the rest of the module, we hope to convince you, if youre not already convinced, that the story of the 1943 bombing of the Amsterdam civil registry office is highly relevant to the work you do as a data scientist. While the data science work you do may be mundane by comparison and not involve a life-or-death decision, ethical data science practices matter to everyday things that affect all of us. The goal of this module is to help you participate in the ethical debates that you will face as a data scientist, infuse your data science work with ethical principles, and inform you to be thoughtful, deliberate, and ethical  the kind of data scientists that we all hope that youre going to be.
[Required Reading]
Please read chapter 3 from Loukides, Mason, H., & Patil, D. (2018). Ethics and Data Science (1st edition). OReilly Media, Inc.
Note: When prompted to select institution, select ""Not listed? Click here"" and enter your CMU email address to access content.",
Collecting and Understanding Data,Data Collection,Study Design,"To connect the studys objectives with the data gathered, data scientists need to come up with a sound study design for execution while ensuring the validity and integrity of the inference. There is a range of study design options, from the exploratory analysis of data that is organically available, to those very highly planned efforts for collecting and analyzing data aligned to a specific question. Study design encompasses everything in preparation for data-driven research. A study can fall into multiple categories of study designs.
Exploratory
Confirmatory
Bottom-up (without a pre-specified question)
Can lead to knowledge discovery or new theory
Uses inductive logic and the logic of discovery
Top-down (with a specified falsifiable hypothesis)
Tests an existing theory
Use deductive logic, the logic of justification, and reconstructed logic
Comparative
Non-Comparative
Contrasts one subject with another based on certain measures.
Estimates or predicts absolute outcomes of the certain subject matter without explicitly making a comparison with its counterpart.
Experimental
Observational
The purpose of experiments is to compare responses of subjects to some outcome measures, under different conditions. Those conditions are levels of a variable that can influence the outcomes. The data scientist has the experimental control of being able to assign subjects to the conditions.
To conduct experiments, there is often a plan of manipulation or assignment of the subjects to treatment. These are called experimental designs.
Good experimental designs use randomization to determine which treatment a subject receives.
The purpose of observational studies is to draw inferences about the effect of an cexposured or intervention on subjects, where the assignment of subjects to groups is observed rather than manipulated (e.g., through randomization) by the data scientist.
Observational research involves the direct observation of individuals in their natural settings. As such, who does or does not receive intervention is determined by individual preferences, practice patterns, or policy decisions.
It is therefore important for readers of observational research to consider if alternative explanations for study results exist.
Establishing causal inference is central to science. However, it is not possible to establish cause and effect relationships definitively with a nonexperimental study, whether it be an observational study with an available sample or a sample survey using random sampling. With an observational study, there is a strong possibility that the sample is not representative of the population. With an observational study or a survey, there is always the possibility that some unmeasured variable could be responsible for patterns observed in the data. With a well-designed experiment that randomly assigns subjects to treatments, those treatments should roughly balance any unmeasured variables. Because a randomized experiment balances the groups being compared on other factors, it is possible to study causal inference more accurately with an experiment than with an observational study. Observational studies are more passive and self-selected as subjects are exposed to a condition rather than being assigned. However, when the random assignment in experimental design is impractical or unethical, observational studies are the next best bet.
In general, more data is better because more data yields more information. However, the manner in which data is collected is arguably more important than the availability of that data itself. If the data that is being collected has too little information to inform about the questions of interest, then the resulting conclusions may not be very informative. Power analysis is a process by which we can assess whether a given study design is likely to yield meaningful findings. Bias is another issue that can result from an unfair sampling of a population, or when measurements are systematically inaccurate on average. In the next section, we will address the issue of bias and validity of a study.",
Exploratory Data Analysis,Performing Exploratory Data Analysis,Statistics & Data Types,"Statistics is the science of using data to learn about the world around us. In this course, we use the term ""statistics"" in the broad sense to refer to methods for obtaining and analyzing data. Specifically, statistics provides methods for:
Design: Planning how to gather data for research studies,
Description: Summarizing the data, and
Inference: Making predictions based on the data.
Design refers to planning how to obtain the data. For a survey, for example, the design aspects would specify how to select the people to interview and would construct the questionnaire to administer.
Description refers to summarizing data to help understand the information they provide. For example, an analysis of the number of siblings based on a survey might start with a list of the number reported for each of the people who responded to that question that year. The raw data are a complete listing of observations, person-by-person. These are not easy to comprehend, however. We get bogged down in numbers. For the presentation of results, instead of listing all observations, we could summarize the data with a graph or table showing the percentages of respondents reporting one sibling, two siblings, three siblings, and so on. Alternatively, we could just report the average number of siblings, lets say 3, or the most common response, lets say 2. Graphs, tables, and numerical summaries are called descriptive statistics.
Inference refers to making predictions based on data. For instance, for the survey data on the number of siblings, suppose 15.6% reported having no siblings. Can we use this information to predict the percentage of all adults in the U.S. at that time who is an only child? Predictions made using data are called statistical inferences. We will explore statistical inference in more detail in the upcoming section.
Description and inference are the two types of statistical analysis - ways of analyzing the data. Data scientists use descriptive and inferential statistics to answer questions about data. For instance, ""Is changing the website layout associated with an increase in visitors?"" or ""Does student performance in schools depends on the amount of money spent per student, the size of the classes, or the teachers' salaries?""
Statistical methods help us determine the factors that explain the variability among subjects. Any characteristic we can measure for each subject is called a variable. The name reflects the values of the characteristics that vary among subjects. In data science, we usually see the term feature used interchangeably with variable.
Different subjects may have different values of variables: the values the variable takes to form the measurement scale. These measurement scales result in data exhibiting different types. In this section, we discuss the measurement scales of data analogous to data types. The valid values for a feature depending on its data type. Thus data types affect the methods we will choose to develop an analytic solution.
When discussing data types in data science, data is typically categorized as numeric or categorical and classified as one of the four measurement scale types. Quantitative data is represented as continuous or discrete values, while categorical data can be nominal or ordinal values.
Discrete: I have one sibling.
Ordinal: I can rate my customer service experience at the grocery store as Good.
Continuous: It takes 1 hour and 20 minutes to get to school.
Nominal: What is your hair color? Brown.
Data is quantitative when the measurement scale has numerical values. The values represent different magnitudes of the variable. Examples of quantitative variables are income, the number of siblings, age, the number of years of education completed, etc.
Meanwhile, data is categorical when the measurement scale is a set of categories. For example, marital status, with categories (single, married, divorced, widowed), is categorical. For Americans, states are categorical, with the categories Pennsylvania, Montana, Utah, and so on; for Canadians, the province of residence is categorical, with the categories Alberta, British Columbia, and so on. Other categorical data are whether employed (yes, no), favorite type of music (classical, country, folk, jazz, rock), and political party preference.
For categorical data, distinct categories differ in quality, not in numerical magnitude. Categorical data are often called qualitative. We distinguish between categorical and quantitative data because different analytical methods apply to each type. For example, the average is a statistical summary of a quantitative variable because it captures numerical values. It's possible to find the average for quantitative data such as income but not for categorical data such as religious affiliation or favorite type of music.
Structured Data is organized facts that are presented in fixed formats and are easy to extract. This data can be stored in spreadsheets, relational databases, and other repositories in, for example, a row and column format. Unstructured data is most difficult to extract. It is not easily stored in typical relational databases and spreadsheets because it does not neatly fit in the row and column structure or cannot be maintained in formats that are uniform. Text, multimedia files, and log files from servers are examples of unstructured data. New generation database frameworks, also known as NoSQL databases, have been developed specifically to handle unstructured data. Unlike structured data, unstructured data can be stored without a predefined schema.
Data can also be classified as internal data, which is data collected and/or controlled by an organization. An example would be personnel data collected and stored by the human resources department. We also have external data, data that is collected from sources outside of an organization. Census data and data gathered from credit reporting agencies are examples of external data.
The different types of data explored earlier are collected through different sources. Primary data sources include data that is collected and processed by an organization and housed internally. Secondary data sources include data that is gathered from sources external to an organization. Keep in mind that internal data can come from a primary or secondary data source and that an organization's data governance framework affects data that is collected from primary and secondary sources as long as they are used by the organization.","Quantitative Data,Categorical Data,Quantitative and Categorical Data,Structured and Unstructured Data,Internal and External Data,Data Sources"
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Module 17 Summary,This is a new page with empty contents.,
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Module 22 Summary,This is a new page with empty contents.,
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 1: Actually Understanding the Business Objective,"The ability to study and understand a projects business objective in sufficient depth in order to maximize the benefit from leveraging data is a critical skill for data scientists. This skill can be thought of as having three aspects:
Engagement with the client and research about the specific circumstances of the clients business.
First, one engages with the client by learning about their business or organization, its customers/clients, the market and its competitors, and the general state of the art in achieving business objectives of its kind. Often this may involve a workshop-like event between the business unit and the data science team, which may lead to a regular communication schedule (e.g., calls, in-person meetings, reports).
Engagement with the client and research around the general domain in which the clients business operates .
Second, preparation will often require additional research around the clients general domain and market using the internet, but also from subject area books and academic publications. A data scientist does not need to become an expert herself in the respective area or market, but she should be convinced that the domain is sufficiently understood and that she can intuitively explain why and how the business wants to reach the objective, as well as engage with the clients experts in a serious conversation about the topic without frequently stumbling over misunderstandings.
Spending continuous efforts to maintain alignment of the ongoing project with changes or new information in both the general domain and specific circumstances.
Third, once the project has progressed to the design and implementation of the analytical models and experiments, the alignment to the business objective must be maintained. A common pitfall is that, once an intriguing machine learning aspect of the project is discovered (e.g., a dataset suitable for deep neural networks), it may appear to be more rewarding to invest resources there and neglect tasks that appear mundane or laborious (e.g., data collection, cleaning, or error analysis) but are equally important for the business objective. In extreme cases, for example, this can lead to the application of overly complex methods to unsuitable datasets, ultimately resulting in project failure. Another bad outcome is the development of models that solve problems the client does not actually have. One of the purposes of this course is to make you appreciate that a data science projects success needs to be gauged in typically two ways: experimental outcomes as measured by technical performance metrics and their contribution to the greater effort of reaching a business objective.
Familiarizing oneself with different substantive domains in order to deliver good data science work takes patience and attention to detail. It is a lifelong learning process but is also one of the most rewarding aspects of this profession. It is not uncommon for data scientists to have some formal education or professional experience in specific disciplines (e.g., medicine, biology, finance, or business), which enables them to do highly effective work in that area. Similarly, trained data scientists may decide to specialize in a certain field because the depth of their expertise makes them sought after consultants, or because they consider it personally satisfying.",
Model Evaluation,Metrics and Interpretation,Regression Evaluation Metrics,"Regression tasks will predict the state of a target variable based on other input variables. As a quick reminder, the target variables in these tasks are continuous values. Let us discuss the metrics that are used to evaluate the outcome of regression tasks:
R-squared, also known as the Coefficient of Determination, is the proportion of the variance in the outcome variable that can be predicted using the predictor variables. It tells you how well-observed outcomes are replicated by a model based on the proportion of total variation of outcomes explained by the model. When interpreting R-squared in a simple linear regression model, it is considered to be the square of the sample correlation coefficient between the outcomes and predictors (r2). If R2 is 0.5, this would mean that 50% of the variation in the dependent variable is explained by the predictor variables. A good model has a high R2.
When there are multiple regressors, then R2 is the square of the coefficient of multiple correlations (""correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables""). This metric will provide an indication of how well new data will be predicted by the model.
Adjusted R2 is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of the target field). The result is then subtracted from 1. It identifies the percentage of variance in the target field that is explained by the input or inputs. Adjusted R2 is always less than or equal to R2. A value of 1 indicates a model that perfectly predicts values in the target field. A value that is less than or equal to 0 indicates a model that has no predictive value. In the real world, adjusted R2 lies between these values.
A weakness of R2 is that it cannot determine whether the coefficient estimates and predictions might be biased. The R2 will typically increase when a predictor is added to a model, and as you add more predictors, your model will likely overfit and result in a high R2. Adjusted R2 attempts to correct this overestimation. Adjusted R2 will only increase if newly added predictors improve the model more than expected and decrease when a predictor or predictors improve the model less than expected. It tells you the percentage of variation explained by predictors that will have an effect on the outcome. Basically, adjusted R2 will calculate R2 from the predictors that have a significant impact on the model. Adjusted R2 is best used to compare models with different numbers of predictors.
Mean Squared Error (MSE) is a measure of the quality of an estimator or a predictor (depending on context). A value closer to zero is always best. Mean squared error refers to the unbiased estimate of error variance: the residual sum of squares divided by the number of degrees of freedom.
Mean Absolute Error (MAE) is the measure of errors between paired observations and is computed as the average of all absolute errors (the absolute error is the absolute value of the difference between a predicted value and the actual value). This metric is used to measure accuracy. You use the Mean Absolute Percentage Error (MAPE) to compare predictions and interpret whether the size of an error is small or large. The MAPE is a model evaluation technique that clearly interprets the relative error.
Root Mean Squared Error (RMSE) gives weight to large errors since it squares the errors before computing the mean. The RMSE is computed by first determining the residuals (difference between the actual and predicted y values). Residuals are squared and the squares are averaged. Finally, the square root of the averaged squares will result in the RMSE. An easier way to think about the formula is: ""square root of (1-r2) multiplied by the standard deviation of y.
Resource: Regression Metrics-scikit-learn",
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Introduction to Natural Language Processing,"Natural Language Processing (NLP hereafter) is a subfield of computer science aiming to  build  systems that:
Enable human-computer communication
Enable/enrich human-human communication
Perform tasks requiring the use of human language faculty, more efficiently and more correctly.
Here are some applications of NLP in these three areas:
Human-computer communication
Dictating email messages
Listening to email messages
With proper pronunciation, tone, and stress!
Using human language to give commands to the operating system
Human-human communication
Real-time text and speech translation
Summarizing meeting conversations
Enabling communication with people with hearing/vision impairments
Improving the efficiency of tasks requiring human language faculty
Correcting typing/grammatical errors
Question answering
Answering e-mails automatically
Searching large document databases
Here are some (and definitely not a comprehensive set of) applications of  NLP in a diverse set of areas:
Information extraction (IE), especially copend IE:  Given a text (or a collection of texts), find out who did what to whom, where, when, how, why, with what, in exchange for what, etc.
Document classification: Classifying a document into topic areas (e.g., news, sports, business, entertainment, sports, etc.), classifying an email as spam or not, or as important or not.
Question answering beyond finding the documents relevant to the question and instead delivering the actual answer.
Conversational Agents (e.g., Siri, Alexa, Google Assistant): Holding a typically multi-stage turn-taking goal-driven conversation with a user, going beyond question answering and helping with other tasks such as making appointments, helping with shopping or entertainment options, etc.
Image (and eventually video) understanding:  Expressing the pictorial or video content in human language (e.g., image captioning or verbalizing a football match action in a TV program setting).
Text (including text on images) and speech translation with additional applications in video call transcription and translation, real-time lecture translation, generating speech output in the right accent and the right lip rendering for much more natural-looking video generation.
Sign-language translation and scene-to-speech for the visually impaired (which would be akin to video understanding above).
Summarization:  Generating a short summary of the salient points of a document or a set of documents.
Opinion and sentiment analysis:  Extracting political or personal sentiments from news pieces, tweets, product or movie reviews.
Essay evaluation: Evaluate an essay (say in an SAT exam setting) for proper structure, sentence, and vocabulary use.
Fake news or fake review detection:  Automatic fact-checking of news, especially in real-time viral settings on social media; verifying that product, restaurant, or movie reviews in online shopping or recommendation settings are genuine and not generated by bots, etc.
The following figure gives a birds eye view of NLP and various functions or tasks that partake in building and evaluating NLP applications.
From Eduard Hovys cThe Past and 3\xbd Futures of NLPd presentation.
There are three high-level functions that NLP tries to automate:
Analysis (or cunderstandingd or cprocessingd ) where the input is language (text or speech), and the output is some representation that supports useful action (e.g., translation or robot movements) in response.
Generation: input is a representation of an utterance and possibly a representation of the context, and the output is text or speech that captures the semantics and the intent encoded in the input representation (e.g., generating the target language sentence in machine translation).
Acquisition: obtaining the representation and necessary algorithms from data (e.g., learning to translate from aligned translated sentences).
The representation that these employs depends on the actual task being solved:  they can be explicit such as morphological analyses of words, syntax trees, or part-of-speech symbol sequences of sentences that capture the linguistic structure of words or sentences. They can also be very opaque, like sentence embeddings that a neural machine translation system computes as it analyzes an input sentence.
NLP is closely related to the following areas of computer science and other fields Machine Learning, Deep Learning, Artificial Intelligence, Statistical modeling, Information Theory, Human-computer interaction, Software Engineering, Linguistics,  Ethics, Cognitive Science, Logic, Social sciences, political science, and psychology.
Currently, almost all functions implemented in NLP applications make heavy use of machine learning, especially deep learning, involving transformers, large-scale neural language models, and the like.  These all necessitate large-scale data sources such as annotated and unannotated text, speech corpora, and large-scale computing resources to train.",
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Errors,"A quick survey of both scholarly and practitioner literature shows that errors can make or break the analytic development process and render your analytic solution of little use to your client. Errors can start from the business understanding phase when a data science team does not set the appropriate analytic objectives due to misunderstanding the business context and needs. This misunderstanding to bloated costs and scope creep (changes, continuous or uncontrolled growth in a projects scope, at any point after the project begins). A data science team might also encounter errors during the data understanding phase, such as issues with data that is not prepared adequately or, even worse, collecting data that is not relevant to the analytic solution. Errors in the data understanding phase can occur due to inexperience within the data science team and an attempt to deliver a solution prematurely.
The model understanding phase also presents errors that we should explore as we are learning about the different data science patterns and techniques that can be used to solve data-related problems. Errors in this phase may show up when training and validating models. Errors will reveal if the expected performance of a model will be sufficient for deploying to production. The models below are errors that you will encounter throughout your career as a data scientist. As we learn about different techniques, we will further explore how to assess models based on certain error estimates.
Training Error is derived by computing the classification error of a model on the exact data that was used to train the model. The training error is defined as the average loss that occurs  at the end of  the training process. It should be noted that the training error will usually be lower  than  the test error.
Test Error is derived by computing the classification error (the loss) of a model on the test set. It is important as it gives insight into the number of errors to expect when making future predictions, and it is used for model selection. No part of the training data set should be part of the test data, as this can affect the accuracy of the test error.  As seen below, the test error may decrease as the model complexity increases up to a certain point and then may start increasing.
Reducible Error is the error resulting from a mismatch between the ground truth and the model estimation, or the estimate of the true relationship between x and y. The reducible error is the element that we can improve. It is the quantity that we reduce when the model is learning on a training dataset, and we try to get this number as close to zero as possible.
Irreducible Error is the noise term in the true relationship that cannot fundamentally be reduced by any model. When x can not determine y because there are other predictors that might improve the prediction error, you can incorporate those variables. The irreducible error is the error that we can not remove with our model or with any model. The error is caused by elements outside our control, such as statistical noise in the observations.",
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4a: Drafting the Methods Statement,"If you have not already, you should notice a pattern at this stage. The conceptual progression Business Need  Business Objective  Analytic Objective including Problem Statement  Task Definition  Method & Data Statement serves the purpose of gradually refining our understanding of what the client needs until we arrive at a technical project specification that the technicians can design against. The statement of methods to be applied will vary in specificity depending on your project, but three elements are important:
The statement  must be precise enough so that the data science team understands it as a concise summary of the technical approach.
At the same time, it should allow for testing different techniques around the main conceptual idea.
The methods should in principle be suitable to produce the target functionality/insight for the task given the available data.
As you become more proficient in the discipline of data science (e.g., after having completed the more advanced units of this course), you will develop refined intuitions about which method to propose in which context. The most general categories of methods one can identify in this statement typically include:
Supervised learning methods which involve learning to predict a target variable (typically through regression or classification) by training on ctrued example data points whose target variable has manually been labeled or is available by other means.
Unsupervised learning methods which deal with finding patterns in unlabeled data without an explicit prediction target.
Semi-Supervised learning methods which encompass hybrid methods that combine supervised and unsupervised learning in different ways.
This course is very focused on the application of machine learning, which has a large overlap with various kinds of statistical methods. It is possible to phrase your method statement around the use of statistics, but it is recommended that one qualifies this rather broad term as something adequate for the project.
In many contexts, the method will need to be stated much more precisely than that, especially if the problem domain is already well-studied in data science or there is some prior work that should be extended. The method can range from specifying a particular family of models (e.g., linear vs. non-linear), using a new feature set, testing the explainability of a particular models predictions, optimizing hyperparameters for faster training and inference in neural networks, and more. We can illustrate the specific vs. general statement in the context of an example:
Your client gives you access to a large dataset of electronics product manufacturing and customer support data and asks you to help improve quality control (business objective) as the quality control personnel do not reliably find all potential defects (problem). Your model should be able to predict product failure within one month after the sale (task 1) and identify predictors measured at quality control time (task 2). In a pilot project, you propose to apply ctraditional supervised learning methods to train a classifier and examine the model for predictive variables.d You then proceed to conduct the project using basic logistic regression and some nonlinear tree-based models as they allow straightforward model explanation.
Variation: Your pilot project was a success, and your models are being used to better inform quality control personnel. However, it turns out that it still produces a high false-positive rate (problem) and causes shipping delays, which the client wants to minimize (business objective). You are asked to improve the models performance by reducing its false-positive rate (task). You are further given access to quality control diagnostic equipment readings, which the engineers believe are useful to discern whether a product is defective or just cneeds to be broken in.d You hence propose to integrate the reading data using a special signal encoding algorithm in combination with a nonlinear model architecture to improve the model.
The method and data are the heart of a data science project. Finding the best tool for the task is, of course, one of the core skills of being a good data scientist. This course will give you an introduction to basic methods and provide you with the opportunity to apply them in course projects. As you gain deeper knowledge and experience, you will become better not only at analyzing problems and tasks in different domains but also at researching data science literature and libraries effectively and coming up with project proposals that are aligned with state-of-the-art solutions. At the same time, thinking through different approaches and deciding on a set of methods and datasets benefit from teamwork, open discussion, and active seeking of advice and feedback from your peers, mentors, and relevant specialists.
A good beginning strategy is to check your proposed method against the target functionality or insight by conceptually thinking through its application and explicitly formulating expected results.
In the above electronics scenario, you imagine training a logistic regression model on the features in the data to predict product failure. Once trained, influential features should receive a high weight in the regression equation, allowing you to identify them easily, similar to correlation analysis. If you train a decision tree, you can identify features by traversing its branches. On the other hand, if you were to propose to train a complex random forest model to predict product failure with maximal accuracy, you may encounter the objection that random forests are not as easily interpretable as simpler models. It would hence be an unsuitable method for task 2 in the pilot stage. A good way would be to go with the simpler models for now and leave more complex models for later phases if needed. In the variation, a colleague may suggest that the signal encoding algorithm you plan to use is outdated and recommends you use a more recently developed encoding. You may research both algorithms and make an informed decision, or even use both algorithms in a comparative experiment.","Proposing Learning Methods,Note: the precise categorization of some methods as supervised or unsupervised methods is subject to some disagreement in the community. After completing this course, you will be able to make an informed decision regarding the use of these terms."
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 4b: Proposing Data Collection Methods,"Just as the methods proposed must be suitable to achieve the analytic goal, so must the data on which the methods operate. The most important criterion in this regard is, of course, that the data contains patterns that are informative for the analytic objective and allow one to successfully tackle the proposed task and make progress towards solving the problem in a data-driven way. Beyond this main requirement, and depending on the project context and proposed methods, specific data sources can be more or less suitable for the project. Possible considerations include:
Is the dataset of the appropriate size for the proposed method? How many data points and how many features does it contain?
What distribution do the individual phenomena in the data follow?
Is the dataset raw or readily processable? What preprocessing is necessary and how will it influence the relevant patterns?
Is the data complete or are some parts of it missing?
Is the dataset clean or noisy? Does measurement error play a role?
Are there access or disclosure restrictions to the data? Is it confidential, private, sensitive, partially redacted, or classified? Is it subject to licensing restrictions?
Well-studied benchmark datasets are available to the data science community for many tasks. While prior work on a dataset is a good indicator of its utility for a task, it cannot replace the process of familiarizing yourself with the data before commencing serious experimentation work. It is good practice to investigate the suitability of the proposed dataset for the task and method before moving on with the project unless the exploration of the dataset itself is understood as part of the analytic goal. This is typically done through research and preliminary data surveys, possibly in collaboration with domain experts.
While statistical analysis and machine learning are, of course, core pillars of data science, effective collection, curation, and interaction with data are equally important and regularly the subject of analytic objectives and even entire projects. As such, the core method and data component of an analytic objective need not necessarily always be about training a model on available data but can also be about collecting data to enable subsequent analysis.
When proposing data collection as part of an analytical objective, the collection methods must be scrutinized as to whether they are likely to succeed in producing a valuable dataset resource. Data collection, especially involving human annotators, is its own research field. Relevant considerations include:
How well will the collection represent/approximate/cover the desired distribution of phenomena relevant to the problem?
Does the collection require human annotators? Can it be done using crowdworkers?
What qualifications do the human annotators need to possess? How can they be effectively trained for the task?
Do the human annotators need to be examined/tested before and/or after collection?
How should the annotation task be designed to ensure productive and correct annotation? How will agreement between annotators be measured?
How much data should be gathered to enable progress towards the analytic objective? How much data can be gathered given the budget?
Once the data has been gathered, what cleaning and curation needs to happen?
Are there any approvals/permissions that need to be obtained before the collection can proceed (e.g., human subject research)?
This course includes an introduction to data collection methods. For purposes of this unit, the main takeaway is that collecting good datasets requires an amount of skill, care, and attention to detail comparable to those needed for doing good data analysis. Like core machine learning efforts, data collection projects are conducted with specific analytic objectives in mind and hence can be framed and assessed using the same template.",Proposing Datasets & Collection Methods
Data Science Project Planning,Developing a Vision,Developing a Vision,"To develop a vision for the project, the project team works with stakeholders to develop a vision document and related artifacts that represent the high-level scope and purpose of the project. The vision document communicates the motivation for the project - what problems does the proposed undertaking attempt to solve, and for what data set(s)? What is novel or unique about the proposed method? How will success be measured and evaluated? What is the anticipated impact (business or research value) if the project is successful? An explicit statement of the problem, proposed solution, and high-level features of the project can establish clear expectations and reduce potential risks.
Before diving into the implementation of the project, the vision must be developed. The vision defines the high-level objective of the entire project and presents clarity with respect to the problem statement, scientific hypothesis, and scope of the proposed solution. It is essential as it introduces the domain of the problem that needs to be addressed and provides a rough timeline of the tasks involved in achieving this objective. It provides context to succeeding developers about why exactly this project is necessary.
Figure 1. Engineering Lifecycle Management (IBM)",
Exploratory Data Analysis,Feature Engineering,Feature Engineering and Bias,"Reading: Avoiding Bias from Feature Selection.
Feature engineering can be performed before the model building process, i.e., during the data wrangling and exploratory data analysis phase, or it can be performed during model building. Later in this course, we will discuss cross-validation, but we must note here that feature engineering can be done during the cross-validation process. Cross-Validation is a model validation technique used to assess how a model will generalize to a new data set. At this time, feature engineering is done during the cross-validation loop.
Feature engineering at any stage can introduce bias to the data. While you manipulate the data, you can unintentionally create a relationship between features that do not otherwise exist. The features that are selected or created during the feature engineering process can shape the insights that are gotten from the model.",Feature Engineering and Bias
Data Science Project Planning,Requirements Gathering,Requirements Gathering Techniques,"The process of discovering the requirements of stakeholders and systems is called Requirements Gathering.  The requirements gathering process involves eliciting user and system needs, defining the formal requirements from those needs, and evaluating the success of the requirements gathering process. The data science project team will understand the needs of the system and stakeholders set by the business team and end-users (elicit needs/expectations), and then analyze and align the expectations to the business and analytic objectives to define the requirements for the project.
Requirements gathering should be performed systematically to ensure that information is extracted from diverse sources. This will ensure that little to no bias is introduced during the process and that all user and system needs are represented and met. It is important to note that different scenarios and circumstances call for a specific type of requirements-gathering technique.
Interviews. Interviews allow the project team to thoroughly investigate the needs of the stakeholder. Interview questions are usually open-ended, providing an opportunity for the respondent to provide information about various aspects of the business and identify performance gaps specific to their roles. Bear in mind that this can be a time-consuming process. In order to ensure that an interview elicits the right information, an interviewer should develop questions that address the right issues and, in certain cases, probe for answers to get useful information related to the business, system, and user. Interviews can be conducted in a one-on-one setting or in a group setting.
Brainstorming. Brainstorming sessions involve gathering ideas from multiple stakeholders at once. In a brainstorming session, a facilitator monitors the session to ensure all possible solutions are identified, and all parties contribute to the process in a reasonable amount of time.
This technique can yield new ideas and themes that would otherwise be difficult for the analyst to produce. A brainstorming session should have five to eight representatives from each shareholder group, including management, users, and support staff.
Questionnaires. The project team prepares and distributes a questionnaire with predefined questions to stakeholders in the company. The responses from these questionnaires are then analyzed and used to define the project requirements. A well-designed questionnaire can be completed by non-technical system users and stakeholders with little or no guidance from the project team. This technique can be used when interviews are not possible due to budget, time, and scheduling constraints.
Document Analysis. This is an information-gathering technique that involves reviewing existing documentation to elicit needs. Document analysis can be the first step in the requirements gathering process and can be used to create questions for questionnaires or interview sessions. Document analysis is considered to be a supporting technique and can serve as a completeness check for requirements.
Facilitated Workshops.  A workshop, also known as a Joint Application Design session is facilitated by a neutral party, usually an outside consultant whose task is to collect information from stakeholders. These workshops should be structured yet interactive, and workshops can lead to the discovery of underlying issues within the business. A successfully executed workshop can result in the development of requirements. Participants of a workshop include a Scribe who records the discussions taking place during the session,  an Executive Sponsor who has the authority to make decisions about the project and who will set the vision of the project and resolve conflicts, and the appropriate client stakeholders, Subject Matter Experts, and Silent Observers.",Requirements Gathering Techniques
Deep Learning and Model Deployment,Model Deployment,Module 23 Summary,This is a new page with empty contents.,
Collecting and Understanding Data,Ethics of Data Science,Informed Consent,"Ethical practices matter in data science because of their impact on human well-being and society at large. When it comes to human subjects research, the concept of informed consent is critical because it involves the right of the individual to know that they are being studied and the right to know how they are being studied. In this module, we will explore the concept of informed consent. First, we will explore what human subject research means, then we will learn about the link to the evolution of informed consent.
Begun in 1932, a study conducted by the United States Public Health Service (USPHS) at Tuskegee University and funded by the Centers for Disease Control (CDC), investigated the cause and development of untreated latent syphilis. Some 399 African American men in Alabama who had syphilis were recruited and matched against 201 uninfected subjects who served as a control group.
Figure 1. Scenes from the Tuskegee Syphilis Study. (Source: https://www.rmpbs.org/)
The subjects were instructed to make regular visits to the clinic, where they would be given a health exam, care for minor medical issues, and a hot meal. The participants were enrolled without their informed consent to a cspecial free treatment,d which was actually intended to study the neurological effects of syphilis.
By the 1950s, when it became clear that penicillin, an antibiotic drug, was a safe and effective treatment for syphilis, the subjects were either prevented from knowing about it or denied such treatment. No subjects were treated with penicillin. The study continued until 1972, when the Department of Health, Education, and Welfare (HEW) terminated the experiment after accounts of the study appeared in the national press, driven by some whistleblowers. At that time, 74 of the test subjects were still alive. An investigatory panel appointed by HEW in August 1972 found the harm being done by the study was cethically unjustifiedd and stated that penicillin should have been used to treat the men. As a result, the National Research Act mandated that all federally funded proposed research with human subjects be approved by an institutional review board (IRB). The IRBs monitor a process called informed consent. Informed consent states that when a study is being done on a human subject, this individual must be informed about the experiment, must consent to the experiment voluntarily without any coercion, and must have the right to withdraw consent at any time.
In the case of the Tuskegee Study of Untreated Syphilis in the African American Male, the subjects were not informed about the study of neurological effects of syphilis. In addition, they were misinformed about possible treatments for syphilis and were told that syphilis could not be treated. The subjects did willingly consent to the experiment, but their consent was not properly informed, and it was not clear if the researcher told the subjects that they had the right to withdraw their consent at any time.
The case here is that the researcher was evaluating the benefit to society or science versus the harm to the participants. A fundamental principle of informed consent is that the party facing potential harm has the right to decide on their own the balance between the benefit to society, as well as any compensation they are receiving from the experiment, and the risk of harm they face. Since full details of the potential harm and benefits are often very complex, it can be nontrivial for the human subject to be fully informed of them. For this reason, an IRB would come in, determine if the study is just and ethical, and ensure that the informed consent principles are appropriately followed.
Informed consent is based on the Fair Information Practice Principles (FIPPs) as part of the Privacy Act created in 1974. It is important to note that, progressive as it may appear, there are still limitations to the principle of informed consent. Informed consent was developed in the context of research that would be conducted on human subjects to collect data prospectively. In todays data science practices and applications, informed is usually something that is hidden in numerous pages of fine print, and users are required to say cI acceptd before the process can begin. From an ethical point of view, setting aside the law, there is a consensus that claiming that somebody has been informed because they were given many pages of fine print to read without an actual opportunity to read them is an unethical means of obtaining consent. The concept of voluntary is also questionable, as consent is being obtained precisely when a user already intends to use a service or technology. Users, in these cases, are typically not given the information well in advance, providing them with adequate time to understand the risks or terms prior to consenting.
There is also a question of what the data will actually be used for once consent has been obtained. For example, a user may consent to give data about themselves to a merchant for a specific service, but it does not mean that the data is authorized to be repurposed. Not all repurposing of data is unethical. On the contrary, repurposing data can bring significant benefits to society in the case of medical data of one patient being studied to help future patients. One caveat here is that, in many cases, what is intended to be studied comes after the data has been collected. Physicians and medical researchers may not know the questions to be asked when data is being collected; they simply know that more information would help. This type of research is called retrospective data analysis.
In terms of informed consent, the problem here is how to inform subjects exactly what they are consenting to while at the same time making it comprehensive enough to include potential research questions that one might ask. So, again, this is a crucial question for conducting meaningful and ethical data science research.",
Data Science Project Planning,Design and Plan Overview,Overview,"After the project design has been developed, it can be divided into sub-tasks. Each of the tasks can be assigned to certain team members, and deliverables falling under each task could be documented along with the deliverable dates. For industry-oriented projects, it might be useful to include budget plans for every individual task. This would help judiciously distribute temporal, human, and monetary resources. The milestone plan developed in this section could be fairly specific, involving every minor update to the project. This could be communicated via a table of tasks, a flow diagram, or even a Gantt chart. These useful tools can further simplify the understanding process for future researchers and developers who would continue working in this domain.
Preparing such a comprehensive collection of documentation for any data science project. Diagrams in the documentation of a data science project are very useful as they help translate requirements and system design much more efficiently.",
Exploratory Data Analysis,Feature Engineering,Feature Vectors,"So far, we have discussed data as an entity in the data science process and how it is transformed during the cleaning/wrangling process, used for exploratory data analysis, and used to draw conclusions with inferential statistics. Now we will focus on the parts of data that can be useful in the model-building process, parts of data that will assist in performing the tasks that you have defined in earlier stages of the data science process, and those tasks that are done to meet our analytic objective. Developing an analytic solution will involve the use of statistical modeling. We must understand that those models consist of formulae that only relate numerical quantities to each other. How then can we build a solution that ranks customer preferences or identifies segments of a customer base that might benefit from a service? How can a mathematical model understand variables that are not numeric?
A feature is a numeric representation of a part of the raw data. The Wikipedia definition of a feature best describes it as ""...an individual measurable property or characteristic of an observation"". Features are the parts of an observation that are represented in a way that a machine learning model can use. Consider an image classification task. To properly represent the features of your image, they are processed into a numerical format that allows the mathematical model to use them.
When raw data is transformed into features, a data scientist must consider the right features that are useful for the data science task. A good feature is one that is appropriate to the statistical modeling technique and data science task. Features should also provide information, i.e., if you are performing a predictive task, your features should have predictive values.
Transforming or processing features from data is an important task in the data science project life cycle but is often glossed over. The price for badly selected features is a costly one that rears its head when you are training your model. As shown in Figure 1, features will directly affect the models that you develop and the insights gleaned from your models. The snowball effect of badly selected features will end up leading decision-makers down the wrong path. As efficiency and accuracy are key in the data science process, it is important to explore available resources that are developed to guide data scientists on feature engineering techniques for data science tasks and modeling. Note that feature engineering requires both domain and technical expertise.
Figure 1. Feature Engineering and Analytic Solution Building. (Source: Zheng & Casari (2018))
Feature engineering is the process of extracting features from raw data and transforming those features into suitable formats for a machine learning model. Feature engineering leads to higher quality models and better insights for decision-makers. When you think about the diverse machine learning techniques, data science tasks, and contexts in which we apply machine learning, you will see that feature engineering can not be generalized. It is not a one size fits all process. It is dependent on the analytic objective and the data. Feature engineering requires domain knowledge and intuition.
During the feature engineering process, the data scientist will remove features from the data that do not provide task-specific information (e.g., the feature has no predictive value) and also features that introduce redundancy. This is called feature selection.
Numeric Data Types: Even though we defined a feature as a numeric representation of data, raw data that is in numeric form should also undergo feature engineering. This is because the data must meet the assumptions of the chosen model.
Scalar: Single numeric feature, e.g., mass.
Vector: Ordered list of scalars; also defined as an object that has both a magnitude and direction.
Spaces: Vectors exist within a vector space and are also a collection of vectors that can be added or multiplied by scalars.
In machine learning, the input to a model is represented as a numeric vector.","Raw Data to Features,Features"
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Hierarchical Clustering,"Let us explore the second type of clustering technique called the Hierarchical Clustering technique. Here, you will begin clustering to form hierarchies of clusters, and those hierarchies are presented using a Dendrogram (reading a Dendrogram). There are two techniques used for hierarchical clustering.
This technique involves starting the clustering process, with each observation forming its own cluster. Clusters are then formed by combining or agglomerating the nearest clusters until there is one cluster left. Essentially, at each step of agglomerating clusters, the clusters with the smallest distance from each other will be combined. As shown in the figure below, the dataset with a-f observations will be combined using the agglomerative technique until there is one cluster left.
The technique can take advantage of any distance measure, but you will find that most studies will use the Euclidean distance as a distance metric.
Raw Data-Source1
The first round of merges finds clusters with observations/clusters b and c merged to form one cluster, and d and e are also merged into one. Now we have clusters a, bc, de and f.
Next, de and f are combined to form cluster a, bc, and def.
Clusters are further combined to form a and bcdef.
Finally, abcdef is formed.
Hierarchical Clustering Technique-Source1
Single Linkage Method is based on grouping clusters using the agglomerative method, with two clusters merged at each step. Those clusters contain the closest observations that are not yet part of the same cluster. The distance between the nearest pair of observations in the two clusters is used to determine the best clusters to combine. This method will produce clusters that have small distances while ignoring observations in clusters that are further from it. As clusters are merged, the agglomerative algorithm uses a linkage method to evaluate the similarity (or dissimilarity) between formed clusters.
Single linkage suffers from chaining. In order to merge two groups, only one pair of points needs to be close, irrespective of all others. Therefore clusters can be too spread out and not compact enough.
Complete Linkage Method uses the maximum distance between data points within each cluster, also known as the farthest neighbor method. Clusters are combined into larger clusters until all data points are in the same cluster. The distance between clusters is the distance between two data points (i.e., one per cluster) that are farthest from each other. Complete linkage avoids chaining but suffers from crowding; because its score is based on the worst-case dissimilarity between pairs, a point can be closer to points in other clusters than to points in its own cluster.
Average Linkage Method uses the average distance between data points within each cluster. You can think about the dissimilarity between clusters using the average linkage method as the average dissimilarity overall points in opposite groups.
Centroid Linkage Method is based on maximum distance and uses the centroid distance between clusters. The mean for data points in a cluster is the centroid. In complete linkage, the dissimilarity between clusters is the largest dissimilarity between two points in opposite groups.
Divisive Clustering. This is the opposite of the agglomerative method. Data are clustered using a top-down approach. All data will belong to one cluster, and then the largest cluster is split until each observation is in its own cluster. This method chooses the observation with the maximum average dissimilarity and then moves all observations to this cluster that are more similar to the new cluster than to the remainder. This method is great at identifying large clusters, and the agglomerative method is great at identifying small clusters.
Reading: Hierarchical Clustering of Words and Application to NLP Tasks","Agglomerative Clustering,Merging Clusters"
Problem Identification and Solution Vision,Distilling the Analytic Objective,Step 2: Identifying and Delimiting the Problem,"The problem underlying an analytic objective should satisfy the following criteria:
Solving the problem must be part of a possible solution vision toward the business objective.
Domain experts should be confident that data exists which, when analyzed, can facilitate that solution. This data must either be available or sources are available to retrieve the data.
The problem must be specific and realistic so that a corresponding data science project can succeed in principle.
First, with the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective. The path to realizing this vision can then be decomposed into sub-problems. The data science team will then be responsible for leveraging data to help solve these sub-problems and/or produce data-derived insight that allows the client to make good decisions along the way.
As data-driven methods are still in the phase of being adopted across many fields, the perspective of using data to support business objectives may even be the main business objective that starts the whole engagement. In such a case, it is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond cwe want to leverage data somehowd and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.
An online company asks for a data science project around the use of social media data for understanding its market, driven by the boards desire to keep up with the competition. After the data science team studies the business and explains to the client a number of possibilities regarding how social media streams are typically used in retail businesses, the client will focus on the need to increase market share and identify the business objective of increasing sales of a particular product. There seem to be two ways forward: (1) Spend money on increased advertising of a unique feature of their product or (2) lower the price. The client asks for an analysis of social media data to inform their decision. This need for gauging consumer preferences now forms the problem component of the analytic goal.
Second, there must be a sound presumption that the problems solution must benefit from the use of data. A collaborating team of domain experts and data scientists should discuss the available data sources and their suitability for the solution, including data that would need to be collected as part of the project. In order to be suitable, the data should exhibit informative patterns relating to the problem. It takes a combination of substantive expertise and data science skills to assess this criterion.
Domain-specific problems around the availability of data include:
Lack of readiness in the organization (e.g., the organization's data is not readily processable), unsuitability of the data for the objective (e.g., data is old/stale, out of domain, incomplete, or suffers from an obvious bias), or difficulty in collecting data because the expert labor involved is too expensive.
Various technical objections, such as too little data for the required methods, fragmentation of data across multiple units with no suitable way of joining, or overwhelming imprecision/noise in the data. In cases where data is available, but the existence of a csignald for the problem is uncertain, exploring whether the signal exists and the extent to which it can be leveraged can become an exploratory analytic objective in and of itself.
Typically, data science projects/consultations involve a preliminary data survey that informs or even precedes a longer substantive discussion. It is very strongly recommended that such a survey be conducted before the team commits to fulfilling any analytical expectations on the part of the client.
Third, both the domain experts and data scientists must be in agreement that the problem is specific and realistic enough so that a data science project attempting to make progress towards it can succeed in principle. In other words, the system and/or insight produced by the project must add enough value to be deemed a success if executed properly. On the client's side, this criterion mandates a moderation of expectations and ensures that the data science component of the whole project can be evaluated. For example, while it should mostly be avoided, a solution vision may prove to be idealistic and somewhat resemble a science-fiction scenario. In such cases, the main purpose of the data science project is to assess its feasibility based on available data and methods and should be explicitly stated as such. On the other hand, the technicians must be very careful not to exaggerate analytical capacities and lead to unwarranted impressions that certain functionality is within reach. For example, once an initial data sample has been surveyed, the results should be communicated as being contingent on the assumption that the sample is representative of the larger dataset. Similarly, if a particular neural network model performs a classification task very well on some domain, the principle feasibility of transferring it to a second domain with reasonable performance should be explicitly tested before promising that it can perform at the same level.",
Data Science Project Planning,Developing a Vision,Overview,"The vision document is the starting point of your documentation set. Besides giving an overview of the project, this document helps remove ambiguities and puts everyone, including your collaborators and others, on the same page. Documenting these details helps direct every effort in the future toward the same goal. This is essentially helpful when there are many collaborators, which is usually the case in the industry. Without a clear vision, the project can go off the rails with changing collaborators or leadership. Take the example of the Virtual Case File study (needs CMU log-in to access the link). The FBI blew more than 100 million on a case-management software it will never use. One of the reasons for failure was an unclear vision for the project.",
Analytic Algorithms and Model Building,Model Selection,Quiz 6,,
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Module 15 Summary,This is a new page with empty contents.,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,k-Nearest Neighbors Method,"Have you come across the term ""lazy learning""? This is a method in which training data is generalized and is most useful for large datasets that will be updated continuously. In such a  case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a recommendation system. A recommendation system relies on certain variables such as ratings, pricing, and country of origin and will continuously update as information on new movies or new items in shoppers' preferences is available.
The opposite of a lazy learning method is an eager learning method. An eager learner usually requires less space than a lazy learner. , An eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will typically learn for a short time but take a longer time to classify data.
A well-known lazy learner is the k-Nearest Neighbor (kNN) method. This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful and is one of the first algorithms or methods that entrants to data science will learn. kNN is also simple to implement in Python or R). kNN will find a predefined number of training samples closest in the distance to a new point or a new observation and predict the label for the new observation. kNN, however, can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a Gaussian distribution.
kNN will perform its task just in time as it does not learn a  model but keeps its training observations in an explicit form, so it spends less time learning and more time classifying or predicting.
There are three steps involved in making predictions with kNN: Compute the Euclidean distance of the new observation to previous classified observations, identify the nearest neighbor(s), and then perform the classification.
Reading: k-Nearest Neighbors: From Global to Local
The kNN method begins by identifying the observations (or the k records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The kNN method uses distance computations to determine the distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the Euclidean Distance. The formula for the Euclidean distance between observations (\\(x_{i}\\) and \\(y_{i}\\)) is shown below:
Euclidean Distance Function. (Source: Sayad (2010))
There are other distance measures, including Manhattan (distance between vectors using the sum of their absolute differences) and Minkowski distances. Euclidean distance is used because it is computationally cheap (keep in mind that predictors should be standardized before implementing the Euclidean distance function). Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables. An alternative distance measure, in this case, would be the Hamming Distance, which can be used as long as you do not have more than two (2) classes.
Once distances are computed, the class that a new observation will be assigned is based on the classes of its neighbors. Now, you can see why kNN is considered a similarity function.
How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can, for example, determine that the new data should be classified in the same class as its single closest/nearest neighbor, i.e., k = 1. Can you guess what will happen in this situation? You are right if you guessed that you would face overfitting. You can mitigate this by using a k that is greater than 1. If you assign k = n, i.e., n being the number of observations in the dataset, there will be over-smoothing, and all new data will be assigned to the majority class. Values of k are historically between 3 and 10 and usually an odd number to avoid ties1. Sometimes k can be chosen using cross-validation, and the validation dataset will validate the best k.
Although kNN is helpful for predicting a categorical response, it is also effective for predicting continuous value responses just like one would with a linear regression model. The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. The best k for a classification task is assessed using the overall error rate but in this instance, the best k is determined using the root mean square (RMS) error.",
Deep Learning and Model Deployment,Model Deployment,About the Author - Kaushik Shakkari,"This module is a special collaboration with a Subject Matter Expert. It was written by Kaushik Shakkari.
Kaushik is a senior data scientist at Cognistx. He proposed and leads SQUARE, an end-end question answering product at Cognistx. He is also a fellow and alumni mentor at the Insights Data Science program. His research and areas of interest include user behavioral analysis, semantic search, and deep learning.
You can reach Kaushik on LinkedIn and read his articles on Medium.
Kaushik Shakkari",
Data Science Project Planning,Requirements Gathering,Overview,"Similar to traditional software development projects, data science projects are also guided by requirements gathering principles. Figure 1 lists the steps that are followed during the process. Requirements gathering for a data science project will involve eliciting the needs of the stakeholders and defining the requirements for the analytic solution(s).
Figure 1. Requirement Gathering Process
The requirements-gathering process involves eliciting user and system needs and defining data and analytic requirements for the successful implementation of a data-related project.
Gather Information: The first step in gathering information is to identify the stakeholders within the business; the stakeholders will be individuals who perform tasks that will meet the business's need, as well as decision-makers within the business. Once stakeholders are identified, the business analyst will elicit information to determine what the solution should do to meet the defined business and analytic objectives. Later on in this unit, we will discuss the techniques used to gather information.
Define and Prioritize Requirements: Stakeholders will provide information according to their view of the business needs; it is the job of the business analyst to lead the effort in defining and prioritizing requirements. It is important to document ""complete"" requirements that capture the needs of the stakeholders, as this will guide the project team in developing the right solution(s). Stakeholders might provide information that can be used for future projects related to the proposed solution; that information should not be discarded; it is prioritized as a low-priority requirement and considered for future implementation.
Evaluate Requirements: The project team must verify and validate all documented requirements. This additional step in the requirements-gathering process will ensure that the solution meets the business needs and satisfies the expectations of the stakeholders.
Receive Sign-Off: This is an indication that the requirements have been approved and agreed upon by the client. Requirements are signed off twice during the development lifecycle; sign-offs take place prior to the start of solution development and after testing the solution.
A requirements management plan can be used to document the requirements-gathering process. This document is made available to the client and the project team as it contains information that affects both parties. There is no standard template for this document, but it is in good practice to include the following sections:
The project description is an overview of your project. This section describes the purpose of your project.
Team Responsibilities are defined in this plan to designate who will be involved in managing activities during the requirements-gathering process. Data science project team members might take on duties outside of their normal roles, e.g., a data analyst on the data science team might serve in the role of Scribe during joint application development sessions.
Tools used to manage the requirements include project management tools and word processing or other dedicated systems used to capture, manage, and track requirements through the requirements-gathering process and throughout the project lifecycle.
Requirements Gathering Process should be defined in this plan. This section will describe the techniques used in eliciting user and system needs, defining the requirements, and evaluating the success of the requirements-gathering process (these techniques are covered later in this unit).
Change control and requirements gathering: Modern-day collaboration tools will support change control. However, a change control process should be documented to ensure that changes to requirements are formally managed.",Requirements Management Plan
Analytic Algorithms and Model Building,Model Selection,Model Selection for Inference,"In inference, models are trained on the entire dataset to derive the relationships between independent and dependent variables. Thus, there is no longer the notion of a train-test split. Instead, model selection is based on probabilistic metrics that reward goodness of fit but also penalize model complexity, with the goal of acquiring the most reasonable model that is sufficiently simple/interpretable. We introduce a number of popular metrics below.
Akaike Information Criterion (AIC). Derived from frequentist statistics, the AIC score of a model M is computed as
\\[ AIC(M)=(2K_{h}-2LL(M))/N \\]
where KM is the number of parameters in h, LL(M) is the maximum log-likelihood of M on the dataset, and N is the size of the dataset. For regression, LL(M) is the mean squared error, and for binary classification, LL(M) is the logistic loss. A model with a smaller AIC value is considered better for inference.
Bayesian Information Criterion (BIC). Derived from Bayesian statistics, the BIC score of a model h is computed as
\\[ BIC(M)=K_{M}\imes logN-2LL(M) \\]
where the variables KM, N, and LL(M) are defined similarly as in AIC. A model with a smaller BIC value is considered better for inference. It can be shown that BIC is proportional to AIC, although the former penalizes complex models more heavily. For small training datasets, it may select models that are too simple.
Minimum Description Length (MDL). Derived from information theory, the MDL score of a model M is computed as
\\[ MDL =L(M)+L(D|H) \\]
Where L(M) is the number of bits required to represent the model h, and L(D|M) is the number of bits required to represent the model predictions on the dataset. A model with a smaller MDL value is considered better for inference.",
Deep Learning and Model Deployment,CPU vs. GPU,Module 21 Summary,This is a new page with empty contents.,
Analytic Algorithms and Model Building,Text Data & Natural Language Processing,Common Language Processing Tasks,"Language data could refer to data from multiple sources like speech, image, video, and text data. Thus, language processing tasks applied to preprocess this largely depend on the form of data and its source. The rest of this module focuses primarily on text data as the input and output.
Tokenization is the process of splitting an input sentence, paragraph, or entire document into a list of tokens, where each token is a linguistic unit in speech or writing. For example, the string sentence ""Today is a good day."", when tokenized, would yield ['Today', 'is', 'a', 'good', 'day',  '. ']. In general, tokenization may split independent punctuation but may need to keep some punctuation if those are deemed to be part of a token. For instance, in cDr. Smithd the token is cDr.d and not cDr.d In English, sometimes tokenizers choose to split contracted words, e.g., cJohnsd is split as John and cs.d  The downstream task in the pipeline may choose the interpret the punctuation or ignore them.
Text in other languages may need to go through more complex operations during tokenization, depending on their writing systems. For instance, Chinese is typically written without any spaces between words, and a sequence of characters needs to be split into individual words before further processing using a process known as word segmentation.  On the other hand, Arabic text is written without short vowels, and for certain downstream applications, one may need to add these through a process known as diacritization.
Stop words are words that typically belong to a closed class of words in the vocabulary of a language that themselves do not carry meaning but function in a sentence to get the syntactic relations right. Typical stop word lists in English words such as ca,d cthe,d cin,d con,d etc. They are also rather frequently used words by nature and usually interfere with some downstream tasks. For example, applications such as document classification rarely benefit from such words. Thus once tokenization is done, one may need to remove all such words in a process called stop word remove.  This can be done by sorting the vocabulary of the text collection by frequency and defining the top 10 vocabulary entries as stop words, or alternatively by using one of the many predefined stop words lists available online.
Many language processing tasks, such as spelling checking and correction, parsing, and surface generation, either need to extract and process the information encoded in words or synthesize words from available semantic and syntactic information. This is especially necessary for languages with rich(er) morphology, such as German, Hungarian, Finnish, Turkish, and Arabic, to name a few.
Morphological analysis typically segments words into their constituent morphemes, taking into account any orthographical variations in orthography dues to morphology. For instance, while processing a word such as cstopped,d a morphological analyzer would need to know about the root words cstopd and the suffix c-ingd and that under certain phonological circumstances, certain consonants at the end of the root words need top duplicate when the following suffix starts with a vowel (cf. cstopsd).  It would then represent this word with something like stop+Verb+Past. Similarly, a word like ceasiestd would be segmented as ceasy+estd using an orthographical rule in English that changes a stem final c-yd to an c-id in orthography, as there is no difference in pronunciation. The output representation would be easy+Adj+Super. Other more mundane mappings include handling special cases such as generating cgod as the root words for words such as cwentd or cgone.d
Morphologically complex languages have many other orthographical processes usually rooted in phonology, such as vowel harmony, consonant and vowel insertions or deletions, or duplications. Morphological analyzers would have to take all of these into account in order to analyze words.
The state-of-the-art tools for morphological analysis rely on the well-established computational formalism of finite state transducers.  There are numerous toolkits that take in a description of the root and affix lexicon of a language and compile these into large finite state transducers which take in a word in a language and generate representations for all possible morphological interpretations of a word. A side benefit of finite state transducers is that they a bidirectional and given a morphological analysis. They can produce the actual word.
When full morphological information is not necessary or not available, a clighterd operation called stemming can be used. Stemming refers to heuristically stripping off known word endings to get to a base word (that itself may not be an actual word) that can be used as a proxy for the word, especially in tasks where morphological details are necessarily needed. Stemming in English, for instance, maps  ""change,"" ""changing,"" ""changes"" to ""chang."" Porter Stemmer is a popular algorithm used for stemming for English? Stemming has limited applicability in languages whose morphology is more complex than English.
A slightly more accurate version of stemming is called lemmatization which does a more informed version of stemming using an additional lexicon and predicts the actual stem of the word or its lemma  the standardized form to look the word up in a dictionary. In the examples above, it should return cchanged as the lemma.
Of course, a full morphological analyzer would do lemmatization in addition to interpreting the affixes and would be a tool of choice for languages with complex morphology.
Morphological analyzers typically assign words a morphological interpretation in addition to a lemma. Such information includes the category of a words morphological interpretation and any additional syntactically (or semantically)  relevant information, such as whether the word is a common or a proper noun, whether it is a singular or plural noun, or what the tense of a verb is, etc.
For English, such morphological information has been coded as part-of-speech tags. The Penn Treebank part-of-speech tag set has been established as the de facto part-of-speech coding scheme for English.  More recently, the universal dependencies project has established a smaller common tag set for many languages, including English.
The Penn Treebank convention assigns short symbols to words as a part-of-speech category. For example, cbooksd would get both VBZ (third person present tense verb) and NNS (plural common noun), while cwentd would be VBD (past tense verb), and cgoned would get VBN  (past participle verb). Many words are ambiguous with respect to part-of-speech (such as cbooksd earlier).  For example, the word cwordd has 6 possible part-of-speech categories:
Adverb (RB) c...up and down Floridad
Particle (RP) c ..keep the ball downd
Preposition (IN) c..down the centerd
Adjective (JJ) c..down payment..d
Verb (VBP) cwe down five glasses of beer every nightd
Noun (NN) cthey fill the comforter with downd
In general, almost every word in an English sentence will have multiple POS tags. For example. In the rather artificial sentence,
the first ccand is a modal verb (MD), the second ccand is a tenseless verb (VB), and the third ccand is a singular noun (NN).
The task of determining the contextually correct POS tag for a word in a sentence is called part-of-speech-tagging (POS tagging).
POS tagging for English is typically done with a sequence-to-sequence mapping approach, with the sequence of the words going in and the sequence of POS tags coming out. Such approaches are trained with manually tagged data from the Penn treebank.
Early such approaches for POS tagging for English were based on Hidden Markov Models, which were trained with standard training data and applied to unseen data, with additional heuristic provisions for handling previously unseen words (e.g., googling was not in the Penn Treebank). The tagging model of transition and emission probabilities learned can then be used with the Viterbi algorithm to select the most likely sequence of tags for the words in an input sentence.
Later more sophisticated but computationally more expensive approaches, such as Conditional Random Fields, were used. These days any machine learning approach for sequence-to-sequence transformation can be used, including recurrent neural networks, etc., provisions in certain tasks like statement disambiguation, sentiment analysis, etc.
While processing text, it is often useful to identify phrases from the text that represent named entities: people, locations, organizations, geopolitical entities, product brand names, etc. Named Entity Recognition (NER) seeks to identify the span and the types of these named entities and group them. Although they are not necessarily named, referrals to dates, times, and other kinds of temporal expressions and prices can be considered under the same umbrella.
Figure 1. Named Entity Recognition with spacy.
NER is another example of sequence-to-sequence transformation. We represent the named entities and their spans using a scheme known as BIO encoding. For each named entity category, we have two symbols, one with B and one with I. So, for example, B-PER labels a token that begins a person-named entity, while I-PER labels any token of a person's name (of a length greater than one)  that is inside a named entity.  O labels any word that is outside a named entity. Thus with k different categories of named entities, there is a set of \\(2k+1\\) labels.
In the figure earlier, any token that is not colored gets the O label. In the last colored blue colored named entity, cThed would get B-ORG label, and cJusticed and cDepartmentd would get the I-ORG label.
The sequence-to-sequence transformation then takes in the sequence of tokens and uses a classifier to assign one of these labels to each of the tokens.  Of course, sophisticated named-entity recognizers also employ other sources of information such as POS tags and capitalization information for the tokens, any phrase markers, etc. Any machine learning approach that can perform classification can be used to implement NER, as in Figure 2.
Figure 2: NER as a classifier  From cJurafsky and Martin, Speech and Language Processing, 2nd Edition.d
A training set of labeled named entities is used to train a classifier which then performs NER on new sentences.
NER systems are evaluated by three metrics:  precision, recall, and \\(F_1\\) as shown in the figure below.
Parsing is the process of assigning syntactic structures to a sentence.  As we saw earlier, the syntactic structures are represented with either constituency trees or dependency trees.  A constituency parser would be based on typically context-free grammar and produces one or parse trees for an input sentence.  The parse trees are relative to the grammar, and different grammars would produce different structures.  The Cocke-Younger-Kasami (CYK) algorithm based on dynamic programming can produce a chart in which all parses of a sentence are encoded in \\(O(n^3)\\) time \\(n\\) being the number of tokens in the input sentence. Extracting all individual parses may take substantially more effort as there may be an exponential number of trees for some edge cases. While the CYK algorithm requires context-free grammar in what is called the Chomsky normal form and may seem restrictive, any context-free grammar can be converted to a Chomsky-normal form grammar with a larger number of intermediate categories at the expense of obtaining structures that may not be linguistically sensible.
Thus if the input sentence is cA boy with a flower sees a girl with a telescope.d  the parser would generate the following two parse trees:
but it would not be able to tell which of these parses is the ccorrectd one.
Manually building grammar for parsing is not a feasible task. Treebanks, like the Penn Treebank discussed above, can be used to build statistical parsers by (1) inducing a large-scale grammar and (2) associating probabilities with the rules in the grammar. Thus the CYK algorithm about can be augmented to produce the most likely parse of an input sentence in the same amount of time.
For dependency parsing, there are several parsing methods. While it is not necessarily the most accurate method, transition-based dependency parsing is the most commonly used technique. Using a stack essentially makes one pass over the input words and decides in \\(O(n)\\) time on the dependency relations between lexical items.
Fundamentally a transition-based parse is a classifier that, at any point in time, looks at what is on the stack and what is left in the rest of the input to decide on action toward building a dependency parse tree. Such a classifier can be trained by using training data obtained by transforming a dependency treebank into a sequence of parser actions.
Other dependency parsing algorithms typically employ more sophisticated algorithms. Graph-based methods typically label a graphical representation of the sentence and assign weights to each arc representing word-to-word relations and then extract the maximum spanning tree to represent the most likely parse.","1. Tokenization,2. Stop word removal,3. Morphological Analysis, Lemmatization,  Stemming,5. Part-of-Speech Tagging,6. Named Entity Recognition,7. Parsing"
Problem Identification and Solution Vision,Problem Identification,Case in Point: MotoManager and the Evidence Value Proposition,"MotoManager Case-in-Point
Here is an example of an AI consulting firm that used the EVP framework to meet the business needs of a popular automotive services provider.
Mr. Tire-Monro Muffler and Brake (a subsidiary of Monro Inc.) is a top-50 automotive part and general repair services provider in the U.S., with over 320 locations nationwide. In general, the automotive services industry struggles with customer retention. Companies record many one-time-only transactions (frequently with a deep-discount coupon) but fewer transactions from repeat customers (who typically provide much more revenue per year per customer). Lack of customer cstickinessd leads to a) less potential revenue and b) less data about customers in general, which could potentially be used to offer specific products and services to individual customers. Attempts to increase customer acquisition and retention via email marketing and television and online advertisements did little to increase the proportion of repeat customers.
Monro Inc. (the parent company of Mr. Tire) approached Cognistx (an AI applications company) to develop a data-driven solution to improve customer acquisition and retention. The video above provides a brief summary of how Cognistx engaged with Mr. Tire to develop MotoManager, a mobile app that was deployed by Mr. Tire. This solution led to measured increases in customer acquisition and retention, as well as increased revenue.
The Cognistx data science team met with the business leaders of Monro Inc. to gain an understanding of the companys business needs related to customer retention. The data science team identified and interviewed all stakeholders from the business and technical teams at Monro Inc., including the Data Management, Information Technology, and Service Management teams. The IT managers provided information about the companys data asset management structure, including data governance, data architecture, and data security management. Accessible and reliable data is important to the solution vision process; a company without adequate data management can not support an analytical solution that might meet its business needs.
Data Management in the Enterprise
Finally, service managers were interviewed on customer service difficulties that could be addressed by the proposed solution. The service managers also identified the hardware and software gaps at various stores around the country. Once the interviews were completed, the Cognistx data science team formulated business objectives that would meet Monro Inc.s business needs. The business objectives included:
Creating an application that provides customers with a customized service experience for their automotive needs.
Offering customers 50 coupon to download Monro Inc mobile app.
Onboarding customers to the application with the creation of customer profiles.
Providing tailored customer service management to very important (VIP) customers.
Classifying customers as VIP customers based on defined characteristics.
Creating a loyalty program to increase repeat customer transactions.
Business objectives should be measurable to ensure that business needs are met. The metrics used to assess the success of the project were:
The number of people who installed the app and on-boarded upon receiving a 50 e-coupon.
The number of times an on-boarded customer visited a store close to them.
The number of transactions completed with the app.
The total amount of revenue generated via the app compared to total cost of  maintaining the app.
A model that can predict a repeat customer from among on-boarded customers with an accuracy of 85%.
The metrics were both technical (precision and accuracy of the model) and business-related (calculate return-on-investment).
Based on the business objectives, Cognistx developed an AI-enabled application for Monro Inc. called MotoManager. MotoManager captures a comprehensive profile of a customer through the onboarding process. Monro Inc. also sends a 50 coupon incentive to current and potential customers, and his coupon can be retrieved when a customer installs the application and completes their user profile. The MotoManager app uses customer data to provide customized reward incentives for booking services and making purchases from a customers local Mr. Tire store. As of 2019, the app has had 53,000 users, and Monro Inc. has reported significant increases in customer engagement and retention. The company has generated over 14 million US dollars from app-based transactions.",
Advanced Natural Language Processing,BERT,Bidirectional Encoder Representations from Transformers (BERT),"Both the encoder and the decoder stacks form a Transformer model as described in the previous module. However, each of the two parts can be used independently too.
Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having cbi-directionald attention and are often called auto-encoding models.
The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.
Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally, word classification), and extractive question answering.
Representatives of this family of models include BERT, ALBERT, RoBERTa.
Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.
The pretraining of decoder models usually revolves around predicting the next word in the sentence.
These models are best suited for tasks involving text generation.
Representatives of this family of models include CTRL, GPT, GPT-2.
We are now finally ready to study arguably the most famous encoder model, BERT and its variants in detail.
One of the latest milestones in NLP is the release of BERT (Bidirectional Encoder Representations from Transformers), an event described as marking the beginning of a new era in NLP. It achieved state-of-the-art performance on several language tasks.
BERT makes use of the Transformer architecture. In its vanilla form, a transformer includes two separate mechanisms  an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERTs goal is to generate a language model, only the encoder mechanism is necessary. In other words, BERT is basically a trained transformer encoder stack.
As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that its non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word). The fundamental units which enable the ability to comprehend the entire context of input without treating it like a sequence are made possible by the mechanism of attention. Attention is a mechanism that can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformer models use multi-head attention to efficiently capture the context and relative importance of input sequence and also enable parallelization of model training.
The original BERT paper presented two variants of BERT based on the number of encoder units (which the paper calls Transformer Blocks) used in the architecture.
BERT BASE is composed of 12 Encoder layers, 768 hidden units in the feedforward network and 12 attention heads for a total 110 million parameters.
BERT LARGE is composed of 24 Encoder layers, 1024 hidden units in the feedforward network and 16 attention heads for a total of 345 million parameters.
For any NLP task, BERT is generally trained in two steps:
First, the model is trained in a semi-supervised manner on textual data. This enables the model to develop a general sense of the language and to grasp its patterns. To build good language understanding, this step requires very large amounts of text and hence, training is resource-intensive.
Then, this pre-trained model is further fine-tuned for a specific task in a supervised manner with a labeled dataset. Additional layers can be added on top of the core model if needed. Since the pre-trained model already has some general language understanding, this step requires comparatively lesser data.
The first step is common across all tasks of a particular language and so models pre-trained on large amounts of text are often distributed publicly for fine-tuning directly for the task at hand. Thus, most of the time we only fine-tune. We will now try to understand the pre-training process which is done using Masked Language Modeling and Next Sentence Prediction.
Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a special [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. Output embedding from BERT corresponding to the [MASK] input token is passed through a final classification layer (feed-forward+softmax) which generates a probability vector of the size of the language vocabulary. Token with the maximum probability represents BERTs prediction of the masked token.
Figure 1. Illustration of masking and input flow across the model in BERT.
While training, the model receives pairs of sentences as input and through this objective, it learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.
Special tokens [CLS] and [SEP] are used to represent the start of the first and the second sentences in the input respectively. Output representation corresponding to the position of the [CLS] token is passed to a final classification layer (feed-forward+softmax) which predicts the likelihood of sentence B belonging with sentence A.
Figure 2: Illustration of the mechanism of next sentence prediction into BERT during training.
When training the BERT model, Masked LM and Next Sentence Prediction are used together, with the goal of minimizing the combined loss function of the two strategies.
Figure 3: Overview of fine-tuning BERT and usage of BERT in various downstream tasks.
Pre-trained BERT models can be used for a wide variety of language tasks by fine-tuning. Some examples are:
Classification tasks such as sentiment analysis are done like Next Sentence Prediction classification, by adding a classification layer on top of the Transformer output for the [CLS] token.
Question Answering tasks, where the system receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.
Named Entity Recognition (NER) where the system receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label. This is similar to what we saw in MLM.
Most plausible sentence continuation tasks where the system should choose between the best continuation for a given sentence given multiple choices. For this, all the possible concatenations are passed through BERT. A task-specific parameter vector is introduced whose dot product with the [CLS] token output representation denotes a score for each choice. These scores are normalized with a softmax layer to choose the best option.
The original BERT architecture has since been modified to improve performance in terms of speed or accuracy for different use cases. A few of the famous variants are discussed below.
ALBERT (A Lite BERT)
ALBERT model has 12 million parameters (with 768 hidden layers and 128 embedding layers) as compared to 110 million parameters of BERT-Base. The lighter model reduced the training time and inference time.
To achieve a lesser number of parameters, cross-layer parameter sharing is used in which the parameter of only the first encoder is learned and the same is used across all encoders. Instead of keeping the embedding layer at 768, the embedding layer is also reduced by factorization to 128 layers.
In addition to ALBERT being light, unlike BERT which works on NSP, ALBERT works on a concept called SOP (Sentence Order Prediction). SOP is a cclassification modeld where the goal is to cclassifyd whether the 2 given sentences are swapped or not i.e., whether they are in the right order.
DistilBERT (Distilled BERT)
DistilBERT has 40% fewer parameters than BERT-Base, and runs 60% faster while preserving over 95% of BERTs performances. It reduced the number of layers in BERT by a factor of two.
DistilBERT uses a technique called distillation, which approximates BERT, the large neural network, by a smaller one. The idea is that once a large neural network (teacher) has been trained, its full output distributions (its knowledge) can be approximated using a smaller network (student). This transfer learning technique is called teacher-student training.
RoBERTa (Robustly Optimized BERT pre-training Approach)
Introduced at Facebook, RoBERTa is a retraining of BERT with improved training methodology, 1000% more data and compute power.
To improve the training procedure, RoBERTa removed the Next Sentence Prediction (NSP) task from BERTs pre-training and introduced a dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure.
ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
Instead of MLM for pre-training, ELECTRA uses a task called cReplaced Token Detectiond (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.
This method of pre-training the model as a discriminator rather than a generator is more sample-efficient. As a result, the learned contextual representations outperform the ones learned by BERT given the same model size, data, and compute.",
Deep Learning and Model Deployment,Deep Learning & Computer Vision,Deep Learning Model Architectures,"Following are some of the key deep learning architecture/algorithms:
Artificial neural networks (ANNs) are composed of node layers containing an input layer, one or more hidden layers, and an output layer. An input node or a node in a hidden layer is connected to nodes in a subsequent hidden layer or an output layer with a weight.  Each node in a hidden layer or an output layer typically computes the weighted sum of its inputs and passes the result through an activation function.  The general name for such an ANN architecture is a multi-layer perceptron.
Figure 1. Deep Neutral Networks - Multiple Hidden Layers. Source: https://www.ibm.com/cloud/learn/neural-networks
A CNN is a multilayer neural network that was biologically inspired by the animal visual cortex. The architecture is particularly useful in image-processing applications. The first CNN was created by Yann LeCun, and his architecture focused on handwritten character recognition, such as postal code interpretation. As a deep network, early layers in a CNN recognize features (such as edges), and later layers recombine these features into higher-level attributes of the input. The LeNet CNN architecture is made up of several layers that implement feature extraction and then classification (see the following image). The image is divided into receptive fields that feed into a convolutional layer, which then extracts features from the input image. The next step is pooling, which reduces the dimensionality of the extracted features (through down-sampling) while retaining the most important information (typically, through max pooling). Another convolution and pooling step is then performed that feeds into a fully connected multilayer perceptron. The final output layer of this network is a set of nodes that identify features of the image (in this case, a node per identified number). You can train the network by using back-propagation.
Figure 2. LeNet CNN. Source: https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures
The use of deep layers of processing, convolutions, pooling, and a fully connected classification layer opened the door to various new applications of deep learning neural networks. In addition to image processing, CNN has been successfully applied to video recognition and various tasks within natural language processing.
The RNN is one of the foundational network architectures from which other deep learning architectures are built. The primary difference between a typical multi-layer network and a recurrent network is that, rather than completely feed-forward connections, a recurrent network might have connections that feed back into prior layers (or into the same layer). This feedback allows RNNs to maintain memory of past inputs and model relationships in time. The key differentiator is feedback within the network, which could manifest itself from a hidden layer, the output layer, or some combination thereof. RNNs can be unfolded in time and trained with standard back-propagation or by using a variant of back-propagation that is called back-propagation in time (BPTT).
RNN architectures suffer from vanishing and exploding gradient problems. To overcome these issues, LSTM and GRU architectures have been  developed and are described below:
Long Short-Term Memory (LSTM) Networks: The LSTM was created in 1997 by Hochreiter and Schmidhuber, but it has grown in popularity in recent years as an RNN architecture for various applications. The LSTM departed from typical neuron-based neural network architectures and instead introduced the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what's important and not just its last computed value.
The LSTM memory cell contains three gates that control how information flows into or out of the cell. The input gate controls when new information can flow into the memory. The forget gate controls when an existing piece of information is forgotten, allowing the cell to remember new data. Finally, the output gate controls when the information that is contained in the cell is used in the output from the cell. The cell also contains weights, which control each gate. The training algorithm, commonly BPTT, optimizes these weights based on the resulting network output error.
Gated Recurrent Unit (GRU) Networks: GRUs were proposed in 2014  as a simplification of the LSTM. This model has two gates, getting rid of the output gate present in the LSTM model. These gates are an update gate and a reset gate. The update gate indicates how much of the previous cell contents to maintain. The reset gate defines how to incorporate the new input with the previous cell contents. A GRU can model a standard RNN simply by setting the reset gate to 1 and the update gate to 0.
The GRU is simpler than the LSTM, can be trained more quickly, and can be more efficient in its execution. However, the LSTM can be more expressive and, with more data, can lead to better results.
Figure 3. Recurrent Neural Networks (RNN). Source: https://clay-atlas.com/blog/2020/06/02/machine-learning-cn-gru-note/",
Problem Identification and Solution Vision,Data Science Lifecycle,Phases of a Data Science Project,"Data Science projects can be complex in nature and require the input and efforts of many stakeholders. A Data Scientist will lead the process, and it is important that a well-defined workflow is followed. The workflow will ensure that all stakeholders are on the same page and requirements are defined and met.
A data scientist will produce a solution that is effective and achieves business and analytic objectives with the end goal of meeting a business need. In this module, we will explore the data science lifecycle. Keep in mind that, just like the system development life cycle (SDLC), the data science life cycle is not linear. Real-world problems will introduce hurdles that require the process to be iterative in nature. The lifecycle will give structure to the process and ensure that the data scientist stays on task.
Figure 2. Data Science Life Cycle (courtesy: Microsoft )
You can draw similarities between the CRISP-DM lifecycle and the data science lifecycle. Perusing the Internet, you will also find that different data science solution vendors have adapted the lifecycle to fit their products and the solutions supported by their tools. The data science lifecycle is briefly explained below:
Business Understanding. cWe fail more often because we solve the wrong problem than because we get the wrong solution to the right problem.d  Professor Russell L. Ackoff.
Data scientists are tasked with providing solutions to difficult business problems, and those solutions should be supported by factual data. Prior to solving a problem, it is important to understand the context of the business and the problem. This must include defining business and analytical objectives, as well as identifying data sources.
Data Acquisition. This process involves obtaining data from various sources and may also require setting up a data collection task and infrastructure.
Subsequently, you will perform data preparation to ensure the data is ready for analysis.
Data Preparation. This is the process of cleaning and transforming raw data prior to processing and analysis. This needs to be done carefully as assumptions made here may influence, or even limit, the use of the data during analysis.
Data Exploration and Cleaning. The quality of your dataset will determine your success in meeting your business objectives. Data exploration includes identifying variables, conducting univariate and multivariate analyses, identifying outliers, anomalies, and missing values, as well as feature creation and selection. We will cover these topics in future units.
Modeling. Later in this course, you will explore modeling and learn about choosing the appropriate model based on the problem. You will study algorithms to implement analytical models and tune their hyper-parameters to achieve the desired performance. We will learn about the balance between generalizability and performance. In general, you want your model to learn and perform well but also to be robust when tested on unseen data.
Feature Engineering is needed to prepare proper datasets that are compatible with the suitable algorithms and to improve the performance of models by leveraging domain knowledge to capture the signal of interest in the features.
Model Training is made efficient when you have adequately prepared your data and engineered new features. Model training involves maximizing performance and finding a balance between performance and generalization. Even in cases when a Data Scientist has collected millions of records, data should be considered and treated as a scarce resource since it may be expensive to obtain.
Models are trained on dedicated training data and evaluated on dedicated test data. Models should not be tested on the data they have been trained on. The ability to match the training performance on unseen test data is referred to as the model's ability to generalize. To operationalize this during training, a validation data set is often sampled from the test data to allow an estimation of test performance during training. In the lifecycle, it is important that this separation is anticipated early because it may influence what parts of the data may be surveyed for feature engineering and model design at the beginning of the project without violating the training/test split.
Model Evaluation is an essential step in the lifecycle. Typically, analytical solutions are meant to provide results when fitted with different datasets or when new data is introduced. Depending on the nature of the task (as stated in the analytic objective), model evaluation will follow corresponding metrics and techniques that will be explored in this course.
Deployment. Once you have evaluated your models to ensure accuracy and performance, you will deploy the model to an environment for application consumption.",
Collecting and Understanding Data,Ethics of Data Science,Accountability,"While data science often includes descriptive analysis (explaining what is actually happening), the prevalence of prescriptive analytics (explaining what needs to be done) continues to grow. As everything grows increasingly computerized and automated, data science has now become something that drives decision-making, sometimes without any input from a human. As these fully autonomous systems are entrusted with ever-greater responsibilities, unintentional and sometimes disastrous results can occur. We focus particularly on large automated systems because that is usually where the question of responsibility is most difficult or most consequential. To properly deal with the question of responsibility, we need to ask: How can we effectively control large automated systems?
When discussing accountability, we are not specifically looking for someone to blame when something goes wrong. What we strive for is for the systems we design to do what they are supposed to and do so responsibly and ethically, according to values or principles we wish them to adhere to. So, as data scientists, we carry considerable responsibility for any ethical failures of the systems weve engineered. When thinking about accountability mechanisms, we need to think about who specifically carries that responsibility. Responsibility is usually personal, but it can also be organizational.
Madeleine Elish wrote a very fascinating article in 2016 titled Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction. In this article, she introduces the concept of the moral crumple zone to describe how systems are designed to absorb the responsibility of human actors who have limited control over the behavior of an automated or autonomous system. Taken from the concept of automotive crumple zones, which are designed to be destroyed in an accident, absorbing the force of the impact and protecting the passengers, the moral crumple zone protects the integrity of the system, at the expense of the nearest human operator. Elish uses the idea of the moral crumple zone pejoratively, saying that someone is picked in advance to be blamed in the event that something goes badly wrong. If the system is designed to have someone intervene if something happens, it is the persons responsibility to intervene and prevent the worst from happening.
The key takeaway from this section is that accountability is not merely about finding who is to blame; blame can be engineered and planned in advance. Accountability is about all the little decisions made by a group of people who created a system, at each step of the way. There are both ex-ante and post hoc accountability, sometimes called anticipatory versus remedial. We need to think about accountability before something goes wrong and design a system for someone whom we are responsible for. If something goes wrong, we also need to figure out what went wrong and ensure it doesnt happen again. We care about both.
In the previous section, we talked about how data governance in an organization must embrace principles of transparency and auditability when making decisions about data. Accountability in the decision-making process is attained by designing and implementing data systems that are transparent and auditable. We will dive deeper into what those descriptors mean in this section.
When an ethical concern arises in a data science solution, transparency means disclosing the involvement and actions of human actors, the data being used and its source, the algorithms being used and their intent, or sometimes, the very presence of data science or AI solutions in the product or service in the first place.
In the past, data scientists have used human involvement or the lack of human involvement to justify the outcome of a data science solution. As the data science field progresses and AI applications become more prominent in our daily lives, governments, regulators, and users have all called for more transparency. Initiatives such as cWhy am I seeing this ad?d (Figure 1) is a progressive step toward solving the challenge of transparency in products and services.
Figure 1. Why am I seeing this ad? (Source: LinkedIn)
Figure 2. High-level Design of cWhy am I seeing this ad?d Initiative (Source: LinkedIn)
Figure 2 shows the flow of control of cWhy am I seeing this ad?d on LinkedIn, how the matching and standardization modules work in the backend, and how the results are displayed to the users. Although it is unlikely that all users of LinkedIn would go through the details of the algorithm that decides which ads to show, it is LinkedIns effort to ensure transparency and control to members. More importantly, such transparency could also ensure that the system can be easily audited, if necessary.
The title of this section is taken from a 2016 article by Mike Ananny and Kate Crawford. In this article, the authors challenge the ideal of transparency, its limitations, and alternative strategies for algorithmic accountability, exploring the following tenets:
Transparency can privilege seeing over understanding
Besides these limitations, other pitfalls of creverse-engineeredd as a strategy for transparency:
Set reasonable expectations to disclose what is known. While we may say that we need to understand and disclose how a data science solution works, there is a chance that we really dont know exactly how it works. We dont know what we dont know. When a customer wonders why their favorite product is being discontinued, it may not be known exactly why this decision was made. The decision-maker could have been acting logically but could also have been acting illogically, with no clear explanation for his or her decision.
""Complexity distributes responsibility"" - Joseph Weizenbaum (1976). When a computer program gets to a certain level of complexity, it is difficult or even impossible to identify who is responsible for which component of the system. This is the matter of ex-ante versus post hoc accountability, as mentioned in the previous section. As data science projects become increasingly complex, there are limitations on deciding who should be responsible when something goes wrong. This is a post hoc accountability matter. In this case, it is much more relevant and sensible to think about ex-ante accountability. That is, we all understand that it might be difficult to trace back who is responsible for the ethical failure, so everyone in a team needs to embrace ethical practices individually to avoid mistakes.
Without a critical audience, algorithms cannot be held accountable. Transparency is not a one-way street. It requires disclosures from the data scientist as well as critical audiences that take in the information and respond to it. Transparency without a proper audience is meaningless. More importantly, without a proper audience, transparency can lead to even greater ethical failures: it can mean knowing about an ethical failure without taking any action to prevent or remedy it.
Another component of accountability in data science is auditing. Auditing has been used in social science research as an experimental test to discover if a system is doing what it was intended to do and whether it results in desirable or undesirable consequences. Auditing in social science was developed in the 1970s by economists at the Department of Housing and Urban Development (HUD) to investigate racial discrimination in housing. The experiment involved sending two people, one black and one white, to apply for the same apartment at the same time, and seeing whether the landlord gave the two applicants different answers. They repeated this process many times. If the results showed that there was a pattern of landlords responding in favor of one group over the other, then this could be considered evidence of racial discrimination.
Auditing has been used in the United States to diagnose employment and racial discrimination. A famous field experiment on labor market discrimination is another example of how audit studies were conducted. The experimenter used made-up names that were likely to be associated with a particular race or gender and sent mocked-up resumes to employers using these names to see whether the applicants from ostensibly different groups received callbacks at different rates. What they found was that even with exactly the same resume, people received callbacks at different rates depending on their names.
In data science applications, when transparency might not be feasible due to the protection of trade secrets or prevention of the system being gamed by bad actors, auditing is a counterpart to transparency for accountability. Auditing can be performed by an internal team whose job is to think through security vulnerabilities within their own organization. Auditing can also be performed by an external party to test whether the system is doing any harm.","Limitations of the Transparency Ideal,Transparency can be disconnected from power,Transparency can be harmful,Transparency can intentionally occlude,Transparency can create false binaries,Transparency can invoke neoliberal models of agency,Transparency does not necessarily build trust,Transparency entails professional boundary work,Transparency has technical limitations,Transparency has temporal limitations"
Analytic Algorithms and Model Building,Bias/Variance Tradeoff,Cross-Validation,"When evaluating the performance of a model, there are methods that allow for your model to be fit multiple times with different subsets of a dataset. Model Assessment and Model Selection are key concepts of importance to every data scientist. You will assess your model to see its performance and select the most fitting model. How then can you test and validate your model to ensure that real-world data can be introduced to it?
There are multiple scenarios where you will not have access to a large enough dataset to estimate the test error rate of a model. However, you can not use this as an excuse not to test and validate your model. You can employ a method called holding out. With holding out, you are using a subset of the observations in your training dataset to be used to validate your model. This process will allow you to predict the responses to the observations used to validate the model. This approach is called the Validation Set Approach, and the data that was used during holding out is called the Validation Dataset. Similar to the results from fitting the model with the training data set, you will assess the error rate of the validation set approach using the mean squared error (MSE), which will provide an estimate of the test error rate for quantitative outputs.
Consider that the test error rate for the validation data set will depend on the observations included in the validation data set and not on the training data set. The validation data set test error rate might be overestimated when this approach is applied to statistical methods that require a large number of observations.
k-Fold Cross Validation
You should think about k as the number of groups that are formed as a result of splitting your dataset. Implementing k-fold cross-validation is straightforward. The dataset should be shuffled randomly and split into groups according to the chosen value of k. Each group will be used as the held-out validation dataset, while the others will be used as a training dataset. Your model will be trained with the training dataset and then evaluated with the held-out dataset. k-fold cross-validation is not costly to implement as other cross-validation techniques. It can be applied to most learning methods. You should assess your model's bias by calculating the mean of all error estimates. The model's variance is assessed by computing the standard deviation of all the error estimates. The lower the value for the bias and variance, the better, and this means your model is balanced.
Selecting k is not a random process, an inappropriate k will lead to a model that has a high bias or high variance. Remember, you want a balanced model with low bias and low variance.  Using a fixed value of k=10 has been empirically tested to show that the resulting model  will be a balanced model (low bias-low variance). k=10 and even k=5 yield test error rates that do not suffer from bias-variance issues.
This technique involves splitting the dataset to use one observation for validation and the rest of the dataset for training. The LOOCV technique presents less bias as it does not overestimate the test error rate as the technique continues to fit the model with as many observations as are in the dataset. There is no randomness in the dataset split. It is costly to implement (think about applying this technique to a large dataset), although it usually provides a reliable and unbiased estimate of model performance. A viable solution involves using polynomial regression to make the cost of this technique similar to that of fitting a single model, which, dues to mathematical convenience, can implement LOOCV with a single training session on all of the data.
LOOCV can be used with any kind of predictive modeling.
Leave One Out Cross Validation (LOOCV)
LOOCV will have a higher variance than the k-fold CV because, with LOOCV, models are trained on almost identical sets of observations, and this means that the outputs will be positively correlated with each other. With k-fold CV, when k is less than n, the output of your models is not as correlated as is the case with the LOOCV models.
Classification Problems: When Y is qualitative, we use the number of misclassified observations as a measure of the model's test error.
Regression Problems: When Y is quantitative, the MSE is used to measure the test error.
Reading: Cross-Validation: Python",
Collecting and Understanding Data,Ethics of Data Science,Algorithmic Bias,"One primary goal of data science is to generate specific conclusions based on the presentation of evidence or data. The term cdata-drivend reflects the central role of information we have about the past in making such inferences, as evidence is always something that was captured in the past, even if it reflects beliefs, attitudes, or plans we have about the future. We try our best as data scientists to make predictions or prescriptions about the futurebut we cannot capture information about the future in the present. It may sound obvious to point this out, but there are profound implications to considering this directionality of time. In an important sense, all of a data scientists work is bound up with information about the past. So is data science backward-looking?
For example, lets say you have a longitudinal dataset about income and education and you want to make some inferences from it. If the data go back far enough, you will get to the time in history when women were either prohibited or otherwise discouraged from furthering their education. As a result, it was often difficult for them to get into many professions, and their incomes were often correspondingly low. Now, if you didnt take that into account or consider it when creating your framing questions, you could end up with conclusions like cgirls arent smart enough to go to colleged or cwomen dont like high-paying jobs.d
The importance of recognizing that these conclusions, while technically possible but maybe problematic to assume, is that in history, there may have been situations where discrimination existed against girls or women getting an education, or against them working in certain fields. As data scientists, we must take these factors into account when analyzing data about the past, so that they do not impact our conclusions in such a way as to reproduce or perpetuate these problems in the future.
One can also argue that data science is discriminatory, in a sense. The nature of data science entails making predictions, and classifications, and separating one group from another. The prevalence of different subgroups in the data will directly impact how much each of those groups affects the final result. If we are trying to infer or optimize some parameter based on our dataset, but the data contains far more examples from group A than from group B, our inference or optimization will be inherently skewed toward members of group A. For example, if your job is to analyze a group of nurses at a hospital in order s to find the best performers, your model may find that most, or perhaps all, of the good nurses are females. If you were to take that model to predict the suitability of a candidate for a nurse position, you might wrongly decline a qualified male candidate.
As a data scientist, it is important to watch out for this bias toward what is most prevalent, or most cnormal,d about a given dataset under analysis. Because minority subgroups often exist in our data, and because many data science techniques can be adversely affected by this data imbalance, we need to be aware of these possibilities, take steps to account for them, and work to ensure that our results are based on sound reasoning and not unwarranted assumptions.
Reading: Introduction section of ""Raw Data"" Is An Oxymoron by Lisa Gitleman and Virginia Jackson.
The location, tier level, and capacity of all 6th grades in Boston for the 2016-17 academic year. Illustration by the Boston Area Research Initiative.
The school assignment algorithm implemented by the Boston Public School (BPS) starting in the 2014-2015 school year, using a school assignment policy to assign students to good schools as close as possible to their homes. The goal is to increase access to high-quality schools while reducing the commute. Researchers found that despite the program's implementation shortening students' distances and minimizing travel time to and from school, the system failed to allocate students to high-quality neighborhood schools due to the disparity in allocations of high-quality schools across Boston's neighborhoods. As a result, the new system had a disproportionately negative impact on minority families with limited access to high-quality schools in their own neighborhoods in the first place. While the program's intention was just, and reducing the distance to school and busing is desirable, the algorithm's design phase did not account for the limited quality-school availability among specific neighborhoods. The resulting algorithm could have been positive and supportive of the minority families but had a negative impact on them instead, in a way that could have been avoided with proper consideration and design.",
Exploratory Data Analysis,Feature Engineering,Module 13 Summary,This is a new page with empty contents.,
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Bayes Method,"The Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event. If you want to assess the risk of a person developing macular degeneration, the Bayes theorem supports accurately assessing that risk based on a certain age range instead of making assumptions.
Bayes Rule (Source: https://www.psychologyinaction.org)
Additional Reading: Bayes Theorem
Additional Reading: Overview of Bayesian Statistics
Bayesian Inference is applied when the Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. It is used in sports, medicine, and law, among other fields. Bayesian Inference derives the posterior probability as a consequence of a likelihood function and a prior probability. It is not the only updating rule, but it is widely used.
Named after Reverend Thomas Bayes, naive Bayes is a simple classifier that can be applied to categorical predictors. When classifying observations using NB, the classifier computes the probability for all possible classes given all the observed evidence and then classifies the observation as belonging to the class with the maximum posterior probability. When the problem calls for predicting the probability that an observation belongs to a class, we can use this method. Naive Bayes is based on applying the Bayes theorem and assumes that all predictors or observed features are independent. Although this is a naive assumption, naive Bayes performs quite well for real-world applications. A fruit that is green, round and 18cm in diameter can be considered to be a honeydew melon.  The NB classifier will assume that all these features independently contribute to the probability that the fruit with these features is honeydew melon. Naive Bayes can perform well with a small training dataset for estimating the right parameters for a classification task. A downside to this model outside of its naivety is that studies have been conducted, showing it does not perform as well as methods like random forests. NB is said to be a good classifier, but as an estimator, its probability outputs should are not as strong. When model complexity is not important, NB can be used for high-dimensional data. This is because when the dimension of a dataset is large, data points are more likely to be further apart than in cases with low-dimensional data.
NB  is not considered the go-to algorithm for estimating the probability of an observation's class as it is biased in its results, but it is quite useful for ranking and classification tasks. Assume that you introduce a new observation to your model, and this new observation has a categorical feature that has not been observed in the training dataset. NB will compute a zero probability to that record. Let's put this in a real context: if your response is has diabetes, and a predictor category is past pregnancy. Now assume that your training dataset has all observations with past pregnancy =0. All new observations with past pregnancy =1 will be classified as not having diabetes.
There are other Bayesian Methods that can be used in Data Science, these are explored in machine learning and applied to statistics courses.",Naive Bayes (NB) Method
Problem Identification and Solution Vision,Data Science Lifecycle,Module 3 Summary,"The data science lifecycle structures the activities of the data science team. It should not be considered to be linear as there must be iterations of questioning and research involved in each phase. The framework consists of several major stages:  The framework involves input from various members of the data science team, as well as the client, as stated below:
Business Understanding involves framing the objectives and assessing data science readiness. The client and data science team are involved in this stage to ensure that the analytic solutions meet the business objectives.
Data Acquisition involves gathering data from various appropriate sources. Data preparation techniques are employed to ensure the data is useful for analysis.
Modeling involves choosing the appropriate model for the problem (we see why business understanding comes first!) even though it is often mistaken as the first stage of the process. Modeling typically consists of feature engineering, algorithm selection, model training, and evaluation.
Deployment involves the implementation of the solution developed in the operating environment of the business. One should always remember that a business needs to measure the impact of the deployed solution to ensure the success of the solution.
The data science lifecycle can seem daunting at first, but a data scientist will not complete all the tasks alone. A productive team will consist of individuals with complementary skills filling various roles that can ensure the project is successfully executed.",
Analytic Algorithms and Model Building,Supervised and Unsupersived Techniques,Active Learning,"This data science pattern is different from what you have studied in this course as it makes assumptions about an algorithm and the data that is used to construct it. Active Learning pattern posits that if an algorithm or learner can choose the data, it will learn from, it will perform better than an algorithm that does not choose its own data, and it will perform better with less training. Active learning is sometimes referred to as query learning. The learning methods you have used so far when you sample and gather data and transform it to train a model are considered the traditional methods. When you have a large data set that is unlabeled (as is typical), active learning can be a useful technique for labeling.
Active learning presents Scenarios that allow a learner to query the labels of observations in a dataset.
Membership Query Synthesis is a scenario that enables a learner will generate an observation that is similar to one or more in the dataset. Once it is created, the new observation can then be labeled by the oracle (an information source or teacher).
Stream-based Selective Sampling scenario involves unlabeled data points or observations that are evaluated by the algorithm as to whether these points should be labeled by for training or discarded. Pool Based Sampling, as shown in the figure below, assumes that you have a pool of unlabeled data, and observations are collected from the pool according to an informativeness measure (certainty that a classifier has when classifying data points). The informativeness measure is applied to all observations in your dataset, and then the observations that have the most important measures are selected. The selected observations are then labeled.
Pool Based Active Learning Cycle-Source: Settles Active Learning Survey1
How does the algorithm decide on the most informative measures? Let's highlight some of the strategies used to evaluate the informativeness of unlabeled data.
Uncertainty Sampling is an approach that allows the active learner to query the observations about which it is not able to label.
Query-by-committee involves using a group or committee of models that have been trained on a labeled dataset, but the catch is that these models have competing hypotheses. Each model in the committee will vote on the labels. Identify the query that all voting models disagree on that becomes the most informative query.
Expected Model Change would use an approach that selects the observation that would introduce the most change to a current model if its label was known.
Expected Error Change involves labeling the data points that would reduce the model's out-of-sample error (a measure of how accurately your learner can make predictions on new data).
Additional Reading: Survey of Active Learning. This report gives an in depth review of active learning in machine learning and artificial intelligence.","Thought: Informative data points equal a data point that your algorithm had difficulty classifying. Informative data points improve your algorithm's abilities (prediction and otherwise).,Query Strategies"
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Processors,"Besides memory, the other main aspect of any hardware system you will need to assess in order to understand if its important for computing are the processors inside the system. In data science, you will see a variety of processors being used, but they tend to split into three main categories, from least expensive to most expensive: CPUs, GPUs, and DSAs.
The primary processor on a system, the Central Processing Unit, is used on most systems for the majority of complex calculations unless the application developer specifically invokes another processor. They can handle less parallelism than GPUs but are more able to handle longer sequences of branching statements with ease.
Also known as the Graphics Processing Unit, this is an additional processor present in systems to help manage graphics and other calculation-intensive operations where there is significant data parallelism, i.e., where we can split the data into chunks and process each chunk separately. While most systems nowadays have an integrated GPU of some kind, in data science, we tend to focus on systems that have a separate GPU, which has performance in mind. As data science applications and graphics applications require similar data-parallel computation, these tend to be much faster in some tasks than CPUs.
Also known as Domain-Specific Architectures, this category ranges from Googles TPU to Intels Crest. These are purpose-built systems to solve computationally expensive modeling problems, like those found in neural networks. These tend to bring the largest performance gains for data science but are further limited in what they can do, as they are built to solve specific problems and can be difficult to program directly.
While it is tempting to use the most-efficient DSAs and try to squeeze as much performance as possible out of the newest systems, remember that, in the data science process, you will need to budget cost as well as time. It might be useful to use a DSA or a GPU, but you should remember to do the following when deciding whether to use either chip for a project or not:
If you have access to trial usage of the DSA/GPU for your project, check that the tools you are using utilize the accelerators at all. As these chips require separate programming APIs to utilize, the tools which you use them might not be compatible out of the box or at all.
Additionally, before setting and forgetting your system, check the usage patterns of the code you are able to run.
If the code is relatively I/O, Network, or Memory intensive, it might not make sense to use an accelerator chip, i.e., a GPU or a DSA, in your system. Instead, it might pay to try to use multiple processors or multiple computers together to solve the work in question.
If the code is computationally expensive, check to see if the memory usage aligns with the memory limits of your GPU/DSA. As these chips have their own memory, keeping to such limits can ensure that your code runs smoothly.
Lastly, and most importantly, use profiling tools on your code. While such tools can be difficult to use at first, they provide the best way to see immediately where the potential slowdowns are and can give you ideas about where you need to optimize your code. If you do not want to use a profiler, you could even use just a simple timer in your program and count the time taken to run some hot sections accordingly.",
Advanced Natural Language Processing,Language Representation and Transformers,Transformers,"The Transformer model was introduced in the famous paper Attention is All You Need in 2017.
A good way to understand Transformers is to think about the fact that in the Sequence2Sequence models with attention, we are replacing the one final context vector with a hidden state generated for every output step. So do we need the hidden states at all? After all, attention alignment is supposed to define which part of the input the given output step should focus on, and the hidden states are only an indirect representation of input embeddings. A given hidden state vector represents the context of all input steps until that point and not just a single input embedding alone. Wouldnt using the input embeddings directly make more sense?
Transformers do exactly this by replacing the sequential processing performed by RNNs in Sequence2Sequence  models with a simpler attention mechanism.
Figure 5: Interactions within components in different architectures.
Instead of using attention to connect the encoder and decoder, Transformers use attention within the encoder and decoder blocks. Instead of deriving hidden states using RNNs, they use self-attention.
Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.
On the encoder side, Transformers use self-attention to generate a richer representation of a given input step \\(x_i\\), with respect to all other items in the input \\(x_1,x_2 \\dots x_n\\). This can be done for all input steps in parallel, unlike hidden state generation in an RNN-based encoder.
On the decoder side, an attention-based decoder is used. There are no hidden states anymore and no computation of a separate context vector for every decoder step. Instead, at a particular time-step, self-attention on all outputs generated till that point \\(y_1,y_2 \\dots y_{i-1}\\) along with the entire encoder output is used to generate \\(y_i\\). In other words, we are applying attention to whatever we know so far.",
Data Science Project Planning,Requirements Gathering,Types of Requirements,"Suppose a utility company wants to introduce personalized service management to its customers, including a secure electronic payment facility and the ability to view usage statistics. This business objective will lead to the development of a mobile application for customer service management. Business requirements for the above scenario will include describing the context, scope, and background of the business need, including the reasons for the proposed change. Business requirements are collected and decomposed to define other types of requirements.
Once business requirements are defined, stakeholders and systems that support the business requirement(s) are identified. System requirements are a detailed description of the system and its operational and development constraints. They include the system software that will support the solution, processing and memory requirements, and other application software considerations. User requirements describe functions or tasks that a user must perform within the system. These tasks will support the business objectives that are defined prior to the requirements gathering process.
Use cases and user stories represent user requirements and provide a big picture of what the user will be able to do within a system. An example of a use case is cMake Paymentd on a mobile application. We will describe user and system requirements later in this unit.
Solution requirements are grounded in software engineering. In this course, we will be tailoring solution requirements to the data science process. The solution requirements for a data science project are classified into functional and non-functional requirements, as well as requirements that consider parts of an analytic solution that are different from the traditional IT systems. The typical analytic solution will consider data and models (e.g., predictive models), and a business intelligence solution will include requirements for reports and dashboards.","Business Requirements,System and User Requirements,Solution Requirements"
Deep Learning and Model Deployment,Model Deployment,"[Additional Resource] Hands-on: Data Versioning + Code Versioning with DVC, GIT, and AWS\xc2\xa0S3","AI projects are data intensive. Data can be
public (from government websites and open data)
paid (from the marketplace, brokers, and other services)
gathered (from customers using the product platform)
owned (from employees like annotators who manually create data)
We get the data mentioned above periodically to build models. Therefore, we need to analyze the quality of data and model performance every time new data is available.
It is critical to track data efficiently. For example, let's say we are working with a customer who provides us with newly collected data weekly. Let us also assume we found a drift in data distribution, and we want to roll back to the model version and keep tuning the model from that checkpoint. The previous model can be reproduced by model parameters (weights) and hyper-parameters. However, we might want to reproduce the model using another library that optimizes the performance, which requires retraining or using old data to build another model. Therefore, we need a tool to version model weights and training data efficiently.
Sometimes, after our initial engagement, we return to old projects based on customer requirements. We might want to reproduce the previous models built by other developers to deliver predictions to customers. However, the performance of models depends on the data used to train the model. Hence, data needs to be tracked periodically.
The Git system (e.g., in Assembla / Atlassian) can only track comparatively small files (e.g., the source code) used for the project. Because git contains a complete history of file changes, disk and memory requirements will grow significantly if we commit data files. Hence, DVC (Data Versioning Control) aims to bring git to projects that use a lot of data and helps to track and version data efficiently. DVC is also easy to learn as it runs on top of git and uses the same git vocabulary.
git-lfs can be a solution for data versioning using pointers and remote storage. However, one of the main advantages of DVC over git-lfs is it doesn't require installing a dedicated server, and it can be used with cloud storage like AWS S3, GCP, Azure, etc. We can also assign tags for each data file version, which allows us to track necessary metadata, such as who gave us the data and when we got it.
Managing the Data Quality
Automating testing and deployment
Easy integration with MLflow, which helps in tracking experiments
1. Install Anaconda
2. Define the project folder structure
Example
Clone the above sample repo here.
3. Create a virtual environment for DVC in the respective project folder and activate it
4. Install dvc
5. Install boto3 for pushing data to AWS S3
6. Install dvc[s3]
7. Initiate DVC which creates .dvc/.gitignore, .dvc/config and .dvcignore files
8. Commit the above dvc files
9. Avoid tracking the data folder for git
10. DVC tracks the data files by adding a data folder to the DVC cache. It prevents adding to GIT by implicitly adding the data folder to .gitignore.
11. Git add and commit data.dvc
12. Create a bucket on S3
13. DVC Remote Adda-acreates a remote section in DVC's config file
14. Set AWS credentials
15. Push the data folder to S3
16. Remove AWS credential information in .dvc/config before pushing to git
17. Push the updates to git
Kudos, you completed the hands-on Data Versioning tutorial with DVC and AWS S3. Now you have the version history of the data and can revisit the respective files in the future.","Why are traditional code versioning tools inefficient in tracking data?,Other advantages of Data Tracking efficiently help in:,Getting started with creating a repo and pushing code to Assembla and data to S3 (using DVC)"
Deep Learning and Model Deployment,CPU vs. GPU,Hardware: An Overlooked Aspect of Data Science,"At the beginning of a  data science process on any task, it is important not just to understand the dataset and the tools available but also the hardware resources available to you for the task at hand. Most data science tasks rely on using statistical or numerical techniques to process the data and/or optimize some discrete/continuous optimization task. In doing so, youll need to assess if a task is feasible with the hardware you have and what hardware to use to make the most of the computational budget youre given to solve the analytic objective.
In this chapter, our goal will be to give a brief tour of what hardware resources one should look at when deciding what hardware to get for a specific task. While it will not be overly exhaustive, our goal here is twofold:
First, you will want to spend time to make sure that the tools you use actively use the computational resources you are given. Taking the time to understand if and when you can scale down any cloud computing resources you are using can save you time and money in performing any computational task, which then can be a boon as you assess different statistical techniques and their performance in your analytic objective.
Second, you will want to assess how you are using the various libraries common to data science. Compared to more traditional single-threaded and multi-threaded applications, data science libraries tend to require users to focus on operations, unlike those more traditionally used, such as conditional indexing and matrix multiplication. In this chapter, youll find motivation for learning these libraries to the depth necessary to perform these tasks, as they form the basis for highly performant and highly readable data science code.",
Deep Learning and Model Deployment,CPU vs. GPU,The Basics of Hardware: Memory,"For our purposes, before we get into the differences between CPUs and GPUs, it is important to take some time to first understand memory and its role in computation.
While it is incredibly difficult, if not impossible, to know the relative speeds of different operations on a computer, accessing and loading data from memory tends to be one of the slowest if poorly done. To better use memory, you need to understand that memory is not a singular shelf from which you can pull and place data. Instead, it operates as a series of increasingly higher shelves, with those shelves at the bottom being much more expensive but way faster to reach than the shelves at the top. This is known as a memory hierarchy; it serves as a great basic model of memory for performance-intensive applications like those found in data science.
The memory hierarchy can frequently be pictured as a pyramid, where at the top lie the bits of memory that are the fastest to access but the most expensive to get, while those at the bottom are the slowest to access but the least expensive to buy:
With this hierarchy of memory, most computer systems will attempt to try to find some memory at the top level and then keep moving down until the datum is found. Once it is found, the system will then note the datum / the datums location in higher levels, with the hope that the datum, or the datum around it, will be useful later on. Youve implemented this partially in P1.
As most of this management is done outside of your control, it can be frustrating to try to figure out how to make data access code faster. However, while the systems managing the data are opaque, it pays to instead focus on two simple facts that underlie all attempts at managing this hierarchy:
Temporal Locality: When you access some data on your computer once, youre probably going to access it again in the near future. Thus, if you can keep the data youre accessing to one small section at a time, you can receive better performance.
Spatial Locality: When you access one element in an array or another complex data structure, you are probably going to access data next to it. Thus, the management system will not just store the data at your location, but also the data near your requested location for future quick access. Thus, if you can access data sequentially in memory, you can receive better performance.
Following these two principles can help guide you towards faster code in general. For example, consider the following two pseudocode sections:
The first code section accesses the elements of <![CDATA[array]]> in order, from 1 to 100, while the second section accesses the elements of <![CDATA[array]]> randomly. While both tasks perform essentially the same task, namely that of summing the elements of an array, the code in section 1 will be much faster than that of section 2, as when we access <![CDATA[array[1]]]> The computer stores the elements near it in the cache, allowing for their rapid access compared to the access pattern in section 2. A good general rule of thumb here is simple and direct access patterns beat complex and potentially insufficient algorithms here. In fact, it is for this reason that the traditional \\(O(n^3)\\) matrix-multiplication algorithm tends to beat fancier algorithms in benchmark tests. While there are analytically faster algorithms for the problem, the simple access pattern, combined with ease of tuning, ensures that competitive performance is maintained for longer than more complicated algorithms.",
Problem Identification and Solution Vision,Problem Identification,"AI Philosophy: A Process, not a Product","AI Philosophy: A Process, not a Product
AI Philosophy: A Process, not a Product
The provision of analytical solutions to an organization requires understanding the organizations needs and its readiness to incorporate and support any analytical solution. A good solution will fail if the organization and its stakeholders are not equipped to support the solution. When engaging with potential clients seeking analytical solutions, it is important to assess the organizations readiness.
Data Science Ready. The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science (statistical analysis, correlation, prediction, etc.). At this point, the organization is ready to begin an engagement with data scientists and AI developers. The Case in point scenario showed that Monro Inc. was data science ready. During interviews with the information technology team, the data manager provided adequate information about the companys data architecture, appropriate data sources that would support the MotoManager app, as well as data that would be useful to build the desired prediction model.
Data Science Enabled. A preliminary analysis indicates that the data supports the desired analytic objectives (useful correlations are identified, predictive models prove to be accurate enough, etc.). At this point, the organization can claim that it is ready to use data science to influence the capabilities of its workflows, products, and services.
AI Ready. The organization has determined how to leverage the insights from data science (e.g., predicting customer preference) as part of an operational process and has implemented the appropriate software or software extensions to integrate data science into a relevant workflow, product, or service. At this point, the organization can claim that it understands how to incorporate AI into its workflows, products, and services to provide enhanced capabilities for end-users.
AI Enabled. The organization has deployed the new software in a relevant context and is able to directly measure the impact (e.g., increased sales). At this point, the organization can claim that it has implemented an AI solution and is gathering feedback to show that it really works with real end-users. An organization can introduce the data science decision into a real-world setting and measure if this implementation works.",
