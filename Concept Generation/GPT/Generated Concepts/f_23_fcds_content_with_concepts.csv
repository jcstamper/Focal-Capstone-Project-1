Filename,Key Concepts
a02cc691384d43edbaf707f306841c3b.xml,"[""Data scientist's role involves utilizing computational and statistical skills to uncover solutions that meet business needs."", 'Developing algorithms and building models are crucial for the analytic solution in data science projects.', 'Data understanding phase involves data acquisition and data preparation.', 'Quality of data has a direct impact on the quality of the analytic solution.', 'Data wrangling is the process of gathering, selecting, and transforming data to ensure usability and minimize bias.', 'Data management ensures data reliability, validity, and integrity.', 'Data management involves planning, oversight, and control over data and data-related resources.', 'Data architecture refers to the overall structure of data and data-related resources.', 'Data modeling and design involve analysis, design, building, testing, and maintenance of data.', 'Data storage and operations involve structured physical data assets storage deployment and management.', 'Data security ensures privacy, confidentiality, and appropriate access to data.', 'Data integration and interoperability involve acquisition, transformation, movement, and delivery of data.', 'Documents and content management involves storing, protecting, indexing, and enabling access to unstructured data.', 'Reference and master data management reduces redundancy and ensures better data quality.', 'Data warehousing and business intelligence involve managing analytical data processing']"
a0abc3fa0e6b44aaaf5e70e36602b61e.xml,"['Each data science project is unique and requires a data collection technique that suits the problem and objectives.', 'The data collection techniques for different projects vary based on the analysis being performed.', 'Business and analytic objectives drive the tasks and methods proposed for a project.', 'The type of data used in a project is influenced by the tasks and methods chosen.', 'Traditional techniques for data collection include conducting interviews, focus groups, document analysis, and observations.', 'The type of data affects the method employed to solve the analytic objective.', 'A data scientist should be aware of different sources of data and how they affect the solution development process.', 'Data can be sourced by the client but controlled by an external source, or collected and owned by the client.']"
a0d6e696158d44cb993ac9175f224c55.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: a0d6e696158d44cb993ac9175f224c55', 'Title: New Page', 'Body content: Empty contents']"
a15a0946f8364c0a81c2d538954588ec.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: a15a0946f8364c0a81c2d538954588ec', 'Title: New Page', 'Body content: Empty contents']"
a28b204fb8954fceb52a3e846fdd5698.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: a28b204fb8954fceb52a3e846fdd5698', 'Title: New Page', 'Body content: Empty contents']"
a2aba8cb20d84feb92654647946ecbb8.xml,"['Prediction: Using a model to predict outcomes and understanding how dependent variables are affected by changes in independent variables.', 'Inference: Understanding how dependent variables are affected by changes in independent variables.', 'Parametric Methods: Assuming that the dataset comes from a population with fixed parameters.', 'Non-Parametric Methods: Seeking an estimate of ""f"" that is close to the data points.', 'Linear Model: A restrictive linear model that shows the relationship between the dependent variable and independent variables.', 'Classification: Labeling outcomes of a new dataset based on an existing dataset with labeled outcomes.', 'Types of Classification Problems: Binary Classification, Multi-Class Classification, and Multi-Label Classification.', 'Logistic Regression: Using a logistic function to model the probability of a class or event.', 'Regression Techniques: Investigating the relationship between input variables and output variables and predicting the average value of an output variable.', 'Simple Linear Regression Model: Predicting a response based on one predictor variable.', 'Multiple Linear Regression Method: Accounting for multiple predictors in the regression model.', 'Goodness of Fit: Assessing how well a model fits the data it is trained with.', 'Cluster Analysis: Studying the hidden structure of data and identifying different groups']"
a2c9cadf120741b1b0709f4ea57eea38.xml,"['Exploratory Data Analysis (EDA)', 'Importance of Exploratory Data Analysis', 'Identify trends and patterns in the data', 'Highlight outlier values in the data', 'Draw conclusions on the data even without modeling', 'Visualize the data to pinpoint points of action', 'Visually inspect the data', 'Perform basic statistical analysis on each column/variable', 'Graph columns using simple visualizations', 'Sample Dataset (Four (4) sets, 11 observations with two (2) variables each)', 'Averages for sample dataset', 'Summary Measures of Sample Data', 'Visual Representation', 'Plots of Sets within Sample Dataset']"
a3e60ec52fd1485daf95a85dac31bbff.xml,"['Validating Requirements', 'Evaluation at each step of the process', 'Errors identification and fixing', 'Validation and verification exercises', 'Confirming requirements meet business and user needs', 'Tracing requirements back to business and analytic objectives', 'Properties of high-quality requirements', 'Performing validation before delivering the solution', 'Testing without implementation', 'Validating requirements throughout the solution development process', 'Steps to validate requirements: ', 'Review requirements', 'Prototype requirements', 'Test requirements', 'Peer review of requirements management plan', 'Producing a summary of defects document', 'Prototyping techniques', 'Acceptance tests and acceptance criteria', 'SMART goals for defining acceptance criteria', 'Formal sign off for approved and accepted requirements', 'Beginning development of the solution/solutions']"
a4167b26abaf41c999d256ed199c8127.xml,"['Technique used to convert raw data from its numeric or categorical format to a format that can be used when performing modeling.', 'Mathematical models mostly ""understand"" data in numeric form, hence the need for transforming raw data.', 'Features should be normalized as needed to meet the assumptions of the mathematical models during the transformation process.', 'Categorical variables can be transformed to numeric format using feature engineering techniques including categorical variable encoding.', 'Bias can be introduced to the data during the feature engineering process, and there are different kinds of biases and ways to control for them.', 'Principal Component Analysis is a way of using modeling techniques for dimensionality reduction in the dataset.', 'Transitioning from the data understanding phase to modeling.']"
a47d2cb6edc24ca7a5bb1b42b9021790.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: a47d2cb6edc24ca7a5bb1b42b9021790', 'Title: New Page', 'Body content: Empty contents']"
a8503d398c5545a28640f6fda9980856.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: a8503d398c5545a28640f6fda9980856', 'Title: New Page', 'Body content: Empty contents']"
a9f596c78e124ac4a71bb63f4af2b672.xml,"['Supervised learning involves training models with labeled input and output data.', 'KNN is a similarity function and a weak learner, useful for large datasets that will be updated continuously.', 'Bayes Theorem describes the probability of an event based on prior knowledge of conditions related to that event.', 'Naive Bayes is a simple classifier that can be applied to categorical predictors.', 'Tree based methods, such as Classification and Regression Trees (CART), are considered simpler methods for prediction and classification.', 'Ensemble models combine other models to produce an optimal predictive model and solve the problem of overfitting.', 'Bagging reduces variance in a decision tree method.', 'Random Forest method is an extension of bagging and remedies overfitting issues.', 'Boosting can be used to improve the predictive accuracy of certain methods, including decision trees.', 'Taking the quiz.']"
aa8999d9a25e49669369e85bd9c12219.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: aa8999d9a25e49669369e85bd9c12219', 'Title: Summary', 'Body content: ', '  - Paragraph with ID: d6a1b227cad74365824ea3bf8a81addf, containing the text ""This is a new page with empty contents.""', '  - Empty paragraph with ID: c4476ff9f352475aa168bb44ba1ff506', '  - Activity with ID reference: newd0be3676060e44379a1de0bf2099a9fa, purpose: quiz']"
aacbbe73fa6f4615b3172bddb51194d8.xml,"['Bias-Variance Trade-Off', 'Data Science Knowledge']"
ac4605b2a7c54814adae760db0cd03d3.xml,"['Completing the task that meets analytic objectives', 'Defining a problem and setting analytic and project objectives', 'Defining requirements', 'Performing exploratory data analysis', 'Building models', 'Examining the impact of the solution on decision makers and those affected', 'The tradeoff between model accuracy and interpretability', 'The relationship between inputs and output in a model', 'The ability of a model to answer questions and support decision making', 'Making tradeoffs between model accuracy and interpretability in data science.']"
af529a5ec36b4e2aa5ef42060df029ce.xml,"['Cluster Analysis: The technique used to study the hidden structure of data and identify different groups within that structure.', 'Types of clustering techniques: There are different types of clustering techniques that will be explored in an upcoming module.', 'K-means clustering: A widely used clustering technique that computes the distance between data points in a group and the center of the group. It can only be applied to numerical variables and uses euclidean distance as a similarity measure.', 'Hierarchical clustering: Connects data points to form clusters based on their distance. It can be agglomerative (grouping clusters based on similarity) or divisive (putting all data points in one cluster and then separating them based on dissimilarity).', 'Dendrogram: A representation of hierarchical clusters, where the y-axis marks the distance where clusters merge and data points are placed on the x-axis.', 'Evaluation of clustering results: Similar to regression and classification techniques, clustering output should be evaluated. Evaluation methods differ based on the clustering technique used. Module 14 will focus on different types of clustering techniques and their evaluation techniques.', 'Reading: A link to a resource on evaluating clustering results.']"
afe43b3ca23848d29a3af1d12f9c426d.xml,"['Dimensionality: The concept of dimensionality in data science, including issues with over-fitting and computing costs, and the curse of dimensionality.', 'Feature Extraction: A technique used to reduce the dimensionality of a dataset, also known as principal component analysis (PCA).', 'Principal Component Analysis (PCA): A technique used to reduce the dimensionality of a dataset by assessing the relationship between variables and selecting the most useful ones.', 'Linear Discriminant Analysis (LDA): Another technique for dimension reduction mentioned in the text.', 'Eigenvectors and Eigenvalues: Mathematical concepts used in PCA to perform the eigendecomposition on the covariance matrix.', 'Steps of PCA: The specific steps involved in performing PCA, including standardizing the data, computing the covariance matrix, computing eigenvectors and eigenvalues, sorting eigenvalues, constructing the projection matrix, and transforming the original data set.', 'PCA in Python Example: A link to an example of implementing PCA in Python.', 'Reading: A link to a brief article on Principal Component Analysis.']"
b05c77bbc0954397a6d500bb6583d1df.xml,"['Accuracy Versus Interpretability', 'Data Science Knowledge']"
b14af454ec8047b2a9c2388d60771c6e.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: b14af454ec8047b2a9c2388d60771c6e', 'Title: New Page', 'Body content: ', '  - Paragraph with ID: d0f61d3b3f4946a48893505faa509f1b', '  - Empty paragraph with ID: c43f5348cd064129bcb715e21d196720', '  - Inline reference with ID: newb90d26b74f994d7aaeaced56c80c55a1 and purpose ""learnbydoing""']"
b18d448de2514761bc6bda97d9b4bb60.xml,"['Analytic objectives: The text mentions the importance of formulating analytic objectives that state how the application of analytical methods to data can facilitate reaching business goals.', '', 'Business goals: The text refers to the identification of business goals as the starting point for formulating analytic objectives.', '', 'Template for analytic objectives: The text mentions a template that can be used to phrase analytic objectives.', '', 'Incremental steps towards business objectives: The text discusses the concept of working towards solving a problem by focusing on specific tasks and applying analytic methods in conjunction with data.', '', 'Analytic methods: The text emphasizes the use of analytic methods in data science work.', '', 'Data: The text highlights the importance of using data in conjunction with analytic methods to create valuable functionality and produce insights.', '', 'Research hypothesis: The text suggests that the formulation of an analytic objective can be considered a research hypothesis, as it posits that a certain technique improves a metric with regard to a population of problem instances.', '', 'Statistical tests: The text mentions that the effectiveness of data science methods is regularly scrutinized using statistical tests in academic settings.', '', 'Methodological rigor: The text encourages striving for methodological rigor in industry applications of data science methods.', '', 'Communication with clients and domain experts: The text']"
b48ebd2cb2c34ab9aa39bb99cbc2a193.xml,"['Data science process emphasizes understanding business needs and objectives, and defining analytic objectives to meet the expectations of the client.', 'Requirements gathering process involves identifying stakeholders, eliciting needs, and defining requirements.', 'Difference between requirements of a traditional IT project and those of a data science project is the focus on requirements for the analytic solution.', 'Functional requirements define the functions of a system and how users will interact with the system.', 'Functional requirements are derived from user and system requirements needed to satisfy business requirements.', 'Use cases describe the interaction between the system and its users.', 'Functional requirements are organized by priority, with high priority requirements being implemented first.', 'Not all functional requirements are implemented in the first iteration of solution development.', 'Traditional functional requirements in software and application development include business rules, process flows, audit tracking, transaction handling, reporting requirements, administration functions, authorization requirements, and data management.', 'Requirements can be captured in different formats including user stories, use case specifications, voice of the customer, and business rules.', 'Initial user requirements are often written too broad and need to be decomposed into functional requirements.', 'Use case specifications provide a textual description of a use case and decompose user requirements into functional requirements.', '3']"
b54b9560ac4f474e948597c8f4cc8ce7.xml,"['Model interpretation', 'Data Scientist', 'Client', 'End user empowerment', 'Trust in a model', 'Doctor', 'Flu prediction', 'Decision making', 'Interpretability', 'Machine learning', 'Human decision makers', 'Predictions', 'Problem solving', 'Doshi Velez & Kim (2017)', 'Bias identification', 'Acceptance', 'Debugging', 'Auditing', 'Impact', 'Context', 'Assessments', 'Metrics', 'Trustworthiness', 'Reading', 'Technique', 'Classifiers', 'Neural networks', 'Local Surrogate Models', 'Black box models', 'Shapley Value', 'Features', 'Importance', 'Additional Resource', 'Sara Hooker', 'Perfect Model']"
b70d1886fc7e48a98d1941febbde3152.xml,"['Formulating the analytic objective/research hypothesis', 'Understanding the business objective', 'Identifying the problem', 'Focusing on a well-defined task', 'Checking proposed method against the target insight', 'Proposing data collection and curation methods', 'Framing the analytic objective', 'Reframing analytic objectives to focus on specific tasks', 'Incremental steps as the main goal', 'Specific functionality, insight, or resource gained from leveraging data and methods', 'Comparison to the current situation', 'Assuming the project is successful as proposed.']"
b7ceae5042564a13b7868f95bddacfcb.xml,"['There is a tradeoff between model accuracy and model interpretation.', 'Models that are easily interpretable tend to be less accurate and models that are highly accurate tend to be less interpretable.', 'We want to be able to explain a model to a human being to ensure better decision making for clients and end users.', 'Interpretability is important in the model building process.', 'Understanding the why behind results leads to early bias detection, better acceptance, model debugging, and satisfying human curiosity.', 'Completing Module 16 is necessary before starting Quiz 10.']"
bac22ecf62614913b53e8b09ee38b8a6.xml,"['Internal Evaluation: Evaluates clusters based on similarity within the cluster and dissimilarity with other clusters. It assigns a score to each cluster and identifies the best cluster based on the score. However, it does not assess the validity of the results in the clusters.', '', 'Silhouette Coefficient: Measures how similar a data point is to its cluster compared to other clusters. It is calculated using the mean intra-cluster distance and the mean nearest cluster distance. A coefficient of 1 is best, -1 is worst (indicating the sample is in the wrong cluster), and close to 0 indicates overlapping clusters.', '', 'Dunn Index: Evaluates clustering techniques based on the compactness and separation of clusters. It calculates the distance between data points within a cluster and between data points in different clusters. The index is computed as min.separation/max.diameter.', '', 'External Evaluation: Measures clustering results based on data not used for the clustering task. It requires ground truth data for evaluation.', '', 'Rand Index: Determines the similarity between clusters and a set benchmark. It is calculated as (TP + TN) / (TP + FP + FN + TN).', '', 'Purity: Assigns each cluster to a class and calculates the accuracy by dividing the number of correctly assigned']"
baf685e044b747728ff285cd468f03da.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: baf685e044b747728ff285cd468f03da', 'Title: New Page', 'Body content: Empty contents']"
bb1ee22518a04be08aab43d3a4817774.xml,"['AI Philosophy: A Process, not a Product', 'AI Philosophy: A Process, not a Product (copy)', 'AI Philosophy', 'Process', 'Product']"
bb899dab4c824bb98d9bd4a3d2aae224.xml,"['Data Science Ready: The organization has identified all the current sources of data to be considered, and those data have been normalized and integrated into a form that supports data science.', 'Data Science Enabled: Preliminary analysis indicates that the data support the desired analytic objectives.', 'AI Ready: The organization has determined how to leverage the insights from data science as part of an operational process and has implemented the appropriate software or software extensions.', 'AI Enabled: The organization has deployed the new software in a relevant context and is able to directly measure the impact.']"
bbc55b94441844e3b7b341ae028c2217.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: bbc55b94441844e3b7b341ae028c2217', 'Title: New Page', 'Body content: Empty contents']"
bbf606c0c1b64c9fb88740e5ac53cc29.xml,"['Data collection process in data science lifecycle', 'Scholarly research', 'Defined business and analytic objectives', 'Valid and reliable data', 'Traditional methods to collect data', 'Questionnaires and surveys', 'Interviews', 'Observations', 'Focus groups', 'Obtrusive data collection methods', 'Unobtrusive data collection methods', 'Web sources for data collection', 'Document analysis', 'Data repository', 'Ethical data gathering', 'Non traditional sources for data collection', 'Non transforming and enriching data for analysis']"
bc51037f932144ffbb61e8b00654c0e1.xml,"['Training Error: The classification error of a model on the exact data used for training.', 'Test Error: The amount of errors to expect when making future predictions and used for model selection.', 'Irreducible Error: The noise term in the true relationship that cannot be reduced by any model.', 'Reducible Error: The error resulting from a mismatch between the estimated relationship and the true relationship.', 'Bias Error: Error from erroneous assumptions in the learning algorithm, causing underfitting.', 'Variance: Error from sensitivity to small fluctuations in the training set, causing overfitting.', 'Bias-Variance Decomposition: Decomposition of the bias-variance trade-off.', 'Model Assessment: Evaluating the performance of a model.', 'Model Selection: Choosing the best model.', 'Cross-validation: Technique for assessing the performance of a model using leave one out or k-fold techniques.', 'Supervised Learning Techniques: Different techniques for supervised learning.', 'CV with Supervised Techniques: Applying cross-validation with supervised learning techniques.', 'Best Model Selection: Selecting the best model using supervised techniques.']"
bc86b2c06c584fada885798b85a0df59.xml,"['Data governance: Defines how data is accessed and managed within an organization.', 'Effective data management: Facilitates effective data management and has positive implications for the quality, security, and integrity of data used for analysis.', 'Data quality: Data governance impacts data quality and the decisions that can be made from the data available to the organization.', 'Data governance strategy: Any organization that stores and utilizes data should have a data governance strategy for internal data or data stored within the organization and external data.', 'Enterprise-wide data: Data governance provides a reliable and consistent view of enterprise-wide data.', 'Improved data quality: Data governance ensures that there is a plan for improved quality of data.', 'Data silos: Maps the location of data in the enterprise, reducing the scourge of data silos.', 'Data management: Data governance improves data management overall.', ""Stakeholders: All users of data in an organization are considered stakeholders of the organization's data."", 'Data-driven decisions: Stakeholders make or are affected by data-driven decisions within an organization.', 'Data governance policies: Data stakeholders have an influence on the state and use of data, in addition to data governance policies.', 'Data as an asset: Data governance influences business']"
bdebfe08ac9049548b99f19762eac40b.xml,"['Model Assessment and Model Selection', 'Cross Validation', 'Validation Set Approach', 'MSE (Mean Square Error)', 'k-Fold Cross Validation', 'Bias and Variance', 'Leave One Out Cross Validation (LOOCV)', 'Classification Problems', 'Regression Problems', 'Misclassification Rate']"
be2904e7ee5f4413a60451a6f582b3b8.xml,"['Importance of Feature Engineering', 'Feature engineering is the process of using domain knowledge to extract features from raw data', 'Algorithms need specific features in the model development process', 'Feature engineering ensures dataset compatibility with algorithms and improves model performance', 'Specialized nature of feature engineering and no one-size-fits-all approach', 'Foundational concepts essential to understanding feature engineering', 'Imputation as a technique for handling missing values in data', 'Numerical data imputation methods', 'Categorical data imputation methods', 'Binning as a technique for making models more robust', 'Trade-off between performance and overfitting in binning', 'Handling outliers using statistical and visualization methods', 'Log transform for handling skewed data and reducing the effect of outliers', 'Categorical encoding techniques: one-hot encoding, binary feature encoding, ordinal feature encoding', 'Feature split for improving model performance by splitting features', 'Scaling continuous features for certain machine learning algorithms', 'Normalization and standardization as scaling techniques', 'Tree-based algorithms not affected by scaling', 'Resources for scaling in Python.']"
bf66164fa2d145259230fd0eb255c2e1.xml,"['Business needs: General necessities for a company to remain competitive or expand on the long term.', 'Business objectives: Specific objectives that a company can work towards to meet a particular need.', ""Data science methods: Evaluation of whether data science methods can be used to facilitate a company's efforts to meet their objectives."", 'Solution vision: A walkthrough of how a full data-driven solution to the problem could look like.', 'Analytic objectives: Specific terms that state what the data science project should produce and serve as the main success criterion for the team.']"
c04ef8f128a54480b25e8d817afab139.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: c04ef8f128a54480b25e8d817afab139', 'Title: New Page', 'Body content: Empty contents']"
c0b1dfda12d741328bdcbe40b2b475bb.xml,"['Data wrangling: The process of extracting, gathering, cleansing, transforming, enriching, and integrating data to support enterprise data needs including analytics and other data related needs.', ""Data management framework: The framework of an organization that is affected by data wrangling as it involves cleansing and enriching data that supports the company's data needs."", 'Raw data: Any data that is extracted for company use and should be inspected to identify the necessary transformation techniques to be applied to the dataset.', 'Missing values: One of the most common data quality issues in a dataset, which can happen due to human error or system issues during data collection. It is important to determine why the dataset has missing values.', 'External source: A dataset that was extracted from an external source, which might not provide context on the reason behind the missing values. Investigation of the missing values is still necessary, as the reasons behind them will determine the techniques used to handle those values.', 'Data transformation techniques: Techniques for transforming categorical and numeric data, which are discussed in this module.', 'Feature transformation: Transformation techniques that further affect the analytic solution, which will be explored in a future unit.', 'Data integration: The process of ingesting, transforming, and integrating the transformed']"
c0d8e2d38a6c4f06844ae7dde6b38d69.xml,"['Prediction involves using a model to predict outcomes.', 'Linear relationship between the number of hospital beds needed and other variables related to the outbreak.', 'Fit a linear model on the training data and select the model that minimizes test error.', 'Inference involves understanding how the dependent variable is affected by changes in independent variables.', 'Estimate f by making inferences or gaining understanding on the relationship between independent and dependent variables.', 'Assess if the relationship between the dependent variable and each independent variable can be explained using a linear equation.', 'Model understanding phase includes using both linear and non-linear approaches to estimate f.', 'Training data set is used to estimate f.', 'Parametric methods assume a fixed set of parameters, while non-parametric methods seek an estimate of f close to the data points.', 'Thin plate spline and spline are non-parametric methods used to estimate f.', 'Parametric methods have strong assumptions about data, while non-parametric methods have fewer assumptions.', 'Parametric methods require less data than non-parametric methods.', 'Parametric methods possess statistical power, while non-parametric methods have less statistical power.', 'Different methods have different levels of flexibility and interpretability.', 'Linear models are best for']"
c0da183109594ae09b9ec5303ac457a9.xml,"['Tree based methods', 'Prediction and classification', 'Interpretable method', 'Decision Trees', 'Numerical and categorical variables', 'Classification Tree', 'Regression Tree', 'Recursive partitioning', 'Pruning', 'Chi-square automatic interaction detection (CHAID)', 'Measures of Impurity (entropy and Gini index)', 'Cost complexity pruning', 'Reduced error pruning', 'Full trees', 'Minimum Error Tree', 'Best Pruned Tree', 'Validation dataset', 'Misclassification rate', 'Building Regression Trees', 'Explainability of decision trees', 'Impurity measure for regression tree', 'Predictive accuracy of CART models', 'Root mean square error (RMSE)', 'Random Forests', 'Bagging', 'Boosting']"
c12ee4e4b63244b69f6d4f9bd03448ae.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: c12ee4e4b63244b69f6d4f9bd03448ae', 'Title: New Page', 'Body content: Empty contents']"
c1d3f498e162477da3a0e170709f2f39.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: c1d3f498e162477da3a0e170709f2f39', 'Title: New Page', 'Body content: ', '  - Paragraph with ID: bb004f454565477da046233d2306d1a7, containing the text ""This is a new page with empty contents.""', '  - Empty paragraph with ID: cfd76a8b20fd4363a2518b8e44b04f9e', '  - Inline reference with ID: newf7f1670b856b49278d1ca5e9fbdc32c6, purpose: learnbydoing']"
c3dfa32556ae42d3af9e83166fa8b0f7.xml,['This is a new page with empty contents.']
c404c7e9a5ba45dca1c9e763351aa76f.xml,"['Data Scientist: This role involves solving business tasks using machine learning model development and statistical techniques. The data scientist identifies trends and patterns within the data and makes predictions based on trends. They also write code to support the data analysis and model building process.', 'Data Engineer: The Data Engineer specializes in data structures and algorithms, as well as working with data through the operation of databases and other large repositories.', 'Solutions Architect: This is a customer-facing role that ensures end-to-end customer deployment for company-related data services. The Solutions Architect interacts with clients to design, coordinate, and execute solution prototypes.', 'Machine Learning (ML) Engineer: The ML Engineer performs modeling and software engineering tasks. They spend a considerable amount of time programming and creating ML solutions but must also have strong statistical skills.', 'Data/Business Analyst: A data analyst has data gathering, analysis, and visualization skills. They provide insights from data to inform decision making and develop key performance indicators. They are typically firmly rooted in the business domain and less technically proficient in systems programming and advanced machine learning.', 'Software Engineer: The Software Engineer handles the alignment between the business objectives and solution and is responsible for integrating the implemented data-driven system into the appropriate applications within the enterprise.', 'Domain Experts']"
c5a0b993dba046cab079b5dfeedaf908.xml,"['Classification: The text discusses the concept of classification in data science, where a dataset with labeled outcomes is used to label the outcomes of a new dataset.', 'Binary Classification: This refers to classification tasks that classify observations into two defined categories.', 'Multi-Class Classification: Also known as multinomial classification, this type of classification involves classifying observations into three or more classes.', 'Multi-Label Classification: Unlike multi-class classification, multi-label classification allows for observations to be classified under multiple classes simultaneously.', 'OneVsRest Technique: This technique assumes that labels are mutually exclusive and does not consider correlations between classes.', 'Binary Relevance Technique: Similar to OneVsRest, this technique trains a single label binary classifier for each class, ignoring any correlation between classes.', 'Multi-Label k-Nearest Neighbors: This is an algorithm that adapts the k-Nearest Neighbors technique for multi-label classification.', 'Multi-Class Classification with Unique Class Combinations: This technique transforms a multi-label classification task into a single multi-class task by training all unique class combinations on one multi-class classifier.', 'Classifier Chains Technique: This technique builds a chain of binary classifiers to consider correlations between classes.', 'Logistic Regression: This is an important classification']"
c68519d2b30e4683a87f94f8b53283ce.xml,"['Valuable Functionality: The specific functionality, insight, or resource gained from leveraging data and proposed methods.', 'Incremental Step: The benefit gained from an incremental yet necessary step towards producing a model that solves a task.', 'Reframing Analytic Objectives: Changing the analytic objective to focus on specific tasks and making an incremental step the main goal.', 'Valuable Insight: The requirement for insight from data to make decisions, also known as ""business intelligence"".', 'Valuable Resource: The production of resources to be used by the organization or further projects.', 'Improvement over the Current Situation: The project should improve over the currently available functionality, information, and resources.', 'Assumption of Project Success: The proposal of a project with confidence to succeed, while being mindful of potential factors beyond control.']"
c768a1f3c4794dc08bef7767fdf20d34.xml,"['Data gathering process', 'Data transformation', 'Handling missing values', 'Missing completely at random (MCAR)', 'Missing at random (MAR)', 'Not missing at random (NMAR)', 'Imputation', 'Omission', 'Subsetting', 'Outliers', 'Transforming categorical data', 'Category reduction', 'Creating category scores', 'Creating dummy variables', 'Transforming numeric data', 'Binning', 'Using mathematics']"
c769c4dcaeb4425d9181d153d65ebdc3.xml,"['Data quality: The quality of data affects decision-making and requires cleaning and formatting.', 'Raw data: Data collected from different sources is considered raw data and should be studied before use.', 'Data wrangling: The process of cleaning, formatting, and enriching raw data for analysis.', 'Data understanding: Clear understanding of the business and analytic objectives is required for successful data understanding.', 'Challenges in data wrangling: Data wrangling presents challenges in data science projects.', 'Enriching data: Techniques used to enrich data through data wrangling.', 'Data wrangling tools: Open source and proprietary tools used for data wrangling, such as Python, R, proprietary tools, and Excel.']"
c925074097744ccba39546b7be2d28c9.xml,"['Ranking: A data science pattern used to solve various tasks such as machine translation and information retrieval.', 'Learning to rank: A technique used to train a model for ranking tasks.', 'Information retrieval: The process of retrieving relevant information from a large collection of data.', 'Collaborative filtering: A technique used in recommendation systems to filter and recommend items based on user preferences.', 'Sentiment analysis: The process of determining the sentiment or emotion expressed in a piece of text.', 'Document retrieval: The process of retrieving relevant documents from a collection based on a query.', 'Ranking algorithms: Algorithms used to rank items or documents based on their relevancy.', 'Pointwise approach: An approach that predicts the score of a single query-document pair, treating the ranking task as a classification or regression task.', 'Listwise approach: An approach that produces an optimal ordering of a list of documents.', 'Pairwise approach: An approach that aims to reduce the number of wrongly ordered rankings compared to the ideal expected result, treating the ranking task as a classification or regression task.', 'RankNet: A learning to rank algorithm that uses gradient descent to update weights or model parameters to minimize the number of wrong orderings in a ranked list.', '2.']"
c92fcac6686c4fd096dec4a854a271f2.xml,"['Classification: Categorizing data points using labels, such as sentiment analysis of text or labeling images.', 'Regression: Predicting a numerical label associated with data instances, such as predicting measurements in medical or demographic data over time.', 'Retrieval & Ranking: Retrieving and presenting data points in a specific order in response to a query, like search engines.', 'Recommendation: Finding items to recommend to users based on their preferences, such as movie and product recommendations.', 'Clustering: Dividing data points into groups based on their similarity in the feature space, often used for exploratory analysis of unlabeled data.', 'Anomaly Detection: Discovering instances that do not adhere to a pattern in the dataset, like detecting fake customer reviews in online retail data.', 'Domain-specific tasks: Specific task patterns and evaluation metrics used in domains like natural language processing and image analysis.', 'Overlapping categories: Some categories can be solved using methods from other categories, such as solving ranking problems using classification or regression methods.', 'Purpose of identifying task type: Characterizing the project for planning and communication purposes, including quantitative evaluation and corresponding metrics.']"
c93c6cf8a0b34f648308f7fd5252bd0d.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: c93c6cf8a0b34f648308f7fd5252bd0d', 'Title: New Page', 'Body content: Empty contents']"
c944a901086345f0a0adbfd4e425d41d.xml,"['Complete the final examination by carefully reading through each question and selecting the best answer among the options.', 'It is best to budget your time wisely for the final examination.', 'After completing the examination, you can privately post questions on Piazza for clarification.']"
c98c595df5fe4c708d82f2a4bb2726a5.xml,"['Data science project complexity: A data science project involves various components such as understanding business needs, stakeholders, existing system environment, and support structure. It requires technical expertise and domain guidance.', '', 'Data gathering: Data for analysis may need to be collected from both internal and external sources.', '', 'Project expectations: The client defines project expectations, and the data science team creates a solution vision to meet those expectations. Careful consideration is required to avoid project failure.', '', 'Reasons for data science project failure: Insufficient or inappropriate data, lack of technical data science skills, issues with project management, inaccurate interpretation of results, and mismanaged client expectations.', '', ""Importance of understanding client's business needs: Misunderstanding regarding the client's business needs and inadequate communication between the data science team and business stakeholders can lead to project issues. Identifying and understanding the client's business needs, environment, and current solution increases the chances of meeting business objectives."", '', 'Broadening the data science process: Data science involves more than just gathering data and applying machine learning methods. It requires a holistic approach that considers the big picture, including scientific research, software engineering principles, and experimentation.', '', 'Problem identification and solution envisioning: Identifying problems and envisioning solutions are crucial skills for a']"
c9c4a0e70194444fb7f47474704b64e7.xml,"['Data Science ready organization', 'Analytics solution', 'Business need(s)', 'Predictive modeling', 'Insights for recruitment and retention', 'Stakeholders and systems related to recruitment and retention', 'User needs and system constraints', 'Data requirements', 'Data sources, management, and extraction', 'Model requirements', 'Model explainability', 'Reports and dashboards', 'Requirements for reports', 'Data sources for reports', 'Customization of reports', 'Packaged analytic solutions', 'Reading: Requirements for an IIoT Predictive Maintenance Solution']"
ca54d3de337f4247b27c5f7ae23dfd2a.xml,"['Requirement: A documented condition or capability needed by a user or system to meet a business need or achieve a business objective.', 'Specifications: Requirements that have been converted into specifications.', 'Characteristics of a good requirement: Completeness, correctness, identification of relevant stakeholders, definition of needs, goals, and objectives, consideration of constraints, traceability.', 'Traceability: Tracking the life cycle of a requirement from development to specification and deployment.', 'Unambiguous requirement: Clear and explicit requirement that has the same meaning for all stakeholders, with defined acceptance criteria and metrics for success.', 'Verifiable requirement: Testers should be able to verify that the requirement was implemented correctly.', 'Complete requirement: A requirement that is verifiable, unambiguous, and traceable.', 'Interpretation: Room for interpretation in a requirement can lead to not meeting client expectations.', 'Improved representation of a requirement: Including a time frame, a responsible party, and a deliverable.', 'Reading: IEEE Guidelines on Software Requirements Gathering.', 'Reading: Traceability in Requirements and Other Artifacts.']"
ca7cb753e5b34aaa9c79d99b5b2ac067.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: ca7cb753e5b34aaa9c79d99b5b2ac067', 'Title: New Page', 'Body content: Empty contents']"
d06e2d1cefb6421b946065b2d2ad647c.xml,"['Unsupervised Learning Techniques', 'Principal Component Analysis', 'Cluster Analysis', 'Market segmentation', 'k-Means Clustering', 'Hard Clustering', 'Soft Clustering', 'Overlapping Clustering', 'Hierarchical Clustering', 'Partition Data', 'Initialize Centroids', 'Iteratively initialize centroids', 'Within Cluster Sum of Squares (WCSS)', 'Elbow method', 'k-Nearest Neighbors (kNN)', 'Supervised classification method', 'Labeled data', 'Unsupervised technique', 'Eager learner', 'Lazy learner']"
d0e07b692e9749d0b85d797b7c52c67b.xml,"['Data science project uniqueness and data collection techniques', 'Business and analytic objectives driving data collection techniques', 'Various data types and sources', 'Numeric and categorical data types', 'Measurement scale types (nominal, ordinal, continuous, discrete)', 'Qualitative and quantitative data', 'Structured and unstructured data', 'Internal and external data', 'Primary and secondary data sources', 'The web as the largest data source', 'Examples of primary and secondary data', 'Examples of internal and external data sources']"
d22b539885d241d5acb1f953be09a54e.xml,"['Model training on labeled data', 'Supervised Learning', 'Classification and Regression tasks', 'Linear and Logistic Regression', 'k-Nearest Neighbors', 'Tree Methods', 'Naive Bayes', 'Machine Learning course', 'Reading: A comprehensive list of Supervised Learning algorithms', 'Reading: Supervised Learning and Natural Language Processing']"
d29b512ad9f54e5c84150164e30a6aa3.xml,"['Framework for formulating questions and defining business needs', 'Extracting business objectives using data science techniques', 'Meeting client expectations and delivering solutions', 'Evidence Value Proposition (EVP) framework', 'Determining analytic objectives and providing evidence', 'Five steps guided by questions to formulate business objectives', 'Figure 1: Evidence Value Proposition Framework']"
d36a94bc5b3e4f6d91845ce57283dafa.xml,"['Cluster Analysis: A technique used to categorize data according to characteristics by placing them in groups.', 'Hard Clustering: Divides data into a number of independent groups, where each data point can only belong to one cluster.', 'Soft Clustering: Groups data into clusters, allowing a data point to belong to more than one cluster to a degree.', 'Overlapping Clustering: Allows data to belong to more than one cluster.', 'Hierarchical Clustering: Organizes data in a hierarchical manner, represented by a dendrogram.', 'k-Means Clustering: A technique that minimizes dispersion within clusters using distance measures, with the most popular being Euclidean Distance.', 'KNN (k-Nearest Neighbors): A supervised classification method that predicts the class of a new observation based on its closest neighbor data points.', 'Agglomerative Clustering: A clustering process that starts with each observation forming its own cluster and combines the nearest clusters until there is one cluster left.']"
d5eabd6a0da14916bdd5a5c7ab564f58.xml,"['Classification metrics are used to evaluate the class or probability output of classifiers. A confusion matrix is a matrix style metric that uses the accuracy, precision, recall, and specificity to assess whether data was classified accordingly.', 'F-measure or F1 Score is a harmonic mean of precision and recall values for classification tasks, it is also an external evaluation for clustering output (only when class labeled data is available).', ""The AUC-ROC is a common metric used for classification problems. The ROC is a visualization of the sensitivity and specificity. It can be used for both class and probability outputs. The AUC ROC does not account for the order of probabilities or the model's ability to predict higher probability for data."", ""Regression problems are evaluated using the RMSE (Root Mean Squared Error) among others and it is considered the most popular evaluation metric used for regression problems. When the RMSE decreases, it means the model's performance will improve."", ""Unsupervised learning algorithms are not as straightforward as the supervised learning algorithms to evaluate. The Dunn index, Silhouette coefficient, and Jaccard's index are used to evaluate clustering techniques.""]"
d6cd1022429a48ce97847ac5cb55421f.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: d6cd1022429a48ce97847ac5cb55421f', 'Title: New Page', 'Body content: Empty contents']"
d746710bc6284e18acd95e112c8252f9.xml,"['Framing Common Forms of Analytical Objectives', 'Constructive', 'Industry Constructive Objective', 'Research Constructive Objective', 'Benchmarking', 'Industry Benchmarking Objective', 'Research Benchmarking Objective', 'Exploratory', 'Industry Exploratory Objective', 'Research Exploratory Objective']"
dc027b76e6f144bfb96391c4f4e26178.xml,"['Data science is the study and practice of extracting insights and knowledge from large amounts of data.', 'Data analysis has been used for real-world problems since ancient times.', 'Interest in developing statistical and programming skills has increased.', 'The field of data science has evolved since the 1960s.', 'Data science is becoming more interdisciplinary.', 'A Data Scientist should have programming and statistical skills, as well as the ability to extract and communicate insights.', 'Data science helps companies make strategic decisions and gain a competitive advantage.', 'The course prepares students for the Masters of Computational Data Science at Carnegie Mellon University.', 'The course focuses on identifying business objectives and developing analytical solutions.', 'The course teaches data gathering, data compilation, data wrangling, and exploratory data analysis.', 'The course also covers building, deploying, and evaluating models for decision making.']"
dd69d3f1ca85431cbe6483368bbcb886.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: dd69d3f1ca85431cbe6483368bbcb886', 'Title: New Page', 'Body content: Empty contents']"
de888370bf4c4bf0a7e9097d7a41e144.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Document Type Declaration (DTD): Workbook Page MathML 3.8', 'Workbook page metadata: bib, cmd, m, pref, theme, wb', 'Workbook page ID: de888370bf4c4bf0a7e9097d7a41e144', 'Page title: Course Wrap-Up', 'Page content: Empty contents']"
dec41bd846734732bcf0a89493b157e8.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: dec41bd846734732bcf0a89493b157e8', 'Title: New Page', 'Body content: Empty contents']"
dee5ac8bd6ce4339a686c56ca2f4a871.xml,"['A data science project does not begin with building models, one must consider the needs of the client, and set objectives to meet those needs.', 'A data scientist must approach a project with a methodology.', 'A data science project follows frameworks that will guide the problem identification process.', 'A data science team will work with a client to understand business need (s).', 'Business objectives will be defined by the company to help meet business needs.', 'Data science methods can be used to facilitate the companies efforts to meet business needs and objectives.', 'A data science team will engage with the client to understand the situation in sufficient depth and establish effective communication with the company’s domain experts.', 'The team will work with the client to develop a solution vision.', 'The data science team will identify a set of analytic objectives that, if achieved, will facilitate realizing the solution and reaching the business goal.', 'The Evidence Value Proposition framework (EVP) can be used to ensure that defined business objectives are met.', 'The EVP helps determine the desired analytic objectives and provide evidence that the objectives can be met with the appropriate data and technology.', 'When engaging with potential clients seeking analytical solutions, it is important to assess the organization’s readiness.']"
e18993e9d0f9473e943d7817917b6586.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: e18993e9d0f9473e943d7817917b6586', 'Title: New Page', 'Body content: Empty contents']"
e22de09db5b34faa9c1d2cda84b950e0.xml,"['Data management: Storing and maintaining data collected by an organization.', 'Data infrastructure: Ensuring data is valid, reliable, secure, and has integrity.', 'Data architect: Responsible for developing a data management plan and working with other IT team members.', 'Data engineers: Collaborate with data architects to collect and store data securely.', 'DAMA-DMBOK: Best practices guide for data management professionals and organizations.', 'Eleven areas of knowledge: Highlighted by DAMA-DMBOK to guide data management professionals.', 'Data governance: Involves all data users in an organization and ensures a common understanding of data, infrastructure, and policies.', 'Quality of data: Impacted by the 11 areas of knowledge in data management.', 'Data science projects: Implications of data governance for successful execution.', 'Handbook of qualitative research: Source reference for data management and analysis methods.']"
e260bc12b0e940a2a88262a8aec11970.xml,"[""Engagement with the client and research about the specific circumstances of the client's business."", ""Engagement with the client and research around the general domain in which the client's business operates."", 'Continuous efforts to maintain alignment of the ongoing project with changes or new information in both the general domain and specific circumstances.', 'Familiarizing oneself with different substantive domains in order to deliver good data science work.', 'The importance of patience and attention to detail in the data science profession.', 'The potential benefits of having formal education or professional experience in specific disciplines.', 'The possibility of specializing in a certain field as a data scientist.']"
e32d2f0eb3bd4fe9a51c2b9fecc17cd6.xml,"['Exploratory data analysis (EDA) process: Visualizing data to gain insights, using non-graphical and graphical techniques, showing skewness and outliers.', 'Summarizing data: Techniques to summarize and describe a dataset, including describing central tendency and assessing measures of spread and relationships.', 'Data visualizations: Evolution of visualizations, including info-graphics, plots, and statistical graphs, importance of effective visualizations and using the right types for data types.', 'Statistical inference: Drawing informed conclusions about the entire dataset using statistical methods, using exploratory data analysis and summary statistics, using probability theory.', 'Estimation and hypothesis testing: Techniques for making statistical inference on population mean and population proportion.']"
e3661493ec9c4370a5ac383ee6701760.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: e3661493ec9c4370a5ac383ee6701760', 'Title: New Page', 'Body content: Empty contents']"
e453a3c17d1f4be8909e27937597da4b.xml,"['Proposing Learning Methods', 'Business Need ⇒ Business Objective ⇒ Analytic Objective', 'Problem Statement ⇒ Task Definition ⇒ Method & Data Statement', 'Precise method statement for data science team understanding', 'Testing different techniques around the main conceptual idea', 'Suitable methods for producing target functionality/insight', 'Supervised learning methods', 'Unsupervised learning methods', 'Semi-Supervised learning methods', 'Statistical methods in data science', 'Categorization of methods as supervised or unsupervised', 'Stating methods more precisely based on problem domain', 'Specific vs. general statement of methods', 'Example of applying methods in a project', 'Method and data as the heart of a data science project', 'Finding the best tool for the task', 'Researching data science literature and libraries effectively', 'Teamwork and seeking advice and feedback', 'Checking proposed method against target functionality or insight', 'Interpreting results of different models', 'Making informed decisions based on research and experimentation.']"
e561bf51b3a14192ad27ee66ea0d9b27.xml,"['Proposing Datasets & Collection Methods', 'Appropriate size of the dataset for the proposed method', 'Distribution of individual phenomena in the data', 'Raw or readily processable dataset and necessary preprocessing', 'Completeness of the data and presence of missing parts', 'Clean or noisy dataset and the role of measurement error', 'Access or disclosure restrictions to the data', 'Suitability of benchmark datasets for the task', 'Investigating the suitability of the proposed dataset', 'Statistical analysis and machine learning as core pillars of data science', 'Effective collection, curation, and interaction with data', 'Data collection as part of an analytical objective', 'Representation, approximation, and coverage of desired distribution in data collection', 'Use of human annotators or crowdworkers in data collection', 'Qualifications and training of human annotators', 'Examination/testing of human annotators before and/or after collection', 'Designing annotation tasks and measuring agreement between annotators', 'Amount of data to be gathered and budget constraints', 'Cleaning and curation of gathered data', 'Approvals/permissions required for data collection', 'Introduction to data collection methods in the course', 'Skill,']"
e656dd356b48423a95d0257066d895d9.xml,"['Bias in Data: The text discusses the possibility of bias being introduced into datasets during the modeling process and how it can affect the performance of models and the decisions made based on them. It also mentions areas of interest related to bias in data science, such as Algorithmic Bias, Computer Ethics, and Roboethics.', '', 'Feature Engineering and Bias: The text explains that feature engineering, which can be performed before or during the model building process, can introduce bias to the data. It mentions that manipulating the data during feature engineering can unintentionally create relationships between features that do not exist and shape the insights obtained from the model.', '', 'Sample Bias: The text describes sample bias as the importance of collecting data from a representative sample of the population. It gives an example of a study that measures the completion rate of graduate students in the United States and how a sample from a specific socioeconomic background, race, or gender would undermine the external validity of the study.', '', 'Confirmation Bias: The text explains that confirmation bias occurs when prior knowledge, beliefs, and values influence the data used to build an analytic solution. It mentions that humans tend to favor evidence that confirms their personal beliefs, values, and hypotheses.', '', 'Information Bias: Also known as measurement bias, the text']"
e6fed9120291434da550600c5d17e699.xml,"['The difference between a traditional IT project and a Data Science project is the focus on the analytical data, model, and operations requirements.', 'Requirements are capabilities needed to achieve an objective. Requirements are gathered from stakeholders of a project including the client, end users, and the data science team. Good requirements are correct, complete, unambiguous, verifiable, measurable, and traceable.', 'Requirements analysis involves activities that identify business needs, evaluate feasibility of solution, and establish constraints.', 'Requirements management is an iterative set of activities to help ensure that elicitation, documentation, refinement and changes of requirements is done during the data science project life cycle.', 'Requirements gathering involves fact-finding that describes the current state of the business, and the business objectives that inform the proposed solution. Requirements gathering is done to ensure that business needs, and solutions are well defined.', 'There are three types of requirements highlighted in this unit. The business, system and user, and solution requirements.']"
e714c309bef9406f971f0ddd02040937.xml,"['Requirements Gathering: The process of discovering the requirements of stakeholders and systems.', 'Elicit needs/expectations: Understanding the needs of the system and stakeholders set by the business team and end-users.', 'Analyze and align expectations: Analyzing and aligning the expectations to the business and analytic objectives.', 'Systematically perform requirements gathering: Performing requirements gathering systematically to ensure information is extracted from diverse sources.', 'Different scenarios call for specific techniques: Different scenarios and circumstances call for a specific type of requirements gathering technique.', 'Interviews: Thoroughly investigating the needs of the stakeholder through open-ended questions.', 'Brainstorming: Gathering ideas from multiple stakeholders at once to identify possible solutions.', 'Questionnaires: Distributing predefined questions to stakeholders and analyzing the feedback to define project requirements.', 'Document Analysis: Reviewing existing documentation to elicit needs and serve as a completeness check for requirements.', 'Facilitated Workshops: Collecting information from stakeholders through structured yet interactive workshops.']"
e836784715544d8ab29d72da3687b322.xml,"['Raw Data to Features', 'Parts of data that can be useful in the model building process', 'Statistical modeling and formulae', 'Building a solution that ranks customer preferences or identifies segments of a customer base', 'Numeric representation of features', 'Processing features from data', 'Importance of selecting the right features', 'Feature engineering techniques for data science tasks and modeling', 'Feature engineering and analytic solution building', 'Higher quality models and better insights through feature engineering', 'Feature selection', 'Numeric data type and feature engineering', 'Scalar, vector, and vector spaces', 'Representation of input to a machine learning model as a numeric vector.']"
e843820862824c77903d30f41a295dac.xml,"['Hierarchical Clustering: A clustering technique that forms hierarchies of clusters and presents them using a dendrogram.', 'Agglomerative Clustering: A technique in hierarchical clustering where clusters are formed by combining the nearest clusters until there is one cluster left.', 'Euclidean Distance: A distance metric commonly used in hierarchical clustering.', 'Merging Clusters: The process of combining clusters in hierarchical clustering.', 'Single Linkage Method: A clustering method that groups clusters using the agglomerative method with two clusters merged at each step.', 'Complete Linkage Method: A clustering method that uses the maximum distance between data points within each cluster to combine clusters.', 'Average Linkage Method: A clustering method that calculates the average distance between data points within each cluster.', 'Centroid Linkage Method: A clustering method that uses the centroid distance between clusters.', 'Divisive Clustering: The opposite of the agglomerative method, where data is clustered using a top-down approach.', 'Reading: A link to a paper on hierarchical clustering of words and its application to NLP tasks.']"
e85359f9e128422393a450d29ea2ad20.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: e85359f9e128422393a450d29ea2ad20', 'Title: New Page', 'Body content: Empty contents']"
e927ef3f635b48fb90fb0e77d3388ba7.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: e927ef3f635b48fb90fb0e77d3388ba7', 'Title: New Page', 'Body content: Empty contents']"
e9e24caf71dd4e7da9c4c51cb2bae76e.xml,"['Unsupervised Learning Techniques', 'This is a new page with empty contents.']"
eb2f3840a8104780912ccad53392a130.xml,"['Solving the problem must be part of a possible solution vision towards the business objective.', 'Domain experts are confident that data exists which, when analyzed, can facilitate that solution; this data must either be available or sources are available to retrieve the data.', 'The problem must be specific and realistic so that a corresponding data science project can succeed in principle.', 'With the business objective clearly stated, one can propose a solution vision describing how the business can reach the objective.', 'The path to realizing this vision can then be decomposed into sub-problems and the data science team will be responsible for leveraging data to help solve these problems and/or produce data-derived insight that allows the client to make good decisions along the way.', 'The perspective of using data to support business objectives may even be the main business objective that starts the whole engagement.', 'It is helpful to recall which higher-level business needs should be fulfilled in order to progress beyond “we want to leverage data somehow” and arrive at a proper project formulation that allows the statement of specific requirements and evaluation criteria, even if they include exploratory tasks.', 'An online company asks for a data science project around the use of social media data for understanding their market driven by the board’s desire to']"
ebc62268ac6c4c188a6719231f3c3f54.xml,"['Requirements Gathering Process: The process of identifying the needs and constraints of stakeholders and the current system or environment related to the business need.', 'Stakeholders: Individuals who perform tasks that will meet the business need and decision makers within the business.', 'Data Science Projects: Similar to traditional software development projects, data science projects also follow requirements gathering principles.', 'Steps in Requirements Gathering: Gather Information, Define and Prioritize Requirements, Evaluate Requirements, Receive Sign-Off.', 'Requirements Management Plan: A document used to document the requirements gathering process, including project description, team responsibilities, tools used, and the requirements gathering process itself.', 'Change Control: The process of formally managing changes to requirements.']"
ec21750cc96c4744ac20259c45ac474b.xml,"['Data integration: Involves ingesting, transforming, and integrating data for access.', 'Analytic solution development: Modeling and analysis of integrated data.', 'Data warehouse: A centralized repository for integrated data that can be accessed by OLAP servers, DSS systems, and other analytic tools.', 'ETL mechanism: Extract, transform, and load mechanism used for data integration in a data warehouse.', 'Feature engineering: Extension of the transformation process during data wrangling.', 'Exploratory Data Analysis (EDA): In-depth look into data exploration techniques to gain insights for a project.', 'Data understanding process: Starting with data wrangling to preprocess data for EDA and beyond.', 'Data quality: Can impact the success of an analytic solution.', 'Iterative data understanding phase: Data wrangling process may need to be repeated if new data is sourced.']"
ec5450c0005241cdb5f7d2b80331aab2.xml,"['Bias error: An error from erroneous assumptions in the learning algorithm that can cause underfitting.', 'Variance: An error from sensitivity to small fluctuations in the training set that can cause overfitting.', 'Bias-Variance tradeoff: The tradeoff between bias and variance in the performance of a model.', 'Decomposition of bias-variance: The decomposition of the error of an estimator into variance of the noise in the data, bias, and the variance of the estimator.', 'Sources of error: Both bias and variance are sources of error in an estimator.', 'Trade-off between bias and variance: Increasing bias decreases variance and vice versa, indicating a complementary relationship.', 'Balanced model: An ideal model with low bias and low variance.', 'Dimensionality reduction techniques: Techniques used to work against overfitting (high variance) by simplifying the model.', 'Optimization of trade-off: The trade-off between bias and variance can be optimized using techniques such as cross-validation.', 'Visual representation of bias-variance: A visual representation of bias-variance with training dataset.', '', 'ote: The numbering is added for clarity and does not exist in the original XML text.']"
ed91e3be86334b32883983125941046e.xml,"['Lazy-learning: A method in which training data is generalized and useful for large datasets that are continuously updated. It relies on a small number of attributes in the dataset.', 'Eager learning: A method that requires less space and learns immediately, but takes a shorter time to classify data compared to lazy learning.', 'k-Nearest Neighbors (kNN): A lazy learner that can be used for both classification and regression problems. It is simple to implement and one of the first algorithms learned in data science.', 'Recommendation system: An application of lazy learning that continuously updates new items based on user preferences and relies on certain variables such as ratings, pricing, and country of origin.', 'Curse of dimensionality: A challenge faced by kNN where the method performs best when data is rescaled and normalized.', 'Euclidean distance: A distance measure used in kNN to determine the similarity between observations in the training dataset.', 'Manhattan and Minkowski distances: Alternative distance measures used in kNN.', 'Hamming distance: A distance measure used in kNN for categorical variables with two classes.', 'Overfitting: Occurs when k = 1 in kNN, leading to assigning new data to the majority class.', '0']"
f0b1f52b9c484fb7b70bbcafcaf64e3d.xml,"['CART methods do not have good predictive performance', 'Ensemble Models', 'Overfitting', 'Tree diagram', 'Bagging', 'Out-of-Bag Error Estimation', 'Random Forests', 'Feature engineering', 'Boosting', 'Adaptive Boosting (AdaBoost)', 'Dimensionality reduction', 'Ensemble methods']"
f1fd5eae72a14a8485c16fb93f4d15dc.xml,"['AI consulting firm', 'EVP framework', 'Automotive services industry', 'Customer retention', 'Customer acquisition', 'Data-driven solution', 'Data science team', 'Business needs', 'Data management', 'Data governance', 'Data architecture', 'Data security management', 'Service managers', 'Hardware and software gaps', 'Business objectives', 'Customized service experience', 'Mobile app', 'Customer profiles', 'VIP customers', 'Loyalty program', 'Measurable metrics', 'App installation', 'Onboarding', 'Store visits', 'Transactions', 'App revenue', 'Repeat customer prediction', 'Model accuracy', 'Return-on-investment', 'AI-enabled application', 'Customer engagement', 'App-based transactions', 'User strength', 'Revenue generation']"
f3df11d058ab4ff68e39e67e58214b6a.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: f3df11d058ab4ff68e39e67e58214b6a', 'Title: Summary', 'Body content: Empty page with no contents']"
f48ee0338e814a2c90e1294d14052375.xml,"['Data Science projects can be complex and require the input of many stakeholders.', 'A well-defined workflow is important in data science projects.', 'The data science lifecycle is not linear and can be iterative.', 'The data science lifecycle gives structure to the process and keeps the data scientist on task.', 'The CRISP-DM lifecycle is similar to the data science lifecycle.', 'Different data science vendors have adapted the lifecycle to fit their products and solutions.', 'Business understanding is an important step in data science projects.', 'Data acquisition involves obtaining data from various sources.', 'Data preparation is the process of cleaning and transforming raw data.', 'Data exploration and cleaning involves analyzing variables, identifying outliers and missing values, and creating/selecting features.', 'Modeling involves choosing the appropriate model and implementing algorithms.', 'Feature engineering is needed to prepare datasets and improve model performance.', 'Model training involves maximizing performance and finding a balance between performance and generalization.', 'Models should be tested on dedicated test data and not on the data they were trained on.', 'Model evaluation is an essential step in the data science lifecycle.', 'Deployment involves deploying the model for application consumption.']"
f4e02624db114a8a9c8cd78840a1f707.xml,"['Errors At All Phases', 'Business understanding phase', 'Data understanding phase', 'Model understanding phase', 'Training Error', 'Test Error', 'Irreducible error', 'Reducible Error']"
f5ff978fd8024e648648d41b9f40e7a5.xml,"['Poorly documented requirements', 'Assumptions and miscommunications', 'Defects in solutions', 'Unmet expectations', 'Requirements gathering process', 'Business analyst', 'Types of requirements (business, user, solution)', 'Collecting requirements for analytic projects', 'Techniques for gathering requirements (interviews, brainstorming sessions, facilitated workshops, document analysis, observations)', 'Analyzing and translating requirements', 'Validating requirements', 'Commencement of solution development']"
f6382cdaac684108b0c2a68f1465e065.xml,"['Supervised Learning Techniques', 'This is a new page with empty contents.']"
f73b5ad409304e1c8a4ec689bab7463d.xml,"['Bayes Theorem: Calculates conditional probabilities and describes the probability of an event based on prior knowledge of conditions related to that event.', 'Bayesian Inference: Applied when Bayes theorem seeks to update the probability for a hypothesis as more information becomes available. Widely used in sports, medicine, and law among other fields.', 'Naive Bayes (NB) Method: A simple classifier that can be applied to categorical predictors. Based on applying the Bayes theorem and assumes that all predictors are independent.', 'Additional Reading: Provides a link to further reading on Bayes Theorem.', 'Naive Bayes Performance: Studies have shown that Naive Bayes does not perform as well as methods like random forests, particularly as an estimator for probability outputs.', 'High Dimensional Data: Naive Bayes can be used for high dimensional data where datapoints are more likely to be further apart.', 'Biased Results: Naive Bayes is biased in its results and may assign zero probability to new records with predictor categories that differ from those in the training dataset.', 'Other Bayesian Methods: There are other Bayesian methods used in Data Science, explored in machine learning and applied statistics courses.']"
f77eeffd875947d69e59a08b5d4076ec.xml,"['Feature engineering: Creating features from raw data to facilitate model building.', 'Principal Component Analysis (PCA): A technique used in the data science lifecycle for modeling.', 'Model understanding phase: Exploring terminology and concepts used in model building.', 'Analytic objectives: Goals set to meet the needs of the client and provide actionable insights.', 'Model training: Partitioning or splitting data into training and test sets to build and validate models.', 'Supervised learning techniques: Predicting outcomes based on independent or predictor variables.', 'Unsupervised learning techniques: Uncovering patterns in data without a known response variable.', 'Goodness of fit measures: Assessing the fit of a regression model to the data.', 'Overfitting: When a model learns all the details in the training set and cannot generalize to new data.', 'Underfitting: When a model does not suit the dataset.', 'Confusion matrix: Assessing the classification performance of models.', 'Misclassification rate: Proportion of observations classified incorrectly.', 'Accuracy rate: Proportion of observations classified correctly.', 'Sensitivity or recall: Proportion of the target class classified correctly.', 'Precision: Proportion of predicted target class observations that belong to the']"
f91876280e8743c1b110a49649a89162.xml,"['The data science lifecycle structures the activities of the data science team.', 'The framework involves input from various members of the data science team and the client.', 'Business Understanding: Framing objectives and assessing data science readiness.', 'Data Acquisition: Involves gathering data from various appropriate sources.', 'Data preparation techniques are employed to ensure the data is useful for analysis.', 'Modeling: Choosing the appropriate model based on the problem.', 'This multi-step process involves feature engineering, algorithm selection, model training, and evaluation.', 'Deployment: The model is deployed to an environment for application consumption.', 'A company needs to measure the impact of the deployed solution to ensure its success.', 'The data science lifecycle can seem daunting, but a productive team consists of various roles.']"
f92219fb485c4e96bbb649d63e54a7c5.xml,"['Active Learning: A data science pattern that involves an algorithm or learner choosing the data it will learn from, resulting in better performance with less training.', 'Query Learning: Another term for active learning, where a learner can query the labels of observations in a dataset.', 'Membership Query Synthesis: A scenario in active learning where a learner generates an observation similar to one or more in the dataset and then labels it.', 'Stream Based Selective Sampling: A scenario in active learning where the algorithm evaluates the informativeness of unlabeled data points and decides whether to assign a label or query the oracle.', 'Pool Based Sampling: A scenario in active learning where a pool of unlabeled data is used, and observations with the most important measures of informativeness are selected and labeled.', ""Informative Data Points: Data points that are difficult for the algorithm to classify, which can improve the algorithm's abilities."", 'Query Strategies: Strategies used to evaluate the informativeness of unlabeled data, including uncertainty sampling, query-by-committee, expected model change, and expected error change.', 'Additional Reading: A link to a survey of active learning in machine learning and artificial intelligence by Burr Settles.']"
f9469fffd5d24f85a8982f29b9fb7a38.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: f9469fffd5d24f85a8982f29b9fb7a38', 'Title: New Page', 'Body content: Empty contents']"
f9851381ff604681bddf45844c5dcee1.xml,"['Data Sampling', 'This is a new page with empty contents.']"
fb31fc1e26af4781813cbb0c071e7b70.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: fb31fc1e26af4781813cbb0c071e7b70', 'Title: New Page', 'Body content: Empty contents']"
fe4c8a5acc54404092ded91e8309e17c.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'DTD: Workbook Page MathML 3.8', 'DTD URL: http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd', 'XML namespaces: bib, cmd, m, pref, theme, wb', 'Workbook page ID: fe4c8a5acc54404092ded91e8309e17c', 'Page title: New Page', 'Page content: This is a new page with empty contents.']"
ff16c81c152740aa9d2d3e312b8cc4f7.xml,"['Interpretability: A very important research area in Data Science and Machine Learning. It ensures that a data scientist can measure the effects of any trade-offs within a model.', 'Explainability: The ability to explain why a model produces certain results and what happens when there are changes within a model.', 'Accuracy: The measurement used to determine the best model for a task. It is important for producing better results and predictions.', 'Trade-offs: The effects of making choices within a model, which can be measured and assessed for interpretability.', 'Generalization: The ability of a model to properly apply its learned knowledge to new data.', 'Restrictions: Certain sectors, such as banking and education, have laws and standards that restrict the use of certain techniques to protect consumer data and prevent bias in decision-making.', 'Retaining interpretability: The challenge of maintaining interpretability while improving accuracy in data science models.', 'Black box models: Models that are not easily interpretable but can be used as benchmarks or in the deployment process.', 'Regression techniques: Different methods used for regression analysis.', 'Interpretable ensemble models: Small models that are more easily interpretable than black box models.', 'Variable importance measures: Techniques used to explain black']"
ff1bce4b2a9a4cdb9ac20c748fe92cfd.xml,"['XML version: 1.0', 'Encoding: UTF-8', 'Workbook page with MathML 3.8', 'Workbook page ID: ff1bce4b2a9a4cdb9ac20c748fe92cfd', 'Title: New Page', 'Body content: Empty contents']"
ff2f9f9615bc439282eed27351aa5b4c.xml,"['Data Sampling: A statistical technique used to capture a representative size of data.', 'Probability Sampling: Creating a sample that is representative of the population using probability methods.', 'Simple random sampling: Reduces selection bias by giving each observation in the population an equal likelihood of inclusion in the sample dataset.', 'Systematic sampling: Involves selecting sample data from a random starting point with a fixed, periodic interval.', 'Stratified sampling: Dividing the dataset into separate groups called strata and taking a probability sample from each stratum.', 'Cluster sampling: Involves dividing the dataset into clusters and selecting a random sample of the clusters for analysis.', 'Non-Probability Sampling Methods: Sampling techniques where the odds of any observation in the sample dataset cannot be calculated.', 'Convenience Sampling: Forming a sample dataset based on convenience to the project team.', ""Purposive Sampling: Sampling done with a specific purpose, often based on the researcher's expert knowledge of the population."", 'Quota Sampling: A sampling method where the sample size is proportional to the demographic makeup of the population.', 'Snowball Sampling: A referral sampling technique where members of a sample group recruit others to become members of the sample group.', 'Data']"
ff70848f356c40d1a1c1bd183d2d194a.xml,"['Human intuition and expert knowledge is not enough to evaluate model performance in data science.', 'Metrics are used to assess algorithms for error, accuracy, and predictive performance.', 'Different metrics are used for different types of problems (classification, prediction, clustering).', 'The metrics discussed in this module are not exhaustive.', 'The choice of metric depends on the problem and the type of algorithm used.', 'Classification problems can have probability or class outputs.', 'Regression problems typically have continuous outputs.']"
ff7cba66391a4fb392fae7dd1f8d98db.xml,"['Business Requirements: Describing the context, scope, and background of a business need, including reasons for proposed changes.', 'System and User Requirements: Detailed description of the system and its operational and development constraints, as well as functions or tasks that a user must perform within the system.', 'Use cases and user stories: Representing user requirements and providing a big picture of what the user will be able to do within a system.', 'Solution Requirements: Tailoring solution requirements to the data science process, including functional and non-functional requirements, as well as requirements for data, models, reports, and dashboards.']"
placeholderitemid.xml,"['xml version=""1.0"" encoding=""UTF-8""', 'workbook_page', 'bib=""http://bibtexml.sf.net/""', 'cmd=""http://oli.web.cmu.edu/content/metadata/2.1/""', 'm=""http://www.w3.org/1998/Math/MathML""', 'pref=""http://oli.web.cmu.edu/preferences/""', 'theme=""http://oli.web.cmu.edu/presentation/""', 'wb=""http://oli.web.cmu.edu/activity/workbook/""', 'id=""placeholderitemid""', 'title', 'Placeholder', 'body', 'p id=""af9025204302f4251b9c92783041e6082""', 'This is a new page with empty content']"
welcome.xml,"['AI Philosophy: A Process, not a Product', 'The provision of analytical solutions to an organization requires understanding the organization’s needs and readiness', ""Assessing the organization's readiness for analytical solutions"", 'Data Science Ready: Identifying and normalizing data sources for data science', 'Data Science Enabled: Using data science to influence workflows, products, and services', 'AI Ready: Leveraging insights from data science and incorporating AI into workflows, products, and services', 'AI Enabled: Deploying new software and measuring its impact', 'Introducing data science decision into a real-world setting and measuring its implementation']"
