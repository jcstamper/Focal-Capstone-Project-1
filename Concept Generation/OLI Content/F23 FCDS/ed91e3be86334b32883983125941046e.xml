<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE workbook_page PUBLIC "-//Carnegie Mellon University//DTD Workbook Page MathML 3.8//EN" "http://oli.web.cmu.edu/dtd/oli_workbook_page_mathml_3_8.dtd"><workbook_page xmlns:bib="http://bibtexml.sf.net/" xmlns:cmd="http://oli.web.cmu.edu/content/metadata/2.1/" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pref="http://oli.web.cmu.edu/preferences/" xmlns:theme="http://oli.web.cmu.edu/presentation/" xmlns:wb="http://oli.web.cmu.edu/activity/workbook/" id="ed91e3be86334b32883983125941046e"><head><title>k-Nearest Neighbors Method</title><objref idref="e31e837cfea64decae6c39ec99b181d9" /><objref idref="d651478ba3e84c03851885b895caea97" /></head><body><p id="c5303ff2322c4e25abf6220c0e1107b7">Have you come across the term, lazy-learning&quot;? This is a method in which training data is generalized and most useful for large datasets that will be updated continuously. In this case, a model typically depends on (or queries) a small number of attributes in the dataset. An application of a lazy learner is a feature of a recommendation system. A recommendation system will continuously update new movies, or new items to shoppers preferences and the recommender relies on certain variables such as ratings, pricing, and country of origin. The opposite of a lazy learning method is an eager learning method. An eager learner requires less space than a lazy learner (remember lazy learners are continuously updating its dataset), an eager learner will learn immediately and sometimes for a long time but will take a shorter time to classify data. A lazy learner will learn for a short time but take a longer time to classify data.</p><p id="c088169ab7bd41b4af3c6bc804399c24">A well known lazy learner is the <em style="italic">k-</em>Nearest Neighbor! This method can be used to solve both classification and regression problems. kNN is considered rather simple yet useful (it is also simple to implement in Python or R) and is one of the first algorithms or methods that entrants to data science will learn.</p><p id="b92f9f6ac70f473a9dd0a45bc58d40bd"><em style="italic">k</em>NN will &quot;find a predefined number of training samples closest in distance to the new point or a new observation, and predict the label for the new observation&quot;. <em style="italic">k</em>NN can also suffer from the curse of dimensionality. This method will perform best when data is rescaled. It is best practice to normalize applicable data to the range of 0,1 and to standardize the data if it has a <link href="https://en.wikipedia.org/wiki/Normal_distribution" target="new" internal="false">gaussian distribution</link>.</p><p id="ef2354cd0b21481bbd9a10e74b86376d">kNN will perform its task just in time as it does not learn the model, remember the characteristic of a lazy learner? It spends less time learning and more time classifying or predicting. </p><p id="c1f2908594394361a1a9cefee5d4a1fa">There are three steps involved in making predictions with <em style="italic">k-NN: </em>Calculate the euclidean distance, identify nearest neighbor(s) and then perform task. </p><table id="cde953af408046d3aba969a72260d9a0" summary="" rowstyle="plain"><cite id="i28cab1043ab047d58def490e7c42b813" /><caption><p id="ec94c24616b14e70929162aa869bf3a0" /></caption><tr><td colspan="1" rowspan="1" align="left"><p id="bb25aa43ef5340c78502b5a782f680be"><em>Reading: </em><link href="https://arxiv.org/pdf/1701.07266.pdf" target="new" internal="false"><em><em style="italic">k-Nearest Neighbors: From Global to Local</em></em></link></p></td></tr></table><p id="c2df056804394a26b5b8f8ad754c6c59"><em style="italic">k-</em>Nearest Neighbors method begins by identifying the observations (or <em style="italic">k </em>records) in the training dataset that are similar to a new observation that should be classified. The neighboring observations can be used to classify the new observation into its class. That class should be its predominant class. Since this method is considered a non-parametric method, it will take information from similarities between the predictor values of the observations in the dataset. The <em style="italic">k</em>NN method uses distance computations to determine distance between each new observation and the observations in the training dataset. The most popular distance measure used in kNN is the <em style="italic">Euclidean Distance. </em>The formula for the Euclidean distance between observations (x<sub>i </sub>and y<sub>i</sub>) is shown below:</p><image id="b024a17deb6d4fd4ae7f59fca4ec051d" src="../webcontent/KNN_similarity_(3).jpg" alt="" style="inline" vertical-align="middle"><caption><p id="c09e8760b2ec4b9f939289e0d47f73d8"><em style="italic">Euclidean Distance Function. Source- Sayad-2010</em></p></caption><popout enable="false"></popout></image><p id="a34f25ef72014c8c96940d67a4f202d0">There are other distance measures including <link href="https://oli.cmu.edu" target="new" internal="false">Manhattan</link> (distance between vectors using the sum of their absolute differences) and <link href="https://en.wikipedia.org/wiki/Minkowski_distance" target="new" internal="false">Minkowski</link> distances.  Euclidean is used because it is computationally cheap, keep in mind that predictors should be standardized before implementing the Euclidean distance function. Euclidean distance will not work with categorical variables unless they are converted into binary dummy variables.  An alternative distance measure in this case would be the <link href="https://arxiv.org/pdf/1708.04321.pdf" target="new" internal="false"><em style="italic">Hamming Distance</em></link><em style="italic">, </em>which can be used as long as you do not have more than two (2) classes. </p><p id="f9fba6ee08e04609ab187661574750f0">Once distances are computed, a rule is set to determine the class that a new observation will be assigned and this is based on the classes of its neighbors. Now, you start to see why <em style="italic">k</em>NN is considered a <em>similarity function</em>. How do you determine the number of neighbors to assess, so that a new observation or data point can be classified correctly? You can determine that the new data should be classified in the same class as its single closest/nearest neighbor i.e. k = 1. Can you guess what will happen in this situation? You are right if you guessed that you will face overfitting. There are options to reduce this issue and that is by using a <em style="italic">k </em>that is greater than 1. If you assign <em style="italic">k = n i.e. n being number of observations in the dataset, </em>there will be over smoothing and all new data will be assigned to the majority class. Values of <em style="italic">k </em>are historically between 3 and 10 and usually an odd number to avoid ties<sup>1</sup>. Sometimes <em style="italic">k </em> can be chosen using cross-validation, and the validation dataset will validate the best <em style="italic">k. </em></p><p id="eaffbcdaac524c30b42d7db53f1ab5cc"><em style="italic">k</em>NN had been described for the purposes of predicting a categorical response but it is also effective for predicting continuous value responses just like you would with a linear regression model. </p><ul id="c571fe43710742efb0b7ec58dafbafaa"><li><p id="ab287a85769e4d5a85c4a4111c7c41a6">The main difference between predicting a categorical response and a continuous value is that the algorithm will use the average (usually weighted) response of the neighbors of the new data point to determine the accurate prediction. The weight will decrease as the distance increases from the data point at which the prediction is required. </p></li><li><p id="ccad33f5b91e4bddbb952e1edb40557f">The best <em style="italic">k </em>for a classification task is assessed using the overall error rate but in this instance, the best <em style="italic">k </em>is determined using the RMS error. </p></li></ul><wb:inline idref="newdfada7a6eeb14e0da337e3c76b5d2329" purpose="learnbydoing" /></body></workbook_page>
