{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('khan_labeled_data/train', 'r', encoding='utf-8') as f:\n",
    "  data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "text = []\n",
    "for line in data:\n",
    "  labels.append(' ' + line[0])\n",
    "  text.append(line[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>certainly , many algorithms are built complex ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but what i would like to ask - what are the al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is it just based on the length of code ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is tere a way to solve a math problem with alg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>could dna be an algorithm ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt completion\n",
       "0  certainly , many algorithms are built complex ...          1\n",
       "1  but what i would like to ask - what are the al...          1\n",
       "2           is it just based on the length of code ?          0\n",
       "3  is tere a way to solve a math problem with alg...          1\n",
       "4                        could dna be an algorithm ?          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(text, labels), columns = ['prompt','completion']) #[:300]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"df.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 5236 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- All prompts end with suffix ` ?`\n",
      "\n",
      "No remediations found.\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `df_prepared_train.jsonl` and `df_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"df_prepared_train.jsonl\" -v \"df_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" 1\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ?` for the model to start generating completions, rather than continuing with the prompt.\n",
      "Once your model starts training, it'll approximately take 2.13 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f df.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from df_prepared_train.jsonl: file-4QOhi018KGejv6HNQAc0oFur\n",
      "Uploaded file from df_prepared_valid.jsonl: file-ep0wHmbiirCeV4luYxzcVMVR\n",
      "Created fine-tune: ft-HWkjULQb7UgpkNlaJ3fBy8sK\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-04-23 20:59:19] Created fine-tune: ft-HWkjULQb7UgpkNlaJ3fBy8sK\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-HWkjULQb7UgpkNlaJ3fBy8sK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload progress:   0%|          | 0.00/461k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|██████████| 461k/461k [00:00<00:00, 462Mit/s]\n",
      "\n",
      "Upload progress:   0%|          | 0.00/110k [00:00<?, ?it/s]\n",
      "Upload progress: 100%|██████████| 110k/110k [00:00<00:00, 109Mit/s]\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"df_prepared_train.jsonl\" -v \"df_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" 1\" -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-23 20:59:19] Created fine-tune: ft-HWkjULQb7UgpkNlaJ3fBy8sK\n",
      "\n",
      "Stream interrupted (client disconnected).\n",
      "To resume the stream, run:\n",
      "\n",
      "  openai api fine_tunes.follow -i ft-HWkjULQb7UgpkNlaJ3fBy8sK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.follow -i ft-HWkjULQb7UgpkNlaJ3fBy8sK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = 'ada:ft-personal-2023-04-24-01-17-14'\n",
    "sample_question = 'What is the very best day of the year?'\n",
    "res = openai.Completion.create(model=ft_model, prompt=sample_question, max_tokens=1, temperature=0, logprobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          38\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.040069122\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \" 1\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" 0\": -3.2578235,\n",
      "            \" 1\": -0.040069122\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \" 1\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1682300098,\n",
      "  \"id\": \"cmpl-78fOU1A5woj3U2Sr6Xll5yn2PjufY\",\n",
      "  \"model\": \"ada:ft-personal-2023-04-24-01-17-14\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1,\n",
      "    \"prompt_tokens\": 10,\n",
      "    \"total_tokens\": 11\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(res['choices'][0]['text'])\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
